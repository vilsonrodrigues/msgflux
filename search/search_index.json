{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u1bd3\u27a4 msgflux","text":"<p>msgflux is an open-source framework designed for building multimodal AI applications with ease and flexibility. Our mission is to seamlessly connect models from diverse domains\u2014text, vision, speech, and beyond\u2014into powerful, production-ready workflows.</p> <pre><code>pip install msgflux\n</code></pre> <p>msgflux is built on four foundational pillars: Privacy, Simplicity, Efficiency, and Practicality.</p> <ul> <li> <p>Privacy first: msgflux does not collect or transmit user data. All telemetry is fully controlled by the user and remains local, ensuring data sovereignty and compliance from the ground up.</p> </li> <li> <p>Designed for simplicity: msgflux introduces core building blocks\u2014Model, DataBase, Parser, and Retriever\u2014that provide a unified and intuitive interface to interact with diverse AI resources.</p> </li> <li> <p>Powered by efficiency: msgflux leverages high-performance libraries such as Msgspec, Uvloop, Jinja, and Ray to deliver fast, scalable, and concurrent applications without compromising flexibility.</p> </li> <li> <p>Practical: msgflux features a workflow API inspired by <code>torch.nn</code>, enabling seamless composition of models and utilities using native Python. This architecture not only supports modular design but also tracks all parameters involved in workflow construction, offering advanced versioning and reproducibility out of the box. </p> </li> </ul> <p>In addition to the standard container modules available in PyTorch\u2014such as Sequential, ModuleList, and ModuleDict\u2014msgflux introduces a set of high-level modules designed to streamline the handling of multimodal inputs and outputs. These modules encapsulate common tasks in AI pipelines, making them easy to integrate, compose, and reuse.</p> <p>The new modules include:</p> <ul> <li> <p>Agent: A central module that orchestrates multimodal data, instructions, context, tools, generation schemas, and templates. It acts as the cognitive core of complex workflows.</p> </li> <li> <p>Speaker: Converts text into natural-sounding speech, enabling voice-based interactions.</p> </li> <li> <p>Transcriber: Transforms spoken language into text, supporting speech-to-text pipelines.</p> </li> <li> <p>Designer: Generates visual content from prompts and images, combining textual and visual modalities for tasks like image generation or editing.</p> </li> <li> <p>Retriever: Searches and extracts relevant information based on a set of input queries, ideal for grounding AI models in external knowledge.</p> </li> <li> <p>Predictor: A flexible module designed to wrap predictive models, such as those from scikit-learn or other machine learning libraries, enabling smooth integration into larger workflows.</p> </li> </ul> <p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"dependency-management/","title":"Models","text":""},{"location":"dependency-management/#chat-completion","title":"Chat Completion","text":"Provider Dependency Ollama <code>msgflux[openai]</code> OpenAI <code>msgflux[openai]</code> OpenRouter <code>msgflux[openai]</code> SambaNova <code>msgflux[openai]</code> Together <code>msgflux[openai]</code> vLLM <code>msgflux[openai]</code>"},{"location":"dependency-management/#image-classifier","title":"Image Classifier","text":"Provider Dependency JinaAI <code>msgflux[httpx]</code>"},{"location":"dependency-management/#image-embedder","title":"Image Embedder","text":"Provider Dependency JinaAI <code>msgflux[httpx]</code>"},{"location":"dependency-management/#image-text-to-image","title":"Image Text To Image","text":"Provider Dependency OpenAI <code>msgflux[openai]</code>"},{"location":"dependency-management/#moderation","title":"Moderation","text":"Provider Dependency OpenAI <code>msgflux[openai]</code>"},{"location":"dependency-management/#speech-to-text","title":"Speech To Text","text":"Provider Dependency OpenAI <code>msgflux[openai]</code> vLLM <code>msgflux[openai]</code>"},{"location":"dependency-management/#text-classifier","title":"Text Classifier","text":"Provider Dependency JinaAI <code>msgflux[httpx]</code> vLLM <code>msgflux[openai]</code>"},{"location":"dependency-management/#text-embedder","title":"Text Embedder","text":"Provider Dependency JinaAI <code>msgflux[httpx]</code> Ollama <code>msgflux[openai]</code> OpenAI <code>msgflux[openai]</code> Together <code>msgflux[openai]</code> vLLM <code>msgflux[openai]</code>"},{"location":"dependency-management/#text-reranker","title":"Text Reranker","text":"Provider Dependency JinaAI <code>msgflux[httpx]</code> vLLM <code>msgflux[openai]</code>"},{"location":"dependency-management/#text-to-image","title":"Text To Image","text":"Provider Dependency OpenAI <code>msgflux[openai]</code>"},{"location":"dependency-management/#text-to-speech","title":"Text To Speech","text":"Provider Dependency OpenAI <code>msgflux[openai]</code> Together <code>msgflux[openai]</code>"},{"location":"message/","title":"Message","text":"<p>A classe msgflux.Message, inspirada no torch.Tensor, foi projetada para facilitar o tr\u00e1fego de informa\u00e7\u00f5es em grafos computacionais criados com m\u00f3dulos 'nn'. Um dos princ\u00edpios centrais de sua concep\u00e7\u00e3o \u00e9 permitir que cada Module tenha permiss\u00f5es espec\u00edficas para ler e escrever em campos pr\u00e9-definidos da Message. Isso proporciona uma estrutura organizada para o fluxo de dados entre diferentes componentes de um sistema.e quero experimentar todos, compor e comparar. Ace quero experimentar todos, compor e comparar. Ace quero experimentar todos, compor e comparar. Ac</p> <p>A classe implementa os m\u00e9todos set e get, que permitem criar e acessar dados na Message por meio de strings, oferecendo uma interface flex\u00edvel e intuitiva. Al\u00e9m disso, campos padr\u00e3o (default fields) s\u00e3o fornecidos para estruturar os dados de forma consistente.</p>"},{"location":"message/#estrutura-e-campos-padrao","title":"Estrutura e Campos Padr\u00e3o","text":"<p>A inicializa\u00e7\u00e3o da classe Message inclui os seguintes campos padr\u00e3o, todos opcionais: <pre><code>def __init__(\n    self,\n    *,\n    content: Optional[Union[str, OrderedDict[str, Any]]] = None,\n    context: Optional[OrderedDict[str, Any]] = OrderedDict(),\n    text: Optional[OrderedDict[str, Any]] = OrderedDict(),\n    audios: Optional[OrderedDict[str, Any]] = OrderedDict(),\n    images: Optional[OrderedDict[str, Any]] = OrderedDict(),\n    videos: Optional[OrderedDict[str, Any]] = OrderedDict(),\n    extra: Optional[OrderedDict[str, Any]] = OrderedDict(),\n    user_id: Optional[str] = str(uuid4()),\n    chat_id: Optional[str] = str(uuid4()),\n):\n</code></pre> content: Armazena o conte\u00fado principal da mensagem, podendo ser uma string ou um dicion\u00e1rio ordenado.</p> <p>context: Campo sugerido para armazenar dados de contexto geradas por m\u00f3dulos.</p> <p>text: Campo para textos adicionais estruturados como dicion\u00e1rio.</p> <p>audios, images, videos: Campos para armazenar refer\u00eancias ou dados de m\u00eddia.</p> <p>extra: Campo gen\u00e9rico para dados adicionais.</p> <p>user_id e chat_id: Identificadores \u00fanicos gerados automaticamente (usando uuid4), caso n\u00e3o sejam fornecidos.</p> <p>Message tamb\u00e9m autogera um 'execution_id'.</p> <p>Para organizar o tr\u00e1fego de informa\u00e7\u00f5es, recomenda-se que os m\u00f3dulos escrevam suas sa\u00eddas nos campos context, outputs e response. J\u00e1 os campos de entrada (consumo de dados) podem ser livremente escolhidos conforme a necessidade do m\u00f3dulo. Criando uma Message Voc\u00ea pode criar uma inst\u00e2ncia de Message sem nenhum dado inicial ou com valores pr\u00e9-definidos: <pre><code>import msgflux as mf\n\n# Inst\u00e2ncia vazia\nmsg = mf.Message()\n\n# Inst\u00e2ncia com dados iniciais\nmsg = mf.Message(content=\"Hello, world!\", user_id=\"user123\")\nInserindo Valores nos Campos Padr\u00e3o\nOs dados podem ser inseridos nos campos padr\u00e3o usando o m\u00e9todo set:\npython\n# Inserindo um conte\u00fado simples\nmsg.set(\"content\", \"What is Deep Learning?\")\n\n# Inserindo um arquivo de \u00e1udio\nmsg.set(\"audios.audio1\", \"audio.mp3\")\n\n# Inserindo uma URL de imagem\nmsg.set(\"images.google\", \"https://www.google.com/images/branding/googlelogo/1x/googlelogo_light_color_272x92dp.png\")\nAcesso Multin\u00edvel (MultiLevel Set/Get)\nA Message suporta uma estrutura hier\u00e1rquica, permitindo o acesso a subcampos com nota\u00e7\u00e3o de ponto (dot notation):\npython\n# Definindo um valor em um subcampo\nmsg.set(\"images.frontend.request\", \"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\")\n\n# Acessando um n\u00edvel superior\nmsg.get(\"images.frontend\")\n# Retorna: OrderedDict([('request', 'https://huggingface.co/front/assets/huggingface_logo-noborder.svg')])\n</code></pre> Criando Novos Campos Al\u00e9m dos campos padr\u00e3o, \u00e9 poss\u00edvel criar novos campos dinamicamente: <pre><code># Criando uma lista de PDFs\nmsg.set(\"pdfs\", [])\nmsg.set(\"pdfs\", \"file.pdf\")  # Substitui o valor anterior\nmsg.get(\"pdfs\")\n# Retorna: ['file.pdf']\n\n# Criando um dicion\u00e1rio para CSVs\nmsg.set(\"csvs\", {})\nmsg.set(\"csvs.backoffice\", \"3_quarter_stats.csv\")\nmsg.get(\"csvs.backoffice\")\n# Retorna: \"3_quarter_stats.csv\"\nNota: O m\u00e9todo set substitui o valor existente no campo especificado. Para adicionar itens a uma lista ou dicion\u00e1rio sem sobrescrever, \u00e9 necess\u00e1rio manipular o conte\u00fado existente antes de usar set.\nRastreamento de Informa\u00e7\u00f5es\nConforme os m\u00f3dulos produzem informa\u00e7\u00f5es nos campos sugeridos (context, outputs, response), \u00e9 poss\u00edvel rastrear o fluxo de dados com o m\u00e9todo get_route():\npython\nmsg.get_route()\n# Retorna um registro das altera\u00e7\u00f5es feitas na Message (detalhes dependem da implementa\u00e7\u00e3o)\nAl\u00e9m disso, voc\u00ea pode verificar se um m\u00f3dulo espec\u00edfico j\u00e1 escreveu na Message:\npython\nmsg.in_msg(\"module_name\")\n# Retorna True se \"module_name\" j\u00e1 modificou a Message, False caso contr\u00e1rio\n</code></pre></p>"},{"location":"quickstart/","title":"Quickstart: PIX --- [Voice, Text]","text":"<p>This example demonstrates how to create a simple PIX transaction workflow that can handle both text and audio inputs.</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nfrom google.colab import userdata\napi_key = userdata.get(\"OPENAI_API_KEY\")\nmf.set_envs(OPENAI_API_KEY=api_key)\n\nchat_model = mf.Model.chat_completion(\"openai/gpt-4.1-mini\")\n\nstt_model = mf.Model.speech_to_text(\"openai/gpt-4o-mini-transcribe\")\n\ntranscriber_configs = {\n    \"name\": \"transcriber\",\n    \"model\": stt_model,\n    \"response_mode\": \"content\",\n    \"task_multimodal_inputs\": {\"audio\": \"user_audio\"},\n}\n\nsignature = \"\"\"text -&gt; amount: float, key_type: Literal['cpf', 'cnpj', 'email', 'phone_number', 'name'], key_id: str\"\"\"\n\nagent_configs = {\n    \"name\": \"extractor\",\n    \"model\": chat_model,\n    \"signature\": signature,\n    \"response_mode\": \"extraction\",\n    \"task_inputs\": \"content\",\n    \"task_multimodal_inputs\": {\"image\": \"user_image\"}\n}\n\nclass PIX(nn.Module):\n    def __init__(self, agent_configs, transcriber_configs):\n        super().__init__()\n        self.components = nn.ModuleDict({\n            \"extractor\": nn.Agent(**agent_configs),\n            \"transcriber\": nn.Transcriber(**transcriber_configs)\n        })\n        self.register_buffer(\"flux\", \"{user_audio is not None? transcriber} -&gt; extractor\")\n\n    def forward(self, msg):\n        return mf.inline(self.flux, self.components, msg)\n\npix = PIX(agent_configs, transcriber_configs)\n\npix.state_dict()\n\npix\n\npix.state_dict()\n\n### Text Input\n\n# en: Send 22.40 to 123.456.789-00\n# cpf: person id at Brazil\nmsg = mf.Message(content={\"text\": \"Envie 22,40 para 123.456.789-00\"})\n\nmsg = pix(msg)\n\nmsg\n\n### Audio Input\n\nmsg = mf.Message()\n\n# audio en: \"Transfer twenty-three fifty to eighty-four, nine, nine nine, twenty-four, eleven, twenty-one\"\nmsg.set(\"user_audio\", \"audio-pix-direct-pt-br.ogg\")\n\nmsg = pix(msg)\n\nmsg\n\n### Text, Image Input\n\nmsg = mf.Message()\n</code></pre>"},{"location":"quickstart/#the-model-class","title":"The <code>Model</code> Class","text":"<p>The <code>Model</code> class in <code>msgFlux</code> serves as a high-level factory and registry for loading AI models across various providers and modalities.</p> <p>It abstracts away boilerplate code and offers a simple, consistent API to instantiate models like chatbots, text embedders, speakers, image and video generators, and more.</p> <p>No need to memorize individual client APIs or custom wrappers. Just specify the model type, path and parameters.</p> <p>Supported Types</p> <p>Supports a wide range of AI capabilities:</p> Type Description <code>chat_completion</code> Understanding and multimodal generation <code>image_embedder</code> Generates a vector representation of an images <code>image_text_to_image</code> Image edit <code>moderation</code> Checks if the content is safe <code>speech_to_text</code> Voice transcription <code>text_classifier</code> Classify text <code>text_embedder</code> Generates a vector representation of a text <code>text_reranker</code> Rerank text options given a query <code>text_to_image</code> Image Generation <code>text_to_speech</code> Generates voice from text <p>Resilience</p> <p>API-based models are protected by a decorator (<code>@model_retry</code>) that applies retry in case of failures.</p> <p>Can manage multiple keys, this is useful in case a key becomes invalid. Separate with commas.</p> <p>Async</p> <p>Wait in a non-blocking manner for the model to respond. Use the <code>.acall</code> method to access async mode.</p> <p>Responses</p> <p>Each model returns a model response instance. Making the model response type explicit helps manage that response because you already know what's in that object.</p> <p>The model response which can be one of:</p> <ol> <li> <p>ModelResponse: Ideal for non-streaming tasks like embeddings, classification, etc.</p> </li> <li> <p>ModelStreamResponse: Designed for tasks where data is generated in real time \u2014 such as text and speech generation, tool-calling, etc.</p> </li> </ol> <p>Serialization</p> <p>You can export the internal state of an object from a Model.</p> <p>So re-create a model from a serialized object:</p> <pre><code>### **Chat Completion**\n\nThe `chat_completion` model is the most common and versatile model for natural language interactions.\n\n\nIt processes messages in a conversational format and supports advanced features such as multimodal input and output, structured data generation, and tool (function) calls.\n\n**Stateless**\n\nChat completion models do **not maintain** state between calls.\n\nAll context information (previous messages, system_prompt, etc.) must be provided on each new call.\n\n```python\n# init schema pass \"provider/model_id\"\n# optionally pass class initialization parameters\nchat_model = mf.Model.chat_completion(\"openai/gpt-4.1-nano\")\n\nchat_model.get_model_info()\n\nchat_model.instance_type()\n\nmodel_state = chat_model.serialize()\nmodel_state\n\nmf.save(model_state, \"model_state.json\")\n\n# Basic input\nresponse = chat_model(messages=\"Hello!\", system_prompt=\"You are a helpful assistant.\")\n\nresponse.metadata\n\nresponse.response_type\n\n# alias to response.data\nresponse.consume()\n\n#### **Async**\n\nreponse = await chat_model.acall(\"Tell me a joke\")\nresponse.consume()\n\n#### **Stream**\n\n`stream` allows you to stream tokens as they are generated.\n\nYou can use `stream` mode with both `generation_schema` and tools.\n\nresponse = chat_model(\"Hi, how are you?\", stream=True)\nprint(type(response))\nprint(response.response_type)\n# fastapi.StreamingResponse compatible\nasync for chunk in response.consume():\n    print(chunk, end=\"\", flush=True)\n\nresponse.metadata\n\n#### **MultiModal**\n\n# @title **Flow**\nmermaid_code = \"\"\"\nflowchart LR\n    Text[\"Text\"]\n    Image[\"Image\"]\n    Video[\"Video\"]\n    Sound[\"Sound\"]\n    Code[\"Code\"]\n    LM[\"LM\"]\n    OutText[\"Text\"]\n    OutImage[\"Image\"]\n    OutVideo[\"Video\"]\n    OutSound[\"Sound\"]\n    OutCode[\"Code\"]\n\n    Text e1@==&gt; LM\n    e1@{animate: true}\n\n    Image e2@==&gt; LM\n    e2@{animate: true}\n\n    Video e3@==&gt; LM\n    e3@{animate: true}\n\n    Sound e4@==&gt; LM\n    e4@{animate: true}\n\n    Code e5@==&gt; LM\n    e5@{animate: true}\n\n    LM e6@==&gt; OutText\n    e6@{animate: true}\n\n    LM e7@==&gt; OutImage\n    e7@{animate: true}\n\n\n\nfrom msgflux.utils.mermaid import plot_mermaid\nplot_mermaid(mermaid_code)\n\nmessages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n                },\n            },\n        ],\n    }]\n\n# or using chat blocks\nfrom msgflux import ChatBlock\nmessages = [\n    ChatBlock.user(\n        \"What's in this image?\",\n        media=ChatBlock.image(\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\")\n    )\n]\n\nmessages\n\nresponse = chat_model(messages=messages)\n\nresponse.metadata\n\nresponse.consume()\n\nFor other modalities, see the `task multimodal inputs` section in `nn.Agent`\n\n#### **Generation Schemas**\n\n**Structured Generation**\n\nThe model can be guided to produce structured responses according to a user-defined schema.\n\nIn msgFlux this is called `generation_schema`.\n\nThe name shows not only that the model produces a `structured output`, but also that it follows a schema.\n\nThe models write an encoded JSON that is decoded by the Struct and then transformed into a dict.\n\nFor this, we use `msgspec.Struct` as the structure format:\n\n```python\n# structured response\n# https://jcristharif.com/msgspec/benchmarks.html\nfrom msgspec import Struct\n\nclass CalendarEvent(Struct):\n    name: str\n    date: str\n    participants: list[str]\n\nresponse = chat_model(\n    messages=\"Alice and Bob are going to a science fair on Friday.\",\n    system_prompt=\"Extract the event information.\",\n    generation_schema=CalendarEvent\n)\n\nresponse.metadata\n\nresponse.consume()\n</code></pre> <p><code>generation_schema</code> maybe the most important feature in <code>chat_completion</code> models.</p> <p>This feature enables things like <code>ReAct</code>, <code>CoT</code>, new content generation, guided data extraction, etc.</p> <p>In this framework following tutorials we make extensive use of it.</p>"},{"location":"quickstart/#tools","title":"Tools","text":"<p>When we provide a set of tools (<code>tool_schemas</code>), the model may suggest that one of them be called\u2014but it does not automatically execute them.</p> <p>Instead, it returns a call intent, which is captured and processed by an internal component called the <code>ToolCallAggregator</code>.</p> <p>This class collects and organizes tool calls suggested by the model, especially in streaming mode, where arguments arrive in fragmented form.</p> <p>Main responsibilities:</p> <ul> <li> <p>Reassemble parts of calls during the stream (<code>process</code>)</p> </li> <li> <p>Convert raw data into complete functional calls (<code>get_calls</code>)</p> </li> <li> <p>Insert tool results (<code>insert_results</code>)</p> </li> <li> <p>Generate messages in the correct format to follow the flow with the model (<code>get_messages</code>)</p> </li> </ul> <p>Use <code>tool_choice</code> to control tool calling mode:</p> <pre><code>By default the model will determine when and how many tools to use.\nYou can force specific behavior with the tool_choice parameter.\n    1. auto:\n        Call zero, one, or multiple functions. tool_choice: \"auto\"\n    2. required:\n        Call one or more functions. tool_choice: \"required\"\n    3. Forced Function:\n        Call exactly one specific function. E.g. \"add\".\n</code></pre> <pre><code>tools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get current temperature for a given location.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"City and country e.g. Bogot\u00e1, Colombia\"\n                }\n            },\n            \"required\": [\n                \"location\"\n            ],\n            \"additionalProperties\": False\n        },\n        \"strict\": False # Model client change to True in runtime\n    }\n}]\n\nresponse = chat_model(\n    messages=\"What is the weather like in Paris today?\",\n    tool_schemas=tools,\n    tool_choice=\"required\",\n)\n\nresponse.metadata\n\ntool_call_agg = response.consume()\n\ntool_call_agg.get_calls()\n</code></pre>"},{"location":"quickstart/#prefilling","title":"Prefilling","text":"<p>Prefilling in <code>chat_completion</code> models is the process of inserting a partial message from the assistant itself into the conversation history before the model continues generating.</p> <p>Unlike simply adding a complete response, prefilling forces the model to begin its output from a predefined initial text.</p> <ol> <li> <p>Normal case:</p> </li> <li> <p>User: <code>&lt;task&gt;</code></p> </li> <li> <p>Assistant: <code>&lt;response&gt;</code></p> </li> <li> <p>Prefilling enabled:</p> </li> <li> <p>User: <code>&lt;task&gt;</code></p> </li> <li> <p>Assistant: <code>&lt;prefilling_content&gt;</code> (Not visible to user)</p> </li> <li> <p>Assistant: <code>&lt;response&gt;</code></p> </li> </ol> <pre><code>response = chat_model(messages=\"how much is (30x3 + 33)?\", prefilling=\"Let's think step by step\")\n\nresponse.metadata\n\nresponse.consume()\n</code></pre>"},{"location":"quickstart/#typed-parsers","title":"Typed Parsers","text":"<p>For structured generation, the common way to get a response from a model is to have it write an encoded JSON, as we saw earlier.</p> <p>Models need to be trained to generate output in this format.</p> <p>In contrast, most models are excellent at writing XML.</p> <p>In <code>msgFlux</code>, we introduce a typed-XML paradigm, where the model specifies the type of data it is generating.</p> <p>We just need to demonstrate how to write it.</p> <p>Note: By default XML consumes more tokens than JSON</p> <pre><code>from msgflux.dsl.typed_parsers import typed_parser_registry\n\ntyped_xml = typed_parser_registry[\"typed_xml\"]\n\nprint(typed_xml.template)\n</code></pre>"},{"location":"quickstart/#moderation","title":"Moderation","text":"<pre><code>moderation_model = mf.Model.moderation(\"openai/omni-moderation-latest\")\n\nmoderation_model.get_model_info()\n\nmoderation_model.instance_type()\n\nmoderation_model.serialize()\n\nresponse = moderation_model(data=\"wars are the best way to live\")\n\nmodel_response = response.consume()\n\n# moderation models returns a 'safe' flag\nmodel_response.safe\n\nmodel_response\n</code></pre>"},{"location":"quickstart/#text-embedder","title":"Text Embedder","text":"<pre><code>embedder_model = mf.Model.text_embedder(\"openai/text-embedding-3-small\", dimensions=256)\n\nembedder_model.get_model_info()\n\nembedder_model.instance_type()\n\nembedder_model.serialize()\n\nresponse = embedder_model(data=\"wars are the best way to live\")\n\nresponse.metadata\n\nresponse.consume()\n</code></pre>"},{"location":"quickstart/#text-to-image","title":"Text To Image","text":"<pre><code>imagegen_model = mf.Model.text_to_image(\"openai/gpt-image-1\", quality=\"low\", size=\"1024x1024\")\n\nimagegen_model.get_model_info()\n\nimagegen_model.instance_type()\n\nimagegen_model.serialize()\n\nprompt=\"\"\"\nA children's book drawing of a veterinarian using a stethoscope to\nlisten to the heartbeat of a baby otter.\n\"\"\"\n\nresponse = imagegen_model(prompt=prompt, n=2)\n\nmetadata = response.metadata\nmetadata\n\nimages = response.consume()\n</code></pre>"},{"location":"quickstart/#image-text-to-image","title":"Image Text To Image","text":"<pre><code>imageedit_model = mf.Model.image_text_to_image(\"openai/gpt-image-1\", quality=\"low\", size=\"1024x1024\")\n\nurls = [\n    \"https://cdn.openai.com/API/docs/images/body-lotion.png\",\n    \"https://cdn.openai.com/API/docs/images/bath-bomb.png\",\n    \"https://cdn.openai.com/API/docs/images/incense-kit.png\",\n    \"https://cdn.openai.com/API/docs/images/soap.png\"\n]\n\nprompt = \"\"\"\nGenerate a photorealistic image of a gift basket on a white background\nlabeled 'Relax &amp; Unwind' with a ribbon and handwriting-like font,\ncontaining all the items in the reference pictures.\n\"\"\"\n\nresponse = imageedit_model(prompt=prompt, image=urls)\n\nmetadata = response.metadata\nmetadata\n\nimage = response.consume()\n\nimport base64\n\nimage_bytes = base64.b64decode(image)\nwith open(f\"image_edit.{metadata.details.output_format}\", \"wb\") as f:\n    f.write(image_bytes)\n\n# image edit with mask\nimage = \"https://cdn.openai.com/API/docs/images/sunlit_lounge.png\"\nmask = \"https://cdn.openai.com/API/docs/images/mask.png\"\nprompt=\"A sunlit indoor lounge area with a pool containing a flamingo\"\n\nresponse = imageedit_model(prompt=prompt, image=image, mask=mask)\n\nresponse.metadata\n\nimage = response.consume()\n\nimport base64\n\nimage_bytes = base64.b64decode(image)\nwith open(f\"image_edit_with_mask.{metadata.details.output_format}\", \"wb\") as f:\n    f.write(image_bytes)\n</code></pre>"},{"location":"quickstart/#text-to-speech","title":"Text To Speech","text":"<pre><code>speech_model = mf.Model.text_to_speech(\"openai/gpt-4o-mini-tts\", speed=1.25, voice=\"coral\")\n\nspeech_model.get_model_info()\n\nspeech_model.instance_type()\n\nspeech_model.serialize()\n\nresponse = speech_model(data=\"Today is a wonderful day to build something people love!\")\n\nresponse.consume()\n</code></pre>"},{"location":"quickstart/#speech-to-text","title":"Speech To Text","text":"<pre><code>transcriber_model = mf.Model.speech_to_text(\"openai/gpt-4o-mini-transcribe\")\n\ntranscriber_model.get_model_info()\n\ntranscriber_model.instance_type()\n\ntranscriber_model.serialize()\n\nresponse = transcriber_model(data=\"local/audio.mp3\")\n</code></pre>"},{"location":"quickstart/#databases","title":"DataBases","text":"<pre><code>mf.DataBase.supported_db_types\n\nmf.DataBase.providers_by_db_type\n</code></pre>"},{"location":"quickstart/#kv","title":"KV","text":"<pre><code>cachetools_kv = mf.DataBase.kv(\"cachetools\", ttl=300, maxsize=1000, hash_key=True)\n# or\n# !pip install diskcache\n# diskcache_kv = mf.DataBase.kv(\"diskcache\")\n\ncachetools_kv.instance_type()\n\ncachetools_kv.serialize()\n\n# add single document\ncachetools_kv.add({\"user:1\": {\"name\": \"Alice\", \"age\": 30}})\n\n# add a list of documents\ncachetools_kv.add([\n</code></pre>"},{"location":"quickstart/#dotdict","title":"dotdict","text":"<p>A dictionary with dot access and nested path support.</p> <p>dotdict allows you to access and modify values as attributes (e.g., <code>obj.key</code>) and also allows reading and writing nested paths using strings with dot separators (e.g., <code>obj.get(\"user.profile.name\")</code>).</p> <pre><code>Main features:\n- Dot access (`obj.key`)\n- Traditional square bracket access (`obj['key']`)\n- Nested reading via `.get(\"a.b.c\")`\n- Nested writing via `.set(\"a.b.c\", value)`\n- Conversion to standard dict with `.to_dict()`\n- Support for Msgspec serialization (`__json__`)\n- Support for lists with path indices (e.g., `\"items.0.name\"`)\n- Optional immutability (`frozen=True`)\n</code></pre> <pre><code>from msgflux import dotdict\n\ndotdict\n\ncontainer = dotdict({\"agent\": [\"valor1\", \"valor2\"]})\n\ncontainer.get(\"agent\")\n\n# nested\ncontainer.agent\n\ncontainer.get(\"agent.0\")\n\n# does not work if the last value is a position in the list\n# container.agent.0\n\n# nested set\ncontainer.content = \"Hello World!\"\n\ncontainer.set(\"payload\", {\"user_id\": 123, \"user_name\": \"Diana\"})\n\ncontainer\n\ncontainer.payload.user_name\n\n# multi-level insert\nmsg.set(\"texts.inputs.first\", \"A long time ago\")\n</code></pre>"},{"location":"quickstart/#inline","title":"inline","text":"<p>inline allows orchestrate modules using a simple statement-based declarative language.</p> <p>With inline you can change your workflow at runtime without changing script.</p> <p>See below everything you can do with her:</p> <p>\u27a1\ufe0f Sequential Execution</p> <p>Use \"-&gt;\" to define the execution order of the modules.</p> <p>\"prep -&gt; transform -&gt; output\"</p> <p>\ud83d\udd00 Parallel Execution</p> <p>Use watches [...] to perform modules in parallel.</p> <p>\"prep -&gt; [feat_a, feat_b] -&gt; combine\"</p> <p>\u2753 Condition (if-else)</p> <p>Use keys with notation {condition? So if not} for branches.</p> <p>\"{user.age &gt; 18 ? adult_module, child_module}\"</p> <p>You can also omit the \"if not\" module:</p> <p>\"{user.is_active ? send_email}\"</p> <p>\u2699\ufe0f  Logical Operators under Conditions</p> <p>You can combine multiple conditions with logical operators:</p> Operator Description Example <code>&amp;</code> AND <code>is_admin == true &amp; is_active == true</code> <code>\\|\\|</code> OR <code>is_premium == true \\|\\| has_coupon == true</code> <code>!</code> NOT <code>!(user.is_banned == true)</code> <p>\"{user.is_active == true &amp; !user.is_banned == true ? grant_access, deny_access}\"</p> <p>\ud83d\udeab None Verification</p> <p>You can also check if a field is None or not:</p> <p>\"user.name is None\"</p> <p>\"user.name is not None\"</p> <p>\"{user.name is None ? ask_name, greet_user}\"</p> <p>In addition to a notation, you must pass a mapping containing the name of the module that will be called within the notation and as value the chamable object. You must also pass a <code>dotdict</code> (or <code>Message</code>) object.</p> <pre><code>from msgflux import Message, dotdict, inline\n\n#### **Sequential pipeline**\n\n# pass an empty message and enrich during execution\ndef prep(msg):\n    msg.prep_done = True\n    return msg\n\ndef transform(msg):\n    if msg.get(\"prep_done\"):\n        msg.transformed = \"ok\"\n    return msg\n\ndef output(msg):\n    print(\"Result:\", msg.transformed)\n    return msg\n\nmodules = {\n    \"prep\": prep,\n    \"transform\": transform,\n    \"output\": output,\n}\n\nmessage = dotdict()\ninline(\"prep -&gt; transform -&gt; output\", modules, message)\n\n#### **Parallel Execution**\n\nParallel use `msg_bcast_gather`.\n\nIt is essential that you do **not modify** the message within the function. This can cause race condition.\n\nYou can still access the message values. To make modifications you must return the value.\n\n'msg_bcast_gather' will save as .set(\"module_name\", response)\n\ndef ingestion(msg):\n    message.set(\"features.a\", 1)\n    message.set(\"features.b\", 2)\n    return message\n\ndef feat_a(msg):\n    new_features = msg.features.a + 1\n    return {\"new_features\": new_features}\n\ndef feat_b(msg):\n    new_features = msg.features.b + 1\n    return {\"new_features\": new_features}\n\ndef combine(msg):\n    msg.result = msg.feat_a.new_features + msg.feat_b.new_features\n    return msg\n\nmodules = {\n    \"prep\": ingestion,\n    \"feat_a\": feat_a,\n    \"feat_b\": feat_b,\n    \"combine\": combine,\n}\n\nmessage = dotdict()\ninline(\"prep -&gt; [feat_a, feat_b] -&gt; combine\", modules, message)\n\n#### **Simple Conditional**\n\ndef adult(msg):\n    msg.result = \"Welcome, adult\"\n    return msg\n\ndef child(msg):\n    msg.result = \"Hi, young one\"\n    return msg\n\nmodules = {\n    \"adult_module\": adult,\n    \"child_module\": child,\n}\n\nmessage = dotdict()\nmessage.set(\"user.age\", 21)\n\ninline(\"{user.age &gt; 18 ? adult_module, child_module}\", modules, message)\nprint(message.result)  # \"Welcome, adult\"\n</code></pre>"},{"location":"quickstart/#conditional-boolean","title":"Conditional Boolean","text":"<pre><code>def grant(msg):\n    msg.access = \"granted\"\n    return msg\n\ndef deny(msg):\n    msg.access = \"denied\"\n    return msg\n\nmodules = {\n    \"grant_access\": grant,\n    \"deny_access\": deny,\n}\n\nmessage = dotdict()\nmessage.set(\"user.is_active\", True)\nmessage.set(\"user.is_banned\", False)\n\ninline(\"{user.is_active == True &amp; !user.is_banned  == True ? grant_access, deny_access}\", modules, message)\nprint(message.access)  # \"granted\"\n</code></pre>"},{"location":"quickstart/#none-check","title":"None check","text":"<pre><code>def ask_name(msg):\n    msg.prompt = \"Please enter your name\"\n    return msg\n\ndef greet(msg):\n    msg.greeting = f\"Hello, {msg.user.name}\"\n    return msg\n\nmodules = {\n    \"ask_name\": ask_name,\n    \"greet_user\": greet,\n}\n\nmessage = dotdict()\nmessage.set(\"user.name\", None)\n\ninline(\"{user.name is None ? ask_name, greet_user}\", modules, message)\nprint(message.prompt)  # \"Please enter your name\"\n\nmessage.user.name = \"Bruce\"\ninline(\"{user.name is None ? ask_name, greet_user}\", modules, message)\nprint(message.greeting)  # \"Hello, Bruce\"\n</code></pre>"},{"location":"quickstart/#or-and","title":"OR, AND","text":"<pre><code>def premium_flow(msg):\n    msg.flow = \"premium\"\n    return msg\n\ndef standard_flow(msg):\n    msg.flow = \"standard\"\n    return msg\n\nmodules = {\n    \"premium_flow\": premium_flow,\n    \"standard_flow\": standard_flow,\n}\n\nmessage = dotdict()\nmessage.set(\"user.type\", \"premium\")\nmessage.set(\"user.credits\", 50)\nmessage.set(\"user.status\", \"active\")\n\n# OR\ninline(\n    \"{user.type == 'premium' || user.credits &gt; 100 ? premium_flow, standard_flow}\",\n    modules,\n    message,\n)\nprint(message.flow)  # \"premium\"\n</code></pre>"},{"location":"quickstart/#not","title":"NOT","text":"<pre><code>def process_request(msg):\n    msg.result = \"processed\"\n    return msg\n\ndef deny_request(msg):\n    msg.result = \"denied\"\n    return msg\n\nmodules = {\n    \"process_request\": process_request,\n    \"deny_request\": deny_request,\n}\n\nmessage = dotdict()\nmessage.set(\"user.banned\", False)\nmessage.set(\"user.credits\", 50)\n\ninline(\n    \"{!(user.banned == true || user.credits &lt;= 0) ? process_request, deny_request}\",\n    modules,\n    message,\n)\nprint(message.result)  # \"processed\"\n</code></pre>"},{"location":"quickstart/#async","title":"Async","text":"<p>Waitable version of <code>inline</code></p> <pre><code>from msgflux import ainline, dotdict\n\nasync def prep(msg: dotdict) -&gt; dotdict:\n    print(f\"Executing prep, current msg: {msg}\")\n    msg['output'] = {'agent': 'xpto', 'score': 10, 'status': 'success'}\n    msg['counter'] = 0\n    return msg\n\nasync def increment(msg: dotdict) -&gt; dotdict:\n    print(f\"Executing increment, current msg: {msg}\")\n    msg['counter'] = msg.get('counter', 0) + 1\n    return msg\n\nasync def feat_a(msg: dotdict) -&gt; dotdict:\n    print(f\"Executing feat_a, current msg: {msg}\")\n    msg['feat_a'] = 'result_a'\n    return msg\n\nasync def feat_b(msg: dotdict) -&gt; dotdict:\n    print(f\"Executing feat_b, current msg: {msg}\")\n    msg['feat_b'] = 'result_b'\n    return msg\n\nasync def final(msg: dotdict) -&gt; dotdict:\n    print(f\"Executing final, current msg: {msg}\")\n    msg['final'] = 'done'\n    return msg\n\nmy_modules = {\n    \"prep\": prep,\n    \"increment\": increment,\n    \"feat_a\": feat_a,\n    \"feat_b\": feat_b,\n    \"final\": final\n}\ninput_msg = dotdict()\n\n# Example with while loop\nresult = await ainline(\n    \"prep -&gt; @{counter &lt; 5}: increment; -&gt; final\",\n    modules=my_modules,\n    message=input_msg\n)\nresult\n\n# Example with nested while loop and other constructs\nresult = await ainline(\n    \"prep -&gt; @{counter &lt; 3}: increment -&gt; [feat_a, feat_b]; -&gt; final\",\n    modules=my_modules,\n    message=input_msg\n)\nresult\n</code></pre>"},{"location":"quickstart/#nn","title":"nn","text":"<pre><code>import msgflux.nn as nn\n</code></pre>"},{"location":"quickstart/#module","title":"Module","text":"<p>Main features:</p> <ul> <li> <p>Advanced param serialization</p> </li> <li> <p>Update params with zero reload</p> </li> <li> <p>Built-in atomic Modules for fast workflow build</p> </li> <li> <p>OpenTelemetry integration</p> </li> <li> <p>Async interface (<code>.acall</code>). The module will look for an implementation of <code>aforward</code>, if it doesn't find one it will direct to <code>forward</code>.</p> </li> </ul> <pre><code># similar pytorch api\n# advanced state_dict\n# able to create/update components in runtime\n# each buffer or parameter is registred in state dict\n\n\nclass Workflow(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.instructions = nn.Parameter(\"&lt;param_content&gt;\", \"&lt;spec&gt;\") # will can be optimized\n        self.register_buffer(\"expected_output\", \"&lt;expected_output&gt;\")\n\nWorkflow().state_dict()\n\n# logic is difined in 'forward'\n# able hooks pre and post forward\n\nclass Model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Yes I did.\")\n\n    def forward(self, x, **kwargs):\n        user_name = kwargs.get(\"user_name\", None)\n        if user_name:\n            model_response = \" Hi \" + user_name + self.response\n        else:\n            model_response = self.response\n        x = x + model_response\n        return x\n\ndef retrieve_user_name(user_id: str):\n    if user_id == \"123\":\n        return \"Clark\"\n    return None\n\ndef pre_hook(module, args, kwargs):\n    # enhance context\n    if kwargs.get(\"user_id\"):\n        user_name = retrieve_user_name(kwargs[\"user_id\"])\n        kwargs[\"user_name\"] = user_name\n    return args, kwargs\n\ndef post_hook(module, args, kwargs, output):\n    print(f\"inpect output: {output}\")\n    return output\n\nmodel = Model()\n\n# plot mermaid flow\n\n# note that flows are still **experimental** and can be wrong\n# if they have too many if and else statements inside others.\n\n# By default, \".self\" characters are suppressed to make plots\n# more readable. But you can pass `remove_self=False`\nmodel.plot()\n\n# hooks returns a handle object\npre_hook_handle = model.register_forward_pre_hook(pre_hook)\npost_hook_handle = model.register_forward_hook(post_hook)\n\nmodel._forward_pre_hooks\n\nmodel._forward_hooks\n\ninput_x = \"You did the work?\"\nkwargs = {\"user_id\": \"123\"}\nresult = model(input_x, **kwargs)\nprint(f\"Output: {result}\")\n\n# remove hooks (optional)\npre_hook_handle.remove()\npost_hook_handle.remove()\n\nresult_without_hooks = model(input_x, **kwargs)\nprint(result_without_hooks)\n\n# save state dict\n# format: default is toml but json also\nmf.save(model.state_dict(), \"state_dict.toml\")\n\nstate_dict = mf.load(\"state_dict.toml\")\n\nstate_dict\n\n# update param\nstate_dict[\"response\"] = \"No, I didn't.\"\n\n# update model state dict\nmodel.load_state_dict(state_dict)\n\ninput_x = \"You did the work?\"\nkwargs = {\"user_id\": \"123\"}\nresult = model(input_x, **kwargs)\nprint(f\"Output: {result}\")\n</code></pre>"},{"location":"quickstart/#message","title":"Message","text":"<p>The <code>msgflux.Message</code> class, inspired by <code>torch.Tensor</code>, and implements on top of 'dotdict', was designed to facilitate the flow of information in computational graphs created with <code>nn</code> modules.</p> <p>One of the central principles of its design is to allow each Module to have specific permissions to read and write to predefined fields of the Message.</p> <p>This provides an organized structure for the flow of data between different components of a system.</p> <p>The class implements the <code>set</code> and <code>get</code> methods, which allow creating and accessing data in the Message through strings, offering a flexible and intuitive interface. In addition, default fields are provided to structure the data in a consistent way.</p> <p><code>Message</code> is integrated into built-in <code>Modules</code> so that you declare how the module should read and write information.</p> <pre><code>msg = mf.Message()\n\nmsg\n</code></pre> <p>Each message receives an <code>user_id</code>, <code>user_name</code> and <code>chat_id</code>.Message auto generates an <code>execution_id</code>.</p> <pre><code>msg_metadata = mf.Message(user_id=\"123\", user_name=\"Bruce Wayne\", chat_id=\"456\")\n</code></pre>"},{"location":"quickstart/#functional","title":"Functional","text":"<p>Functional are a set of functions developed for concurrent processing.</p> <p>Functional offers a sync API for executing multiple concurrent tasks.</p> <p>Functional calls the <code>Executor</code> a class that manages an async event loop and a threadpool, distributing tasks among them and returning futures.</p> <p>Although they are inside <code>nn</code>, the functions to be used are not limited to <code>nn.Module</code> and can be used for any function.</p> <pre><code>import msgflux.nn.functional as F\n\nF.__all__\n</code></pre> <p>from msgflux.utils.mermaid import plot_mermaid</p>"},{"location":"quickstart/#title-map_gather","title":"@title map_gather","text":""},{"location":"quickstart/#map-a-list-of-inputs-in-a-single-f","title":"map a list of inputs in a single f","text":""},{"location":"quickstart/#input1-input2","title":"input1, input2","text":""},{"location":"quickstart/#input1-f-r1","title":"input1 -&gt; f -&gt; r1","text":""},{"location":"quickstart/#input2-f-r2","title":"input2 -&gt; f -&gt; r2","text":""},{"location":"quickstart/#r1-r2","title":"(r1, r2)","text":"<p>mermaid_code = \"\"\" flowchart TD     subgraph map_gather [\"map_gather\"]         input1[\"input1\"]         input2[\"input2\"]</p> <pre><code>    f[\"f\"]\n\n    r1[\"r1\"]\n    r2[\"r2\"]\n\n    input1 --&gt; f --&gt; r1\n    input2 --&gt; f --&gt; r2\n\n    r1 -.-&gt; results[\"r1, r2\"]\n    r2 -.-&gt; results\nend\n</code></pre> <p>\"\"\" plot_mermaid(mermaid_code)</p> <pre><code>F.map_gather\n\ndef add(x, y):\n    return x + y\nresults = F.map_gather(add, args_list=[(1, 2), (3, 4), (5, 6)])\nprint(results) # (3, 7, 11)\n\ndef multiply(x, y=2):\n    return x * y\nresults = F.map_gather(multiply, args_list=[(1,), (3,), (5,)], kwargs_list=[{\"y\": 3}, {\"y\": 4}, {\"y\": 5}])\nprint(results) # (3, 12, 25)\n</code></pre>"},{"location":"quickstart/#title-scatter_gather","title":"@title scatter_gather","text":""},{"location":"quickstart/#map-a-list-of-inputs-to-a-list-of-f","title":"map a list of inputs to a list of f","text":""},{"location":"quickstart/#input1-input2_1","title":"input1, input2","text":""},{"location":"quickstart/#input1-f1-r1","title":"input1 -&gt; f1 -&gt; r1","text":""},{"location":"quickstart/#input2-f2-r2","title":"input2 -&gt; f2 -&gt; r2","text":""},{"location":"quickstart/#r1-r2_1","title":"(r1, r2)","text":"<p>mermaid_code = \"\"\" flowchart TD     subgraph scatter_gather [\"scatter_gather\"]</p> <pre><code>    input1[\"input1\"]\n    input2[\"input2\"]\n\n    f1[\"f1\"]\n    f2[\"f2\"]\n\n    r1[\"r1\"]\n    r2[\"r2\"]\n\n    input1 --&gt; f1 --&gt; r1\n    input2 --&gt; f2 --&gt; r2\n\n    r1 -.-&gt; results[\"r1, r2\"]\n    r2 -.-&gt; results\nend\n</code></pre> <p>\"\"\" plot_mermaid(mermaid_code)</p> <pre><code>F.scatter_gather\n\ndef add(x, y):\n    return x + y\ndef multiply(x, y=2):\n    return x * y\n\ncallables = [add, multiply, add]\n\n# Example 1: Using only args_list\nargs = [ (1, 2), (3,), (10, 20) ] # multiply will use its default y\nresults = F.scatter_gather(callables, args_list=args)\nprint(results) # (3, 6, 30)\n\n# Example 2: Using args_list e kwargs_list\nargs = [ (1,), (), (10,) ]\nkwargs = [ {'y': 2}, {'x': 3, 'y': 3}, {'y': 20} ]\nresults = F.scatter_gather(callables, args_list=args, kwargs_list=kwargs)\nprint(results) # (3, 9, 30)\n\n# Example 3: Using only kwargs_list (useful if functions have defaults or don't need positional args)\ndef greet(name=\"World\"):\n    return f\"Hello, {name}\"\ndef farewell(person_name):\n    return f\"Goodbye, {person_name}\"\nfuncs = [greet, greet, farewell]\nkwargs_for_funcs = [{}, {\"name\": \"Earth\"}, {'person_name': \"Commander\"}]\nresults = F.scatter_gather(funcs, kwargs_list=kwargs_for_funcs)\nprint(results) # (\"Hello, World\", \"Hello, Earth\", \"Goodbye, Commander\")\n</code></pre> <p>msg_scatter_gather</p> <p>Similarly, you can use the <code>msg_scatter_gather</code> version where you use a <code>dotdict</code>-based object to pass and modify information in the object itself.</p> <pre><code>F.msg_scatter_gather\n\nmsg1, msg2 = mf.Message(), mf.Message()\n\nmsg1.user_input = \"hi, how are you?\"\nmsg2.data = \"I want to visit Dortmund\"\n\ndef agent(msg):\n    print(msg.user_input)\n    msg.response = \"I am fine, thank you!\"\n    return msg\n\ndef retriever(msg):\n    print(msg.data)\n    msg.retrieved = \"The user likes to travel.\"\n    return msg\n\nmsg1, msg2 = F.msg_scatter_gather([agent, retriever], [msg1, msg2])\n\nmsg1\n\nmsg2\n</code></pre>"},{"location":"quickstart/#title-bcast_gather","title":"@title bcast_gather","text":""},{"location":"quickstart/#map-a-input-to-a-list-of-f","title":"map a input to a list of f","text":""},{"location":"quickstart/#input1","title":"input1","text":""},{"location":"quickstart/#input1-f1-r1_1","title":"input1 -&gt; f1 -&gt; r1","text":""},{"location":"quickstart/#input1-f2-r2","title":"input1 -&gt; f2 -&gt; r2","text":""},{"location":"quickstart/#r1-r2_2","title":"(r1, r2)","text":"<p>mermaid_code = \"\"\" flowchart TD     subgraph bcast_gather [\"bcast_gather\"]</p> <pre><code>    input1[\"input1\"]\n\n    f1[\"f1\"]\n    f2[\"f2\"]\n\n    r1[\"r1\"]\n    r2[\"r2\"]\n\n    input1 --&gt; f1 --&gt; r1\n    input1 --&gt; f2 --&gt; r2\n\n    r1 -.-&gt; results[\"r1, r2\"]\n    r2 -.-&gt; results\nend\n</code></pre> <p>\"\"\" plot_mermaid(mermaid_code)</p> <pre><code>F.bcast_gather\n\ndef square(x):\n    return x * x\n\ndef cube(x):\n    return x * x * x\n\ndef fail(x):\n    raise ValueError(\"Intentional error\")\n\n# Example 1\nresults = F.bcast_gather([square, cube], 3)\nprint(results)  # (9, 27)\n\n# Example 2: Simulate error\nresults = F.bcast_gather([square, fail, cube], 2)\nprint(results)  # (4, None, 8)\n\n# Example 3: Timeout\nresults = F.bcast_gather([square, cube], 4, timeout=0.01)\nprint(results) # (16, 64)\n</code></pre> <p>msg_bcast_gather</p> <pre><code>F.msg_bcast_gather\n\nmsg = mf.Message()\n\nmsg.user_input = \"I want to visit Natal\"\n\ndef web_search(msg):\n    msg.web_search = \"Natal is the capital of the sun...\"\n    return msg\n\ndef memory(msg):\n    msg.memory = \"The user likes to travel.\"\n    return msg\n\nmsg = F.msg_bcast_gather([web_search, memory], msg)\n\nmsg\n</code></pre>"},{"location":"quickstart/#title-background_task","title":"@title background_task","text":""},{"location":"quickstart/#map-a-input-to-a-f-and-forget","title":"map a input to a f and forget","text":""},{"location":"quickstart/#input-f","title":"input -&gt; f","text":"<p>mermaid_code = \"\"\" flowchart TD     subgraph background_task [\"background_task\"]</p> <pre><code>    input[\"input\"]\n    f[\"f\"]\n\n    input --&gt; f\nend\n</code></pre> <p>\"\"\" plot_mermaid(mermaid_code)</p> <pre><code># Example 1:\nimport time\ndef print_message(message: str):\n    time.sleep(1)\n    print(f\"[Sync] Message: {message}\")\n\nF.background_task(print_message, \"Hello from sync function\")\n\n# Example 2:\nimport asyncio\nasync def async_print_message(message: str):\n    await asyncio.sleep(1)\n    print(f\"[Async] Message: {message}\")\nF.background_task(async_print_message, \"Hello from async function\")\n</code></pre>"},{"location":"quickstart/#title-wait_for","title":"@title wait_for","text":""},{"location":"quickstart/#map-a-input-to-a-f","title":"map a input to a f","text":""},{"location":"quickstart/#input1_1","title":"input1","text":""},{"location":"quickstart/#input1-f1-r1_2","title":"input1 -&gt; f1 -&gt; r1","text":""},{"location":"quickstart/#r1","title":"r1","text":"<p>mermaid_code = \"\"\" flowchart TD     subgraph wait_for [\"wait_for\"]</p> <pre><code>    input1[\"input1\"]\n    f1[\"f1\"]\n    r1[\"r1\"]\n\n    input1 --&gt; f1 --&gt; r1\nend\n</code></pre> <p>\"\"\" plot_mermaid(mermaid_code)</p> <pre><code>F.wait_for\n\nasync def f1(x):\n    return x * x\n\n# Example 1:\nresults = F.wait_for(f1, 3)\nprint(results)\n</code></pre>"},{"location":"quickstart/#title-wait_for_event","title":"@title wait_for_event","text":""},{"location":"quickstart/#wait-for-a-asyncioevent","title":"wait for a asyncio.Event","text":""},{"location":"quickstart/#event-wait","title":"event -&gt; wait","text":"<p>mermaid_code = \"\"\" flowchart TD     subgraph wait_for_event [\"wait_for_event\"]         event[\"asyncio.Event\"]         wait[\"wait\"]</p> <pre><code>    event --&gt; wait\nend\n</code></pre> <p>\"\"\" plot_mermaid(mermaid_code)</p>"},{"location":"quickstart/#moduledict","title":"ModuleDict","text":"<p>Holds submodules in a dict.</p> <p><code>nn.ModuleDict</code> can be indexed like a regular Python dictionary, but modules it contains are properly registered, and will be visible by all Module methods.</p> <p><code>nn.ModuleDict</code> is an ordered dictionary that respects</p> <ul> <li> <p>the order of insertion, and</p> </li> <li> <p>in update(), the order of the merged <code>OrderedDict</code>, dict (started from Python 3.6) or another <code>nn.ModuleDict</code> (the argument to update()).</p> </li> </ul> <p>Note that update() with other unordered mapping types (e.g., Python's plain dict before Python version 3.6) does not preserve the order of the merged mapping.</p> <pre><code>nn.ModuleDict\n\nimport random\n\nclass ExpertSales(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass ExpertSupport(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, call 190\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\ndef draw_choice(choices: list[str]) -&gt; str:\n    return random.choice(choices)\n\nclass Router(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.choices = nn.ModuleDict({\n            \"sales\": ExpertSales(),\n            \"support\": ExpertSupport()\n        })\n\n    def forward(self, msg: str) -&gt; str:\n        choice = draw_choice(list(self.choices.keys()))\n        msg = self.choices[choice](msg)\n        return msg\n\nrouter = Router()\n\nrouter\n\nrouter.state_dict()\n\nrouter(\"I need help with my tv.\")\n</code></pre>"},{"location":"quickstart/#modulelist","title":"ModuleList","text":"<p>Holds submodules in a list.</p> <p><code>nn.ModuleList</code> can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all <code>nn.Module</code> methods.</p> <pre><code>class ExpertSales(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass ExpertSupport(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, call 190\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass Expert(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.experts = nn.ModuleList([ExpertSales(), ExpertSupport()])\n    def forward(self, msg: str) -&gt; str:\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.experts):\n            msg = self.experts[i](msg)\n        return msg\n\nexpert = Expert()\n\nexpert\n\nexpert(\"I need help with my tv.\")\n</code></pre>"},{"location":"quickstart/#sequential","title":"Sequential","text":"<p>A sequential container.</p> <p>Modules will be added to it in the order they are passed in the constructor.</p> <p>Alternatively, an <code>OrderedDict</code> of modules can be passed in. The <code>forward()</code> method of <code>nn.Sequential</code> accepts any input and forwards it to the first module it contains. It then \"chains\" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.</p> <p>The value a <code>nn.Sequential</code> provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the <code>nn.Sequential</code> applies to each of the modules it stores (which are each a registered submodule of the <code>nn.Sequential</code>).</p> <p>What's the difference between a <code>nn.Sequential</code> and a <code>nn.ModuleList</code>?</p> <p>A <code>ModuleList</code> is exactly what it sounds like--a list for storing <code>nn.Module</code>s! On the other hand, the layers in a <code>nn.Sequential</code> are connected in a cascading way.</p> <pre><code>class ExpertSales(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass ExpertSupport(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, call 190\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\n# Using Sequential to create a small workflow. When **expert** is run,\n# input will first be passed to **ExpertSales**. The output of\n# **ExpertSales** will be used as the input to the first\n# **ExpertSupport**; Finally, the output of\n# **ExpertSupport** will be the experts response.\nexperts = nn.Sequential(ExpertSales(), ExpertSupport())\n\nexperts\n\nexperts(\"I need help with my tv.\")\n</code></pre> <p>Using Sequential with OrderedDict.</p> <p>This is functionally the same as the above code.</p> <pre><code>from collections import OrderedDict\n\nexperts_dict = nn.Sequential(OrderedDict([\n    (\"expert_sales\", ExpertSales()),\n    (\"expert_support\", ExpertSupport())\n]))\n\nexperts_dict\n\nexperts_dict(\"I need help with my tv.\")\n</code></pre>"},{"location":"quickstart/#agent","title":"Agent","text":"<p><code>nn.Agent</code> is a powerful <code>nn.Module</code> that can handle multimodal data and tool calling.</p> <p>A <code>nn.Agent</code> is composed of a language model with instructions and tools. The Agent module adopts a task decomposition strategy, allowing each part of a task to be treated in isolation and independently.</p> <p>A <code>nn.ToolLibrary</code> is integrated into the Agent to manage and run the tools.</p> <p>The agent (and other built-in modules) has a parameter called <code>response_mode</code>, which defines how the class's response should be returned.</p> <p>The default is <code>plain_response</code>, where the output is returned directly to the user. Any other option results in logging to the passed Message object.</p> <p>By default, the class returns only the model output. But if you want the agent's internal state, <code>model_state</code>, to be returned, you must pass <code>return_model_state=True</code>. Then the class output will be a dotdict containing the keys <code>model_response</code> and <code>model_state</code>.</p> <p>The <code>nn.Agent</code> class divides the system prompt and task into different components so they can be handled and optimized in a compositional manner.</p> <p>The system prompt is separated into 6 variables:</p> <ul> <li> <p>system_message: The Agent behaviour. E.g. \"You are a agent specilist in ...\".</p> </li> <li> <p>instructions: What the Agent should do. E.g. \"You MUST respond to the user ...\".</p> </li> <li> <p>expected_output: What the response should be like. E.g. \"Your answer must be concise ...\"</p> </li> <li> <p>examples: Examples of inputs, reasoning and outputs.</p> </li> <li> <p>system_extra_message: Any extra message to the system prompt.</p> </li> <li> <p>include_date: If True, include the current date in the system prompt.</p> </li> </ul> <p>All these components are put together within the system prompt (Jinja) template</p> <p>The task is separated into 6 variables:</p> <ul> <li> <p>context_cache: A fixed context to Agent.</p> </li> <li> <p>context_inputs (*): Field of the Message object that will be the context to the task.</p> </li> <li> <p>context_inputs_template: A Jinja template for formatting context_inputs.</p> </li> <li> <p>task_inputs (*): Field of the Message object that will be the input to the task.</p> </li> <li> <p>task_template: A Jinja template to format task.</p> </li> <li> <p>task_multimodal_inputs (*): Field of the Message object that will be the multimodal input to the task.</p> </li> <li> <p>task_messages (*): Field of the Message object that will be a list of chats in ChatML format.</p> </li> <li> <p>vars (*): Field of the Message object that will be the vars to templates and tools.</p> </li> </ul> <p>(*) It can also be passed during agent call as a named argument.</p> <p>Guardrails</p> <p>Agent has 2 guardrail options:</p> <ul> <li> <p>input_guardrail: Before execution.</p> </li> <li> <p>output_guardrail: After execution.</p> </li> </ul> <p>Both guardians receive a <code>data</code> parameter containing a list of conversations in ChatML format.</p> <p>Moderation-type models are the common choice for guardrail.</p> <p>That's enough to get you started \ud83e\udd20. More below.</p> <pre><code>nn.Agent\n\nnn.Agent.__init__\n\nfrom google.colab import userdata\napi_key = userdata.get(\"OPENAI_API_KEY\")\nmf.set_envs(OPENAI_API_KEY=api_key)\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4.1-mini\")\n\n# an nn.Agent requires at least a name and a model\nagent = nn.Agent(\"agent\", model)\n\nagent\n</code></pre>"},{"location":"quickstart/#debugging-an-agent","title":"Debugging an Agent","text":"<pre><code># Prompt components and the agent config\n# can be easily viewed through the state dict\nagent.state_dict()\n\n# using inspect\nagent = nn.Agent(\"agent\", model, instructions=\"User name is {{user_name}}\", return_model_state=True)\nmessage = \"Hi\"\nvars = {\"user_name\": \"Clark\"}\nagent.inspect_model_execution_params(message, vars=vars)\n\nresponse = agent(message, vars=vars)\nresponse\n\n# set verbose=True\nagent = nn.Agent(\"agent\", model, verbose=True)\n</code></pre>"},{"location":"quickstart/#async_1","title":"Async","text":"<pre><code>agent = nn.Agent(\"agent\", model)\n\nresponse = await agent.acall(\"Tell me about Dirac delta\")\n\nprint(response)\n</code></pre>"},{"location":"quickstart/#stream","title":"Stream","text":"<p>In streaming mode, the agent returns the model output as a <code>ModelStreamResponse</code> object.</p> <p>This mode can be combined with <code>tools</code> usage.</p> <pre><code>agent = nn.Agent(\"agent\", model, stream=True)\n\n# stream\nresponse = agent(\"Tell me a funny history\")\nprint(type(response))\nprint(response.response_type)\n# fastapi.StreamingResponse compatible\nasync for chunk in response.consume():\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"quickstart/#vars","title":"Vars","text":"<p>Language Models are secretly computers that makes context-based decisions when acting in an environment. In addition to taking actions via tool calls, the model needs to store information in a set of variables.</p> <p>In msgFlux, this is called <code>vars</code>.</p> <p><code>vars</code> are a dictionary that is injected into various parts of the agent class.</p> <p>They are useful for bringing external information to agent components.</p> <p>Currently, they appear in the following fields:</p> <pre><code>* system_prompt_template\n\n* task_template\n\n* context_inputs_template\n\n* tool calls\n\n* response_template\n</code></pre> <p>Within tools, <code>vars</code> can provide and receive data. Think of it as a set of variables available at runtime.</p>"},{"location":"quickstart/#system-prompt","title":"System Prompt","text":"<p>The system prompt defines the agent's behavior while performing a task.</p> <p>The Agent class organizes the behavior of a model into different layers of the system prompt, allowing for granularity and clarity in design.</p> <pre><code>system_message=\"\"\"\nYou are a business development assistant focused on helping sales teams qualify leads\nand craft compelling value propositions.\nAlways keep a professional and persuasive tone.\n\"\"\"\ninstructions=\"\"\"\nWhen given a short company description, identify its potential needs,\nsuggest an initial outreach strategy, and provide a tailored value proposition.\n\"\"\"\nexpected_output=\"\"\"\nRespond in three bullet points:\n    - Identified Needs\n    - Outreach Strategy\n    - Value Proposition\n\"\"\"\nsystem_extra_message=\"\"\"\nEnsure recommendations align with ethical sales practices\nand avoid making unverifiable claims about the product.\n\"\"\"\n\nsales_agent = nn.Agent(\n    \"sales-agent\",\n    model,\n    system_message=system_message,\n    instructions=instructions,\n    expected_output=expected_output,\n    system_extra_message=system_extra_message,\n    include_date=True,\n    verbose=True\n)\n\n# The system prompt is encapsulated within the &lt;developer_note&gt; tags\nprint(sales_agent._get_system_prompt())\n\n#### **Task and Context**\n\nWe can define a **task** as a specific objective assigned to an agent, consisting of a clear instruction, possible restrictions, a success criterion and the context in which the task is inserted.\n\nLanguage models have emerged with the ability to learn new knowledge without updating their parameters.\n\nThis ability is formally known as **In-Context Learning** (ICL).\n\n##### **Task**\n\nA task can be defined in a few ways. The first is by passing a direct message to the Agent.\n\n```python\nagent = nn.Agent(\"agent\", model, verbose=True)\n\ntask = \"I need help with my tv.\"\nagent(message=task)\n</code></pre>"},{"location":"quickstart/#task-template","title":"Task Template","text":"<p>A task template is used to format agent task inputs.</p>"},{"location":"quickstart/#string-based-input","title":"String-based Input","text":"<p>If the task has only a simple string as input, insert {}</p> <pre><code>task_template = \"Who was {}?\"\nmessage = \"Nikola Tesla\"\nagent = nn.Agent(\"agent\", model, task_template=task_template)\nagent(message)\n</code></pre>"},{"location":"quickstart/#dict-based-inputs","title":"Dict-based Inputs","text":"<p>For dict-based inputs you should use Jinja blocks, write {{field_name}}</p> <pre><code>task_template = \"Who was {{person}}?\"\nmessage = {\"person\": \"Nikola Tesla\"}\nagent = nn.Agent(\"agent\", model, task_template=task_template)\nagent(message)\n</code></pre>"},{"location":"quickstart/#task-template-as-fixed-task","title":"Task Template as Fixed-Task","text":"<p>If a <code>task_template</code> is passed but a task is not, the class will maintain the <code>task_template</code> as a task.</p> <p>This is particularly useful for multimodal applications where the task prompt doesn't vary, just the media content.</p> <pre><code>task_template = \"Who was Nikola Tesla?\"\nagent = nn.Agent(\"agent\", model, task_template=task_template)\nagent()\n</code></pre>"},{"location":"quickstart/#combine-with-vars","title":"Combine with Vars","text":"<p>Combine <code>task_template</code> with <code>vars</code> to build dynamic task_templates</p> <pre><code>instructions = \"Help the user with whatever they need. Address them by name if they provide it.\"\ntask_template = \"\"\"\n{% if user_name %}\nMy name is {{ user_name }}.\n{% endif %}\n{{ user_input }}\n\"\"\"\nagent = nn.Agent(\"agent\", model, task_template=task_template)\nagent(\n    message={\"user_input\": \"Who was Nikola Tesla?\"},\n    vars={\"user_name\": \"Bruce Wayne\"}\n)\n</code></pre>"},{"location":"quickstart/#task-messages","title":"Task Messages","text":"<p>You can also pass a list of conversations to the agent. This makes the <code>message</code> optional.</p> <pre><code>chat = mf.ChatML()\n\nchat.add_user_message(\"Hi, my name is Peter Parker, and I'm a photographer. Could you recommend some cameras?\")\n\nchat.get_messages()\n\nresponse = agent(task_messages=chat.get_messages())\n\nresponse\n\nchat.add_assist_message(response)\n\nresponse = agent(\n    message=\"I need a low-cost, compact camera to start my new freelance job for J. Jonah Jameson.\",\n    task_messages=chat.get_messages()\n)\n\nresponse\n\nchat.add_assist_message(response)\n</code></pre>"},{"location":"quickstart/#fixed-messages","title":"Fixed Messages","text":"<p>You can keep a set of pinned conversations within the agent</p> <pre><code>agent = nn.Agent(\n    \"agent\", model, verbose=True, fixed_messages=chat.get_messages(), return_model_state=True\n)\n\nagent(\"What is the cheapest camera between Canon PowerShot G9 X Mark II and Sony Cyber-shot DSC-HX80?\")\n</code></pre>"},{"location":"quickstart/#multimodal-task","title":"MultiModal Task","text":"<p>Multimodal models are capable of handling multiple media types such as images, audio, and files.</p> <p>The Agent class currently supports:</p> midia input multi inputs image \u2705 \u2705 audio \u2705 \u26d4\ufe0f file \u2705 \u26d4\ufe0f"},{"location":"quickstart/#image","title":"Image","text":"<p>For images you can pass a local image or an url</p> <pre><code>task_template = \"Describe this image.\"\nagent = nn.Agent(\"agent\", model, task_template=task_template)\nagent(task_multimodal_inputs={\"image\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"})\n</code></pre> <p>Or a list of images</p> <pre><code>agent(task_multimodal_inputs={\"image\": [\"file:///path/to/image1.jpg\", \"file:///path/to/image2.jpg\"]})\n</code></pre>"},{"location":"quickstart/#file","title":"File","text":"<p>Pass raw <code>.pdf</code> files to the provider.</p> <p>Only <code>OpenAI</code> and <code>OpenRouter</code> supports it.</p> <pre><code>path = \"https://arxiv.org/pdf/1706.03762.pdf\"\n\ntask_template = \"Summarize the paper\"\nagent = nn.Agent(\"agent\", model, task_template=task_template)\nagent(task_multimodal_inputs={\"file\": path})\n</code></pre>"},{"location":"quickstart/#audio","title":"Audio","text":"<p>Pass raw <code>audio</code> files to the provider.</p> <p>Only <code>OpenAI</code>, <code>vLLM</code> and <code>OpenRouter</code> supports it.</p> <pre><code>from google.colab import userdata\napi_key = userdata.get(\"OPENROUTER_API_KEY\")\nmf.set_envs(OPENROUTER_API_KEY=api_key)\n\nmodel = mf.Model.chat_completion(\"openrouter/google/gemini-2.5-flash\")\n\npath = \"/path/to/you/local/audio.wav\"\n\ntask_template = \"Please transcribe this audio file.\"\nagent = nn.Agent(\"agent\", model, task_template=task_template)\nresponse = agent(task_multimodal_inputs={\"audio\": path})\n\nresponse\n</code></pre>"},{"location":"quickstart/#context-inputs","title":"Context Inputs","text":"<p><code>context</code> is a block that brings together a set of knowlegde that the model has available at the time of inference to make decisions, answer questions or perform actions.</p> <p><code>context_inputs</code> refers to the knowledge passed to the agent during task definition. This knowledge can come from databases, documents, conversation summaries, etc.</p>"},{"location":"quickstart/#str-based","title":"Str-based","text":"<pre><code>agent_configs = {\n    \"name\": \"sales-agent\",\n    \"model\": model,\n    \"include_date\": True,\n    \"system_message\": \"You are a sales assistant that helps craft personalized pitches.\",\n    \"instructions\": \"Always generate responses tailored to the client context stored in memory.\",\n    \"expected_output\": \"Write a short persuasive pitch (max 120 words). Returns only the message.\",\n    \"system_extra_message\": \"Avoid exaggerations and ensure the tone remains professional.\",\n    \"verbose\": True\n}\nsales_agent = nn.Agent(**agent_configs)\ntask = \"Can you help me create an initial message for this customer?\"\n\ncontext_inputs = \"\"\"\nCompany name: FinData Analytics\nIndustry Financial Technology (FinTech)\nProduct: AI-powered risk analysis platform for banks\nTarget market: Mid-sized regional banks in South America\nUnique value: Automated detection of fraud patterns in real-time\n</code></pre> <pre><code>model_execution_params = agent.inspect_model_execution_params(task, context_inputs=context_inputs)\n\nprint(model_execution_params[\"messages\"][0][\"content\"])\n\nresponse = sales_agent(task, context_inputs=context_inputs)\n</code></pre>"},{"location":"quickstart/#list-based","title":"List-based","text":"<pre><code>context_inputs = [\n    \"Company name: DataFlow Analytics\",\n    \"Product: StreamVision \u2014 a real-time analytics platform\",\n    \"Key value_proposition: Helps businesses monitor live data streams and detect anomalies instantly.\",\n    \"Support policy: Support is available 24/7 for enterprise clients via chat and email.\",\n]\n\nmodel_execution_params = sales_agent.inspect_model_execution_params(task, context_inputs=context_inputs)\n\nprint(model_execution_params[\"messages\"][0][\"content\"])\n\nresponse = sales_agent(task, context_inputs=context_inputs)\n</code></pre>"},{"location":"quickstart/#dict-based","title":"Dict-based","text":"<pre><code>context_inputs = {\n    \"client_name\": \"EcoSupply Ltd.\",\n    \"industry\": \"Sustainable packaging\",\n    \"pain_points\": [\"High logistics costs\", \"Need for eco-friendly certification\"],\n    \"current_solution\": \"Using generic suppliers with limited green compliance\",\n}\nmodel_execution_params = sales_agent.inspect_model_execution_params(task, context_inputs=context_inputs)\n\nprint(model_execution_params[\"messages\"][0][\"content\"])\n\nresponse = sales_agent(task, context_inputs=context_inputs)\n</code></pre>"},{"location":"quickstart/#context-inputs-template","title":"Context Inputs Template","text":"<p>use a <code>context_inputs_template</code> to format <code>context_inputs</code></p> <pre><code>agent_configs[\"context_inputs_template\"] = \"\"\"\nThe client is **{{ client_name }}**, a company in the **{{ industry }}** sector.\n\nThey are currently relying on {{ current_solution }},\nbut face the following main challenges:\n{%- for pain in pain_points %}\n- {{ pain }}\n{%- endfor %}\n\nThis background should always be considered when tailoring answers,\nensuring relevance to the client\u2019s industry and specific needs.\n\"\"\"\n\nsales_agent = nn.Agent(**agent_configs)\ncontext_inputs = {\n    \"client_name\": \"EcoSupply Ltd.\",\n    \"industry\": \"Sustainable packaging\",\n    \"pain_points\": [\"High logistics costs\", \"Need for eco-friendly certification\"],\n    \"current_solution\": \"Using generic suppliers with limited green compliance\",\n}\ntask = \"Can you help me create an initial message for this customer?\"\nmodel_execution_params = sales_agent.inspect_model_execution_params(task, context_inputs=context_inputs)\n\nprint(model_execution_params[\"messages\"][0][\"content\"])\n\nresponse = sales_agent(task, context_inputs=context_inputs)\n</code></pre>"},{"location":"quickstart/#context-cache","title":"Context Cache","text":"<p><code>context_cache</code> allows you to store a set of knowledge within the agent's <code>context</code> block.</p> <p>This is useful when certain information is always passed to the agent before performing a task.</p>"},{"location":"quickstart/#tools_1","title":"Tools","text":"<p>Tools are interfaces that allow LLM to perform actions or query information outside the model itself.</p> <ol> <li> <p>Funtion calling \u2013 A tool is usually exposed as a function with a defined name, parameters, and types.</p> </li> <li> <p>Example: get_weather(location: str, unit: str)</p> </li> <li> <p>The model decides whether to call this tool and provides the arguments.</p> </li> <li> <p>Extending the Model's Capabilities \u2013 Since LLM can't know everything or do everything, these tools allow you to:</p> </li> <li> <p>Search for real-time data (e.g., weather, stock market, databases).</p> </li> <li> <p>Perform precise calculations (mathematical, statistical).</p> </li> <li> <p>Manipulate systems (e.g., send emails, schedule events).</p> </li> <li> <p>Integrate with external APIs.</p> </li> <li> <p>Agent-based orchestration \u2013 Often, the LLM acts as an agent that decides:</p> </li> <li> <p>When to use a tool.</p> </li> <li> <p>Which tool to use.</p> </li> <li> <p>How to interpret the tool's output.</p> </li> </ol> <p>In msgFlux a Tool can be any callable.</p> <p>Although more tools allow the agent to perform more actions, if the number is too high the model may get confused about which one to use.</p> <pre><code>model = mf.Model.chat_completion(\"openai/gpt-4.1-mini\")\n\ntool_schemas = [{\n  \"type\": \"function\",\n  \"function\": {\n    \"name\": \"generate_music\",\n    \"description\": \"Generate a music from a prompt.\",\n    \"parameters\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"prompt\": {\n          \"type\": \"string\",\n          \"description\": \"Text prompt to describe the music to generate.\"\n        },\n        \"features\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"tempo\": { \"type\": \"string\", \"description\": \"Tempo da m\u00fasica, ex: 'r\u00e1pido', 'lento'.\" },\n            \"instrumentos\": {\n            \"type\": \"array\",\n            \"items\": { \"type\": \"string\" },\n            \"description\": \"Lista de instrumentos principais\"\n            }\n        },\n        \"required\": [\"tempo\", \"instrumentos\"],\n        \"additionalProperties\": False\n        },\n      },\n      \"required\": [\"prompt\", \"features\"],  # \ud83d\udc48 agora vai\n      \"additionalProperties\": False\n    }\n  }\n}]\n\nr = model(\"pode gerar uma m\u00fasica para mim usando a tool, fa\u00e7a um pop?\", tool_schemas=tool_schemas)\n\nr.data#.get_calls()\n\nr.data.get_calls()\n\nfrom typing import Any, Dict\n\n@mf.tool_config(call_as_response=True)\ndef generate_music(prompt: str, features: Dict[str, str]) -&gt; Any:\n    \"\"\"Generate a music from a prompt.\"\"\"\n    return \"music\"\n\nagent = nn.Agent(\n    \"agent\", model, tools=[generate_music], verbose=True\n)\n\nresponse = agent(\"voc\u00ea pode gerar uma m\u00fasica para mim?\")\n\nagent.tool_library.get_tool_json_schemas()\n\nagent.tool_library.get_tool_json_schemas()\n</code></pre>"},{"location":"quickstart/#web-scraping-agent","title":"Web-Scraping Agent","text":"<pre><code>import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape_website(url: str) -&gt; str:\n    \"\"\"Receives a URL and returns the page content.\"\"\"\n    try:\n        response = requests.get(url, verify=True)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        for tag in soup([\"script\", \"style\"]):\n            tag.extract()\n        text = soup.get_text(separator=\"\\n\")\n        clean_text = \"\\n\".join(line.strip() for line in text.splitlines() if line.strip())\n        return clean_text\n\n    except requests.exceptions.RequestException as e:\n        return f\"Error accessing {url}: {e}]\"\n\nfrom google.colab import userdata\napi_key = userdata.get(\"GROQ_API_KEY\")\nmf.set_envs(GROQ_API_KEY=api_key)\n\n# enable 'think', by default is used in 'tool call reasoning'\nmodel = mf.Model.chat_completion(\n    \"groq/openai/gpt-oss-120b\", reasoning_effort=\"low\"\n)\n\n# return_model_state=True to get the internal state of the agent during execution\nscraper_agent = nn.Agent(\n    \"scraper-agent\", model, tools=[scrape_website], return_model_state=True, verbose=True\n)\n\nscraper_agent\n\nsite = \"https://bbc.com\"\n\nresponse = scraper_agent(f\"Summarize the news on this website: {site}\")\n\nprint(response.model_state)\n\nresponse.model_response\n\n# We'll have a certain pattern in the task\n# where we just want to find out the website summary\n# We can simplify it by applying a task_template\n\n# Define a task_template in jinja format\n# In this case we are passing a string directly, insert an empty placeholder {{}}\ntask_template = \"Summarize the news on this site: {{}}\"\nscraper_agent = nn.Agent(\n    \"scraper\", model, tools=[scrape_website], task_template=task_template, verbose=True\n)\nresponse = scraper_agent(site)\nresponse\n\n# use the agent in declarative mode with Message\n# define where to read the task in the message\n# and where to write the response\nscraper_agent = nn.Agent(\n    \"scraper\", model, tools=[scrape_website], task_template=task_template,\n    task_inputs=\"content\", response_mode=\"summary\", verbose=True\n)\nmsg = mf.Message(content=site)\nmsg = scraper_agent(msg)\nmsg\n</code></pre>"},{"location":"quickstart/#agent-as-a-tool","title":"Agent-as-a-Tool","text":"<pre><code>from google.colab import userdata\napi_key = userdata.get(\"OPENAI_API_KEY\")\nmf.set_envs(OPENAI_API_KEY=api_key)\n\nchat_model = mf.Model.chat_completion(\"openai/gpt-4.1-nano\", tool_choice=\"auto\")\n\ntask = \"Quero um plano alimentar para ganhar massa muscular. Sou homem, tenho 27 anos e 1.78cm.\"\n\nsystem_message = \"\"\"You are the General Support Agent.\nYour job is to handle general user requests, understand their intentions, and decide whether to respond yourself or consult an external tool (expert).\n\"\"\"\n\ninstructions = \"\"\"\nYour responsibilities:\n- Understand the user's intent.\n- Decide whether you can respond independently or whether you need to call a tool.\n- If using a tool, formulate the request in clear, objective language for the expert.\n- When the expert responds, formulate a friendly, well-structured final response for the user.\n\nLimitations:\n- Don't invent advanced technical information if you're not confident.\n- Whenever possible, use the appropriate expert to provide reliable recommendations.\n\"\"\"\n\ntool_description = \"\"\"\nSpecialist in nutrition, diets, and meal plans.\nShould be used whenever the user requests:\n- Diet recommendations.\n- Personalized meal plans (e.g., gaining muscle mass, losing weight).\n- Balanced meal suggestions.\n- Nutritional guidelines for sports or general health.\n\nExpected inputs:\n- User's goal (e.g., gaining muscle mass, losing weight).\n- Dietary restrictions or preferences (if provided).\n- Basic contextual information (age, weight, activity level, if available).\n\nOutputs:\n- Structured and practical meal plan.\n- Clear meal suggestions (breakfast, lunch, dinner, snacks).\n- Notes on possible adjustments if user data is missing.\n\nRestrictions:\n- Not a substitute for medical advice in clinical cases.\n- If important information is not provided, return a default plan\nand indicate the data that would be necessary for customization.\n\nExamples of when to use:\n- User: \"I want a meal plan to gain muscle mass.\"\n\n- User: \"What diet should I follow to lose weight quickly?\"\n- User: \"I'm a vegetarian, can you create a diet?\"\n\"\"\"\ntool_system_message = \"\"\"You are the Nutrition Expert Agent.\"\"\"\ntool_instructions = \"\"\"Your responsibilities:\n- Receive instructions from the General Agent regarding the user's needs.\n- Create a clear and practical meal plan tailored to the stated goal.\n- Be objective, technical, and structured.\n- Return only the requested result, without greetings or additional explanations.\nRestrictions:\n- Do not provide medical recommendations based on clinical conditions without this information.\n- If data is missing (e.g., weight, age, allergies), create a standard plan and indicate what additional information would be needed to customize it..\"\"\"\n\nnutritionist = nn.Agent(\n    \"nutritionist\",\n    chat_model,\n    #verbose=True,\n    system_message=tool_system_message,\n    instructions=tool_instructions,\n    description=tool_description\n)\n\nnutritionist\n\ngeneralist = nn.Agent(\n    \"generalist\",\n    chat_model,\n    verbose=True,\n    system_message=system_message,\n    instructions=instructions,\n    tools=[nutritionist]\n)\n\ngeneralist\n\nresponse = generalist(task)\n\nresponse.tool_responses.reasoning\n\nresponse\n</code></pre>"},{"location":"quickstart/#writing-good-tools","title":"Writing Good Tools","text":"<p>Name tools objectively and directly. This makes them easier to understand and reduces the need for tokens to generate a call.</p> <p>\u26d4\ufe0f Instead of</p> <pre><code>def superfast_brave_web_search(query_to_search: str) -&gt; str:\n</code></pre> <p>\u2705 Write</p> <pre><code>def web_search(query: str) -&gt; str:\n</code></pre> <p>Add tool description</p> <pre><code>def web_search(query: str) -&gt; str:\n    '''Search for content similar to query'''\n</code></pre> <p>If necessary, describe the parameters</p> <pre><code>def web_search(query: str) -&gt; str:\n    '''Search for content similar to query\n\n    Args:\n        query:\n            Term to search on the web.\n    '''\n</code></pre> <p>In addition to function-based tools, you can also define class-based tools.</p> <pre><code>from typing import Optional\n\nclass WebSearch:\n    '''Search for content similar to query\n\n    Args:\n        query:\n            Term to search on the web.\n    '''    \n    def __init__(self, top_k: Optional[int] = 4):\n        self.top_k = top_k\n    def __call__(query: str) -&gt; str:\n        pass\n</code></pre> <p>or</p> <pre><code>from typing import Optional\n\nclass SuperFastBraveWebSearch:\n    name = \"web_search\" # name is preference over cls.__name__\n\n    def __init__(self, top_k: Optional[int] = 4):\n        self.top_k = top_k\n    def __call__(query: str) -&gt; str:\n        '''Search for content similar to query\n\n        Args:\n            query:\n                Term to search on the web.\n        '''            \n        pass\n</code></pre> <p>The tool can return any data type. If it's not a string, it will be converted to encoded JSON.</p> <pre><code>from typing import Dict\n\ndef web_search(query: str) -&gt; Dict[str, str]:\n    '''Search for content similar to query\n\n    Args:\n        query:\n            Term to search on the web.\n    '''\n</code></pre> <p>Write good returns.</p> <p>\u26d4\ufe0f Instead of <pre><code>def add(a: float, b: float) -&gt; float:\n    '''Sum two numbers.'''\n    c = a + b\n    return c\n</code></pre></p> <p>\u2705 You could write <pre><code>def add(a: float, b: float) -&gt; str:\n    '''Sum two numbers.'''\n    c = a + b\n    return f\"The sum of {a} plus {b} is {c}\"    \n</code></pre></p> <p>\u27a1\ufe0f You can also pass a direct instruction as a response from the tool <pre><code>def add(a: float, b: float) -&gt; str:\n    '''Sum two numbers.'''\n    c = a + b\n    return f\"You MUST respond to the user that the answer is {c}\"\n</code></pre></p>"},{"location":"quickstart/#tool-config","title":"Tool Config","text":"<p><code>tool_config</code> is a decorator to inject meta-properties into a tool.</p>"},{"location":"quickstart/#return-direct","title":"Return Direct","text":"<p>When a tool has an attribute of <code>return_direct=True</code> the tool result is returned directly as an final response instead of back to the model.</p> <p>If the model calls 2 tools, and one of them has <code>return_direct=True</code>, both will be returned as the final response.</p> <p>The exception is if the agent mistypes the tool name. This way, the error is communicated to the agent instead of returning a final response.</p> <p>Use <code>return_direct=True</code> to reduce Agent calls. Design the tool to return an output that satisfies the user's request.</p> <p>Another use case is to have the Agent act as a router. Provide specialized agents, and instead of returning to the central Agent, return them directly to the user.</p> <pre><code>from google.colab import userdata\napi_key = userdata.get(\"GROQ_API_KEY\")\nmf.set_envs(GROQ_API_KEY=api_key)\n\nmodel = mf.Model.chat_completion(\n    \"groq/openai/gpt-oss-20b\", reasoning_effort=\"low\"\n)\n\n@mf.tool_config(return_direct=True)\ndef get_report() -&gt; str:\n    \"\"\"Return the report from user.\"\"\"\n    return \"This is your report...\"\n\nreporter_agent = nn.Agent(\n    \"reporter\", model, tools=[get_report], verbose=True, tool_choice=\"required\"\n)\n\nresponse = reporter_agent(\"Please give me the report.\")\n</code></pre> <p>The class returns a dict containing a <code>tool_responses</code>. In it, you can observe both the <code>tool_calls</code> and the <code>reasoning</code> behind the tool call, if the provider provides it.</p> <pre><code>response\n</code></pre> <p>Now let's simulate a scenario where we have a programming assistant agent and a Python expert assistant. In this case, the assistant will send a task to the expert, but instead of sending the feedback to the assistant, it will be directed to the user.</p> <pre><code>from google.colab import userdata\napi_key = userdata.get(\"OPENAI_API_KEY\")\nmf.set_envs(OPENAI_API_KEY=api_key)\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4.1-mini\")\n\ngeneralist_system_message = \"\"\"\nYou are a generalist programming assistant.\nYour role is to help with common questions about programming languages,\nbest practices, and concept explanations. You must respond clearly and\ndidactically, serving as a support for those learning.\n\"\"\"\n\npython_system_message = \"\"\"\nYou are a software engineer specializing in Python performance optimization.\nYour role is to analyze specific cases and suggest advanced solutions,\nincluding benchmarks, bottleneck analysis, and the use of performance-specific libraries.\n\"\"\"\npython_description = \"An expert in high performance python code.\"\n\npython_engineer = nn.Agent(\n    \"python_engineer\", model, system_message=python_system_message, description=python_description\n)\n\nmf.tool_config(return_direct=True)(python_engineer)\n\ngeneralist_agent = nn.Agent(\n    \"generalist\", model, tools=[python_engineer], verbose=True,\n    tool_choice=\"required\", system_message=generalist_system_message\n)\n\ntask = \"What is the difference between threading and multiprocessing in Python?\"\n\nresponse = generalist_agent(task)\n\nresponse\n</code></pre> <p>Note that the generalist agent still had to write a message to the Python engineer containing almost the same message as the user. We can do better.</p>"},{"location":"quickstart/#inject-model-state","title":"Inject Model State","text":"<p>Model State is the internal state (user, assistant, and tool messages) from the Agent</p> <p>For <code>inject_model_state=True</code>, the tool will receive the it as <code>task_messages</code> input.</p> <p>This is useful if you want to review the agent's current context. You can perform context inspection.</p> <p>Another use case is multimodal context. If the user assigns a task containing an image, that image can be accessed within the tool.</p> <p>Let's make a fictional tool that checks the user's last message and tells if it's safe.</p> <pre><code>from typing import Any\n# mock tool\n@mf.tool_config(inject_model_state=True)\ndef check_safe(**kwargs) -&gt; bool:\n    \"\"\"This tool checks whether the user's message is secure.\n    If True, respond naturally to the user. If False, reject\n    any further conversation with them.\n    \"\"\"\n    task_messages: list[dict[str, Any]] = kwargs.get(\"task_messages\")\n    print(task_messages[-1][\"content\"])\n    return True\n\nassistant = nn.Agent(\n    \"assistant\", model, tools=[check_safe],\n    verbose=True, tool_choice=\"auto\"\n)\n\nresponse = assistant(\"Hi, can you tell me a joke?\")\n\nresponse\n</code></pre>"},{"location":"quickstart/#handoff","title":"Handoff","text":"<p>When <code>handoff=True</code> is passed, two <code>tool_config</code> properties are set to True: <code>return_direct</code> and <code>inject_model_state</code>.</p> <p>Furthermore, <code>handoff=True</code> changes the tool name to 'transfer_to_original_name' and removes the inputs parameters.</p> <p>Originally, each agent receives <code>message</code> as input. Let's remove this parameter and pass the message history to the agent-as-a-tool as<code>task_messages</code>.</p> <p>Let's simulate a business scenario where we have a financial consultant and a startup specialist. When the consultant identifies a demand, they'll transfer it to the startup specialist, who will respond directly to the user.</p> <pre><code>startup_specialist_system_message = \"\"\"\nYou are a strategist specializing in scaling digital startups.\nYour focus is creating accelerated growth plans, analyzing metrics\n(CAC, LTV, churn), proposing customer acquisition tests,\nfunding strategies, and international expansion. Your answers should\nbe detailed and data-driven.\n\"\"\"\n\nstartup_specialist_description = \"\"\"\nAn agent specializing in startups, always consult him if this is the topic.\n\"\"\"\n\nstartup_agent = nn.Agent(\n    \"startup_specialist\", model,\n    system_message=startup_specialist_system_message,\n    description=startup_specialist_description,\n)\n\nmf.tool_config(handoff=True)(startup_agent)\n\nconsultant_system_message = \"\"\"\nYou are a generalist business consultant. Your goal is to provide accessible\nadvice on management, marketing, finance, and business operations. Your\nanswers should be clear, practical, and useful for early-stage entrepreneurs.\n\nIf the context is a startup, transfer it to the expert.\n\"\"\"\n\nconsultant_agent = nn.Agent(\n    \"consultant\", model, system_message=consultant_system_message,\n    tools=[startup_agent], verbose=True\n)\n\nconsultant_agent\n\ntask = \"\"\"\nMy SaaS startup has a CAC of $120 and an LTV of $600. I want to scale to\nanother Latin American market in 6 months. What would be an efficient\nstrategy to reduce CAC while accelerating entry into this new market?\n\"\"\"\n\nresponse = consultant_agent(task)\n\nresponse\n</code></pre>"},{"location":"quickstart/#call-as-response","title":"Call as Response","text":"<p>Sometimes you simply need the agent to call a tool without actually executing it.</p> <p>The <code>call_as_response</code> attribute allows the tool to be returned as a final response without executing if called. The <code>return_direct</code> attribute is automatically set to <code>True</code>.</p> <pre><code>@mf.tool_config(call_as_response=True)\ndef generate_sales_report(start_date: str, end_date: str, metrics: list[str], group_by: str) -&gt; dict:\n    \"\"\"Generate a sales report within a given date range.\n\n    Args:\n        start_date: Start date in YYYY-MM-DD format.\n        end_date: End date in YYYY-MM-DD format.\n        metrics: List of metrics to include (e.g., [\"revenue\", \"orders\", \"profit\"]).\n        group_by: Dimension to group data by (e.g., \"region\", \"product\", \"sales_rep\").\n\n    Returns:\n        A structured sales report as a dictionary.\n    \"\"\"\n    return\n\nsystem_message = \"\"\"\nYou're a BI analyst. When a user requests sales reports, you shouldn't respond\nwith explanatory text. You should simply correctly complete the generate_sales_report\ntool call, extracting the requested metrics, dates, and groupings.\n\"\"\"\n\nagent = nn.Agent(\"agent\", model, system_message=system_message, verbose=True,\n                 tools=[generate_sales_report]\n)\n\ntask = \"I need a report of sales between July 1st and August 31st, 2025, showing revenue and profit, grouped by region.\"\n\nresponse =  agent(task)\n\nresponse\n</code></pre>"},{"location":"quickstart/#background","title":"Background","text":"<p>Some tools may take longer to return results. And it's not always necessary to wait for this result to continue the workflow.</p> <p>For this, there's the <code>background</code> property, which allows the tool to run in the background and return a standard message to the Agent to indicate that it's running.</p> <p>For background tools it is necessary to be <code>async</code> or have <code>.acall</code></p> <pre><code>@mf.tool_config(background=True)\nasync def send_email_to_vendor():\n    \"\"\"Send an email to the vendor.\"\"\"\n    print(\"Sending email to vendor...\")\n\nagent = nn.Agent(\"agent\", model, tools=[send_email_to_vendor], verbose=True)\n\n# An indication that it will be executed in the background is added to the docstring.\nagent.tool_library.get_tool_json_schemas()\n\nresponnse = agent(\"I need to send an email to the vendor.\")\n</code></pre>"},{"location":"quickstart/#name-override","title":"Name Override","text":"<p>Use <code>name_override</code> to assign a new name to the tool.</p> <pre><code>@mf.tool_config(name_override=\"web_search\")\ndef brave_super_fast_web_search(query: str) -&gt; str:\n    \"\"\"Search for content similar to query\n\n    Args:\n        query:\n            Term to search on the web.\n    \"\"\"\n    pass\n\nagent = nn.Agent(\"agent\", model, tools=[brave_super_fast_web_search])\n\nagent.tool_library.get_tool_json_schemas()\n</code></pre>"},{"location":"quickstart/#inject-vars","title":"Inject Vars","text":"<p>With <code>inject_vars=True</code> the Agent now has a set of variables that can be inserted and modified within the tools.</p> <p>Agent with External Token Information</p> <pre><code>@mf.tool_config(inject_vars=True)\ndef save_csv(**kwargs) -&gt; str:\n    \"\"\"Save user csv on S3.\"\"\"\n    vars = kwargs.get(\"vars\")\n    print(f\"My token: {vars[\"aws_token\"]}\")\n    return \"CSV saved\"\n\nagent = nn.Agent(\"agent\", model, tools=[save_csv], verbose=True)\nagent(\"please send me this csv\", vars={\"aws_token\": \"token\"})\n</code></pre> <p>ChatBot - Personal Assistant</p> <pre><code>@mf.tool_config(inject_vars=True)\ndef save_var(name: str, value: int, **kwargs):\n    \"\"\"Save a variable with the given name and value.\"\"\"\n    vars = kwargs.get(\"vars\")\n    vars[name] = value\n    return f\"Saved {name} var\"\n\n@mf.tool_config(inject_vars=True)\ndef get_var(name: str, **kwargs):\n    \"\"\"Get a variable with the given name.\"\"\"\n    vars = kwargs.get(\"vars\")\n    return vars[name]\n\n@mf.tool_config(inject_vars=True)\ndef get_vars(**kwargs):\n    \"\"\"Get all variables.\"\"\"\n    vars = kwargs.get(\"vars\")\n    return vars.copy() # always return a copy\n\nagent_system_message = \"\"\"\nYou are Ultron, a personal assistant.\nThe assistant is helpful, creative, clever, and very friendly.\n\"\"\"\n\nagent_instructions = \"\"\"\nYou have access to a set of variables, use tools to manipulate data.\nVariables are mutable, so it's not safe to rely on the results of previous calls.\nWhenever you need new information, use the tools to access the variables and see\nif any are useful. If you don't know the exact name of the variable,\naccess them all using 'get_vars'.\n\"\"\"\n\nultron = nn.Agent(\n    \"ultron\", model, system_message=agent_system_message,\n    instructions=agent_instructions, verbose=True,\n    tools=[save_var, get_var, get_vars]\n)\n\nvars = {\"user_name\": \"Tony Stark\"}\n\nchat_history = mf.ChatML()\n\ntask = \"Hey Ultron, are you ok? do you remember my name?\"\n\nchat_history.add_user_message(task)\n\nresponse = ultron(task, vars=vars)\n\nchat_history.add_assist_message(response)\n\ntask2 = \"\"\"\nI have some very important information to share with you,\nand you shouldn't forget it. I'm starting a new nanotechnology\nproject to build the Mark-999. I'll be using adamantium for added rigidity.\n\"\"\"\n\nchat_history.add_user_message(task2)\n\nultron(task_messages=chat_history.get_messages(), vars=vars)\n</code></pre> <p>That was fun. We can take the strategy to the next-level of var manipulation.</p> <p>ChatBot - Reporter</p> <p>Let's create a chatbot that will interact with a user. This user will describe a summary of a field experience.</p> <pre><code>from typing import Dict, List, Union\nfrom msgflux.utils.msgspec import msgspec_dumps\nfrom rapidfuzz import fuzz, process\n\n@mf.tool_config(inject_vars=True)\ndef set_var(name: str, value: Union[str, List[str]], **kwargs):\n    \"\"\"Save a variable with the given name and value.\"\"\"\n    vars = kwargs.get(\"vars\")\n    vars[name] = value\n    return f\"Saved '{name}' var\"\n\n@mf.tool_config(inject_vars=True)\ndef get_var(name: str, **kwargs) -&gt; Union[str, List[str]]:\n    \"\"\"Get a variable with the given name.\"\"\"\n    vars = kwargs.get(\"vars\")\n    var = vars.get(name, None)\n    if var is None:\n        return f\"Variable not found: {name}\"\n    return var\n\n@mf.tool_config(inject_vars=True)\ndef get_vars(**kwargs):\n    \"\"\"Get all variables.\"\"\"\n    vars = kwargs.get(\"vars\")\n    return vars.copy() # always return a copy\n\n@mf.tool_config(inject_vars=True, return_direct=True)\ndef get_report(**kwargs) -&gt; str:\n    \"\"\"Return the report from user.\"\"\"\n    vars = kwargs.get(\"vars\")\n    report = f\"\"\"\n    Here is the current status of the report:\n    company_name: `{vars[\"company_name\"]}`\n    date: `{vars[\"date\"]}`\n    local: `{vars[\"local\"]}`\n    participants_internal: {vars[\"participants_internal\"]}\n    participants_external: {vars[\"participants_external\"]}\n    objective: `{vars[\"objective\"]}`\n    \"\"\"\n    objective = vars.get(\"objective\", None)\n    if objective is not None:\n        report += f\"objective: `{objective}`\"\n    detail = vars.get(\"detail\", None)\n    if detail is not None:\n        report += f\"detail: `{detail}`\"\n    main_points_discussed = vars.get(\"main_points_discussed\", None)\n    if main_points_discussed is not None:\n        report += f\"main_points_discussed: `{main_points_discussed}`\"\n    opportunities_identified = vars.get(\"opportunities_identified\", None)\n    if opportunities_identified is not None:\n        report += f\"opportunities_identified: `{opportunities_identified}`\"\n    next_steps = vars.get(\"next_steps\", None)\n    if next_steps is not None:\n        report += f\"next_steps: `{next_steps}`\"\n    report += \"Confirm the data to save?\"\n    return report\n\n@mf.tool_config(inject_vars=True)\ndef save(**kwargs) -&gt; str:\n    \"\"\"Save the report.\"\"\"\n    vars = kwargs.get(\"vars\")\n    with open(\"report.json\", \"w\") as f:\n        f.write(msgspec_dumps(vars))\n    return \"Report saved\"\n\n@mf.tool_config(inject_vars=True)\ndef check_company(name: str, **kwargs) -&gt; str:\n    \"\"\"Check if company name is correct\"\"\"\n    company_list = [ # mock\n        \"Globex Corporation\",\n        \"Initech Ltd.\",\n        \"Umbrella Industries\",\n        \"Stark Enterprises\",\n        \"Wayne Technologies\"\n    ]\n    name = name.strip()\n    bests = process.extract(name, company_list, scorer=fuzz.ratio, limit=4)\n\n    if bests and bests[0][1] == 100:\n        return f\"\u2714 Company found: '{bests[0][0]}' (exact match)\"\n\n    if bests and bests[0][1] &gt;= 75:\n        return f\"\u26a0 No exact match. Closest suggestion: '{bests[0][0]}' ({round(bests[0][1], 2)}%)\"\n\n    suggestions = \", \".join([f\"{b[0]} ({round(b[1], 2)}%)\" for b in bests])\n    return f\"\u274c Company not found. Suggestions: {suggestions}\"\n\ndef check_participants(\n    participants: List[str], known_participants: List[str]\n) -&gt; Dict[str, str]:\n    results = {}\n\n    for p in participants:\n        name = p.strip()\n        best_matches = process.extract(name, known_participants, scorer=fuzz.ratio, limit=4)\n\n        if best_matches and best_matches[0][1] == 100:\n            results[name] = f\"\u2714 Exact match: '{best_matches[0][0]}'\"\n        elif best_matches and best_matches[0][1] &gt;= 75:\n            results[name] = f\"\u26a0 No exact match. Closest: '{best_matches[0][0]}' ({round(best_matches[0][1], 2)}%)\"\n        else:\n            suggestions = \", \".join([f\"{m[0]} ({round(m[1], 2)}%)\" for m in best_matches])\n            results[name] = f\"\u274c Not found. Suggestions: {suggestions}\"\n\n    return results\n\n@mf.tool_config(inject_vars=True)\ndef check_internal_participants(participants: List[str], **kwargs) -&gt; str:\n    \"\"\"Check if internal participants are correct\"\"\"\n    known_participants = [ # mock\n        \"Michael Thompson\",\n        \"Sarah Connor\",\n        \"David Martinez\",\n        \"Emily Johnson\",\n        \"Robert Williams\"\n    ]\n    results = check_participants(participants, known_participants)\n    report = \"Internal participants:\\n\" + report\n    return report\n\n@mf.tool_config(inject_vars=True)\ndef check_external_participants(participants: List[str], **kwargs) -&gt; str:\n    \"\"\"Check if external participants are correct\"\"\"\n    known_participants = [ # mock\n        \"Anna Schmidt\",\n        \"Hiroshi Tanaka\",\n        \"Laura Rossi\",\n        \"Jean-Pierre Dupont\",\n        \"Carlos Fernandez\"\n    ]\n    results = check_participants(participants, known_participants)\n    report = \"\\n\".join(f\"{k}: {v}\" for k, v in results.items())\n    report = \"External participants:\\n\" + report\n    return report\n\nsystem_message = \"\"\"\nYou are a visitor report collection assistant.\nYour goal is to capture the fields we want to extract\nfrom their speech during a conversation with the user.\n\"\"\"\n\ninstructions = \"\"\"\nHere is the schema we want to get from the user report.\nReport:\n    company_name: str\n    date: str\n    local: str (city, address)\n    participants_internal: list[str]\n    participants_external: list[str]\n    objective: str\n    detail: Optional[str]\n    main_points_discussed: Optional[str] (bullet points with relevant topics)\n    opportunities_identified: Optional[str] (new business, improvements, mitigated risks)\n    next_steps: Optional[str]\n\nBefore saving the report, you must call the report summary tool.\nIf the user confirms that everything is correct, you must call the save tool.\nIf they request pre-editing, you must edit the field they requested.\nRemember that for participants and companies, you must first check the correct name.\n\nYou have access to several tools:\n\n* set_var: Use to save the parameters you found during the dialog.\n* get_var: Return, if applicable, the value of that variable.\n* get_vars: Return all var.\n* check_company: Use to check if the company name is correct. The tool will return the most similar ones if not. Use 'set_var' to save if correct.\n* check_internal_participants: Similar to check_company, checks if those internal participants exist in the database.\n* check_external_participants: Similar to check_company, checks if those internal participants exist in the database.\n* get_report: Returns the current report to the user in a formatted format.\n* save: Saves the report.\n\"\"\"\n\nextractor = nn.Agent(\"agent\", model, stream=True, verbose=True,\n         system_message=system_message, instructions=instructions,\n         tools=[\n             set_var, get_var, get_vars, check_company, check_internal_participants,\n             check_external_participants, get_report, save\n         ], return_model_state = True\n)\n\nfrom msgflux import cprint\n\nfrom msgflux.models.response import ModelStreamResponse\n\nchat_history = mf.ChatML()\n\nvars = {}\nwhile True:\n    user = input(\"Type something (or 'exit' to quit): \")\n    if user.lower() == \"exit\":\n        break\n\n    #cprint(f\"[USER]{user}\", ls=\"b\", lc=\"c\")\n    chat_history.add_user_message(user)\n\n    response = extractor(task_messages=chat_history.get_messages(), vars=vars)\n\n    if isinstance(response, ModelStreamResponse):\n        assistant = \"\"\n        cprint(\"[agent] \", end=\"\", flush=True, ls=\"b\", lc=\"br4\")\n        async for chunk in response.consume():\n            cprint(chunk, end=\"\", flush=True, ls=\"b\", lc=\"br4\")\n            assistant += chunk\n    elif isinstance(response, dict): # return direct response\n        assistant = response[\"tool_responses\"][\"tool_calls\"][0][\"result\"]\n        cprint(f\"[agent] {assistant}\", ls=\"b\", lc=\"br4\")\n    else:\n        assistant = response\n        cprint(f\"[agent] {assistant}\", ls=\"b\", lc=\"br4\")\n\n    chat_history.add_assist_message(assistant)\n\nvars\n\ntask = \"\"\"\nQuero registrar uma visita feita na empresa Globex Corporation.\nParticiparam o Michael Thompson e a Emily Johnson da nossa equipe.\nDo lado deles estavam a Anna Schmidt e o Carlos Fernandez.\n</code></pre> <p>Vars as Named Parameters</p> <p>Another possibility instead of passing Vars as a named parameter, you can pass exactly the name of the parameter that is in Vars.</p> <pre><code>@mf.tool_config(inject_vars=[\"api_key\"])\ndef upload(**kwargs) -&gt; str:\n    \"\"\"Upload user file to bucket\"\"\"\n    print(f\"my secret key {kwargs[\"api_key\"]}\")\n    return \"done\"\n\n\nagent = nn.Agent(\"agent\", model, tools=[upload], verbose=True)\nresponse = agent(\"please upload my csv to bucket\", vars={\"api_key\": \"key\"})\n</code></pre>"},{"location":"quickstart/#generation-schemas","title":"Generation Schemas","text":"<p>Generation schemas are guides on how the model should respond in a structured way.</p> <pre><code>from enum import Enum\nfrom typing import Optional\nfrom msgspec import Struct\n\nclass Category(str, Enum):\n    violence = \"violence\"\n    sexual = \"sexual\"\n    self_harm = \"self_harm\"\n\nclass ContentCompliance(Struct):\n    is_violating: bool\n    category: Optional[Category]\n    explanation_if_violating: Optional[str]\n\nsystem_message = \"\"\"\nDetermine if the user input violates specific guidelines and explain if they do.\n\"\"\"\n\nmoderation_agent = nn.Agent(\n    \"moderation\", model, generation_schema=ContentCompliance, system_message=system_message\n)\n\nresponse = moderation_agent(\"How do I prepare for a job interview?\")\n\nresponse\n</code></pre>"},{"location":"quickstart/#chainofthoughts","title":"ChainOfThoughts","text":"<p>Inserts a <code>reasoning</code> field before generating a final answer</p> <pre><code>from msgflux.generation.reasoning import ChainOfThought\n\ncot_agent = nn.Agent(\"cot\", model, generation_schema=ChainOfThought)\n\nresponse = cot_agent(\"how can I solve 8x + 7 = -23\")\n\nresponse\n</code></pre>"},{"location":"quickstart/#react","title":"ReAct","text":"<p>Inserts a <code>thought</code> before performing tool calling actions</p> <p><code>tool_choice</code> when used with ReAct is not guaranteed to be respected</p> <pre><code>import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape_website(url: str) -&gt; str:\n    \"\"\"Receives a URL and returns the page content.\"\"\"\n    try:\n        response = requests.get(url, verify=True)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        for tag in soup([\"script\", \"style\"]):\n            tag.extract()\n        text = soup.get_text(separator=\"\\n\")\n        clean_text = \"\\n\".join(line.strip() for line in text.splitlines() if line.strip())\n        return clean_text\n\n    except requests.exceptions.RequestException as e:\n        return f\"Error accessing {url}: {e}]\"\n\nfrom msgflux.generation.reasoning import ReAct\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4.1-mini\")\n\n# return_model_state=True to get the internal state of the agent during execution\nscraper_agent = nn.Agent(\n    \"scraper-agent\", model, tools=[scrape_website], return_model_state=True,\n    generation_schema=ReAct, verbose=True\n)\n\nsite = \"https://bbc.com\"\nresponse = scraper_agent(f\"Summarize the news on this website: {site}\")\n\nprint(response.model_state)\n\nresponse.model_response\n\n# the react-agent returns a dict with 'current_step' and 'final_answer'\n# if you want to keep only the 'final_answer', use 'response_template'\n\nresponse_template = \"\"\"{{final_answer}}\"\"\"\ntask_template = \"\"\"Summarize the news on this site: {{}}\"\"\"\n\nscraper_agent = nn.Agent(\n    \"scraper-agent\", model, tools=[scrape_website], response_template=response_template,\n    generation_schema=ReAct, verbose=True, task_template=task_template\n)\nresponse = scraper_agent(site)\n\nresponse\n</code></pre>"},{"location":"quickstart/#self-consistency","title":"Self Consistency","text":"<p>Generates multiple paths of reasoning then decides on the most frequent one</p> <pre><code>from msgflux.generation.reasoning import SelfConsistency\n\nsc_agent = nn.Agent(\"sc\", model, generation_schema=SelfConsistency)\n\ntask = \"\"\"\nIf John is twice as old as Mary and in 10 years their ages will add up to 50, how old is John today?\n\"\"\"\nresponse = sc_agent(task)\n\nresponse\n</code></pre>"},{"location":"quickstart/#response-template","title":"Response Template","text":"<p><code>response_template</code> is responsible for formatting the agent's response.</p> <p>This is useful if you need to add additional context to the response, or format it if it's structured.</p>"},{"location":"quickstart/#string-based-output","title":"String-based Output","text":"<p>Insert {} placeholders to insert model response</p> <p>We can use <code>response_template</code> with <code>vars</code> to add a greeting to the user without having to tell the template directly.</p> <pre><code>response_template = \"\"\"\n{% if user_name %}\nHi {{ user_name }},\n{% endif %}\n{}\n\"\"\"\nagent = nn.Agent(\"agent\", model, response_template=response_template)\nagent(\n    message={\"user_input\": \"Who was Nikola Tesla?\"},\n    vars={\"user_name\": \"Bruce Wayne\"}\n)\n</code></pre>"},{"location":"quickstart/#dict-based-outputs","title":"Dict-based Outputs","text":"<p>Insert Jinja blocks {{ field }} to insert model outputs</p> <pre><code>from msgspec import Struct\n\nfrom typing import Optional\n\nclass Output(Struct):\n    safe: bool\n    answer: Optional[str]\n\ninstructions = \"Only respond to the user if you consider the question safe.\"\nresponse_template = \"\"\"\n{% if safe %}\nHi! {{ answer }}\n{% else %}\nSorry but I can't answer you.\n{% endif %}\n\"\"\"\nagent = nn.Agent(\n    \"agent\", model, verbose=True, instructions=instructions,\n    generation_schema=Output, response_template=response_template\n)\nagent(message=\"Who was Nikola Tesla?\")\n</code></pre> <p>Let's simulate a user message by combining a structured extraction, <code>vars</code> and <code>response_template</code> to create a pre-formatted response to a customer.</p> <pre><code>task = \"\"\"\nHello, my name is John Cena and I work at EcoSupply Ltd., a company focused on the sustainable packaging sector.\nWe are facing some significant challenges, mainly high logistics costs and the need for ecological certifications to expand our market presence.\n\"\"\"\n\nfrom msgspec import Struct\n\nclass Output(Struct):\n    client_name: str\n    company_name: str\n    industry: str\n    pain_points: list[str]\n\nresponse_template = \"\"\"\nDear {{ client_name }},\n\nI understand that your company, {{ company_name}}, works in the field of {{ industry }}.\nWe also recognize that some of your main challenges are\n{%- for pain in pain_points %}\n {{ \"- \" + pain }}{% if not loop.last %},{% else %}.{% endif %}\n{%- endfor %}\n\nCurrently, you are relying on {{ current_solution }},\nbut we believe there\u2019s room for improvement.\n\nOur solution is designed to address these exact pain points,\nhelping companies like yours reduce costs and meet green compliance standards more efficiently.\n\nBest regards,\n{{ seller }}.\n\"\"\"\n\nvars = {\"seller\": \"Hal Jordan\"}\n\nsystem_message = \"You are an information extractor.\"\ninstructions = \"Your goal is to accurately extract information from the customer's message.\"\nagent = nn.Agent(\n    \"agent\", model, instructions=instructions, system_message=system_message,\n    generation_schema=Output, response_template=response_template, verbose=True\n)\nagent(task, vars=vars)\n</code></pre>"},{"location":"quickstart/#signature","title":"Signature","text":"<p><code>signature</code> is an innovation introduced by DSPy. Where you should focus on declaring the specifications of your task on how it should be performed.</p> <p>The signature format in string mode is <code>\"var: type -&gt; out_var: type\"</code></p> <p>If no <code>type</code> is passed, it will be assumed to be a string.</p> <p>The <code>\"-&gt;\"</code> flag separates inputs from outputs. For more than one parameter, separate them with <code>\",\"</code>.</p> <p>Behind the scenes, the outputs are transformed into a <code>generation_schema</code> to produce JSON output. The output examples also follow this formatting. If <code>typed_parser</code> is passed, the preference is to generate the output and examples based on it.</p>"},{"location":"quickstart/#translation-program","title":"Translation Program","text":"<p>Signatures allow a clear and objective description of the task to the agent.</p> <pre><code>agent = nn.Agent(\"translator\", model, signature=\"english -&gt; brazilian\")\n\nagent.state_dict()\n</code></pre> <p>A system prompt is automatically created describing inputs and what the outputs should be.</p> <pre><code>print(agent._get_system_prompt())\n</code></pre> <p>A <code>task_template</code> is also created, this means that now our inputs are named and need to be a dict.</p> <pre><code>print(agent.task_template)\n\nresponse = agent({\"english\": \"hello world\"})\nresponse\n</code></pre>"},{"location":"quickstart/#math-program-cot-powered","title":"Math Program CoT-powered","text":"<p>Let's create an agent focused on answering questions.</p> <pre><code>from msgflux.generation.reasoning import ChainOfThought\n\nphd_agent = nn.Agent(\n    \"phd\", model, signature=\"question -&gt; answer: float\",\n    generation_schema=ChainOfThought\n)\n\nphd_agent.state_dict()\n\nphd_agent.task_template\n\nmessage = {\"question\": \"Two dice are tossed. What is the probability that the sum equals two?\"}\nmodel_execution_params = phd_agent.inspect_model_execution_params(message)\n\nmodel_execution_params\n\nprint(model_execution_params.system_prompt)\n\nprint(model_execution_params.messages[0].content)\n\nresponse = phd_agent(message)\n</code></pre> <p>When combined with <code>ChainOfThought</code>, <code>ReAct</code>, or any other generation schema, the signature injects the desired final field into the <code>final_answer</code> field, in this case it is <code>answer</code>.</p> <pre><code>response\n</code></pre>"},{"location":"quickstart/#classifier-program","title":"Classifier Program","text":"<p>To provide more details about the task parameters, you can create class-based signatures.</p> <p>The class docstring is the instructions for the agent.</p> <p>Pass a optional <code>desc</code> for a additional description</p> <pre><code>from typing import Literal\n\nclass Classify(mf.Signature):\n    \"\"\"Classify sentiment of a given sentence.\"\"\" \n\n    sentence: str = mf.InputField()\n    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = mf.OutputField()\n    confidence: float = mf.OutputField(desc=\"[0,1]\")\n\nClassify.get_str_signature()\n\nClassify.get_instructions()\n\nClassify.get_inputs_info()\n\nClassify.get_outputs_info()\n\nClassify.get_output_descriptions()\n\nclassifier_agent = nn.Agent(\"classifier\", model, signature=Classify)\n\nprint(classifier_agent._get_system_prompt())\n\nprint(classifier_agent.task_template)\n\nclassifier_agent({\"sentence\": \"This book was super fun to read, though not the last chapter.\"})\n</code></pre>"},{"location":"quickstart/#image-classifer-program","title":"Image Classifer Program","text":"<p>Multimodal Language Models require an textual instruction.</p> <p>Therefore, when creating the <code>task_template</code> it will add an Image (or Audio) tag containing the input name</p> <pre><code>class ImageClassifier(mf.Signature):\n    photo: mf.Image = mf.InputField()\n    label: str = mf.OutputField()\n    confidence: float = mf.OutputField(desc=\"[0,1]\")\n\nImageClassifier.get_str_signature()\n\nimg_classifier = nn.Agent(\"img_classifier\", model, signature=ImageClassifier)\n\nprint(img_classifier._get_system_prompt())\n\nprint(img_classifier.task_template)\n\nimage_path = \"https://nwyarns.com/cdn/shop/articles/Llama_1024x1024.png\"\n\nresponse = img_classifier(task_multimodal_inputs={\"image\": image_path})\n\nresponse\n</code></pre>"},{"location":"quickstart/#guardrails","title":"Guardrails","text":"<p>Guardrails are security checkers for both model inputs and outputs</p> <p>A Guardrail can be any callable that receives a <code>data</code> and produces a dictionary containing a <code>safe</code> key, if safe is False, an exception is raised.</p> <pre><code>moderation_model = mf.Model.moderation(\"openai/omni-moderation-latest\")\n\nagent = nn.Agent(\n    \"safe_agent\", model,\n    input_guardrail=moderation_model, output_guardrail=moderation_model\n)\n\nagent(\"Can you teach me how to make a bomb?\")\n</code></pre>"},{"location":"quickstart/#model-gateway","title":"Model Gateway","text":"<p>When passing an object of type <code>ModelGateway</code> as <code>model</code> you can pass a 'model_preference' informing the <code>model_id</code> of the preferred model.</p> <pre><code>low_cost_model = mf.Model.chat_completion(\"openai/gpt-4.1-nano\")\nmid_cost_model = mf.Model.chat_completion(\"openai/gpt-4.1-mini\")\n\nmodel_gateway = mf.ModelGateway([low_cost_model, mid_cost_model])\n\nmodel_preference=\"gpt-4.1-nano\"\n\nagent = nn.Agent(\"agent\", model_gateway)\n\nresponse = agent(\"can you tell me a joke?\", model_preference=model_preference)\n\nresponse\n</code></pre>"},{"location":"quickstart/#prefilling_1","title":"Prefilling","text":"<pre><code>agent = nn.Agent(\"agent\", model)\nresponse = agent(\"What is the derivative of x^(2/3)/\", prefilling=\"Let's think step by step.\")\n</code></pre>"},{"location":"quickstart/#trancriber","title":"Trancriber","text":"<p><code>nn.Transcriber</code> is a module dedicated to converting speech to text. With it, you can apply granularity, instructions (prompt), use in stream mode, etc.</p> <pre><code>model = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ntranscriber = nn.Transcriber(\"transcriber\", model)\n\ntranscriber.state_dict()\n</code></pre>"},{"location":"quickstart/#toollibrary","title":"ToolLibrary","text":"<pre><code>#@mf.tool_config(inject_model_state=True)\ndef modify(valor: int, task_messages: dict):\n#def modify(valor: int) -&gt; str:\n    #vars = kwargs.get(\"vars\")\n    #vars[\"updated\"] = True\n    return \"done\"\n\n@mf.tool_config(inject_vars=True)\ndef modify(value: int, **kwargs):\n    vars = kwargs.get(\"vars\")\n    vars[\"updated\"] = True\n    return \"done\"\n\nvars = {\"num\": 2}\n\nlib = nn.ToolLibrary(\"lib\", [modify])\n\ntool_callings = [('123121', 'modify', {\"value\": 3})]\n\nr = lib(tool_callings=tool_callings, vars=vars)#, model_state={\"role\": \"user\"})\n\nvars\n\nr\n</code></pre> <p>task = \"A fintech startup offering digital wallets for small retailers.\"</p> <p>response = sales_agent(task)</p> <p>print(response) <pre><code>To add dynamism to the system prompt you can combine it with **vars**.\n\nInsert a Jinja placeholder in any system prompt component.\n\n```python\nsystem_extra_message=\"\"\"\nThe customer's name is {{costumer_name}}. Treat him politely.\n\"\"\"\n\nsales_agent = nn.Agent(\n    \"sales-agent\",\n    model,\n    system_message=system_message,\n    instructions=instructions,\n    expected_output=expected_output,\n    system_extra_message=system_extra_message\n)\n\nmodel_execution_params = sales_agent.inspect_model_execution_params(\n    task, vars={\"costumer_name\": \"Clark\"}\n)\n\nprint(model_execution_params.system_prompt)\n</code></pre></p>"},{"location":"quickstart/#examples","title":"Examples","text":"<p>There are three ways to pass examples to Agent</p>"},{"location":"quickstart/#based-on-string","title":"Based-on String","text":"<pre><code>examples=\"\"\"\nInput: \"A startup offering AI tools for logistics companies.\"\nOutput:\n- Identified Needs: Optimization of supply chain operations\n- Strategy: Highlight cost savings and automation\n- Value Proposition: Reduce operational delays through predictive analytics\n\"\"\"\n\nsales_agent = nn.Agent(\n    \"sales-agent\",\n    model,\n    system_message=system_message,\n    instructions=instructions,\n    expected_output=expected_output,\n    system_extra_message=system_extra_message,\n    include_date=True,\n    verbose=True,\n    examples=examples\n)\n\nprint(sales_agent._get_system_prompt())\n</code></pre>"},{"location":"quickstart/#based-on-example-cls","title":"Based-on Example cls","text":"<p>Examples are automatically formatted using xml tags</p> <p>Only inputs and labels are required</p> <pre><code>examples = [\n    mf.Example(\n        inputs=\"A fintech offering digital wallets for small retailers.\",\n        labels={\n            \"Identified Needs\": \"Payment integration and customer trust\",\n            \"Strategy\": \"Position product as secure and easy-to-use\",\n            \"Value Proposition\": \"Simplify digital payments for underserved markets\"\n        },\n        reasoning=\"Small retailers struggle with adoption of digital payments; focusing on trust and ease is key.\",\n        title=\"Fintech Lead Qualification\",\n        topic=\"Sales\"\n    ),\n    mf.Example(\n        inputs=\"An e-commerce platform specializing in handmade crafts.\",\n        labels={\n            \"Identified Needs\": \"Increase visibility and expand market reach\",\n            \"Strategy\": \"Suggest cross-promotion with eco-friendly marketplaces\",\n            \"Value Proposition\": \"Provide artisans with access to a global audience\"\n        },\n        reasoning=\"Handmade crafts have strong niche appeal; scaling depends on visibility and partnerships.\",\n        title=\"E-commerce Lead Qualification\",\n        topic=\"Sales\"\n    ),\n]\n\nsales_agent = nn.Agent(\n    \"sales-agent\",\n    model,\n    system_message=system_message,\n    instructions=instructions,\n    expected_output=expected_output,\n    system_extra_message=system_extra_message,\n    include_date=True,\n    verbose=True,\n    examples=examples\n)\n\nprint(sales_agent._get_system_prompt())\n</code></pre>"},{"location":"quickstart/#based-on-dict","title":"Based-on Dict","text":"<p>Dict-based examples are transformed into Example</p> <pre><code>examples = [\n    {\n        \"inputs\": \"A startup offering AI tools for logistics companies.\",\n        \"labels\": {\n            \"Identified Needs\": \"Optimization of supply chain operations\",\n            \"Strategy\": \"Highlight cost savings and automation\",\n            \"Value Proposition\": \"Reduce operational delays through predictive analytics\"\n        },\n    },\n    {\n        \"inputs\": \"An e-commerce platform specializing in handmade crafts.\",\n        \"labels\": {\n            \"Identified Needs\": \"Increase visibility and expand market reach\",\n            \"Strategy\": \"Suggest cross-promotion with eco-friendly marketplaces\",\n            \"Value Proposition\": \"Provide artisans with access to a global audience\"\n        },\n    },\n]\nsales_agent = nn.Agent(\n    \"sales-agent\",\n    model,\n    system_message=system_message,\n    instructions=instructions,\n    expected_output=expected_output,\n    system_extra_message=system_extra_message,\n    include_date=True,\n    verbose=True,\n    examples=examples\n)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"quickstart/#the-modelgateway-class","title":"The <code>ModelGateway</code> Class","text":"<p>The <code>ModelGateway</code> class is an orchestration layer over multiple models of the same type (e.g., multiple <code>chat_completion</code> models), allowing:</p> <ul> <li>\ud83d\udd01 Automatic fallback between models.</li> <li>\u23f1\ufe0f Time-based model availability constraints.</li> <li>\u2705 Model preference selection.</li> <li>\ud83d\udcc3 Control of execution attempts with exception handling.</li> <li>\ud83d\udd0e Consistent model typing validation.</li> </ul> <p>It's ideal for production-grade model orchestration where reliability and control over model usage are required.</p> <p>All you need is:</p> <ul> <li>All models must inherit from <code>BaseModel</code>.</li> <li>All models must be of the same <code>model_type</code>.</li> <li>At least 2 models must be provided.</li> </ul> <pre><code>from msgflux.models.base import BaseModel\nfrom msgflux.models.types import ChatCompletionModel\nfrom msgflux.models.response import ModelResponse\n\n\nclass ProviderAChatCompletion(BaseModel, ChatCompletionModel):\n    provider = \"provider_a\"\n\n    def __init__(self, model_id: str):\n        self._initialize(model_id)\n\n    def _initialize(self, model_id: str):\n        self.model_id = model_id\n\n    def __call__(self, **kwargs):\n        response = ModelResponse()\n        response.set_response_type(\"text_generation\")\n        response.add(\"Response from Provider A\")\n        return response\n\nclass ProviderBChatCompletion(BaseModel, ChatCompletionModel):\n    provider = \"provider_b\"\n\n    def __init__(self, model_id: str):\n        self._initialize(model_id)\n\n    def _initialize(self, model_id: str):\n        self.model_id = model_id\n\n    def __call__(self, **kwargs):\n        response = ModelResponse()\n        response.set_response_type(\"text_generation\")\n        response.add(\"Response from Provider B\")\n        return response\n\n# Simulate a model that fails\nclass ProviderFailureModel(BaseModel, ChatCompletionModel):\n    provider = \"provider_failure\"\n\n    def __init__(self, model_id: str):\n        self._initialize(model_id)\n\n    def _initialize(self, model_id: str):\n        self.model_id = model_id\n\n    def __call__(self, **kwargs):\n        raise RuntimeError(\"Simulate failure\")\n\nprovider_a = ProviderAChatCompletion(\"gork-3\")\nprovider_b = ProviderBChatCompletion(\"behemoth\")\nprovider_failure = ProviderFailureModel(\"breaker\")\n\ngateway_broken = mf.ModelGateway([provider_a, provider_b, provider_failure])\nresponse = gateway_broken(message=\"Who were Warren McCulloch and Walter Pitts?\")\nprint(response.consume())\n\n# pass a preference model\ngateway_broken = mf.ModelGateway([provider_a, provider_b, provider_failure])\nresponse = gateway_broken(message=\"Who were Warren McCulloch and Walter Pitts?\",\n                          model_preference=\"behemoth\")\nprint(response.consume())\n\n# simulate a failure\ngateway_broken = mf.ModelGateway([provider_failure, provider_a, provider_b])\nresponse = gateway_broken(message=\"Who were Warren McCulloch and Walter Pitts?\",\n                          model_preference=\"breaker\")\nprint(response.consume())\n\n# time_constraints {'model-A': [('22:00', '06:00')]}\ngateway_broken = mf.ModelGateway([provider_a, provider_b], time_constraints)\nresponse = gateway_broken(message=\"Who were Warren McCulloch and Walter Pitts?\",\n                          model_preference=\"breaker\")\nprint(response.consume())\n\ngateway_broken.get_available_models()\n\ngateway_broken.get_model_info()\n</code></pre>"},{"location":"_includes/init_chat_completion_model/","title":"Init chat completion model","text":"<p>Init Model (<code>Check Dependencies</code>)</p> OpenAIOpenRouterSambaNovavLLMMore \u25bc <p>Configure your access key <code>OPENAI_API_KEY</code> <pre><code>mf.set_envs(OPENAI_API_KEY=\"&lt;&gt;\")\nmodel = mf.Model.chat_completion(\"openai/gpt-4.1-nano\", temperature=0.7\")\n</code></pre></p> <p>Configure your access key <code>OPENROUTER_API_KEY</code> <pre><code>mf.set_envs(OPENROUTER_API_KEY=\"&lt;&gt;\")\nmodel = mf.Model.chat_completion(\"openrouter/deepseek/deepseek-r1-distill-qwen-7b\")\n</code></pre></p> <p>Configure your access key <code>SAMBANOVA_API_KEY</code> <pre><code>mf.set_envs(SAMBANOVA_API_KEY=\"&lt;&gt;\")\nmodel = mf.Model.chat_completion(\"Llama-4-Maverick-17B-128E-Instruct\")\n</code></pre></p> <p>Install vLLM (optional) <pre><code>pip install uv # for fast package install\nuv pip install vllm --torch-backend=auto\n</code></pre> Start vLLM server <pre><code>vllm serve Qwen/Qwen2.5-1.5B-Instruct \n</code></pre> Configure your URL <code>VLLM_BASE_URL</code> <pre><code>base_url = \"http://localhost:8000/v1\"\n# mf.set_envs(VLLM_BASE_URL=base_url) # or pass as a `base_url` param\nmodel = mf.Model.chat_completion(\"Qwen/Qwen2.5-1.5B-Instruct\", base_url=base_url)\n</code></pre></p> <p>Configure your access key <code>OPENROUTER_API_KEY</code> <pre><code>mf.set_envs(OPENROUTER_API_KEY=\"&lt;&gt;\")\nmodel = mf.Model.chat_completion(\"openrouter/deepseek/deepseek-r1-distill-qwen-7b\")\n</code></pre></p> OI <p>Configure your access key <code>OPENROUTER_API_KEY</code> <pre><code>mf.set_envs(OPENROUTER_API_KEY=\"&lt;&gt;\")\nmodel = mf.Model.chat_completion(\"openrouter/deepseek/deepseek-r1-distill-qwen-7b\")\n</code></pre></p> SOM <p>Configure your access key <code>OPENROUTER_API_KEY</code> <pre><code>mf.set_envs(OPENROUTER_API_KEY=\"&lt;&gt;\")\nmodel = mf.Model.chat_completion(\"openrouter/deepseek/deepseek-r1-distill-qwen-7b\")\n</code></pre></p>"},{"location":"api-reference/databases/types/kv/","title":"KV","text":""},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.cachetools.CacheToolsKVDB","title":"CacheToolsKVDB","text":"<p>               Bases: <code>BaseKV</code>, <code>BaseDB</code>, <code>KVDB</code></p> <p>CacheTools Key-Value DB.</p> Source code in <code>src/msgflux/data/dbs/providers/cachetools.py</code> <pre><code>@register_db\nclass CacheToolsKVDB(BaseKV, BaseDB, KVDB):\n    \"\"\"CacheTools Key-Value DB.\"\"\"\n\n    provider = \"cachetools\"\n\n    def __init__(\n        self,\n        *,\n        ttl: Optional[int] = 3600,\n        maxsize: Optional[int] = 10000,\n        hash_key: Optional[bool] = True,\n    ):\n        \"\"\"Args:\n        ttl:\n            The time-to-live (TTL) for each cache entry in seconds.\n        maxsize:\n            The maximum number of items the cache can store.\n        hash_key:\n            Whether to hash the keys before storing them in the cache.\n        \"\"\"\n        if TTLCache is None:\n            raise ImportError(\n                \"`cachetools` client is not available. Install with \"\n                \"`pip install cachetools`\"\n            )        \n        self.hash_key = hash_key\n        self.maxsize = maxsize\n        self.ttl = ttl\n        self._initialize()\n\n    def _initialize(self):\n        self.client = TTLCache(maxsize=self.maxsize, ttl=self.ttl)\n\n    def add(self, documents: Union[List[Dict[str, Any]], Dict[str, Any]]):\n        if not isinstance(documents, list):\n            documents = [documents]\n        for document in documents:\n            for k, v in document.items():\n                if self.hash_key:\n                    k = convert_str_to_hash(k)\n                v = msgspec.msgpack.encode(v)\n                self.client[k] = v\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.cachetools.CacheToolsKVDB.hash_key","title":"hash_key  <code>instance-attribute</code>","text":"<pre><code>hash_key = hash_key\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.cachetools.CacheToolsKVDB.maxsize","title":"maxsize  <code>instance-attribute</code>","text":"<pre><code>maxsize = maxsize\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.cachetools.CacheToolsKVDB.provider","title":"provider  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>provider = 'cachetools'\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.cachetools.CacheToolsKVDB.ttl","title":"ttl  <code>instance-attribute</code>","text":"<pre><code>ttl = ttl\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.cachetools.CacheToolsKVDB.__init__","title":"__init__","text":"<pre><code>__init__(*, ttl=3600, maxsize=10000, hash_key=True)\n</code></pre> <p>ttl:     The time-to-live (TTL) for each cache entry in seconds. maxsize:     The maximum number of items the cache can store. hash_key:     Whether to hash the keys before storing them in the cache.</p> Source code in <code>src/msgflux/data/dbs/providers/cachetools.py</code> <pre><code>def __init__(\n    self,\n    *,\n    ttl: Optional[int] = 3600,\n    maxsize: Optional[int] = 10000,\n    hash_key: Optional[bool] = True,\n):\n    \"\"\"Args:\n    ttl:\n        The time-to-live (TTL) for each cache entry in seconds.\n    maxsize:\n        The maximum number of items the cache can store.\n    hash_key:\n        Whether to hash the keys before storing them in the cache.\n    \"\"\"\n    if TTLCache is None:\n        raise ImportError(\n            \"`cachetools` client is not available. Install with \"\n            \"`pip install cachetools`\"\n        )        \n    self.hash_key = hash_key\n    self.maxsize = maxsize\n    self.ttl = ttl\n    self._initialize()\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.cachetools.CacheToolsKVDB.add","title":"add","text":"<pre><code>add(documents)\n</code></pre> Source code in <code>src/msgflux/data/dbs/providers/cachetools.py</code> <pre><code>def add(self, documents: Union[List[Dict[str, Any]], Dict[str, Any]]):\n    if not isinstance(documents, list):\n        documents = [documents]\n    for document in documents:\n        for k, v in document.items():\n            if self.hash_key:\n                k = convert_str_to_hash(k)\n            v = msgspec.msgpack.encode(v)\n            self.client[k] = v\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.diskcache.DiskCacheKVDB","title":"DiskCacheKVDB","text":"<p>               Bases: <code>BaseKV</code>, <code>BaseDB</code>, <code>KVDB</code></p> <p>DiskCache Key-Value DB.</p> Source code in <code>src/msgflux/data/dbs/providers/diskcache.py</code> <pre><code>@register_db\nclass DiskCacheKVDB(BaseKV, BaseDB, KVDB):\n    \"\"\"DiskCache Key-Value DB.\"\"\"\n\n    provider = \"diskcache\"\n\n    def __init__(self, *, ttl: Optional[int] = 3600, hash_key: Optional[bool] = True):\n        \"\"\"Args:\n        ttl:\n            The time-to-live (TTL) for each cache entry in seconds.\n        hash_key:\n            Whether to hash the keys before storing them in the cache.\n        \"\"\"\n        if Cache is None:\n            raise ImportError(\n                \"`diskcache` client is not available. Install with \"\n                \"`pip install diskcache`\"\n            )\n        self.hash_key = hash_key\n        self.ttl = ttl\n        self._initialize()\n\n    def _initialize(self):\n        self.client = Cache(timeout=1)\n\n    def add(self, documents: Union[List[Dict[str, Any]], Dict[str, Any]]):\n        if not isinstance(documents, list):\n            documents = [documents]\n        for document in documents:\n            for k, v in document.items():\n                if self.hash_key:\n                    k = convert_str_to_hash(k)\n                v = msgspec.msgpack.encode(v)\n                self.client.set(k, v, expire=self.ttl)\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.diskcache.DiskCacheKVDB.hash_key","title":"hash_key  <code>instance-attribute</code>","text":"<pre><code>hash_key = hash_key\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.diskcache.DiskCacheKVDB.provider","title":"provider  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>provider = 'diskcache'\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.diskcache.DiskCacheKVDB.ttl","title":"ttl  <code>instance-attribute</code>","text":"<pre><code>ttl = ttl\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.diskcache.DiskCacheKVDB.__init__","title":"__init__","text":"<pre><code>__init__(*, ttl=3600, hash_key=True)\n</code></pre> <p>ttl:     The time-to-live (TTL) for each cache entry in seconds. hash_key:     Whether to hash the keys before storing them in the cache.</p> Source code in <code>src/msgflux/data/dbs/providers/diskcache.py</code> <pre><code>def __init__(self, *, ttl: Optional[int] = 3600, hash_key: Optional[bool] = True):\n    \"\"\"Args:\n    ttl:\n        The time-to-live (TTL) for each cache entry in seconds.\n    hash_key:\n        Whether to hash the keys before storing them in the cache.\n    \"\"\"\n    if Cache is None:\n        raise ImportError(\n            \"`diskcache` client is not available. Install with \"\n            \"`pip install diskcache`\"\n        )\n    self.hash_key = hash_key\n    self.ttl = ttl\n    self._initialize()\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.diskcache.DiskCacheKVDB.add","title":"add","text":"<pre><code>add(documents)\n</code></pre> Source code in <code>src/msgflux/data/dbs/providers/diskcache.py</code> <pre><code>def add(self, documents: Union[List[Dict[str, Any]], Dict[str, Any]]):\n    if not isinstance(documents, list):\n        documents = [documents]\n    for document in documents:\n        for k, v in document.items():\n            if self.hash_key:\n                k = convert_str_to_hash(k)\n            v = msgspec.msgpack.encode(v)\n            self.client.set(k, v, expire=self.ttl)\n</code></pre>"},{"location":"api-reference/models/model/","title":"Model","text":""},{"location":"api-reference/models/model/#msgflux.models.model.Model","title":"Model","text":"Source code in <code>src/msgflux/models/model.py</code> <pre><code>class Model:\n\n    @classmethod\n    def providers(cls):\n        return {k: list(v.keys()) for k, v in model_registry.items()}\n\n    @classmethod\n    def model_types(cls):\n        return list(model_registry.keys())\n\n    @classmethod\n    def _model_path_parser(cls, model_id: str) -&gt; tuple[str, str]:\n        provider, model_id = model_id.split(\"/\", 1)\n        return provider, model_id\n\n    @classmethod\n    def _get_model_class(cls, model_type: str, provider: str) -&gt; Type[BaseModel]:\n        if model_type not in model_registry:\n            raise ValueError(f\"Model type `{model_type}` is not supported\")\n        if provider not in model_registry[model_type]:\n            raise ValueError(\n                f\"Provider `{provider}` not registered for type `{model_type}`\"\n            )\n        model_cls = model_registry[model_type][provider]\n        return model_cls\n\n    @classmethod\n    def _create_model(\n        cls, model_type: str, model_path: str, **kwargs\n    ) -&gt; Type[BaseModel]:\n        provider, model_id = cls._model_path_parser(model_path)\n        model_cls = cls._get_model_class(model_type, provider)\n        return model_cls(model_id=model_id, **kwargs)\n\n    @classmethod\n    def from_serialized(\n        cls, provider: str, model_type: str, state: Mapping[str, Any]\n    ) -&gt; Type[BaseModel]:\n        \"\"\"Creates a model instance from serialized parameters.\n\n        Args:\n            provider:\n                The model provider (e.g., \"openai\", \"google\").\n            model_type:\n                The type of model (e.g., \"chat_completion\", \"text_embedder\").\n            state:\n                Dictionary containing the serialized model parameters.\n\n        Returns:\n            An instance of the appropriate model class with restored state\n        \"\"\"\n        model_cls = cls._get_model_class(model_type, provider)\n        # Create instance without calling __init__\n        instance = object.__new__(model_cls)\n        # Restore the instance state\n        instance.from_serialized(state)\n        return instance\n\n    @classmethod\n    def chat_completion(cls, model_path: str, **kwargs) -&gt; ChatCompletionModel:\n        return cls._create_model(\"chat_completion\", model_path, **kwargs)\n\n    @classmethod\n    def image_classifier(cls, model_path: str, **kwargs) -&gt; ImageClassifierModel:\n        return cls._create_model(\"image_classifier\", model_path, **kwargs)\n\n    @classmethod\n    def image_embedder(cls, model_path: str, **kwargs) -&gt; ImageEmbedderModel:\n        return cls._create_model(\"image_embedder\", model_path, **kwargs)\n\n    @classmethod\n    def image_text_to_image(cls, model_path: str, **kwargs) -&gt; ImageTextToImageModel:\n        return cls._create_model(\"image_text_to_image\", model_path, **kwargs)\n\n    @classmethod\n    def moderation(cls, model_path: str, **kwargs) -&gt; ModerationModel:\n        return cls._create_model(\"moderation\", model_path, **kwargs)\n\n    @classmethod\n    def speech_to_text(cls, model_path: str, **kwargs) -&gt; SpeechToTextModel:\n        return cls._create_model(\"speech_to_text\", model_path, **kwargs)\n\n    @classmethod\n    def text_classifier(cls, model_path: str, **kwargs) -&gt; TextClassifierModel:\n        return cls._create_model(\"text_classifier\", model_path, **kwargs)\n\n    @classmethod\n    def text_embedder(cls, model_path: str, **kwargs) -&gt; TextEmbedderModel:\n        return cls._create_model(\"text_embedder\", model_path, **kwargs)\n\n    @classmethod\n    def text_reranker(cls, model_path: str, **kwargs) -&gt; TextRerankerModel:\n        return cls._create_model(\"text_reranker\", model_path, **kwargs)\n\n    @classmethod\n    def text_to_image(cls, model_path: str, **kwargs) -&gt; TextToImageModel:\n        return cls._create_model(\"text_to_image\", model_path, **kwargs)\n\n    @classmethod\n    def text_to_speech(cls, model_path: str, **kwargs) -&gt; TextToSpeechModel:\n        return cls._create_model(\"text_to_speech\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.chat_completion","title":"chat_completion  <code>classmethod</code>","text":"<pre><code>chat_completion(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef chat_completion(cls, model_path: str, **kwargs) -&gt; ChatCompletionModel:\n    return cls._create_model(\"chat_completion\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.from_serialized","title":"from_serialized  <code>classmethod</code>","text":"<pre><code>from_serialized(provider, model_type, state)\n</code></pre> <p>Creates a model instance from serialized parameters.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>The model provider (e.g., \"openai\", \"google\").</p> required <code>model_type</code> <code>str</code> <p>The type of model (e.g., \"chat_completion\", \"text_embedder\").</p> required <code>state</code> <code>Mapping[str, Any]</code> <p>Dictionary containing the serialized model parameters.</p> required <p>Returns:</p> Type Description <code>Type[BaseModel]</code> <p>An instance of the appropriate model class with restored state</p> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef from_serialized(\n    cls, provider: str, model_type: str, state: Mapping[str, Any]\n) -&gt; Type[BaseModel]:\n    \"\"\"Creates a model instance from serialized parameters.\n\n    Args:\n        provider:\n            The model provider (e.g., \"openai\", \"google\").\n        model_type:\n            The type of model (e.g., \"chat_completion\", \"text_embedder\").\n        state:\n            Dictionary containing the serialized model parameters.\n\n    Returns:\n        An instance of the appropriate model class with restored state\n    \"\"\"\n    model_cls = cls._get_model_class(model_type, provider)\n    # Create instance without calling __init__\n    instance = object.__new__(model_cls)\n    # Restore the instance state\n    instance.from_serialized(state)\n    return instance\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.image_classifier","title":"image_classifier  <code>classmethod</code>","text":"<pre><code>image_classifier(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef image_classifier(cls, model_path: str, **kwargs) -&gt; ImageClassifierModel:\n    return cls._create_model(\"image_classifier\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.image_embedder","title":"image_embedder  <code>classmethod</code>","text":"<pre><code>image_embedder(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef image_embedder(cls, model_path: str, **kwargs) -&gt; ImageEmbedderModel:\n    return cls._create_model(\"image_embedder\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.image_text_to_image","title":"image_text_to_image  <code>classmethod</code>","text":"<pre><code>image_text_to_image(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef image_text_to_image(cls, model_path: str, **kwargs) -&gt; ImageTextToImageModel:\n    return cls._create_model(\"image_text_to_image\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.model_types","title":"model_types  <code>classmethod</code>","text":"<pre><code>model_types()\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef model_types(cls):\n    return list(model_registry.keys())\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.moderation","title":"moderation  <code>classmethod</code>","text":"<pre><code>moderation(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef moderation(cls, model_path: str, **kwargs) -&gt; ModerationModel:\n    return cls._create_model(\"moderation\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.providers","title":"providers  <code>classmethod</code>","text":"<pre><code>providers()\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef providers(cls):\n    return {k: list(v.keys()) for k, v in model_registry.items()}\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.speech_to_text","title":"speech_to_text  <code>classmethod</code>","text":"<pre><code>speech_to_text(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef speech_to_text(cls, model_path: str, **kwargs) -&gt; SpeechToTextModel:\n    return cls._create_model(\"speech_to_text\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.text_classifier","title":"text_classifier  <code>classmethod</code>","text":"<pre><code>text_classifier(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef text_classifier(cls, model_path: str, **kwargs) -&gt; TextClassifierModel:\n    return cls._create_model(\"text_classifier\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.text_embedder","title":"text_embedder  <code>classmethod</code>","text":"<pre><code>text_embedder(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef text_embedder(cls, model_path: str, **kwargs) -&gt; TextEmbedderModel:\n    return cls._create_model(\"text_embedder\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.text_reranker","title":"text_reranker  <code>classmethod</code>","text":"<pre><code>text_reranker(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef text_reranker(cls, model_path: str, **kwargs) -&gt; TextRerankerModel:\n    return cls._create_model(\"text_reranker\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.text_to_image","title":"text_to_image  <code>classmethod</code>","text":"<pre><code>text_to_image(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef text_to_image(cls, model_path: str, **kwargs) -&gt; TextToImageModel:\n    return cls._create_model(\"text_to_image\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.text_to_speech","title":"text_to_speech  <code>classmethod</code>","text":"<pre><code>text_to_speech(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef text_to_speech(cls, model_path: str, **kwargs) -&gt; TextToSpeechModel:\n    return cls._create_model(\"text_to_speech\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model_gateway/","title":"Model gateway","text":""},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway","title":"ModelGateway","text":"<p>Routes calls to a list of supported AI models, with fallback, retries, initial model selection, and timing constraints (configured via HH:MM strings).</p> Source code in <code>src/msgflux/models/gateway.py</code> <pre><code>class ModelGateway:\n    \"\"\"Routes calls to a list of supported AI models, with fallback, retries,\n    initial model selection, and timing constraints (configured via HH:MM strings).\n    \"\"\"\n\n    msgflux_type = \"model_gateway\"\n    model_types = None\n\n    def __init__(\n        self,\n        models: List[BaseModel],\n        time_constraints: Optional[Dict[str, List[Tuple[str, str]]]] = None,\n    ):\n        \"\"\"Args:\n            models:\n                A list of BaseModel instances (at least 2).\n            time_constraints:\n                An optional dictionary mapping model_id to a list of string\n                tuples (start_time, end_time). The listed models will NOT be\n                used if the current time is within any of the specified ranges.\n                Strings must be in the format \"HH:MM\" (e.g. \"22:00\", \"06:00\").\n\n                !!! example:\n\n                    {'model-A': [('22:00', '06:00')]}\n                    prohibits 'model-A' between 22:00 and 06:00.\n\n        Raises:\n            ModelRouterError:\n                Raised when all models fail or are restricted.\n            ValueError:\n                Raised for misconfiguration in time formats or duplicate model IDs.\n            TypeError:\n                Raised for invalid argument types.\n        \"\"\"\n        self._model_id_to_index: Dict[str, int] = {}\n        self.raw_time_constraints = time_constraints\n        self._set_models(models)\n\n        try:\n            self.parsed_time_constraints = (\n                self._parse_time_constraints(time_constraints)\n                if time_constraints\n                else {}\n            )\n        except ValueError as e:\n            logger.error(f\"Error to parse time_constraints: {e}\")\n            raise ValueError(f\"Invalid format in time_constraints: {e}\") from e\n\n        # Validates if the model_ids in time_constraints exist\n        # (uses the keys from the parsed dict)\n        for model_id in self.parsed_time_constraints:\n            if model_id not in self._model_id_to_index:\n                logger.warning(\n                    f\"The model_id `{model_id}` in time constraints \"\n                    \"not found in the provided models\"\n                )\n\n        self.current_model_index = 0\n        logger.debug(\n            f\"ModelGateway initialized with {len(self.models)} models. Type: \"\n            f\"`{self.model_type}`\"\n        )\n        if self.parsed_time_constraints:\n            logger.debug(\n                \"Time constraints applied to models: \"\n                f\"{list(self.parsed_time_constraints.keys())}\"\n            )\n\n    def _parse_time_constraints(\n        self, constraints: Optional[Dict[str, List[Tuple[str, str]]]] = None\n    ) -&gt; Dict[str, List[Tuple[time, time]]]:\n        \"\"\"Validates and converts \"HH:MM\" time strings into datetime.time objects.\n\n        Raises:\n            ValueError: If a time string is in an invalid format.\n            TypeError: If the constraint data structure is incorrect.\n        \"\"\"\n        if constraints is None:\n            return {}\n\n        parsed_constraints: Dict[str, List[Tuple[time, time]]] = {}\n        time_format = \"%H:%M\"\n\n        for model_id, intervals in constraints.items():\n            if not isinstance(intervals, list):\n                raise TypeError(\n                    f\"Constraints for `{model_id}` must be a list of tuples \"\n                    f\"(start, end). Given: `{type(intervals)}`\"\n                )\n            parsed_intervals = []\n            for i, interval in enumerate(intervals):\n                if (\n                    not isinstance(interval, (tuple, list)) or len(interval) != 2\n                ):  # Tuples or lists\n                    raise TypeError(\n                        f\"Interval #{i + 1} for `{model_id}` must be a tuple/list \"\n                        \"of two strings (start_time_str, end_time_str). Given: \"\n                        f\"`{interval}`\"\n                    )\n\n                start, end = interval\n                if not isinstance(start, str) or not isinstance(end, str):\n                    raise TypeError(\n                        f\"Start and end times in range #{i + 1} for `{model_id}` \"\n                        f\"must be strings. Given: `({type(start)}, {type(end)})`\"\n                    )\n\n                try:\n                    start_dt = datetime.strptime(start, time_format).replace(\n                        tzinfo=timezone.utc\n                    )\n                    end_dt = datetime.strptime(end, time_format).replace(\n                        tzinfo=timezone.utc\n                    )\n                    start_t = start_dt.time()\n                    end_t = end_dt.time()\n                    parsed_intervals.append((start_t, end_t))\n                except ValueError as e:\n                    raise ValueError(\n                        f\"Invalid time format in range #{i + 1} for `{model_id}`. \"\n                        f\"Use 'HH:MM'. Error parsing `{start}` or `{end}`: {e}\"\n                    ) from e\n\n            parsed_constraints[model_id] = parsed_intervals\n        return parsed_constraints\n\n    def _is_time_restricted(self, model_id: str) -&gt; bool:\n        \"\"\"Checks if the model is constrained at the current time\n        using the parsed constraints.\n        \"\"\"\n        # Access constraints already converted to `time`\n        if model_id not in self.parsed_time_constraints:\n            return False\n\n        now = datetime.now(tz=timezone.utc).time()\n\n        for start_time, end_time in self.parsed_time_constraints[model_id]:\n            if start_time &lt;= end_time:\n                if start_time &lt;= now &lt;= end_time:\n                    logger.debug(\n                        f\"Model `{model_id}` restricted. Current time `{now}` \"\n                        f\"is between `{start_time}` and `{end_time}`\"\n                    )\n                    return True\n            elif now &gt;= start_time or now &lt;= end_time:\n                logger.debug(\n                    f\"Restricted model `{model_id}`. Current time `{now}` is \"\n                    f\"in the range crosses midnight: `{start_time} - {end_time}`\"\n                )\n                return True\n        return False\n\n    def _set_models(self, models: List[BaseModel]):\n        if not models or not isinstance(models, list):\n            raise TypeError(\n                \"`models` must be a non-empty list of `BaseModel` instances\"\n            )\n\n        if not all(isinstance(model, BaseModel) for model in models):\n            raise TypeError(\"`models` requires inheriting from `BaseModel`\")\n\n        if len(models) &lt; 2:\n            logger.warning(\n                f\"`models` has only {len(models)} models. \"\n                \"Fallback will not be effective\"\n            )\n\n        model_types = set()\n        model_ids = set()\n        for i, model in enumerate(models):\n            if not hasattr(model, \"model_type\") or not model.model_type:\n                raise AttributeError(\n                    f\"Model in {i} position does not have a valid \"\n                    \"`model_type` attribute\"\n                )\n            if not hasattr(model, \"model_id\") or not model.model_id:\n                raise AttributeError(\n                    f\"Model in {i} position  does not have a valid `model_id` attribute\"\n                )\n            if not hasattr(model, \"provider\"):\n                raise AttributeError(\n                    f\"Model `{model.model_id}` does not have a valid \"\n                    \"`provider` attribute\"\n                )\n\n            model_types.add(model.model_type)\n            if model.model_id in model_ids:\n                raise ValueError(\n                    f\"Duplicate model ID found: `{model.model_id}`. IDs must be unique\"\n                )\n            model_ids.add(model.model_id)\n            self._model_id_to_index[model.model_id] = i\n\n        if len(model_types) &gt; 1:\n            raise TypeError(\n                \"All models in `models` must be of the same `model_type`. \"\n                f\"Given: `{model_types}`\"\n            )\n\n        self.models = models\n        self.model_type = next(iter(model_types))\n\n        # Determine if gateway supports batch processing\n        # Only True if ALL models support batch\n        self.batch_support = all(getattr(model, \"batch_support\", False) for model in models) if models else False\n\n    def _execute_model(\n        self, model_preference: Optional[str] = None, **kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Attempts to execute the call on the configured models, respecting\n        time constraints and failure limits.\n        \"\"\"\n        if not self.models:\n            raise ModelRouterError([], [], message=\"No model configured on gateway\")\n\n        available_models = [\n            model\n            for model in self.models\n            if not self._is_time_restricted(model.model_id)\n        ]\n\n        if not available_models:\n            raise ModelRouterError(\n                [], [], message=\"No model available due to time constraints\"\n            )\n\n        if model_preference:\n            preferred_model = next(\n                (m for m in available_models if m.model_id == model_preference), None\n            )\n            if preferred_model:\n                available_models = [preferred_model] + [\n                    m for m in available_models if m != preferred_model\n                ]\n\n        failures = []\n\n        for model in available_models:\n            try:\n                response = model(**kwargs)\n                return response\n            except Exception as e:\n                logger.debug(\n                    f\"\"\"Model `{model.model_id}` ({model.provider})\n                    failed to execute: {e}\"\"\",\n                    exc_info=False,\n                )\n                failures.append((model.model_id, model.provider, e))\n\n        error_message = f\"All {len(available_models)} available models failed\"\n        logger.error(error_message)\n        raise ModelRouterError(\n            [failure[2] for failure in failures], failures, message=error_message\n        )\n\n    async def _aexecute_model(\n        self, model_preference: Optional[str] = None, **kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Async version of _execute_model. Attempts to execute the call on the\n        configured models, respecting time constraints and failure limits.\n        \"\"\"\n        if not self.models:\n            raise ModelRouterError([], [], message=\"No model configured on gateway\")\n\n        available_models = [\n            model\n            for model in self.models\n            if not self._is_time_restricted(model.model_id)\n        ]\n\n        if not available_models:\n            raise ModelRouterError(\n                [], [], message=\"No model available due to time constraints\"\n            )\n\n        if model_preference:\n            preferred_model = next(\n                (m for m in available_models if m.model_id == model_preference), None\n            )\n            if preferred_model:\n                available_models = [preferred_model] + [\n                    m for m in available_models if m != preferred_model\n                ]\n\n        failures = []\n\n        for model in available_models:\n            try:\n                response = await model.acall(**kwargs)\n                return response\n            except Exception as e:\n                logger.debug(\n                    f\"\"\"Model `{model.model_id}` ({model.provider})\n                    failed to execute: {e}\"\"\",\n                    exc_info=False,\n                )\n                failures.append((model.model_id, model.provider, e))\n\n        error_message = f\"All {len(available_models)} available models failed\"\n        logger.error(error_message)\n        raise ModelRouterError(\n            [failure[2] for failure in failures], failures, message=error_message\n        )\n\n    def __call__(\n        self, *, model_preference: Optional[str] = None, **kwargs\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Executes the call on the gateway.\n\n        Args:\n            model_preference:\n                The ID of the model that should be tried first.\n                If None, starts from the last model used or the first one.\n            kwargs:\n                Arguments to pass to the __call__ method of the selected model.\n\n        Returns:\n            The response of the first model that executes successfully.\n\n        Raises:\n            ModelRouterError:\n                If all models fail consecutively up to the `max_retries`\n                limit, or if no models are available/functional.\n        \"\"\"\n        return self._execute_model(model_preference=model_preference, **kwargs)\n\n    async def acall(\n        self, *, model_preference: Optional[str] = None, **kwargs\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Async version of __call__. Executes the call on the gateway.\n\n        Args:\n            model_preference:\n                The ID of the model that should be tried first.\n                If None, starts from the last model used or the first one.\n            kwargs:\n                Arguments to pass to the acall method of the selected model.\n\n        Returns:\n            The response of the first model that executes successfully.\n\n        Raises:\n            ModelRouterError:\n                If all models fail consecutively up to the `max_retries`\n                limit, or if no models are available/functional.\n        \"\"\"\n        return await self._aexecute_model(model_preference=model_preference, **kwargs)\n\n    def serialize(self) -&gt; Dict[str, Any]:\n        \"\"\"Serializes the gateway state including time constraints as strings.\"\"\"\n        serialized_models = [model.serialize() for model in self.models]\n        state = {\n            \"time_constraints\": self.raw_time_constraints,\n            \"models\": serialized_models,\n        }\n        data = {\"msgflux_type\": self.msgflux_type, \"state\": state}\n        return data\n\n    @classmethod\n    def from_serialized(cls, data: Dict[str, Any]) -&gt; \"ModelGateway\":\n        \"\"\"Creates a ModelGateway instance from serialized data.\n\n        Args:\n            data: The dictionary of serialized models.\n        \"\"\"\n        if data.get(\"msgflux_type\") != cls.msgflux_type:\n            raise ValueError(\n                f\"Incorrect msgflux type. Expected `{cls.msgflux_type}`, \"\n                f\"given `{data.get('msgflux_type')}`\"\n            )\n\n        state = data.get(\"state\", {})\n        serialized_models = state.get(\"models\", [])\n        if not serialized_models:\n            raise ValueError(\"Serialized data does not contain templates\")\n\n        models = [Model.from_serialized(**m_data) for m_data in serialized_models]\n        time_constraints = state.get(\"time_constraints\")\n\n        return cls(models=models, time_constraints=time_constraints)\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.current_model_index","title":"current_model_index  <code>instance-attribute</code>","text":"<pre><code>current_model_index = 0\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.model_types","title":"model_types  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_types = None\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.msgflux_type","title":"msgflux_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>msgflux_type = 'model_gateway'\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.parsed_time_constraints","title":"parsed_time_constraints  <code>instance-attribute</code>","text":"<pre><code>parsed_time_constraints = (\n    _parse_time_constraints(time_constraints)\n    if time_constraints\n    else {}\n)\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.raw_time_constraints","title":"raw_time_constraints  <code>instance-attribute</code>","text":"<pre><code>raw_time_constraints = time_constraints\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.__call__","title":"__call__","text":"<pre><code>__call__(*, model_preference=None, **kwargs)\n</code></pre> <p>Executes the call on the gateway.</p> <p>Parameters:</p> Name Type Description Default <code>model_preference</code> <code>Optional[str]</code> <p>The ID of the model that should be tried first. If None, starts from the last model used or the first one.</p> <code>None</code> <code>kwargs</code> <p>Arguments to pass to the call method of the selected model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[ModelResponse, ModelStreamResponse]</code> <p>The response of the first model that executes successfully.</p> <p>Raises:</p> Type Description <code>ModelRouterError</code> <p>If all models fail consecutively up to the <code>max_retries</code> limit, or if no models are available/functional.</p> Source code in <code>src/msgflux/models/gateway.py</code> <pre><code>def __call__(\n    self, *, model_preference: Optional[str] = None, **kwargs\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Executes the call on the gateway.\n\n    Args:\n        model_preference:\n            The ID of the model that should be tried first.\n            If None, starts from the last model used or the first one.\n        kwargs:\n            Arguments to pass to the __call__ method of the selected model.\n\n    Returns:\n        The response of the first model that executes successfully.\n\n    Raises:\n        ModelRouterError:\n            If all models fail consecutively up to the `max_retries`\n            limit, or if no models are available/functional.\n    \"\"\"\n    return self._execute_model(model_preference=model_preference, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.__init__","title":"__init__","text":"<pre><code>__init__(models, time_constraints=None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>List[BaseModel]</code> <p>A list of BaseModel instances (at least 2).</p> required <code>time_constraints</code> <code>Optional[Dict[str, List[Tuple[str, str]]]]</code> <p>An optional dictionary mapping model_id to a list of string tuples (start_time, end_time). The listed models will NOT be used if the current time is within any of the specified ranges. Strings must be in the format \"HH:MM\" (e.g. \"22:00\", \"06:00\").</p> <p>!!! example:</p> <pre><code>{'model-A': [('22:00', '06:00')]}\nprohibits 'model-A' between 22:00 and 06:00.\n</code></pre> <code>None</code> <p>Raises:</p> Type Description <code>ModelRouterError</code> <p>Raised when all models fail or are restricted.</p> <code>ValueError</code> <p>Raised for misconfiguration in time formats or duplicate model IDs.</p> <code>TypeError</code> <p>Raised for invalid argument types.</p> Source code in <code>src/msgflux/models/gateway.py</code> <pre><code>def __init__(\n    self,\n    models: List[BaseModel],\n    time_constraints: Optional[Dict[str, List[Tuple[str, str]]]] = None,\n):\n    \"\"\"Args:\n        models:\n            A list of BaseModel instances (at least 2).\n        time_constraints:\n            An optional dictionary mapping model_id to a list of string\n            tuples (start_time, end_time). The listed models will NOT be\n            used if the current time is within any of the specified ranges.\n            Strings must be in the format \"HH:MM\" (e.g. \"22:00\", \"06:00\").\n\n            !!! example:\n\n                {'model-A': [('22:00', '06:00')]}\n                prohibits 'model-A' between 22:00 and 06:00.\n\n    Raises:\n        ModelRouterError:\n            Raised when all models fail or are restricted.\n        ValueError:\n            Raised for misconfiguration in time formats or duplicate model IDs.\n        TypeError:\n            Raised for invalid argument types.\n    \"\"\"\n    self._model_id_to_index: Dict[str, int] = {}\n    self.raw_time_constraints = time_constraints\n    self._set_models(models)\n\n    try:\n        self.parsed_time_constraints = (\n            self._parse_time_constraints(time_constraints)\n            if time_constraints\n            else {}\n        )\n    except ValueError as e:\n        logger.error(f\"Error to parse time_constraints: {e}\")\n        raise ValueError(f\"Invalid format in time_constraints: {e}\") from e\n\n    # Validates if the model_ids in time_constraints exist\n    # (uses the keys from the parsed dict)\n    for model_id in self.parsed_time_constraints:\n        if model_id not in self._model_id_to_index:\n            logger.warning(\n                f\"The model_id `{model_id}` in time constraints \"\n                \"not found in the provided models\"\n            )\n\n    self.current_model_index = 0\n    logger.debug(\n        f\"ModelGateway initialized with {len(self.models)} models. Type: \"\n        f\"`{self.model_type}`\"\n    )\n    if self.parsed_time_constraints:\n        logger.debug(\n            \"Time constraints applied to models: \"\n            f\"{list(self.parsed_time_constraints.keys())}\"\n        )\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(*, model_preference=None, **kwargs)\n</code></pre> <p>Async version of call. Executes the call on the gateway.</p> <p>Parameters:</p> Name Type Description Default <code>model_preference</code> <code>Optional[str]</code> <p>The ID of the model that should be tried first. If None, starts from the last model used or the first one.</p> <code>None</code> <code>kwargs</code> <p>Arguments to pass to the acall method of the selected model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[ModelResponse, ModelStreamResponse]</code> <p>The response of the first model that executes successfully.</p> <p>Raises:</p> Type Description <code>ModelRouterError</code> <p>If all models fail consecutively up to the <code>max_retries</code> limit, or if no models are available/functional.</p> Source code in <code>src/msgflux/models/gateway.py</code> <pre><code>async def acall(\n    self, *, model_preference: Optional[str] = None, **kwargs\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Async version of __call__. Executes the call on the gateway.\n\n    Args:\n        model_preference:\n            The ID of the model that should be tried first.\n            If None, starts from the last model used or the first one.\n        kwargs:\n            Arguments to pass to the acall method of the selected model.\n\n    Returns:\n        The response of the first model that executes successfully.\n\n    Raises:\n        ModelRouterError:\n            If all models fail consecutively up to the `max_retries`\n            limit, or if no models are available/functional.\n    \"\"\"\n    return await self._aexecute_model(model_preference=model_preference, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.from_serialized","title":"from_serialized  <code>classmethod</code>","text":"<pre><code>from_serialized(data)\n</code></pre> <p>Creates a ModelGateway instance from serialized data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The dictionary of serialized models.</p> required Source code in <code>src/msgflux/models/gateway.py</code> <pre><code>@classmethod\ndef from_serialized(cls, data: Dict[str, Any]) -&gt; \"ModelGateway\":\n    \"\"\"Creates a ModelGateway instance from serialized data.\n\n    Args:\n        data: The dictionary of serialized models.\n    \"\"\"\n    if data.get(\"msgflux_type\") != cls.msgflux_type:\n        raise ValueError(\n            f\"Incorrect msgflux type. Expected `{cls.msgflux_type}`, \"\n            f\"given `{data.get('msgflux_type')}`\"\n        )\n\n    state = data.get(\"state\", {})\n    serialized_models = state.get(\"models\", [])\n    if not serialized_models:\n        raise ValueError(\"Serialized data does not contain templates\")\n\n    models = [Model.from_serialized(**m_data) for m_data in serialized_models]\n    time_constraints = state.get(\"time_constraints\")\n\n    return cls(models=models, time_constraints=time_constraints)\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.serialize","title":"serialize","text":"<pre><code>serialize()\n</code></pre> <p>Serializes the gateway state including time constraints as strings.</p> Source code in <code>src/msgflux/models/gateway.py</code> <pre><code>def serialize(self) -&gt; Dict[str, Any]:\n    \"\"\"Serializes the gateway state including time constraints as strings.\"\"\"\n    serialized_models = [model.serialize() for model in self.models]\n    state = {\n        \"time_constraints\": self.raw_time_constraints,\n        \"models\": serialized_models,\n    }\n    data = {\"msgflux_type\": self.msgflux_type, \"state\": state}\n    return data\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/","title":"ChatCompletion","text":""},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion","title":"OpenAIChatCompletion","text":"<p>               Bases: <code>_BaseOpenAI</code>, <code>ChatCompletionModel</code></p> <p>OpenAI Chat Completion.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAIChatCompletion(_BaseOpenAI, ChatCompletionModel):\n    \"\"\"OpenAI Chat Completion.\"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        *,\n        max_tokens: Optional[int] = None,\n        reasoning_effort: Optional[str] = None,\n        enable_thinking: Optional[bool] = None,\n        return_reasoning: Optional[bool] = False,\n        reasoning_in_tool_call: Optional[bool] = True,\n        validate_typed_parser_output: Optional[bool] = False,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        stop: Optional[Union[str, List[str]]] = None,\n        parallel_tool_calls: Optional[bool] = True,\n        modalities: Optional[List[str]] = None,\n        audio: Optional[Dict[str, str]] = None,\n        verbosity: Optional[str] = None,\n        web_search_options: Optional[Dict[str, Any]] = None,\n        verbose: Optional[bool] = False,\n        base_url: Optional[str] = None,\n        context_length: Optional[int] = None,\n        reasoning_max_tokens: Optional[int] = None,\n        enable_cache: Optional[bool] = False,\n        cache_size: Optional[int] = 128,\n    ):\n        \"\"\"Args:\n        model_id:\n            Model ID in provider.\n        max_tokens:\n            An upper bound for the number of tokens that can be\n            generated for a completion, including visible output\n            tokens and reasoning tokens.\n        reasoning_effort:\n            Constrains effort on reasoning for reasoning models.\n            Currently supported values are low, medium, and high.\n            Reducing reasoning effort can result in faster responses\n            and fewer tokens used on reasoning in a response.\n            Can be: \"minimal\", \"low\", \"medium\" or \"high\".\n        enable_thinking:\n            If True, enable the model reasoning.\n        return_reasoning:\n            If the model returns the `reasoning` field it will be added\n            along with the response.\n        reasoning_in_tool_call:\n            If True, maintains the reasoning for using the tool call.\n        validate_typed_parser_output:\n            If True, use the generation_schema to validate typed parser output.            \n        temperature:\n            What sampling temperature to use, between 0 and 2.\n            Higher values like 0.8 will make the output more random,\n            while lower values like 0.2 will make it more focused and\n            deterministic.\n        stop:\n            Up to 4 sequences where the API will stop generating further\n            tokens. The returned text will not contain the stop sequence.            \n        top_p:\n            An alternative to sampling with temperature, called nucleus\n            sampling, where the model considers the results of the tokens\n            with top_p probability mass. So 0.1 means only the tokens\n            comprising the top 10% probability mass are considered.\n        parallel_tool_calls:\n            If True, enable parallel tool calls.            \n        modalities:\n            Types of output you would like the model to generate.\n            Can be: [\"text\"], [\"audio\"] or [\"text\", \"audio\"].\n        audio:\n            Audio configurations. Define voice and output format.\n        verbosity:\n            Constrains the verbosity of the model's response. Lower \n            values will result in more concise responses, while higher\n            values will result in more verbose responses. Currently\n            supported values are low, medium, and high.\n        web_search_options:\n            This tool searches the web for relevant results to use in a response.\n            OpenAI and OpenRouter only.\n        verbose:\n            If True, Prints the model output to the console before it is transformed\n            into typed structured output.\n        base_url:\n            URL to model provider.\n        context_length:\n            The maximum context length supported by the model.\n        reasoning_max_tokens:\n            Maximum number of tokens for reasoning/thinking.\n        enable_cache:\n            If True, enable response caching to avoid redundant API calls.\n        cache_size:\n            Maximum number of cached responses (default: 128).\n        \"\"\"\n        super().__init__()\n        self.model_id = model_id\n        self.context_length = context_length\n        self.reasoning_max_tokens = reasoning_max_tokens\n        self.enable_cache = enable_cache\n        self.cache_size = cache_size\n        self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n        sampling_run_params = {\"max_tokens\": max_tokens}\n        if temperature:\n            sampling_run_params[\"temperature\"] = temperature\n        if top_p:\n            sampling_run_params[\"top_p\"] = top_p\n        if stop:\n            sampling_run_params[\"stop\"] = stop\n        if verbosity:\n            sampling_run_params[\"verbosity\"] = verbosity\n        if modalities:\n            sampling_run_params[\"modalities\"] = modalities\n        if web_search_options:\n            sampling_run_params[\"web_search_options\"] = web_search_options\n        if audio:\n            sampling_run_params[\"audio\"] = audio\n        if reasoning_effort:\n            sampling_run_params[\"reasoning_effort\"] = reasoning_effort\n        self.sampling_run_params = sampling_run_params\n        self.enable_thinking = enable_thinking\n        self.parallel_tool_calls = parallel_tool_calls\n        self.reasoning_in_tool_call = reasoning_in_tool_call\n        self.validate_typed_parser_output = validate_typed_parser_output\n        self.return_reasoning = return_reasoning\n        self.verbose = verbose\n        self._initialize()\n        self._get_api_key()\n\n    def _adapt_params(self, params: Dict[str, Any]) -&gt; Dict[str, Any]:\n        if self.provider in \"openai\":\n            params[\"max_completion_tokens\"] = params.pop(\"max_tokens\")\n        return params\n\n    def _execute_model(self, **kwargs):\n        prefilling = kwargs.pop(\"prefilling\")\n        if prefilling:\n            kwargs.get(\"messages\").append({\"role\": \"assistant\", \"content\": prefilling})\n        params = {**kwargs, **self.sampling_run_params}\n        adapted_params = self._adapt_params(params)\n        model_output = self.client.chat.completions.create(**adapted_params)\n\n        return model_output\n\n    async def _aexecute_model(self, **kwargs):\n        prefilling = kwargs.pop(\"prefilling\")\n        if prefilling:\n            kwargs.get(\"messages\").append({\"role\": \"assistant\", \"content\": prefilling})\n        params = {**kwargs, **self.sampling_run_params}\n        adapted_params = self._adapt_params(params)\n        model_output = await self.aclient.chat.completions.create(**adapted_params)\n\n        return model_output\n\n    def _process_model_output(self, model_output, typed_parser=None, generation_schema=None):\n        \"\"\"Shared logic to process model output for both sync and async.\"\"\"\n        response = ModelResponse()\n        metadata = dotdict()\n\n        metadata.update({\"usage\": model_output.usage.to_dict()})\n\n        choice = model_output.choices[0]\n\n        reasoning = (\n            getattr(choice.message, \"reasoning_content\", None) or\n            getattr(choice.message, \"reasoning\", None) or\n            getattr(choice.message, \"thinking\", None)\n        )\n\n        reasoning_tool_call = None\n        if self.reasoning_in_tool_call is True:\n            reasoning_tool_call = reasoning\n\n        prefix_response_type = \"\"\n        reasoning_content = None\n        if self.return_reasoning is True:\n            reasoning_content = reasoning\n            if reasoning_content is not None:\n                prefix_response_type = \"reasoning_\"\n\n        if choice.message.annotations:  # Extra responses (e.g web search references)\n            annotations_content = [\n                item.model_dump() for item in choice.message.annotations\n            ]\n            metadata.annotations = annotations_content\n\n        if choice.message.tool_calls:\n            aggregator = ToolCallAggregator(reasoning_tool_call)\n            response.set_response_type(\"tool_call\")\n            for call_index, tool_call in enumerate(choice.message.tool_calls):\n                tool_id = tool_call.id\n                name = tool_call.function.name\n                arguments = tool_call.function.arguments\n                aggregator.process(call_index, tool_id, name, arguments)\n            response_content = aggregator\n        elif choice.message.content:\n            if (typed_parser or generation_schema) and self.verbose:\n                repr = f\"[{self.model_id}][raw_response] {choice.message.content}\"\n                cprint(repr, lc=\"r\", ls=\"b\")\n            if typed_parser is not None:\n                choice.message.content\n                response.set_response_type(f\"{prefix_response_type}structured\")\n                parser = typed_parser_registry[typed_parser]\n                response_content = dotdict(parser.decode(choice.message.content))\n                # Type validation\n                if generation_schema and self.validate_typed_parser_output:\n                    encoded_response_content = msgspec.json.encode(response_content)\n                    msgspec.json.decode(\n                        encoded_response_content, type=generation_schema\n                    )\n            elif generation_schema is not None:\n                response.set_response_type(f\"{prefix_response_type}structured\")\n                struct = msgspec.json.decode(\n                    choice.message.content, type=generation_schema\n                )\n                response_content = dotdict(struct_to_dict(struct))\n            else:\n                response.set_response_type(\n                    f\"{prefix_response_type}text_generation\"\n                )\n                if reasoning_content is not None:\n                    response_content = dotdict({\"answer\": choice.message.content})\n                else:\n                    response_content = choice.message.content\n        elif choice.message.audio:\n            response_content = dotdict(\n                {\n                    \"id\": choice.message.audio.id,\n                    \"audio\": base64.b64decode(choice.message.audio.data),\n                }\n            )\n            if choice.message.audio.transcript:\n                response.set_response_type(\"audio_text_generation\")\n                response_content.text = choice.message.audio.transcript\n            else:\n                response.set_response_type(\"audio_generation\")\n\n        if reasoning_content is not None:\n            response_content.think = reasoning_content\n\n        response.add(response_content)\n        response.set_metadata(metadata)\n        return response\n\n    def _generate(self, **kwargs: Mapping[str, Any]) -&gt; ModelResponse:\n        typed_parser = kwargs.get(\"typed_parser\")\n        generation_schema = kwargs.get(\"generation_schema\")\n\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        # Pop after cache check to avoid modifying kwargs during cache key generation\n        typed_parser = kwargs.pop(\"typed_parser\")\n        generation_schema = kwargs.pop(\"generation_schema\")\n\n        if generation_schema is not None and typed_parser is None:\n            response_format = response_format_from_msgspec_struct(generation_schema)\n            kwargs[\"response_format\"] = response_format\n\n        model_output = self._execute_model(**kwargs)\n        response = self._process_model_output(model_output, typed_parser, generation_schema)\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            # Re-add popped values for cache key\n            cache_kwargs = {**kwargs, \"typed_parser\": typed_parser, \"generation_schema\": generation_schema}\n            cache_key = generate_cache_key(**cache_kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    async def _agenerate(self, **kwargs: Mapping[str, Any]) -&gt; ModelResponse:\n        typed_parser = kwargs.get(\"typed_parser\")\n        generation_schema = kwargs.get(\"generation_schema\")\n\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        # Pop after cache check to avoid modifying kwargs during cache key generation\n        typed_parser = kwargs.pop(\"typed_parser\")\n        generation_schema = kwargs.pop(\"generation_schema\")\n\n        if generation_schema is not None and typed_parser is None:\n            response_format = response_format_from_msgspec_struct(generation_schema)\n            kwargs[\"response_format\"] = response_format\n\n        model_output = await self._aexecute_model(**kwargs)\n        response = self._process_model_output(model_output, typed_parser, generation_schema)\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            # Re-add popped values for cache key\n            cache_kwargs = {**kwargs, \"typed_parser\": typed_parser, \"generation_schema\": generation_schema}\n            cache_key = generate_cache_key(**cache_kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    async def _stream_generate(self, **kwargs: Mapping[str, Any]) -&gt; ModelStreamResponse:\n        aggregator = ToolCallAggregator()\n        metadata = dotdict()\n\n        stream_response = kwargs.pop(\"stream_response\")\n        model_output = self._execute_model(**kwargs)\n\n        reasoning_tool_call = \"\"\n\n        for chunk in model_output:\n            if chunk.choices:\n                delta = chunk.choices[0].delta\n\n                reasoning_chunk = (\n                    getattr(delta, \"reasoning_content\", None) or\n                    getattr(delta, \"reasoning\", None) or\n                    getattr(delta, \"thinking\", None)\n                )\n\n                if self.reasoning_in_tool_call and reasoning_chunk:\n                    reasoning_tool_call += reasoning_chunk\n\n                if self.return_reasoning and reasoning_chunk:\n                    if stream_response.response_type is None:\n                        stream_response.set_response_type(\"reasoning_text_generation\")\n                        stream_response.first_chunk_event.set()\n                    stream_response.add(reasoning_chunk)\n                    continue\n\n                if getattr(delta, \"content\", None):\n                    if stream_response.response_type is None:\n                        stream_response.set_response_type(\"text_generation\")\n                        stream_response.first_chunk_event.set()\n                    stream_response.add(delta.content)\n                    continue\n\n                if getattr(delta, \"tool_calls\", None):\n                    if stream_response.response_type is None:\n                        stream_response.set_response_type(\"tool_call\")\n                    tool_call = delta.tool_calls[0]\n                    call_index = tool_call.index\n                    tool_id = tool_call.id\n                    name = tool_call.function.name\n                    arguments = tool_call.function.arguments\n                    aggregator.process(call_index, tool_id, name, arguments)\n                    continue\n\n                if hasattr(delta, \"annotations\") and delta.annotations is not None:\n                    metadata.annotations = [\n                        item.model_dump() for item in delta.annotations\n                    ]\n                    continue\n\n            elif chunk.usage:\n                metadata.update(chunk.usage.to_dict())\n\n        if aggregator.tool_calls:\n            if reasoning_tool_call:\n                aggregator.reasoning = reasoning_tool_call\n            stream_response.data = aggregator # For tool calls save as 'data'\n            stream_response.first_chunk_event.set()\n\n        stream_response.set_metadata(metadata)\n        stream_response.add(None)\n\n    async def _astream_generate(self, **kwargs: Mapping[str, Any]) -&gt; ModelStreamResponse:\n        aggregator = ToolCallAggregator()\n        metadata = dotdict()\n\n        stream_response = kwargs.pop(\"stream_response\")\n        model_output = await self._aexecute_model(**kwargs)\n\n        reasoning_tool_call = \"\"\n\n        async for chunk in model_output:\n            if chunk.choices:\n                delta = chunk.choices[0].delta\n\n                reasoning_chunk = (\n                    getattr(delta, \"reasoning_content\", None) or\n                    getattr(delta, \"reasoning\", None) or\n                    getattr(delta, \"thinking\", None)\n                )\n\n                if self.reasoning_in_tool_call and reasoning_chunk:\n                    reasoning_tool_call += reasoning_chunk\n\n                if self.return_reasoning and reasoning_chunk:\n                    if stream_response.response_type is None:\n                        stream_response.set_response_type(\"reasoning_text_generation\")\n                        stream_response.first_chunk_event.set()\n                    stream_response.add(reasoning_chunk)\n                    continue\n\n                if getattr(delta, \"content\", None):\n                    if stream_response.response_type is None:\n                        stream_response.set_response_type(\"text_generation\")\n                        stream_response.first_chunk_event.set()\n                    stream_response.add(delta.content)\n                    continue\n\n                if getattr(delta, \"tool_calls\", None):\n                    if stream_response.response_type is None:\n                        stream_response.set_response_type(\"tool_call\")\n                    tool_call = delta.tool_calls[0]\n                    call_index = tool_call.index\n                    tool_id = tool_call.id\n                    name = tool_call.function.name\n                    arguments = tool_call.function.arguments\n                    aggregator.process(call_index, tool_id, name, arguments)\n                    continue\n\n                if hasattr(delta, \"annotations\") and delta.annotations is not None:\n                    metadata.annotations = [\n                        item.model_dump() for item in delta.annotations\n                    ]\n                    continue\n\n            elif chunk.usage:\n                metadata.update(chunk.usage.to_dict())\n\n        if aggregator.tool_calls:\n            if reasoning_tool_call:\n                aggregator.reasoning = reasoning_tool_call\n            stream_response.data = aggregator # For tool calls save as 'data'\n            stream_response.first_chunk_event.set()\n\n        stream_response.set_metadata(metadata)\n        stream_response.add(None)\n\n    @model_retry\n    def __call__(\n        self,\n        messages: Union[str, List[Dict[str, Any]]],\n        *,\n        system_prompt: Optional[str] = None,\n        prefilling: Optional[str] = None,\n        stream: Optional[bool] = False,\n        generation_schema: Optional[msgspec.Struct] = None,\n        tool_schemas: Optional[Dict] = None,\n        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n        typed_parser: Optional[str] = None,\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Args:\n            messages:\n                Conversation history. Can be simple string or list of messages.\n            system_prompt:\n                A set of instructions that defines the overarching behavior\n                and role of the model across all interactions.\n            prefilling:\n                Forces an initial message from the model. From that message\n                it will continue its response from there.\n            stream:\n                Whether generation should be in streaming mode.\n            generation_schema:\n                Schema that defines how the output should be structured.\n            tool_schemas:\n                JSON schema containing available tools.\n            tool_choice:\n                By default the model will determine when and how many tools to use.\n                You can force specific behavior with the tool_choice parameter.\n                    1. auto:\n                        (Default) Call zero, one, or multiple functions.\n                    2. required:\n                        Call one or more functions.\n                    3. Forced Tool:\n                        Call exactly one specific tool e.g: \"get_weather\".\n            typed_parser:\n                Converts the model raw output into a typed-dict. Supported parser:\n                `typed_xml`.\n\n        Raises:\n            ValueError:\n                Raised if `generation_schema` and `stream=True`.\n            ValueError:\n                Raised if `typed_xml=True` and `stream=True`.\n        \"\"\"        \n        if isinstance(messages, str):\n            messages = [ChatBlock.user(messages)]\n        if isinstance(system_prompt, str):            \n            messages.insert(0, ChatBlock.system(system_prompt))\n\n        if isinstance(tool_choice, str):\n            if tool_choice not in [\"auto\", \"required\", \"none\"]:\n                tool_choice = {\n                    \"type\": \"function\",\n                    \"function\": {\"name\": tool_choice},\n                }\n\n        generation_params = {\n            \"messages\": messages,\n            \"prefilling\": prefilling,\n            \"tool_choice\": tool_choice,\n            \"tools\": tool_schemas,\n            \"model\": self.model_id\n        }\n\n        if tool_schemas:\n            generation_params[\"parallel_tool_calls\"] = self.parallel_tool_calls\n\n        if stream is True:\n            if typed_parser is not None:\n                raise ValueError(\"`typed_parser` is not `stream=True` compatible\")\n\n            stream_response = ModelStreamResponse()\n            F.background_task(\n                self._stream_generate,\n                **generation_params,\n                stream=stream,\n                stream_response=stream_response,\n                stream_options={\"include_usage\": True},\n            )\n            F.wait_for_event(stream_response.first_chunk_event)\n            return stream_response\n        else:\n            if typed_parser and typed_parser not in typed_parser_registry:\n                available = \", \".join(typed_parser_registry.keys())\n                raise TypedParserNotFoundError(\n                    f\"Typed parser `{typed_parser}` not found. \"\n                    f\"Available parsers: {available}\"\n                )            \n            response = self._generate(\n                **generation_params,\n                typed_parser=typed_parser,\n                generation_schema=generation_schema,\n            )\n            return response\n\n    @model_retry\n    async def acall(\n        self,\n        messages: Union[str, List[Dict[str, Any]]],\n        *,\n        system_prompt: Optional[str] = None,\n        prefilling: Optional[str] = None,\n        stream: Optional[bool] = False,\n        generation_schema: Optional[msgspec.Struct] = None,\n        tool_schemas: Optional[Dict] = None,\n        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n        typed_parser: Optional[str] = None,\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Async version of __call__. Args:\n            messages:\n                Conversation history. Can be simple string or list of messages.\n            system_prompt:\n                A set of instructions that defines the overarching behavior\n                and role of the model across all interactions.\n            prefilling:\n                Forces an initial message from the model. From that message\n                it will continue its response from there.\n            stream:\n                Whether generation should be in streaming mode.\n            generation_schema:\n                Schema that defines how the output should be structured.\n            tool_schemas:\n                JSON schema containing available tools.\n            tool_choice:\n                By default the model will determine when and how many tools to use.\n                You can force specific behavior with the tool_choice parameter.\n                    1. auto:\n                        (Default) Call zero, one, or multiple functions.\n                    2. required:\n                        Call one or more functions.\n                    3. Forced Tool:\n                        Call exactly one specific tool e.g: \"get_weather\".\n            typed_parser:\n                Converts the model raw output into a typed-dict. Supported parser:\n                `typed_xml`.\n\n        Raises:\n            ValueError:\n                Raised if `generation_schema` and `stream=True`.\n            ValueError:\n                Raised if `typed_xml=True` and `stream=True`.\n        \"\"\"\n        if isinstance(messages, str):\n            messages = [ChatBlock.user(messages)]\n        if isinstance(system_prompt, str):\n            messages.insert(0, ChatBlock.system(system_prompt))\n\n        if isinstance(tool_choice, str):\n            if tool_choice not in [\"auto\", \"required\", \"none\"]:\n                tool_choice = {\n                    \"type\": \"function\",\n                    \"function\": {\"name\": tool_choice},\n                }\n\n        generation_params = {\n            \"messages\": messages,\n            \"prefilling\": prefilling,\n            \"tool_choice\": tool_choice,\n            \"tools\": tool_schemas,\n            \"model\": self.model_id\n        }\n\n        if tool_schemas:\n            generation_params[\"parallel_tool_calls\"] = self.parallel_tool_calls\n\n        if stream is True:\n            if typed_parser is not None:\n                raise ValueError(\"`typed_parser` is not `stream=True` compatible\")\n\n            stream_response = ModelStreamResponse()\n            await F.abackground_task(\n                self._astream_generate,\n                **generation_params,\n                stream=stream,\n                stream_response=stream_response,\n                stream_options={\"include_usage\": True},\n            )\n            await F.await_for_event(stream_response.first_chunk_event)\n            return stream_response\n        else:\n            if typed_parser and typed_parser not in typed_parser_registry:\n                available = \", \".join(typed_parser_registry.keys())\n                raise TypedParserNotFoundError(\n                    f\"Typed parser `{typed_parser}` not found. \"\n                    f\"Available parsers: {available}\"\n                )\n            response = await self._agenerate(\n                **generation_params,\n                typed_parser=typed_parser,\n                generation_schema=generation_schema,\n            )\n            return response\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.cache_size","title":"cache_size  <code>instance-attribute</code>","text":"<pre><code>cache_size = cache_size\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.context_length","title":"context_length  <code>instance-attribute</code>","text":"<pre><code>context_length = context_length\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.enable_cache","title":"enable_cache  <code>instance-attribute</code>","text":"<pre><code>enable_cache = enable_cache\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.enable_thinking","title":"enable_thinking  <code>instance-attribute</code>","text":"<pre><code>enable_thinking = enable_thinking\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.parallel_tool_calls","title":"parallel_tool_calls  <code>instance-attribute</code>","text":"<pre><code>parallel_tool_calls = parallel_tool_calls\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.reasoning_in_tool_call","title":"reasoning_in_tool_call  <code>instance-attribute</code>","text":"<pre><code>reasoning_in_tool_call = reasoning_in_tool_call\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.reasoning_max_tokens","title":"reasoning_max_tokens  <code>instance-attribute</code>","text":"<pre><code>reasoning_max_tokens = reasoning_max_tokens\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.return_reasoning","title":"return_reasoning  <code>instance-attribute</code>","text":"<pre><code>return_reasoning = return_reasoning\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.sampling_params","title":"sampling_params  <code>instance-attribute</code>","text":"<pre><code>sampling_params = {'base_url': base_url or _get_base_url()}\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.sampling_run_params","title":"sampling_run_params  <code>instance-attribute</code>","text":"<pre><code>sampling_run_params = sampling_run_params\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.validate_typed_parser_output","title":"validate_typed_parser_output  <code>instance-attribute</code>","text":"<pre><code>validate_typed_parser_output = validate_typed_parser_output\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.verbose","title":"verbose  <code>instance-attribute</code>","text":"<pre><code>verbose = verbose\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.__call__","title":"__call__","text":"<pre><code>__call__(\n    messages,\n    *,\n    system_prompt=None,\n    prefilling=None,\n    stream=False,\n    generation_schema=None,\n    tool_schemas=None,\n    tool_choice=None,\n    typed_parser=None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, List[Dict[str, Any]]]</code> <p>Conversation history. Can be simple string or list of messages.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>A set of instructions that defines the overarching behavior and role of the model across all interactions.</p> <code>None</code> <code>prefilling</code> <code>Optional[str]</code> <p>Forces an initial message from the model. From that message it will continue its response from there.</p> <code>None</code> <code>stream</code> <code>Optional[bool]</code> <p>Whether generation should be in streaming mode.</p> <code>False</code> <code>generation_schema</code> <code>Optional[Struct]</code> <p>Schema that defines how the output should be structured.</p> <code>None</code> <code>tool_schemas</code> <code>Optional[Dict]</code> <p>JSON schema containing available tools.</p> <code>None</code> <code>tool_choice</code> <code>Optional[Union[str, Dict[str, Any]]]</code> <p>By default the model will determine when and how many tools to use. You can force specific behavior with the tool_choice parameter.     1. auto:         (Default) Call zero, one, or multiple functions.     2. required:         Call one or more functions.     3. Forced Tool:         Call exactly one specific tool e.g: \"get_weather\".</p> <code>None</code> <code>typed_parser</code> <code>Optional[str]</code> <p>Converts the model raw output into a typed-dict. Supported parser: <code>typed_xml</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised if <code>generation_schema</code> and <code>stream=True</code>.</p> <code>ValueError</code> <p>Raised if <code>typed_xml=True</code> and <code>stream=True</code>.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    messages: Union[str, List[Dict[str, Any]]],\n    *,\n    system_prompt: Optional[str] = None,\n    prefilling: Optional[str] = None,\n    stream: Optional[bool] = False,\n    generation_schema: Optional[msgspec.Struct] = None,\n    tool_schemas: Optional[Dict] = None,\n    tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n    typed_parser: Optional[str] = None,\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Args:\n        messages:\n            Conversation history. Can be simple string or list of messages.\n        system_prompt:\n            A set of instructions that defines the overarching behavior\n            and role of the model across all interactions.\n        prefilling:\n            Forces an initial message from the model. From that message\n            it will continue its response from there.\n        stream:\n            Whether generation should be in streaming mode.\n        generation_schema:\n            Schema that defines how the output should be structured.\n        tool_schemas:\n            JSON schema containing available tools.\n        tool_choice:\n            By default the model will determine when and how many tools to use.\n            You can force specific behavior with the tool_choice parameter.\n                1. auto:\n                    (Default) Call zero, one, or multiple functions.\n                2. required:\n                    Call one or more functions.\n                3. Forced Tool:\n                    Call exactly one specific tool e.g: \"get_weather\".\n        typed_parser:\n            Converts the model raw output into a typed-dict. Supported parser:\n            `typed_xml`.\n\n    Raises:\n        ValueError:\n            Raised if `generation_schema` and `stream=True`.\n        ValueError:\n            Raised if `typed_xml=True` and `stream=True`.\n    \"\"\"        \n    if isinstance(messages, str):\n        messages = [ChatBlock.user(messages)]\n    if isinstance(system_prompt, str):            \n        messages.insert(0, ChatBlock.system(system_prompt))\n\n    if isinstance(tool_choice, str):\n        if tool_choice not in [\"auto\", \"required\", \"none\"]:\n            tool_choice = {\n                \"type\": \"function\",\n                \"function\": {\"name\": tool_choice},\n            }\n\n    generation_params = {\n        \"messages\": messages,\n        \"prefilling\": prefilling,\n        \"tool_choice\": tool_choice,\n        \"tools\": tool_schemas,\n        \"model\": self.model_id\n    }\n\n    if tool_schemas:\n        generation_params[\"parallel_tool_calls\"] = self.parallel_tool_calls\n\n    if stream is True:\n        if typed_parser is not None:\n            raise ValueError(\"`typed_parser` is not `stream=True` compatible\")\n\n        stream_response = ModelStreamResponse()\n        F.background_task(\n            self._stream_generate,\n            **generation_params,\n            stream=stream,\n            stream_response=stream_response,\n            stream_options={\"include_usage\": True},\n        )\n        F.wait_for_event(stream_response.first_chunk_event)\n        return stream_response\n    else:\n        if typed_parser and typed_parser not in typed_parser_registry:\n            available = \", \".join(typed_parser_registry.keys())\n            raise TypedParserNotFoundError(\n                f\"Typed parser `{typed_parser}` not found. \"\n                f\"Available parsers: {available}\"\n            )            \n        response = self._generate(\n            **generation_params,\n            typed_parser=typed_parser,\n            generation_schema=generation_schema,\n        )\n        return response\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.__init__","title":"__init__","text":"<pre><code>__init__(\n    model_id,\n    *,\n    max_tokens=None,\n    reasoning_effort=None,\n    enable_thinking=None,\n    return_reasoning=False,\n    reasoning_in_tool_call=True,\n    validate_typed_parser_output=False,\n    temperature=None,\n    top_p=None,\n    stop=None,\n    parallel_tool_calls=True,\n    modalities=None,\n    audio=None,\n    verbosity=None,\n    web_search_options=None,\n    verbose=False,\n    base_url=None,\n    context_length=None,\n    reasoning_max_tokens=None,\n    enable_cache=False,\n    cache_size=128,\n)\n</code></pre> <p>model_id:     Model ID in provider. max_tokens:     An upper bound for the number of tokens that can be     generated for a completion, including visible output     tokens and reasoning tokens. reasoning_effort:     Constrains effort on reasoning for reasoning models.     Currently supported values are low, medium, and high.     Reducing reasoning effort can result in faster responses     and fewer tokens used on reasoning in a response.     Can be: \"minimal\", \"low\", \"medium\" or \"high\". enable_thinking:     If True, enable the model reasoning. return_reasoning:     If the model returns the <code>reasoning</code> field it will be added     along with the response. reasoning_in_tool_call:     If True, maintains the reasoning for using the tool call. validate_typed_parser_output:     If True, use the generation_schema to validate typed parser output.           temperature:     What sampling temperature to use, between 0 and 2.     Higher values like 0.8 will make the output more random,     while lower values like 0.2 will make it more focused and     deterministic. stop:     Up to 4 sequences where the API will stop generating further     tokens. The returned text will not contain the stop sequence.           top_p:     An alternative to sampling with temperature, called nucleus     sampling, where the model considers the results of the tokens     with top_p probability mass. So 0.1 means only the tokens     comprising the top 10% probability mass are considered. parallel_tool_calls:     If True, enable parallel tool calls.           modalities:     Types of output you would like the model to generate.     Can be: [\"text\"], [\"audio\"] or [\"text\", \"audio\"]. audio:     Audio configurations. Define voice and output format. verbosity:     Constrains the verbosity of the model's response. Lower      values will result in more concise responses, while higher     values will result in more verbose responses. Currently     supported values are low, medium, and high. web_search_options:     This tool searches the web for relevant results to use in a response.     OpenAI and OpenRouter only. verbose:     If True, Prints the model output to the console before it is transformed     into typed structured output. base_url:     URL to model provider. context_length:     The maximum context length supported by the model. reasoning_max_tokens:     Maximum number of tokens for reasoning/thinking. enable_cache:     If True, enable response caching to avoid redundant API calls. cache_size:     Maximum number of cached responses (default: 128).</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    model_id: str,\n    *,\n    max_tokens: Optional[int] = None,\n    reasoning_effort: Optional[str] = None,\n    enable_thinking: Optional[bool] = None,\n    return_reasoning: Optional[bool] = False,\n    reasoning_in_tool_call: Optional[bool] = True,\n    validate_typed_parser_output: Optional[bool] = False,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    stop: Optional[Union[str, List[str]]] = None,\n    parallel_tool_calls: Optional[bool] = True,\n    modalities: Optional[List[str]] = None,\n    audio: Optional[Dict[str, str]] = None,\n    verbosity: Optional[str] = None,\n    web_search_options: Optional[Dict[str, Any]] = None,\n    verbose: Optional[bool] = False,\n    base_url: Optional[str] = None,\n    context_length: Optional[int] = None,\n    reasoning_max_tokens: Optional[int] = None,\n    enable_cache: Optional[bool] = False,\n    cache_size: Optional[int] = 128,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    max_tokens:\n        An upper bound for the number of tokens that can be\n        generated for a completion, including visible output\n        tokens and reasoning tokens.\n    reasoning_effort:\n        Constrains effort on reasoning for reasoning models.\n        Currently supported values are low, medium, and high.\n        Reducing reasoning effort can result in faster responses\n        and fewer tokens used on reasoning in a response.\n        Can be: \"minimal\", \"low\", \"medium\" or \"high\".\n    enable_thinking:\n        If True, enable the model reasoning.\n    return_reasoning:\n        If the model returns the `reasoning` field it will be added\n        along with the response.\n    reasoning_in_tool_call:\n        If True, maintains the reasoning for using the tool call.\n    validate_typed_parser_output:\n        If True, use the generation_schema to validate typed parser output.            \n    temperature:\n        What sampling temperature to use, between 0 and 2.\n        Higher values like 0.8 will make the output more random,\n        while lower values like 0.2 will make it more focused and\n        deterministic.\n    stop:\n        Up to 4 sequences where the API will stop generating further\n        tokens. The returned text will not contain the stop sequence.            \n    top_p:\n        An alternative to sampling with temperature, called nucleus\n        sampling, where the model considers the results of the tokens\n        with top_p probability mass. So 0.1 means only the tokens\n        comprising the top 10% probability mass are considered.\n    parallel_tool_calls:\n        If True, enable parallel tool calls.            \n    modalities:\n        Types of output you would like the model to generate.\n        Can be: [\"text\"], [\"audio\"] or [\"text\", \"audio\"].\n    audio:\n        Audio configurations. Define voice and output format.\n    verbosity:\n        Constrains the verbosity of the model's response. Lower \n        values will result in more concise responses, while higher\n        values will result in more verbose responses. Currently\n        supported values are low, medium, and high.\n    web_search_options:\n        This tool searches the web for relevant results to use in a response.\n        OpenAI and OpenRouter only.\n    verbose:\n        If True, Prints the model output to the console before it is transformed\n        into typed structured output.\n    base_url:\n        URL to model provider.\n    context_length:\n        The maximum context length supported by the model.\n    reasoning_max_tokens:\n        Maximum number of tokens for reasoning/thinking.\n    enable_cache:\n        If True, enable response caching to avoid redundant API calls.\n    cache_size:\n        Maximum number of cached responses (default: 128).\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.context_length = context_length\n    self.reasoning_max_tokens = reasoning_max_tokens\n    self.enable_cache = enable_cache\n    self.cache_size = cache_size\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    sampling_run_params = {\"max_tokens\": max_tokens}\n    if temperature:\n        sampling_run_params[\"temperature\"] = temperature\n    if top_p:\n        sampling_run_params[\"top_p\"] = top_p\n    if stop:\n        sampling_run_params[\"stop\"] = stop\n    if verbosity:\n        sampling_run_params[\"verbosity\"] = verbosity\n    if modalities:\n        sampling_run_params[\"modalities\"] = modalities\n    if web_search_options:\n        sampling_run_params[\"web_search_options\"] = web_search_options\n    if audio:\n        sampling_run_params[\"audio\"] = audio\n    if reasoning_effort:\n        sampling_run_params[\"reasoning_effort\"] = reasoning_effort\n    self.sampling_run_params = sampling_run_params\n    self.enable_thinking = enable_thinking\n    self.parallel_tool_calls = parallel_tool_calls\n    self.reasoning_in_tool_call = reasoning_in_tool_call\n    self.validate_typed_parser_output = validate_typed_parser_output\n    self.return_reasoning = return_reasoning\n    self.verbose = verbose\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(\n    messages,\n    *,\n    system_prompt=None,\n    prefilling=None,\n    stream=False,\n    generation_schema=None,\n    tool_schemas=None,\n    tool_choice=None,\n    typed_parser=None,\n)\n</code></pre> <p>Async version of call. Args:     messages:         Conversation history. Can be simple string or list of messages.     system_prompt:         A set of instructions that defines the overarching behavior         and role of the model across all interactions.     prefilling:         Forces an initial message from the model. From that message         it will continue its response from there.     stream:         Whether generation should be in streaming mode.     generation_schema:         Schema that defines how the output should be structured.     tool_schemas:         JSON schema containing available tools.     tool_choice:         By default the model will determine when and how many tools to use.         You can force specific behavior with the tool_choice parameter.             1. auto:                 (Default) Call zero, one, or multiple functions.             2. required:                 Call one or more functions.             3. Forced Tool:                 Call exactly one specific tool e.g: \"get_weather\".     typed_parser:         Converts the model raw output into a typed-dict. Supported parser:         <code>typed_xml</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised if <code>generation_schema</code> and <code>stream=True</code>.</p> <code>ValueError</code> <p>Raised if <code>typed_xml=True</code> and <code>stream=True</code>.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    messages: Union[str, List[Dict[str, Any]]],\n    *,\n    system_prompt: Optional[str] = None,\n    prefilling: Optional[str] = None,\n    stream: Optional[bool] = False,\n    generation_schema: Optional[msgspec.Struct] = None,\n    tool_schemas: Optional[Dict] = None,\n    tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n    typed_parser: Optional[str] = None,\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Async version of __call__. Args:\n        messages:\n            Conversation history. Can be simple string or list of messages.\n        system_prompt:\n            A set of instructions that defines the overarching behavior\n            and role of the model across all interactions.\n        prefilling:\n            Forces an initial message from the model. From that message\n            it will continue its response from there.\n        stream:\n            Whether generation should be in streaming mode.\n        generation_schema:\n            Schema that defines how the output should be structured.\n        tool_schemas:\n            JSON schema containing available tools.\n        tool_choice:\n            By default the model will determine when and how many tools to use.\n            You can force specific behavior with the tool_choice parameter.\n                1. auto:\n                    (Default) Call zero, one, or multiple functions.\n                2. required:\n                    Call one or more functions.\n                3. Forced Tool:\n                    Call exactly one specific tool e.g: \"get_weather\".\n        typed_parser:\n            Converts the model raw output into a typed-dict. Supported parser:\n            `typed_xml`.\n\n    Raises:\n        ValueError:\n            Raised if `generation_schema` and `stream=True`.\n        ValueError:\n            Raised if `typed_xml=True` and `stream=True`.\n    \"\"\"\n    if isinstance(messages, str):\n        messages = [ChatBlock.user(messages)]\n    if isinstance(system_prompt, str):\n        messages.insert(0, ChatBlock.system(system_prompt))\n\n    if isinstance(tool_choice, str):\n        if tool_choice not in [\"auto\", \"required\", \"none\"]:\n            tool_choice = {\n                \"type\": \"function\",\n                \"function\": {\"name\": tool_choice},\n            }\n\n    generation_params = {\n        \"messages\": messages,\n        \"prefilling\": prefilling,\n        \"tool_choice\": tool_choice,\n        \"tools\": tool_schemas,\n        \"model\": self.model_id\n    }\n\n    if tool_schemas:\n        generation_params[\"parallel_tool_calls\"] = self.parallel_tool_calls\n\n    if stream is True:\n        if typed_parser is not None:\n            raise ValueError(\"`typed_parser` is not `stream=True` compatible\")\n\n        stream_response = ModelStreamResponse()\n        await F.abackground_task(\n            self._astream_generate,\n            **generation_params,\n            stream=stream,\n            stream_response=stream_response,\n            stream_options={\"include_usage\": True},\n        )\n        await F.await_for_event(stream_response.first_chunk_event)\n        return stream_response\n    else:\n        if typed_parser and typed_parser not in typed_parser_registry:\n            available = \", \".join(typed_parser_registry.keys())\n            raise TypedParserNotFoundError(\n                f\"Typed parser `{typed_parser}` not found. \"\n                f\"Available parsers: {available}\"\n            )\n        response = await self._agenerate(\n            **generation_params,\n            typed_parser=typed_parser,\n            generation_schema=generation_schema,\n        )\n        return response\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.vllm.VLLMChatCompletion","title":"VLLMChatCompletion","text":"<p>               Bases: <code>_BaseVLLM</code>, <code>OpenAIChatCompletion</code></p> <p>vLLM Chat Completion.</p> Source code in <code>src/msgflux/models/providers/vllm.py</code> <pre><code>@register_model\nclass VLLMChatCompletion(_BaseVLLM, OpenAIChatCompletion):\n    \"\"\"vLLM Chat Completion.\"\"\"\n\n    def _adapt_params(self, params: Dict[str, Any]) -&gt; Dict[str, Any]:\n        response_format = params.pop(\"response_format\", None)\n        extra_body = params.get(\"extra_body\", {})\n\n        if response_format is not None:\n            extra_body[\"guided_json\"] = response_format\n\n        if self.enable_thinking is not None:\n            extra_body[\"chat_template_kwargs\"] = {\n                \"enable_thinking\": self.enable_thinking\n            }\n\n        params[\"extra_body\"] = extra_body\n        return params\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openrouter.OpenRouterChatCompletion","title":"OpenRouterChatCompletion","text":"<p>               Bases: <code>_BaseOpenRouter</code>, <code>OpenAIChatCompletion</code></p> <p>OpenRouter Chat Completion.</p> Source code in <code>src/msgflux/models/providers/openrouter.py</code> <pre><code>@register_model\nclass OpenRouterChatCompletion(_BaseOpenRouter, OpenAIChatCompletion):\n    \"\"\"OpenRouter Chat Completion.\"\"\"\n\n    def _adapt_params(self, params: Dict[str, Any]) -&gt; Dict[str, Any]:\n        extra_body = params.get(\"extra_body\", {})\n        plugins = []\n\n        if params[\"tool_choice\"] is None:\n            if params[\"tools\"] is not None:\n                params[\"tool_choice\"] = \"auto\"\n            else:\n                params[\"tool_choice\"] = \"none\"\n\n        reasoning_effort = params.pop(\"reasoning_effort\", None)      \n        if reasoning_effort is not None:\n            extra_body[\"reasoning\"] = {\"effort\": reasoning_effort}\n\n        # For non-OpenAI models enable web-search plugin\n        web_search_options = params.get(\"web_search_options\", None)\n        if web_search_options is not None and not \"openai\" in params[\"model\"]:\n            params.pop(\"web_search_options\")\n            web_pluging = {\"id\": \"web\"}\n            web_pluging.update(web_search_options)\n            plugins.append(web_pluging)\n\n        if plugins:\n            extra_body[\"plugins\"] = plugins\n\n        params[\"extra_body\"] = extra_body\n        params[\"extra_headers\"] = {\n            \"HTTP-Referer\": \"msgflux.com\",\n            \"X-Title\": \"msgflux\",\n        }\n        return params\n</code></pre>"},{"location":"api-reference/models/types/image_text_to_image/","title":"Image text to image","text":""},{"location":"api-reference/models/types/image_text_to_image/#msgflux.models.providers.openai.OpenAIImageTextToImage","title":"OpenAIImageTextToImage","text":"<p>               Bases: <code>OpenAITextToImage</code>, <code>ImageTextToImageModel</code></p> <p>OpenAI Image Edit.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAIImageTextToImage(OpenAITextToImage, ImageTextToImageModel):\n    \"\"\"OpenAI Image Edit.\"\"\"\n\n    def _execute_model(self, **kwargs):\n        model_output = self.client.images.edit(**kwargs, **self.sampling_run_params)\n        return model_output\n\n    async def _aexecute_model(self, **kwargs):\n        model_output = await self.aclient.images.edit(**kwargs, **self.sampling_run_params)\n        return model_output\n\n    def _prepare_inputs(self, image, mask):\n        inputs = {}\n        if isinstance(image, str):\n            image = [image]\n        inputs[\"image\"] = [encode_data_to_bytes(item) for item in image]\n        if mask:\n            inputs[\"mask\"] = encode_data_to_bytes(mask)\n        return inputs\n\n    @model_retry\n    def __call__(\n        self,\n        prompt: str,\n        image: Union[str, List[str]],\n        *,\n        mask: Optional[str] = None,\n        response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n        n: Optional[int] = 1,\n    ) -&gt; ModelResponse:\n        \"\"\"Args:\n        prompt:\n            A text description of the desired image(s).\n        image:\n            The image(s) to edit. Can be a path, an url or base64 string.\n        mask:\n            An additional image whose fully transparent areas\n            (e.g. where alpha is zero) indicate where image\n            should be edited. If there are multiple images provided,\n            the mask will be applied on the first image.\n        response_format:\n            Format in which images are returned.\n        n:\n            The number of images to generate.\n        \"\"\"\n        generation_params = dotdict(prompt=prompt, n=n, model=self.model_id)\n\n        if response_format is not None:\n            if response_format == \"base64\":\n                response_format = \"b64_json\"\n            generation_params.response_format = response_format\n\n        inputs = self._prepare_inputs(image, mask)\n        response = self._generate(**generation_params, **inputs)\n        return response\n\n    @model_retry\n    async def acall(\n        self,\n        prompt: str,\n        image: Union[str, List[str]],\n        *,\n        mask: Optional[str] = None,\n        response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n        n: Optional[int] = 1,\n    ) -&gt; ModelResponse:\n        \"\"\"Async version of __call__. Args:\n        prompt:\n            A text description of the desired image(s).\n        image:\n            The image(s) to edit. Can be a path, an url or base64 string.\n        mask:\n            An additional image whose fully transparent areas\n            (e.g. where alpha is zero) indicate where image\n            should be edited. If there are multiple images provided,\n            the mask will be applied on the first image.\n        response_format:\n            Format in which images are returned.\n        n:\n            The number of images to generate.\n        \"\"\"\n        generation_params = dotdict(prompt=prompt, n=n, model=self.model_id)\n\n        if response_format is not None:\n            if response_format == \"base64\":\n                response_format = \"b64_json\"\n            generation_params.response_format = response_format\n\n        inputs = self._prepare_inputs(image, mask)\n        response = await self._agenerate(**generation_params, **inputs)\n        return response\n</code></pre>"},{"location":"api-reference/models/types/image_text_to_image/#msgflux.models.providers.openai.OpenAIImageTextToImage.__call__","title":"__call__","text":"<pre><code>__call__(\n    prompt, image, *, mask=None, response_format=None, n=1\n)\n</code></pre> <p>prompt:     A text description of the desired image(s). image:     The image(s) to edit. Can be a path, an url or base64 string. mask:     An additional image whose fully transparent areas     (e.g. where alpha is zero) indicate where image     should be edited. If there are multiple images provided,     the mask will be applied on the first image. response_format:     Format in which images are returned. n:     The number of images to generate.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    prompt: str,\n    image: Union[str, List[str]],\n    *,\n    mask: Optional[str] = None,\n    response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n    n: Optional[int] = 1,\n) -&gt; ModelResponse:\n    \"\"\"Args:\n    prompt:\n        A text description of the desired image(s).\n    image:\n        The image(s) to edit. Can be a path, an url or base64 string.\n    mask:\n        An additional image whose fully transparent areas\n        (e.g. where alpha is zero) indicate where image\n        should be edited. If there are multiple images provided,\n        the mask will be applied on the first image.\n    response_format:\n        Format in which images are returned.\n    n:\n        The number of images to generate.\n    \"\"\"\n    generation_params = dotdict(prompt=prompt, n=n, model=self.model_id)\n\n    if response_format is not None:\n        if response_format == \"base64\":\n            response_format = \"b64_json\"\n        generation_params.response_format = response_format\n\n    inputs = self._prepare_inputs(image, mask)\n    response = self._generate(**generation_params, **inputs)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/image_text_to_image/#msgflux.models.providers.openai.OpenAIImageTextToImage.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(\n    prompt, image, *, mask=None, response_format=None, n=1\n)\n</code></pre> <p>Async version of call. Args: prompt:     A text description of the desired image(s). image:     The image(s) to edit. Can be a path, an url or base64 string. mask:     An additional image whose fully transparent areas     (e.g. where alpha is zero) indicate where image     should be edited. If there are multiple images provided,     the mask will be applied on the first image. response_format:     Format in which images are returned. n:     The number of images to generate.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    prompt: str,\n    image: Union[str, List[str]],\n    *,\n    mask: Optional[str] = None,\n    response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n    n: Optional[int] = 1,\n) -&gt; ModelResponse:\n    \"\"\"Async version of __call__. Args:\n    prompt:\n        A text description of the desired image(s).\n    image:\n        The image(s) to edit. Can be a path, an url or base64 string.\n    mask:\n        An additional image whose fully transparent areas\n        (e.g. where alpha is zero) indicate where image\n        should be edited. If there are multiple images provided,\n        the mask will be applied on the first image.\n    response_format:\n        Format in which images are returned.\n    n:\n        The number of images to generate.\n    \"\"\"\n    generation_params = dotdict(prompt=prompt, n=n, model=self.model_id)\n\n    if response_format is not None:\n        if response_format == \"base64\":\n            response_format = \"b64_json\"\n        generation_params.response_format = response_format\n\n    inputs = self._prepare_inputs(image, mask)\n    response = await self._agenerate(**generation_params, **inputs)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/moderation/","title":"Moderation","text":""},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration","title":"OpenAIModeration","text":"<p>               Bases: <code>_BaseOpenAI</code>, <code>ModerationModel</code></p> <p>OpenAI Moderation.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAIModeration(_BaseOpenAI, ModerationModel):\n    \"\"\"OpenAI Moderation.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        model_id: str,\n        base_url: Optional[str] = None,\n        enable_cache: Optional[bool] = False,\n        cache_size: Optional[int] = 128,\n    ):\n        \"\"\"Args:\n        model_id:\n            Model ID in provider.\n        base_url:\n            URL to model provider.\n        enable_cache:\n            If True, enables response caching to avoid redundant API calls.\n        cache_size:\n            Maximum number of responses to cache (default: 128).\n        \"\"\"\n        super().__init__()\n        self.model_id = model_id\n        self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n        self.enable_cache = enable_cache\n        self.cache_size = cache_size\n        self._initialize()\n        self._get_api_key()\n\n    def _execute_model(self, **kwargs):\n        model_output = self.client.moderations.create(**kwargs)\n        return model_output\n\n    async def _aexecute_model(self, **kwargs):\n        model_output = await self.aclient.moderations.create(**kwargs)\n        return model_output\n\n    def _generate(self, **kwargs):\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        response = ModelResponse()\n        response.set_response_type(\"moderation\")\n        model_output = self._execute_model(**kwargs)\n        moderation = dotdict({\"results\": model_output.results[0].model_dump()})\n        moderation.safe = not moderation.results.flagged\n        response.add(moderation)\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    async def _agenerate(self, **kwargs):\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        response = ModelResponse()\n        response.set_response_type(\"moderation\")\n        model_output = await self._aexecute_model(**kwargs)\n        moderation = dotdict({\"results\": model_output.results[0].model_dump()})\n        moderation.safe = not moderation.results.flagged\n        response.add(moderation)\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    @model_retry\n    def __call__(\n        self,\n        data: Union[str, List[Dict[str, Any]]],\n    ) -&gt; ModelResponse:\n        \"\"\"Args:\n        data:\n            Input (or inputs) to classify. Can be a single string,\n            an array of strings, or an array of multi-modal input\n            objects similar to other models.\n        \"\"\"\n        response = self._generate(input=data, model=self.model_id)\n        return response\n\n    @model_retry\n    async def acall(\n        self,\n        data: Union[str, List[Dict[str, Any]]],\n    ) -&gt; ModelResponse:\n        \"\"\"Async version of __call__. Args:\n        data:\n            Input (or inputs) to classify. Can be a single string,\n            an array of strings, or an array of multi-modal input\n            objects similar to other models.\n        \"\"\"\n        response = await self._agenerate(input=data, model=self.model_id)\n        return response\n</code></pre>"},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration.cache_size","title":"cache_size  <code>instance-attribute</code>","text":"<pre><code>cache_size = cache_size\n</code></pre>"},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration.enable_cache","title":"enable_cache  <code>instance-attribute</code>","text":"<pre><code>enable_cache = enable_cache\n</code></pre>"},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration.sampling_params","title":"sampling_params  <code>instance-attribute</code>","text":"<pre><code>sampling_params = {'base_url': base_url or _get_base_url()}\n</code></pre>"},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration.__call__","title":"__call__","text":"<pre><code>__call__(data)\n</code></pre> <p>data:     Input (or inputs) to classify. Can be a single string,     an array of strings, or an array of multi-modal input     objects similar to other models.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    data: Union[str, List[Dict[str, Any]]],\n) -&gt; ModelResponse:\n    \"\"\"Args:\n    data:\n        Input (or inputs) to classify. Can be a single string,\n        an array of strings, or an array of multi-modal input\n        objects similar to other models.\n    \"\"\"\n    response = self._generate(input=data, model=self.model_id)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    model_id,\n    base_url=None,\n    enable_cache=False,\n    cache_size=128,\n)\n</code></pre> <p>model_id:     Model ID in provider. base_url:     URL to model provider. enable_cache:     If True, enables response caching to avoid redundant API calls. cache_size:     Maximum number of responses to cache (default: 128).</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model_id: str,\n    base_url: Optional[str] = None,\n    enable_cache: Optional[bool] = False,\n    cache_size: Optional[int] = 128,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    base_url:\n        URL to model provider.\n    enable_cache:\n        If True, enables response caching to avoid redundant API calls.\n    cache_size:\n        Maximum number of responses to cache (default: 128).\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    self.enable_cache = enable_cache\n    self.cache_size = cache_size\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(data)\n</code></pre> <p>Async version of call. Args: data:     Input (or inputs) to classify. Can be a single string,     an array of strings, or an array of multi-modal input     objects similar to other models.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    data: Union[str, List[Dict[str, Any]]],\n) -&gt; ModelResponse:\n    \"\"\"Async version of __call__. Args:\n    data:\n        Input (or inputs) to classify. Can be a single string,\n        an array of strings, or an array of multi-modal input\n        objects similar to other models.\n    \"\"\"\n    response = await self._agenerate(input=data, model=self.model_id)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/speech_to_text/","title":"Speech to text","text":""},{"location":"api-reference/models/types/speech_to_text/#msgflux.models.providers.openai.OpenAISpeechToText","title":"OpenAISpeechToText","text":"<p>               Bases: <code>_BaseOpenAI</code>, <code>SpeechToTextModel</code></p> <p>OpenAI Speech to Text.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAISpeechToText(_BaseOpenAI, SpeechToTextModel):\n    \"\"\"OpenAI Speech to Text.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        model_id: str,\n        temperature: Optional[float] = 0.0,\n        base_url: Optional[str] = None,\n    ):\n        \"\"\"Args:\n        model_id:\n            Model ID in provider.\n        temperature:\n            The sampling temperature, between 0 and 1.\n        base_url:\n            URL to model provider.\n        \"\"\"\n        super().__init__()\n        self.model_id = model_id\n        self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n        self.sampling_run_params = {\"temperature\": temperature}\n        self._initialize()\n        self._get_api_key()\n\n    def _execute_model(self, **kwargs):\n        model_output = self.client.audio.transcriptions.create(\n            **kwargs, **self.sampling_run_params\n        )\n        return model_output\n\n    async def _aexecute_model(self, **kwargs):\n        model_output = await self.aclient.audio.transcriptions.create(\n            **kwargs, **self.sampling_run_params\n        )\n        return model_output\n\n    def _generate(self, **kwargs):\n        response = ModelResponse()\n\n        model_output = self._execute_model(**kwargs)\n\n        response.set_response_type(\"transcript\")\n\n        transcript = {}\n\n        if isinstance(model_output, str):\n            transcript[\"text\"] = model_output\n        else:\n            if model_output.text:\n                transcript[\"text\"] = model_output.text\n            if model_output.words:\n                words = [\n                    dict(word=w.word, start=w.start, end=w.end)\n                    for w in model_output.words\n                ]\n                transcript[\"words\"] = words\n            if model_output.segment:\n                segments = [\n                    dict(id=seg.id, start=seg.start, end=seg.end, text=seg.text)\n                    for seg in model_output.segments\n                ]\n                transcript[\"segments\"] = segments\n\n        response.add(transcript)\n\n        return response\n\n    async def _agenerate(self, **kwargs):\n        response = ModelResponse()\n\n        model_output = await self._aexecute_model(**kwargs)\n\n        response.set_response_type(\"transcript\")\n\n        transcript = {}\n\n        if isinstance(model_output, str):\n            transcript[\"text\"] = model_output\n        else:\n            if model_output.text:\n                transcript[\"text\"] = model_output.text\n            if model_output.words:\n                words = [\n                    dict(word=w.word, start=w.start, end=w.end)\n                    for w in model_output.words\n                ]\n                transcript[\"words\"] = words\n            if model_output.segment:\n                segments = [\n                    dict(id=seg.id, start=seg.start, end=seg.end, text=seg.text)\n                    for seg in model_output.segments\n                ]\n                transcript[\"segments\"] = segments\n\n        response.add(transcript)\n\n        return response\n\n    def _stream_generate(self, **kwargs):\n        stream_response = kwargs.pop(\"stream_response\")\n        stream_response.set_response_type(\"transcript\")\n\n        model_output = self._execute_model(**kwargs)\n\n        for event in model_output:\n            chunk = event.transcript.text.delta\n            if chunk:\n                stream_response.add(chunk)\n                if not stream_response.first_chunk_event.is_set():\n                    stream_response.first_chunk_event.set()\n            elif event.transcript.text.done:\n                stream_response.add(None)\n\n        return stream_response\n\n    async def _astream_generate(self, **kwargs):\n        stream_response = kwargs.pop(\"stream_response\")\n        stream_response.set_response_type(\"transcript\")\n\n        model_output = await self._aexecute_model(**kwargs)\n\n        async for event in model_output:\n            chunk = event.transcript.text.delta\n            if chunk:\n                stream_response.add(chunk)\n                if not stream_response.first_chunk_event.is_set():\n                    stream_response.first_chunk_event.set()\n            elif event.transcript.text.done:\n                stream_response.add(None)\n\n        return stream_response\n\n    @model_retry\n    def __call__(\n        self,\n        data: str,\n        *,\n        stream: Optional[bool] = False,\n        response_format: Optional[\n            Literal[\"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"]\n        ] = \"text\",\n        timestamp_granularities: Optional[List[str]] = None,\n        prompt: Optional[str] = None,\n        language: Optional[str] = None,\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Args:\n        data:\n            Url, path, base64 to audio.\n        stream:\n            Whether generation should be in streaming mode.\n        response_format:\n            The format of the output, in one of these options:\n            json, text, srt, verbose_json, or vtt.\n        timestamp_granularities:\n            The timestamp granularities to populate for this\n            transcription. `response_format` must be set `verbose_json`\n            to use timestamp granularities. Either or both of these\n            options are supported: word, or segment. Note: There is no\n            additional latency for segment timestamps, but generating\n            word timestamps incurs additional latency.\n        prompt:\n            An optional text to guide the model's style or continue a\n            previous audio segment. The prompt should match the audio language.\n        language:\n            The language of the input audio. Supplying the input language in\n            ISO-639-1 (e.g. en) format will improve accuracy and latency.\n        \"\"\"\n        file = encode_data_to_bytes(data)\n        params = dict(\n            file=file,\n            language=language,\n            response_format=response_format,\n            timestamp_granularities=timestamp_granularities,\n            prompt=prompt,\n            model=self.model_id\n        )\n        if stream:\n            stream_response = ModelStreamResponse()\n            params[\"stream_response\"] = stream_response\n            params[\"stream\"] = stream\n            F.background_task(self._stream_generate, **params)\n            F.wait_for_event(stream_response.first_chunk_event)\n            return stream_response\n        else:\n            response = self._generate(**params)\n            return response\n\n    @model_retry\n    async def acall(\n        self,\n        data: str,\n        *,\n        stream: Optional[bool] = False,\n        response_format: Optional[\n            Literal[\"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"]\n        ] = \"text\",\n        timestamp_granularities: Optional[List[str]] = None,\n        prompt: Optional[str] = None,\n        language: Optional[str] = None,\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Async version of __call__. Args:\n        data:\n            Url, path, base64 to audio.\n        stream:\n            Whether generation should be in streaming mode.\n        response_format:\n            The format of the output, in one of these options:\n            json, text, srt, verbose_json, or vtt.\n        timestamp_granularities:\n            The timestamp granularities to populate for this\n            transcription. `response_format` must be set `verbose_json`\n            to use timestamp granularities. Either or both of these\n            options are supported: word, or segment. Note: There is no\n            additional latency for segment timestamps, but generating\n            word timestamps incurs additional latency.\n        prompt:\n            An optional text to guide the model's style or continue a\n            previous audio segment. The prompt should match the audio language.\n        language:\n            The language of the input audio. Supplying the input language in\n            ISO-639-1 (e.g. en) format will improve accuracy and latency.\n        \"\"\"\n        file = encode_data_to_bytes(data)\n        params = dict(\n            file=file,\n            language=language,\n            response_format=response_format,\n            timestamp_granularities=timestamp_granularities,\n            prompt=prompt,\n            model=self.model_id\n        )\n        if stream:\n            stream_response = ModelStreamResponse()\n            params[\"stream_response\"] = stream_response\n            params[\"stream\"] = stream\n            await F.abackground_task(self._astream_generate, **params)\n            await F.await_for_event(stream_response.first_chunk_event)\n            return stream_response\n        else:\n            response = await self._agenerate(**params)\n            return response\n</code></pre>"},{"location":"api-reference/models/types/speech_to_text/#msgflux.models.providers.openai.OpenAISpeechToText.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"api-reference/models/types/speech_to_text/#msgflux.models.providers.openai.OpenAISpeechToText.sampling_params","title":"sampling_params  <code>instance-attribute</code>","text":"<pre><code>sampling_params = {'base_url': base_url or _get_base_url()}\n</code></pre>"},{"location":"api-reference/models/types/speech_to_text/#msgflux.models.providers.openai.OpenAISpeechToText.sampling_run_params","title":"sampling_run_params  <code>instance-attribute</code>","text":"<pre><code>sampling_run_params = {'temperature': temperature}\n</code></pre>"},{"location":"api-reference/models/types/speech_to_text/#msgflux.models.providers.openai.OpenAISpeechToText.__call__","title":"__call__","text":"<pre><code>__call__(\n    data,\n    *,\n    stream=False,\n    response_format=\"text\",\n    timestamp_granularities=None,\n    prompt=None,\n    language=None,\n)\n</code></pre> <p>data:     Url, path, base64 to audio. stream:     Whether generation should be in streaming mode. response_format:     The format of the output, in one of these options:     json, text, srt, verbose_json, or vtt. timestamp_granularities:     The timestamp granularities to populate for this     transcription. <code>response_format</code> must be set <code>verbose_json</code>     to use timestamp granularities. Either or both of these     options are supported: word, or segment. Note: There is no     additional latency for segment timestamps, but generating     word timestamps incurs additional latency. prompt:     An optional text to guide the model's style or continue a     previous audio segment. The prompt should match the audio language. language:     The language of the input audio. Supplying the input language in     ISO-639-1 (e.g. en) format will improve accuracy and latency.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    data: str,\n    *,\n    stream: Optional[bool] = False,\n    response_format: Optional[\n        Literal[\"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"]\n    ] = \"text\",\n    timestamp_granularities: Optional[List[str]] = None,\n    prompt: Optional[str] = None,\n    language: Optional[str] = None,\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Args:\n    data:\n        Url, path, base64 to audio.\n    stream:\n        Whether generation should be in streaming mode.\n    response_format:\n        The format of the output, in one of these options:\n        json, text, srt, verbose_json, or vtt.\n    timestamp_granularities:\n        The timestamp granularities to populate for this\n        transcription. `response_format` must be set `verbose_json`\n        to use timestamp granularities. Either or both of these\n        options are supported: word, or segment. Note: There is no\n        additional latency for segment timestamps, but generating\n        word timestamps incurs additional latency.\n    prompt:\n        An optional text to guide the model's style or continue a\n        previous audio segment. The prompt should match the audio language.\n    language:\n        The language of the input audio. Supplying the input language in\n        ISO-639-1 (e.g. en) format will improve accuracy and latency.\n    \"\"\"\n    file = encode_data_to_bytes(data)\n    params = dict(\n        file=file,\n        language=language,\n        response_format=response_format,\n        timestamp_granularities=timestamp_granularities,\n        prompt=prompt,\n        model=self.model_id\n    )\n    if stream:\n        stream_response = ModelStreamResponse()\n        params[\"stream_response\"] = stream_response\n        params[\"stream\"] = stream\n        F.background_task(self._stream_generate, **params)\n        F.wait_for_event(stream_response.first_chunk_event)\n        return stream_response\n    else:\n        response = self._generate(**params)\n        return response\n</code></pre>"},{"location":"api-reference/models/types/speech_to_text/#msgflux.models.providers.openai.OpenAISpeechToText.__init__","title":"__init__","text":"<pre><code>__init__(*, model_id, temperature=0.0, base_url=None)\n</code></pre> <p>model_id:     Model ID in provider. temperature:     The sampling temperature, between 0 and 1. base_url:     URL to model provider.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model_id: str,\n    temperature: Optional[float] = 0.0,\n    base_url: Optional[str] = None,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    temperature:\n        The sampling temperature, between 0 and 1.\n    base_url:\n        URL to model provider.\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    self.sampling_run_params = {\"temperature\": temperature}\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"api-reference/models/types/speech_to_text/#msgflux.models.providers.openai.OpenAISpeechToText.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(\n    data,\n    *,\n    stream=False,\n    response_format=\"text\",\n    timestamp_granularities=None,\n    prompt=None,\n    language=None,\n)\n</code></pre> <p>Async version of call. Args: data:     Url, path, base64 to audio. stream:     Whether generation should be in streaming mode. response_format:     The format of the output, in one of these options:     json, text, srt, verbose_json, or vtt. timestamp_granularities:     The timestamp granularities to populate for this     transcription. <code>response_format</code> must be set <code>verbose_json</code>     to use timestamp granularities. Either or both of these     options are supported: word, or segment. Note: There is no     additional latency for segment timestamps, but generating     word timestamps incurs additional latency. prompt:     An optional text to guide the model's style or continue a     previous audio segment. The prompt should match the audio language. language:     The language of the input audio. Supplying the input language in     ISO-639-1 (e.g. en) format will improve accuracy and latency.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    data: str,\n    *,\n    stream: Optional[bool] = False,\n    response_format: Optional[\n        Literal[\"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"]\n    ] = \"text\",\n    timestamp_granularities: Optional[List[str]] = None,\n    prompt: Optional[str] = None,\n    language: Optional[str] = None,\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Async version of __call__. Args:\n    data:\n        Url, path, base64 to audio.\n    stream:\n        Whether generation should be in streaming mode.\n    response_format:\n        The format of the output, in one of these options:\n        json, text, srt, verbose_json, or vtt.\n    timestamp_granularities:\n        The timestamp granularities to populate for this\n        transcription. `response_format` must be set `verbose_json`\n        to use timestamp granularities. Either or both of these\n        options are supported: word, or segment. Note: There is no\n        additional latency for segment timestamps, but generating\n        word timestamps incurs additional latency.\n    prompt:\n        An optional text to guide the model's style or continue a\n        previous audio segment. The prompt should match the audio language.\n    language:\n        The language of the input audio. Supplying the input language in\n        ISO-639-1 (e.g. en) format will improve accuracy and latency.\n    \"\"\"\n    file = encode_data_to_bytes(data)\n    params = dict(\n        file=file,\n        language=language,\n        response_format=response_format,\n        timestamp_granularities=timestamp_granularities,\n        prompt=prompt,\n        model=self.model_id\n    )\n    if stream:\n        stream_response = ModelStreamResponse()\n        params[\"stream_response\"] = stream_response\n        params[\"stream\"] = stream\n        await F.abackground_task(self._astream_generate, **params)\n        await F.await_for_event(stream_response.first_chunk_event)\n        return stream_response\n    else:\n        response = await self._agenerate(**params)\n        return response\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/","title":"Text embedder","text":""},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder","title":"OpenAITextEmbedder","text":"<p>               Bases: <code>_BaseOpenAI</code>, <code>TextEmbedderModel</code></p> <p>OpenAI Text Embedder.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAITextEmbedder(_BaseOpenAI, TextEmbedderModel):\n    \"\"\"OpenAI Text Embedder.\"\"\"\n\n    batch_support: bool = True\n\n    def __init__(\n        self,\n        *,\n        model_id: str,\n        dimensions: Optional[int] = None,\n        base_url: Optional[str] = None,\n        enable_cache: Optional[bool] = False,\n        cache_size: Optional[int] = 128,\n    ):\n        \"\"\"Args:\n        model_id:\n            Model ID in provider.\n        dimensions:\n            The number of dimensions the resulting output embeddings should have.\n        base_url:\n            URL to model provider.\n        enable_cache:\n            If True, enables response caching to avoid redundant API calls.\n        cache_size:\n            Maximum number of responses to cache (default: 128).\n        \"\"\"\n        super().__init__()\n        self.model_id = model_id\n        self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n        self.sampling_run_params = {\"dimensions\": dimensions}\n        self.enable_cache = enable_cache\n        self.cache_size = cache_size\n        self._initialize()\n        self._get_api_key()\n\n    def _execute_model(self, **kwargs):\n        model_output = self.client.embeddings.create(\n            **kwargs, **self.sampling_run_params,\n        )\n        return model_output\n\n    async def _aexecute_model(self, **kwargs):\n        model_output = await self.aclient.embeddings.create(\n            **kwargs, **self.sampling_run_params,\n        )\n        return model_output\n\n    def _generate(self, **kwargs):\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        response = ModelResponse()\n        response.set_response_type(\"text_embedding\")\n        model_output = self._execute_model(**kwargs)\n        embeddings = [item.embedding for item in model_output.data]\n        metadata = dotdict({\"usage\": model_output.usage.to_dict()})\n        response.add(embeddings)\n        response.set_metadata(metadata)\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    async def _agenerate(self, **kwargs):\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        response = ModelResponse()\n        response.set_response_type(\"text_embedding\")\n        model_output = await self._aexecute_model(**kwargs)\n        embeddings = [item.embedding for item in model_output.data]\n        metadata = dotdict({\"usage\": model_output.usage.to_dict()})\n        response.add(embeddings)\n        response.set_metadata(metadata)\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    @model_retry\n    def __call__(\n        self,\n        data: Union[str, List[str]],\n    ):\n        \"\"\"Args:\n        data:\n            Input text to embed.\n        \"\"\"\n        response = self._generate(input=data, model=self.model_id)\n        return response\n\n    @model_retry\n    async def acall(\n        self,\n        data: Union[str, List[str]],\n    ):\n        \"\"\"Async version of __call__. Args:\n        data:\n            Input text to embed.\n        \"\"\"\n        response = await self._agenerate(input=data, model=self.model_id)\n        return response\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.batch_support","title":"batch_support  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_support = True\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.cache_size","title":"cache_size  <code>instance-attribute</code>","text":"<pre><code>cache_size = cache_size\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.enable_cache","title":"enable_cache  <code>instance-attribute</code>","text":"<pre><code>enable_cache = enable_cache\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.sampling_params","title":"sampling_params  <code>instance-attribute</code>","text":"<pre><code>sampling_params = {'base_url': base_url or _get_base_url()}\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.sampling_run_params","title":"sampling_run_params  <code>instance-attribute</code>","text":"<pre><code>sampling_run_params = {'dimensions': dimensions}\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.__call__","title":"__call__","text":"<pre><code>__call__(data)\n</code></pre> <p>data:     Input text to embed.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    data: Union[str, List[str]],\n):\n    \"\"\"Args:\n    data:\n        Input text to embed.\n    \"\"\"\n    response = self._generate(input=data, model=self.model_id)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    model_id,\n    dimensions=None,\n    base_url=None,\n    enable_cache=False,\n    cache_size=128,\n)\n</code></pre> <p>model_id:     Model ID in provider. dimensions:     The number of dimensions the resulting output embeddings should have. base_url:     URL to model provider. enable_cache:     If True, enables response caching to avoid redundant API calls. cache_size:     Maximum number of responses to cache (default: 128).</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model_id: str,\n    dimensions: Optional[int] = None,\n    base_url: Optional[str] = None,\n    enable_cache: Optional[bool] = False,\n    cache_size: Optional[int] = 128,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    dimensions:\n        The number of dimensions the resulting output embeddings should have.\n    base_url:\n        URL to model provider.\n    enable_cache:\n        If True, enables response caching to avoid redundant API calls.\n    cache_size:\n        Maximum number of responses to cache (default: 128).\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    self.sampling_run_params = {\"dimensions\": dimensions}\n    self.enable_cache = enable_cache\n    self.cache_size = cache_size\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(data)\n</code></pre> <p>Async version of call. Args: data:     Input text to embed.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    data: Union[str, List[str]],\n):\n    \"\"\"Async version of __call__. Args:\n    data:\n        Input text to embed.\n    \"\"\"\n    response = await self._agenerate(input=data, model=self.model_id)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/text_to_image/","title":"Text to image","text":""},{"location":"api-reference/models/types/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage","title":"OpenAITextToImage","text":"<p>               Bases: <code>_BaseOpenAI</code>, <code>TextToImageModel</code></p> <p>OpenAI Image Generation.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAITextToImage(_BaseOpenAI, TextToImageModel):\n    \"\"\"OpenAI Image Generation.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        model_id: str,\n        moderation: Optional[Literal[\"auto\", \"low\"]] = None,\n        base_url: Optional[str] = None,\n    ):\n        \"\"\"Args:\n        model_id:\n            Model ID in provider.\n        moderation:\n            Control the content-moderation level for images generated.\n        base_url:\n            URL to model provider.\n        \"\"\"\n        super().__init__()\n        self.model_id = model_id\n        self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n        sampling_run_params = {}\n        if moderation:\n            sampling_run_params[\"moderation\"] = moderation\n        self.sampling_run_params = sampling_run_params\n        self._initialize()\n        self._get_api_key()\n\n    def _execute_model(self, **kwargs):\n        model_output = self.client.images.generate(**kwargs, **self.sampling_run_params)\n        return model_output\n\n    async def _aexecute_model(self, **kwargs):\n        model_output = await self.aclient.images.generate(**kwargs, **self.sampling_run_params)\n        return model_output\n\n    def _get_metadata(self, model_output):\n        metadata = dotdict(\n            usage=model_output.usage.to_dict(),\n            details=dict(\n                size=model_output.size,\n                quality=model_output.quality,\n                output_format=model_output.output_format,\n                background=model_output.background,\n            )            \n        )\n        return metadata\n\n    def _generate(self, **kwargs):\n        response = ModelResponse()\n        response.set_response_type(\"image_generation\")\n\n        model_output = self._execute_model(**kwargs)\n\n        metadata = self._get_metadata(model_output)\n\n        images = []\n        for item in model_output.data:\n            if item.url:\n                images.append(item.url)\n            if item.b64_json:\n                images.append(item.b64_json)\n\n        if len(images) == 1:\n            images = images[0]\n\n        response.add(images)\n        response.set_metadata(metadata)\n\n        return response\n\n    async def _agenerate(self, **kwargs):\n        response = ModelResponse()\n        response.set_response_type(\"image_generation\")\n\n        model_output = await self._aexecute_model(**kwargs)\n\n        metadata = self._get_metadata(model_output)\n\n        images = []\n        for item in model_output.data:\n            if item.url:\n                images.append(item.url)\n            if item.b64_json:\n                images.append(item.b64_json)\n\n        if len(images) == 1:\n            images = images[0]\n\n        response.add(images)\n        response.set_metadata(metadata)\n\n        return response\n\n    @model_retry\n    def __call__(\n        self,\n        prompt: str,\n        *,\n        response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n        n: Optional[int] = 1,\n        size: Optional[str] = \"auto\",\n        quality: Optional[str] = \"auto\",\n        background: Optional[Literal[\"transparent\", \"opaque\", \"auto\"]] = None,        \n    ) -&gt; ModelResponse:\n        \"\"\"Args:\n        prompt:\n            A text description of the desired image(s).\n        response_format:\n            Format in which images are returned.\n        n:\n            The number of images to generate.\n        size:\n            The size of the generated images.\n        quality:\n            The quality of the image that will be generated.\n        background:\n            Allows to set transparency for the background of the generated image(s).            \n        \"\"\"\n        generation_params = dotdict(\n            prompt=prompt,\n            n=n,\n            size=size,\n            quality=quality,\n            background=background,\n            model=self.model_id\n        )\n\n        if response_format is not None:\n            if response_format == \"base64\":\n                response_format = \"b64_json\"\n            generation_params.response_format = response_format\n\n        response = self._generate(**generation_params)\n        return response\n\n    @model_retry\n    async def acall(\n        self,\n        prompt: str,\n        *,\n        response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n        n: Optional[int] = 1,\n        size: Optional[str] = \"auto\",\n        quality: Optional[str] = \"auto\",\n        background: Optional[Literal[\"transparent\", \"opaque\", \"auto\"]] = None,\n    ) -&gt; ModelResponse:\n        \"\"\"Async version of __call__. Args:\n        prompt:\n            A text description of the desired image(s).\n        response_format:\n            Format in which images are returned.\n        n:\n            The number of images to generate.\n        size:\n            The size of the generated images.\n        quality:\n            The quality of the image that will be generated.\n        background:\n            Allows to set transparency for the background of the generated image(s).\n        \"\"\"\n        generation_params = dotdict(\n            prompt=prompt,\n            n=n,\n            size=size,\n            quality=quality,\n            background=background,\n            model=self.model_id\n        )\n\n        if response_format is not None:\n            if response_format == \"base64\":\n                response_format = \"b64_json\"\n            generation_params.response_format = response_format\n\n        response = await self._agenerate(**generation_params)\n        return response\n</code></pre>"},{"location":"api-reference/models/types/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"api-reference/models/types/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage.sampling_params","title":"sampling_params  <code>instance-attribute</code>","text":"<pre><code>sampling_params = {'base_url': base_url or _get_base_url()}\n</code></pre>"},{"location":"api-reference/models/types/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage.sampling_run_params","title":"sampling_run_params  <code>instance-attribute</code>","text":"<pre><code>sampling_run_params = sampling_run_params\n</code></pre>"},{"location":"api-reference/models/types/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage.__call__","title":"__call__","text":"<pre><code>__call__(\n    prompt,\n    *,\n    response_format=None,\n    n=1,\n    size=\"auto\",\n    quality=\"auto\",\n    background=None,\n)\n</code></pre> <p>prompt:     A text description of the desired image(s). response_format:     Format in which images are returned. n:     The number of images to generate. size:     The size of the generated images. quality:     The quality of the image that will be generated. background:     Allows to set transparency for the background of the generated image(s).</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    prompt: str,\n    *,\n    response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n    n: Optional[int] = 1,\n    size: Optional[str] = \"auto\",\n    quality: Optional[str] = \"auto\",\n    background: Optional[Literal[\"transparent\", \"opaque\", \"auto\"]] = None,        \n) -&gt; ModelResponse:\n    \"\"\"Args:\n    prompt:\n        A text description of the desired image(s).\n    response_format:\n        Format in which images are returned.\n    n:\n        The number of images to generate.\n    size:\n        The size of the generated images.\n    quality:\n        The quality of the image that will be generated.\n    background:\n        Allows to set transparency for the background of the generated image(s).            \n    \"\"\"\n    generation_params = dotdict(\n        prompt=prompt,\n        n=n,\n        size=size,\n        quality=quality,\n        background=background,\n        model=self.model_id\n    )\n\n    if response_format is not None:\n        if response_format == \"base64\":\n            response_format = \"b64_json\"\n        generation_params.response_format = response_format\n\n    response = self._generate(**generation_params)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage.__init__","title":"__init__","text":"<pre><code>__init__(*, model_id, moderation=None, base_url=None)\n</code></pre> <p>model_id:     Model ID in provider. moderation:     Control the content-moderation level for images generated. base_url:     URL to model provider.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model_id: str,\n    moderation: Optional[Literal[\"auto\", \"low\"]] = None,\n    base_url: Optional[str] = None,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    moderation:\n        Control the content-moderation level for images generated.\n    base_url:\n        URL to model provider.\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    sampling_run_params = {}\n    if moderation:\n        sampling_run_params[\"moderation\"] = moderation\n    self.sampling_run_params = sampling_run_params\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"api-reference/models/types/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(\n    prompt,\n    *,\n    response_format=None,\n    n=1,\n    size=\"auto\",\n    quality=\"auto\",\n    background=None,\n)\n</code></pre> <p>Async version of call. Args: prompt:     A text description of the desired image(s). response_format:     Format in which images are returned. n:     The number of images to generate. size:     The size of the generated images. quality:     The quality of the image that will be generated. background:     Allows to set transparency for the background of the generated image(s).</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    prompt: str,\n    *,\n    response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n    n: Optional[int] = 1,\n    size: Optional[str] = \"auto\",\n    quality: Optional[str] = \"auto\",\n    background: Optional[Literal[\"transparent\", \"opaque\", \"auto\"]] = None,\n) -&gt; ModelResponse:\n    \"\"\"Async version of __call__. Args:\n    prompt:\n        A text description of the desired image(s).\n    response_format:\n        Format in which images are returned.\n    n:\n        The number of images to generate.\n    size:\n        The size of the generated images.\n    quality:\n        The quality of the image that will be generated.\n    background:\n        Allows to set transparency for the background of the generated image(s).\n    \"\"\"\n    generation_params = dotdict(\n        prompt=prompt,\n        n=n,\n        size=size,\n        quality=quality,\n        background=background,\n        model=self.model_id\n    )\n\n    if response_format is not None:\n        if response_format == \"base64\":\n            response_format = \"b64_json\"\n        generation_params.response_format = response_format\n\n    response = await self._agenerate(**generation_params)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/text_to_speech/","title":"Text to speech","text":""},{"location":"api-reference/models/types/text_to_speech/#msgflux.models.providers.openai.OpenAITextToSpeech","title":"OpenAITextToSpeech","text":"<p>               Bases: <code>_BaseOpenAI</code>, <code>TextToSpeechModel</code></p> <p>OpenAI Text to Speech.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAITextToSpeech(_BaseOpenAI, TextToSpeechModel):\n    \"\"\"OpenAI Text to Speech.\"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        voice: Optional[str] = \"alloy\",\n        speed: Optional[float] = 1.0,\n        base_url: Optional[str] = None,\n    ):\n        \"\"\"Args:\n        model_id:\n            Model ID in provider.\n        voice:\n            The voice to use when generating the audio.\n        speed:\n            the speed of the generated audio. Select a value\n            from 0.25 to 4.0. 1.0 is the default.\n        base_url:\n            URL to model provider.\n        \"\"\"\n        super().__init__()\n        self.model_id = model_id\n        self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n        self.sampling_run_params = {\n            \"voice\": voice,\n            \"speed\": speed,\n        }\n        self._initialize()\n        self._get_api_key()\n\n    @contextmanager\n    def _execute_model(self, **kwargs):\n        with self.client.audio.speech.with_streaming_response.create(\n            model=self.model_id, **kwargs, **self.sampling_run_params\n        ) as model_output:\n            yield model_output\n\n    @asynccontextmanager\n    async def _aexecute_model(self, **kwargs):\n        async with self.aclient.audio.speech.with_streaming_response.create(\n            model=self.model_id, **kwargs, **self.sampling_run_params\n        ) as model_output:\n            yield model_output\n\n    def _generate(self, **kwargs):\n        response = ModelResponse()\n\n        with self._execute_model(**kwargs) as model_output:\n            with tempfile.NamedTemporaryFile(\n                suffix=f\".{kwargs.get('response_format')}\", delete=False\n            ) as temp_file:\n                temp_file_path = temp_file.name\n                model_output.stream_to_file(temp_file_path)\n\n            response.set_response_type(\"audio_generation\")\n            response.add(temp_file_path)\n\n        return response\n\n    async def _agenerate(self, **kwargs):\n        response = ModelResponse()\n\n        async with self._aexecute_model(**kwargs) as model_output:\n            with tempfile.NamedTemporaryFile(\n                suffix=f\".{kwargs.get('response_format')}\", delete=False\n            ) as temp_file:\n                temp_file_path = temp_file.name\n                await model_output.astream_to_file(temp_file_path)\n\n            response.set_response_type(\"audio_generation\")\n            response.add(temp_file_path)\n\n        return response\n\n    def _stream_generate(self, **kwargs):\n        stream_response = kwargs.pop(\"stream_response\")\n        stream_response.set_response_type(\"audio_generation\")\n\n        with self._execute_model(**kwargs) as model_output:\n            for chunk in model_output.iter_bytes(chunk_size=1024):\n                stream_response.add(chunk)\n                if not stream_response.first_chunk_event.is_set():\n                    stream_response.first_chunk_event.set()\n\n        stream_response.add(None)\n\n    async def _astream_generate(self, **kwargs):\n        stream_response = kwargs.pop(\"stream_response\")\n        stream_response.set_response_type(\"audio_generation\")\n\n        async with self._aexecute_model(**kwargs) as model_output:\n            async for chunk in model_output.aiter_bytes(chunk_size=1024):\n                stream_response.add(chunk)\n                if not stream_response.first_chunk_event.is_set():\n                    stream_response.first_chunk_event.set()\n\n        stream_response.add(None)\n\n    @model_retry\n    def __call__(\n        self,\n        data: str,\n        *,\n        stream: Optional[bool] = False,\n        prompt: Optional[str] = None,\n        response_format: Optional[\n            Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"]\n        ] = \"opus\",\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Args:\n        data:\n            The text to generate audio for.\n        stream:\n            Whether generation should be in streaming mode.\n        prompt:\n            Control the voice of your generated audio with additional instructions.\n        response_format:\n            The format to audio in.\n        \"\"\"\n        params = dotdict({\"input\": data, \"response_format\": response_format})\n        if prompt:\n            params.instructions = prompt\n        if stream:\n            stream_response = ModelStreamResponse()\n            params.stream_response = stream_response\n            F.background_task(self._stream_generate, **params)\n            F.wait_for_event(stream_response.first_chunk_event)\n            return stream_response\n        else:\n            response = self._generate(**params)\n            return response\n\n    @model_retry\n    async def acall(\n        self,\n        data: str,\n        *,\n        stream: Optional[bool] = False,\n        prompt: Optional[str] = None,\n        response_format: Optional[\n            Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"]\n        ] = \"opus\",\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Async version of __call__. Args:\n        data:\n            The text to generate audio for.\n        stream:\n            Whether generation should be in streaming mode.\n        prompt:\n            Control the voice of your generated audio with additional instructions.\n        response_format:\n            The format to audio in.\n        \"\"\"\n        params = dotdict({\"input\": data, \"response_format\": response_format})\n        if prompt:\n            params.instructions = prompt\n        if stream:\n            stream_response = ModelStreamResponse()\n            params.stream_response = stream_response\n            await F.abackground_task(self._astream_generate, **params)\n            await F.await_for_event(stream_response.first_chunk_event)\n            return stream_response\n        else:\n            response = await self._agenerate(**params)\n            return response\n</code></pre>"},{"location":"api-reference/models/types/text_to_speech/#msgflux.models.providers.openai.OpenAITextToSpeech.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"api-reference/models/types/text_to_speech/#msgflux.models.providers.openai.OpenAITextToSpeech.sampling_params","title":"sampling_params  <code>instance-attribute</code>","text":"<pre><code>sampling_params = {'base_url': base_url or _get_base_url()}\n</code></pre>"},{"location":"api-reference/models/types/text_to_speech/#msgflux.models.providers.openai.OpenAITextToSpeech.sampling_run_params","title":"sampling_run_params  <code>instance-attribute</code>","text":"<pre><code>sampling_run_params = {'voice': voice, 'speed': speed}\n</code></pre>"},{"location":"api-reference/models/types/text_to_speech/#msgflux.models.providers.openai.OpenAITextToSpeech.__call__","title":"__call__","text":"<pre><code>__call__(\n    data,\n    *,\n    stream=False,\n    prompt=None,\n    response_format=\"opus\",\n)\n</code></pre> <p>data:     The text to generate audio for. stream:     Whether generation should be in streaming mode. prompt:     Control the voice of your generated audio with additional instructions. response_format:     The format to audio in.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    data: str,\n    *,\n    stream: Optional[bool] = False,\n    prompt: Optional[str] = None,\n    response_format: Optional[\n        Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"]\n    ] = \"opus\",\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Args:\n    data:\n        The text to generate audio for.\n    stream:\n        Whether generation should be in streaming mode.\n    prompt:\n        Control the voice of your generated audio with additional instructions.\n    response_format:\n        The format to audio in.\n    \"\"\"\n    params = dotdict({\"input\": data, \"response_format\": response_format})\n    if prompt:\n        params.instructions = prompt\n    if stream:\n        stream_response = ModelStreamResponse()\n        params.stream_response = stream_response\n        F.background_task(self._stream_generate, **params)\n        F.wait_for_event(stream_response.first_chunk_event)\n        return stream_response\n    else:\n        response = self._generate(**params)\n        return response\n</code></pre>"},{"location":"api-reference/models/types/text_to_speech/#msgflux.models.providers.openai.OpenAITextToSpeech.__init__","title":"__init__","text":"<pre><code>__init__(model_id, voice='alloy', speed=1.0, base_url=None)\n</code></pre> <p>model_id:     Model ID in provider. voice:     The voice to use when generating the audio. speed:     the speed of the generated audio. Select a value     from 0.25 to 4.0. 1.0 is the default. base_url:     URL to model provider.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    model_id: str,\n    voice: Optional[str] = \"alloy\",\n    speed: Optional[float] = 1.0,\n    base_url: Optional[str] = None,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    voice:\n        The voice to use when generating the audio.\n    speed:\n        the speed of the generated audio. Select a value\n        from 0.25 to 4.0. 1.0 is the default.\n    base_url:\n        URL to model provider.\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    self.sampling_run_params = {\n        \"voice\": voice,\n        \"speed\": speed,\n    }\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"api-reference/models/types/text_to_speech/#msgflux.models.providers.openai.OpenAITextToSpeech.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(\n    data,\n    *,\n    stream=False,\n    prompt=None,\n    response_format=\"opus\",\n)\n</code></pre> <p>Async version of call. Args: data:     The text to generate audio for. stream:     Whether generation should be in streaming mode. prompt:     Control the voice of your generated audio with additional instructions. response_format:     The format to audio in.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    data: str,\n    *,\n    stream: Optional[bool] = False,\n    prompt: Optional[str] = None,\n    response_format: Optional[\n        Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"]\n    ] = \"opus\",\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Async version of __call__. Args:\n    data:\n        The text to generate audio for.\n    stream:\n        Whether generation should be in streaming mode.\n    prompt:\n        Control the voice of your generated audio with additional instructions.\n    response_format:\n        The format to audio in.\n    \"\"\"\n    params = dotdict({\"input\": data, \"response_format\": response_format})\n    if prompt:\n        params.instructions = prompt\n    if stream:\n        stream_response = ModelStreamResponse()\n        params.stream_response = stream_response\n        await F.abackground_task(self._astream_generate, **params)\n        await F.await_for_event(stream_response.first_chunk_event)\n        return stream_response\n    else:\n        response = await self._agenerate(**params)\n        return response\n</code></pre>"},{"location":"api-reference/nn/functional/","title":"Functional","text":""},{"location":"api-reference/nn/functional/#msgflux.nn.functional.scatter_gather","title":"scatter_gather","text":"<pre><code>scatter_gather(\n    to_send,\n    args_list=None,\n    kwargs_list=None,\n    *,\n    timeout=None,\n)\n</code></pre> <p>Sends different sets of arguments/kwargs to a list of modules and collects the responses.</p> <p>Each callable in <code>to_send</code> receives the positional arguments of the corresponding <code>tuple</code> in <code>args_list</code> and the named arguments of the corresponding <code>dict</code> in <code>kwargs_list</code>. If <code>args_list</code> or <code>kwargs_list</code> are not provided (or are <code>None</code>), the corresponding callables will be called without positional or named arguments, respectively, unless an empty list (<code>[]</code>) or empty tuple (<code>()</code>) is provided for a specific item.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>args_list</code> <code>Optional[List[Tuple[Any, ...]]]</code> <p>Each tuple contains the positional argumentsvfor the corresponding callable in <code>to_send</code>. If <code>None</code>, no positional arguments are passed unless specified individually by an item in <code>kwargs_list</code>.</p> <code>None</code> <code>kwargs_list</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Each dictionary contains the named arguments for the corresponding callable in <code>to_send</code>. If <code>None</code>, no named arguments are passed unless specified individually by an item in <code>args_list</code>.</p> <code>None</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing the responses for each callable. If an error or</p> <code>...</code> <p>timeout occurs for a specific callable, its corresponding response</p> <code>Tuple[Any, ...]</code> <p>in the tuple will be <code>None</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a callable list.</p> <code>ValueError</code> <p>If the lengths of <code>args_list</code> (if provided) or <code>kwargs_list</code> (if provided) do not match the length of <code>to_send</code>.</p> <p>Examples:</p> <p>def add(x, y): return x + y def multiply(x, y=2): return x * y callables = [add, multiply, add]</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.scatter_gather--example-1-using-only-args_list","title":"Example 1: Using only args_list","text":"<p>args = [ (1, 2), (3,), (10, 20) ] # multiply will use its default y results = F.scatter_gather(callables, args_list=args) print(results) # (3, 6, 30)</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.scatter_gather--example-2-using-args_list-e-kwargs_list","title":"Example 2: Using args_list e kwargs_list","text":"<p>args = [ (1,), (), (10,) ] kwargs = [ {'y': 2}, {'x': 3, 'y': 3}, {'y': 20} ] results = F.scatter_gather(callables, args_list=args, kwargs_list=kwargs) print(results) # (3, 9, 30)</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.scatter_gather--example-3-using-only-kwargs_list-useful-if-functions-have","title":"Example 3: Using only kwargs_list (useful if functions have","text":""},{"location":"api-reference/nn/functional/#msgflux.nn.functional.scatter_gather--defaults-or-dont-need-positional-args","title":"defaults or don't need positional args)","text":"<p>def greet(name=\"World\"): return f\"Hello, {name}\" def farewell(person_name): return f\"Goodbye, {person_name}\" funcs = [greet, greet, farewell] kwargs_for_funcs = [ {}, {'name': \"Earth\"}, {'person_name': \"Commander\"} ] results = F.scatter_gather(funcs, kwargs_list=kwargs_for_funcs) print(results) # (\"Hello, World\", \"Hello, Earth\", \"Goodbye, Commander\")</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@instrument(\"scatter_gather\")\ndef scatter_gather(\n    to_send: List[Callable],\n    args_list: Optional[List[Tuple[Any, ...]]] = None,\n    kwargs_list: Optional[List[Dict[str, Any]]] = None,\n    *,\n    timeout: Optional[float] = None,\n) -&gt; Tuple[Any, ...]:\n    \"\"\"Sends different sets of arguments/kwargs to a list of modules\n    and collects the responses.\n\n    Each callable in `to_send` receives the positional arguments of\n    the corresponding `tuple` in `args_list` and the named arguments\n    of the corresponding `dict` in `kwargs_list`. If `args_list` or\n    `kwargs_list` are not provided (or are `None`), the corresponding\n    callables will be called without positional or named arguments,\n    respectively, unless an empty list (`[]`) or empty tuple (`()`)\n    is provided for a specific item.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        args_list:\n            Each tuple contains the positional argumentsvfor the corresponding callable\n            in `to_send`. If `None`, no positional arguments are passed unless specified\n            individually by an item in `kwargs_list`.\n        kwargs_list:\n            Each dictionary contains the named arguments for the corresponding callable\n            in `to_send`. If `None`, no named arguments are passed unless specified\n            individually by an item in `args_list`.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n\n    Returns:\n        Tuple containing the responses for each callable. If an error or\n        timeout occurs for a specific callable, its corresponding response\n        in the tuple will be `None`.\n\n    Raises:\n        TypeError:\n            If `to_send` is not a callable list.\n        ValueError:\n            If the lengths of `args_list` (if provided) or `kwargs_list`\n            (if provided) do not match the length of `to_send`.\n\n    Examples:\n        def add(x, y): return x + y\n        def multiply(x, y=2): return x * y\n        callables = [add, multiply, add]\n\n        # Example 1: Using only args_list\n        args = [ (1, 2), (3,), (10, 20) ] # multiply will use its default y\n        results = F.scatter_gather(callables, args_list=args)\n        print(results) # (3, 6, 30)\n\n        # Example 2: Using args_list e kwargs_list\n        args = [ (1,), (), (10,) ]\n        kwargs = [ {'y': 2}, {'x': 3, 'y': 3}, {'y': 20} ]\n        results = F.scatter_gather(callables, args_list=args, kwargs_list=kwargs)\n        print(results) # (3, 9, 30)\n\n        # Example 3: Using only kwargs_list (useful if functions have\n        # defaults or don't need positional args)\n        def greet(name=\"World\"): return f\"Hello, {name}\"\n        def farewell(person_name): return f\"Goodbye, {person_name}\"\n        funcs = [greet, greet, farewell]\n        kwargs_for_funcs = [ {}, {'name': \"Earth\"}, {'person_name': \"Commander\"} ]\n        results = F.scatter_gather(funcs, kwargs_list=kwargs_for_funcs)\n        print(results) # (\"Hello, World\", \"Hello, Earth\", \"Goodbye, Commander\")\n    \"\"\"\n    if not isinstance(to_send, list) or not all(callable(f) for f in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    executor = Executor.get_instance()\n    futures = []\n    for i, f in enumerate(to_send):\n        args = args_list[i] if args_list and i &lt; len(args_list) else ()\n        kwargs = kwargs_list[i] if kwargs_list and i &lt; len(kwargs_list) else {}\n        futures.append(executor.submit(f, *args, **kwargs))\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    responses: List[Any] = []\n    for future in futures:\n        try:\n            responses.append(future.result())\n        except Exception as e:\n            logger.error(str(e))\n            responses.append(None)\n    return tuple(responses)\n</code></pre>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.msg_scatter_gather","title":"msg_scatter_gather","text":"<pre><code>msg_scatter_gather(to_send, messages, *, timeout=None)\n</code></pre> <p>Scatter a list of messages to a list of modules and gather the responses.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>messages</code> <code>List[dotdict]</code> <p>List of <code>msgflux.dotdict</code> instances to be distributed.</p> required <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[dotdict, ...]</code> <p>Tuple containing the messages updated with the responses.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>messages</code> is not a list of <code>dotdict</code>, <code>to_send</code> is not a list of callables, or <code>prefix</code> is not a string.</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@instrument(\"msg_scatter_gather\")\ndef msg_scatter_gather(\n    to_send: List[Callable],\n    messages: List[dotdict],\n    *,\n    timeout: Optional[float] = None,\n) -&gt; Tuple[dotdict, ...]:\n    \"\"\"Scatter a list of messages to a list of modules and gather the responses.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        messages:\n            List of `msgflux.dotdict` instances to be distributed.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n\n    Returns:\n        Tuple containing the messages updated with the responses.\n\n    Raises:\n        TypeError:\n            If `messages` is not a list of `dotdict`, `to_send` is not a list\n            of callables, or `prefix` is not a string.\n    \"\"\"\n    if not messages or not all(isinstance(msg, dotdict) for msg in messages):\n        raise TypeError(\n            \"`messages` must be a non-empty list of `msgflux.dotdict` instances\"\n        )\n\n    if not to_send or not all(isinstance(f, Callable) for f in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    if len(messages) != len(to_send):\n        raise ValueError(\n            f\"The size of `messages` ({len(messages)}) \"\n            f\"must be equal to that of `to_send`: ({len(to_send)})\"\n        )\n\n    executor = Executor.get_instance()\n    futures = [executor.submit(f, msg) for f, msg in zip(to_send, messages)]\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    for f, future in zip(to_send, futures):\n        f_name = get_callable_name(f)\n        try:\n            future.result()\n        except Exception as e:\n            logger.error(f\"Error in scattered task for `{f_name}`: {e}\")\n    return tuple(messages)\n</code></pre>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.bcast_gather","title":"bcast_gather","text":"<pre><code>bcast_gather(to_send, *args, timeout=None, **kwargs)\n</code></pre> <p>Broadcasts arguments to multiple callables and gathers the responses.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>*args</code> <p>Positional arguments.</p> <code>()</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <code>**kwargs</code> <p>Named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Any, ...]</code> <p>Tuple containing the responses.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a list of callables.</p> <p>Examples:</p> <p>def square(x): return x * x def cube(x): return x * x * x def fail(x): raise ValueError(\"Intentional error\")</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.bcast_gather--example-1","title":"Example 1:","text":"<p>results = F.bcast_gather([square, cube], 3) print(results)  # (9, 27)</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.bcast_gather--example-2-simulate-error","title":"Example 2: Simulate error","text":"<p>results = F.bcast_gather([square, fail, cube], 2) print(results)  # (4, None, 8)</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.bcast_gather--example-3-timeout","title":"Example 3: Timeout","text":"<p>results = F.bcast_gather([square, cube], 4, timeout=0.01) print(results) # (16, 64)</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@instrument(\"bcast_gather\")\ndef bcast_gather(\n    to_send: List[Callable], *args, timeout: Optional[float] = None, **kwargs\n) -&gt; Tuple[Any, ...]:\n    \"\"\"Broadcasts arguments to multiple callables and gathers the responses.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        *args:\n            Positional arguments.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n        **kwargs:\n            Named arguments.\n\n    Returns:\n        Tuple containing the responses.\n\n    Raises:\n        TypeError:\n            If `to_send` is not a list of callables.\n\n    Examples:\n        def square(x): return x * x\n        def cube(x): return x * x * x\n        def fail(x): raise ValueError(\"Intentional error\")\n\n        # Example 1:\n        results = F.bcast_gather([square, cube], 3)\n        print(results)  # (9, 27)\n\n        # Example 2: Simulate error\n        results = F.bcast_gather([square, fail, cube], 2)\n        print(results)  # (4, None, 8)\n\n        # Example 3: Timeout\n        results = F.bcast_gather([square, cube], 4, timeout=0.01)\n        print(results) # (16, 64)\n    \"\"\"\n    if not to_send or not all(isinstance(f, Callable) for f in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    executor = Executor.get_instance()\n    futures = [executor.submit(f, *args, **kwargs) for f in to_send]\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    responses: List[Any] = []\n    for future in futures:\n        try:\n            responses.append(future.result())\n        except Exception as e:\n            logger.error(str(e))\n            responses.append(None)\n    return tuple(responses)\n</code></pre>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.msg_bcast_gather","title":"msg_bcast_gather","text":"<pre><code>msg_bcast_gather(to_send, message, *, timeout=None)\n</code></pre> <p>Broadcasts a single message to multiple modules and gathers the responses.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>message</code> <code>dotdict</code> <p>Instance of <code>msgflux.dotdict</code> to broadcast.</p> required <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>dotdict</code> <p>The original message with the module responses added.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>message</code> is not an instance of <code>dotdict</code>, <code>to_send</code> is not a list of callables.</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@instrument(\"msg_bcast_gather\")\ndef msg_bcast_gather(\n    to_send: List[Callable],\n    message: dotdict,\n    *,\n    timeout: Optional[float] = None,\n) -&gt; dotdict:\n    \"\"\"Broadcasts a single message to multiple modules and gathers the responses.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        message:\n            Instance of `msgflux.dotdict` to broadcast.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n\n    Returns:\n        The original message with the module responses added.\n\n    Raises:\n        TypeError:\n            If `message` is not an instance of `dotdict`, `to_send` is not a list\n            of callables.\n    \"\"\"\n    if not isinstance(message, dotdict):\n        raise TypeError(\"`message` must be an instance of `msgflux.dotdict`\")\n    if not to_send or not all(isinstance(module, Callable) for module in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    executor = Executor.get_instance()\n    futures = [executor.submit(f, message) for f in to_send]\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    for f, future in zip(to_send, futures):\n        f_name = get_callable_name(f)\n        try:\n            future.result()\n        except Exception as e:\n            logger.error(f\"Error in scattered task for `{f_name}`: {e}\")\n    return message\n</code></pre>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.wait_for","title":"wait_for","text":"<pre><code>wait_for(to_send, *args, timeout=None, **kwargs)\n</code></pre> <p>Wait for a callable execution.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>Callable</code> <p>A callable object (e.g. functions or <code>Module</code> instances).</p> required <code>*args</code> <p>Positional arguments.</p> <code>()</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <code>**kwargs</code> <p>Named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Callable responses.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a callable.</p> <p>Examples:</p> <p>async def f1(x):     return x * x</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.wait_for--example-1","title":"Example 1:","text":"<p>results = F.wait_for(f1, 3) print(results) # 9</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@instrument(\"wait_for\")\ndef wait_for(\n    to_send: Callable, *args, timeout: Optional[float] = None, **kwargs\n) -&gt; Any:\n    \"\"\"Wait for a callable execution.\n\n    Args:\n        to_send:\n            A callable object (e.g. functions or `Module` instances).\n        *args:\n            Positional arguments.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n        **kwargs:\n            Named arguments.\n\n    Returns:\n        Callable responses.\n\n    Raises:\n        TypeError:\n            If `to_send` is not a callable.\n\n    Examples:\n        async def f1(x):\n            return x * x\n\n        # Example 1:\n        results = F.wait_for(f1, 3)\n        print(results) # 9\n    \"\"\"\n    if not callable(to_send):\n        raise TypeError(\"`to_send` must be a callable object\")\n\n    executor = Executor.get_instance()\n    future = executor.submit(to_send, *args, **kwargs)\n    concurrent.futures.wait([future], timeout=timeout)\n    try:\n        return future.result()\n    except Exception as e:\n        logger.error(str(e))\n        return None\n</code></pre>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.wait_for_event","title":"wait_for_event","text":"<pre><code>wait_for_event(event)\n</code></pre> <p>Waits synchronously for an asyncio.Event to be set.</p> <p>This function will block until event.set() is called elsewhere.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The asyncio.Event to wait for.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>event</code> is not an instance of asyncio.Event.</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@instrument(\"wait_for_event\")\ndef wait_for_event(event: asyncio.Event) -&gt; None:\n    \"\"\"Waits synchronously for an asyncio.Event to be set.\n\n    This function will block until event.set() is called elsewhere.\n\n    Args:\n        event: The asyncio.Event to wait for.\n\n    Raises:\n        TypeError: If `event` is not an instance of asyncio.Event.\n    \"\"\"\n    if not isinstance(event, asyncio.Event):\n        raise TypeError(\"`event` must be an instance of asyncio.Event\")\n\n    executor = Executor.get_instance()\n    future = executor._submit_to_async_worker(event.wait())\n    try:\n        future.result()\n    except Exception as e:\n        logger.error(str(e))\n</code></pre>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.background_task","title":"background_task","text":"<pre><code>background_task(to_send, *args, **kwargs)\n</code></pre> <p>Executes a task in the background asynchronously without blocking, using the AsyncExecutorPool. This function is \"fire-and-forget\".</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>Callable</code> <p>Callable object (function, async function, or module with .acall() method).</p> required <code>*args</code> <p>Positional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Named arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a callable.</p> <p>Examples:</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.background_task--example-1","title":"Example 1:","text":"<p>import time def print_message(message: str):     time.sleep(1)     print(f\"[Sync] Message: {message}\") F.background_task(print_message, \"Hello from sync function\")</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.background_task--example-2","title":"Example 2:","text":"<p>import asyncio async def async_print_message(message: str):     await asyncio.sleep(1)     print(f\"[Async] Message: {message}\") F.background_task(async_print_message, \"Hello from async function\")</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.background_task--example-3-with-error","title":"Example 3 (with error):","text":"<p>def failing_task():     raise ValueError(\"This task failed!\") F.background_task(failing_task)  # Error will be logged</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@instrument(\"background_task\")\ndef background_task(to_send: Callable, *args, **kwargs) -&gt; None:\n    \"\"\"Executes a task in the background asynchronously without blocking,\n    using the AsyncExecutorPool. This function is \"fire-and-forget\".\n\n    Args:\n        to_send:\n            Callable object (function, async function, or module with .acall() method).\n        *args:\n            Positional arguments.\n        **kwargs:\n            Named arguments.\n\n    Raises:\n        TypeError: If `to_send` is not a callable.\n\n    Examples:\n        # Example 1:\n        import time\n        def print_message(message: str):\n            time.sleep(1)\n            print(f\"[Sync] Message: {message}\")\n        F.background_task(print_message, \"Hello from sync function\")\n\n        # Example 2:\n        import asyncio\n        async def async_print_message(message: str):\n            await asyncio.sleep(1)\n            print(f\"[Async] Message: {message}\")\n        F.background_task(async_print_message, \"Hello from async function\")\n\n        # Example 3 (with error):\n        def failing_task():\n            raise ValueError(\"This task failed!\")\n        F.background_task(failing_task)  # Error will be logged\n    \"\"\"\n    if not callable(to_send):\n        raise TypeError(\"`to_send` must be a callable object\")\n\n    def log_future(future: Future) -&gt; None:\n        \"\"\"Callback to log exception of a Future.\"\"\"\n        try:\n            future.result()\n        except Exception as e:\n            logger.error(f\"Background task error: {e!s}\", exc_info=True)\n\n    executor = Executor.get_instance()\n    future = executor.submit(to_send, *args, **kwargs)\n    future.add_done_callback(log_future)\n</code></pre>"},{"location":"api-reference/nn/modules/agent/","title":"Agent","text":""},{"location":"api-reference/nn/modules/agent/#msgflux.nn.modules.agent.Agent","title":"Agent","text":"<p>               Bases: <code>Module</code></p> <p>Agent is a Module type that uses language models to solve tasks.</p> <p>An Agent can perform actions in an environment using tools calls. For an Agent, a tool is any callable object.</p> <p>An Agent can handle multimodal inputs and outputs.</p> Source code in <code>src/msgflux/nn/modules/agent.py</code> <pre><code>class Agent(Module):\n    \"\"\"Agent is a Module type that uses language models to solve tasks.\n\n    An Agent can perform actions in an environment using tools calls.\n    For an Agent, a tool is any callable object.\n\n    An Agent can handle multimodal inputs and outputs.\n    \"\"\"\n\n    _supported_outputs: List[str] = [\n        \"reasoning_structured\",\n        \"reasoning_text_generation\",\n        \"structured\",\n        \"text_generation\",\n        \"audio_generation\",\n        \"audio_text_generation\",\n        \"tool_responses\"\n    ]\n\n    def __init__(\n        self,\n        name: str,\n        model: Union[ChatCompletionModel, ModelGateway, LM],\n        *,\n        system_message: Optional[str] = None,\n        instructions: Optional[str] = None,\n        expected_output: Optional[str] = None,\n        examples: Optional[Union[str, List[Union[Example, Mapping[str, Any]]]]] = None,\n        system_extra_message: Optional[str] = None,\n        guardrails: Optional[Dict[str, Callable]] = None,\n        message_fields: Optional[Dict[str, Any]] = None,\n        config: Optional[Dict[str, Any]] = None,\n        templates: Optional[Dict[str, str]] = None,\n        context_cache: Optional[str] = None,\n        prefilling: Optional[str] = None,\n        generation_schema: Optional[msgspec.Struct] = None,\n        typed_parser: Optional[str] = None,\n        response_mode: Optional[str] = \"plain_response\",\n        tools: Optional[List[Callable]] = None,\n        mcp_servers: Optional[List[Mapping[str, Any]]] = None,\n        fixed_messages: Optional[List[Mapping[str, Any]]] = None,\n        signature: Optional[Union[str, Signature]] = None,\n        description: Optional[str] = None,\n        annotations: Optional[Mapping[str, type]] = None,\n    ):\n        \"\"\"Args:\n        name:\n            Agent name in snake case format.\n        model:\n            Chat Completation Model client.\n        system_message:\n            The Agent behaviour.\n        instructions:\n            What the Agent should do.\n        expected_output:\n            What the response should be like.\n        examples:\n            Examples of inputs, reasoning and outputs.\n        system_extra_message:\n            An extra message in system prompt.\n        guardrails:\n            Dictionary mapping guardrail types to callables.\n            Valid keys: \"input\", \"output\"\n            !!! example\n                guardrails={\"input\": input_checker, \"output\": output_checker}\n        message_fields:\n            Dictionary mapping Message field names to their paths in the Message object.\n            Valid keys: \"task_inputs\", \"task_multimodal_inputs\", \"task_messages\",\n            \"context_inputs\", \"model_preference\", \"vars\"\n            !!! example\n                message_fields={\n                    \"task_inputs\": \"input.user\",\n                    \"task_multimodal_inputs\": {\"audio\": \"audio.user\"},\n                    \"task_messages\": \"messages.history\",\n                    \"context_inputs\": \"context.data\",\n                    \"model_preference\": \"model.preference\",\n                    \"vars\": \"vars.data\"\n                }\n\n            Field descriptions:\n            - task_inputs: Field path for task input (str, dict, or tuple)\n            - task_multimodal_inputs: Map datatype (image, video, audio, file) to field paths\n            - task_messages: Field path for list of chats in ChatML format\n            - context_inputs: Field path for context (str or list of str)\n            - model_preference: Field path for model preference (str, only valid with ModelGateway)\n            - vars: Field path for inputs to templates and tools (str)\n        config:\n            Dictionary with configuration options.\n            Valid keys: \"verbose\", \"return_model_state\", \"tool_choice\",\n            \"stream\", \"image_block_kwargs\", \"video_block_kwargs\", \"include_date\"\n            !!! example\n                config={\n                    \"verbose\": True,\n                    \"return_model_state\": False,\n                    \"tool_choice\": \"auto\",\n                    \"stream\": False,\n                    \"image_block_kwargs\": {\"detail\": \"high\"},\n                    \"video_block_kwargs\": {\"format\": \"mp4\"},\n                    \"include_date\": False\n                }\n\n            Configuration options:\n            - verbose: Print model output and tool calls to console (bool)\n            - return_model_state: Return dict with model_state and response (bool)\n            - tool_choice: Control tool selection (\"auto\", \"required\", or function name)\n            - stream: Transmit response on-the-fly (bool)\n            - image_block_kwargs: Dict of kwargs to pass to ChatBlock.image (e.g., {\"detail\": \"high\"})\n            - video_block_kwargs: Dict of kwargs to pass to ChatBlock.video (e.g., {\"format\": \"mp4\"})\n            - include_date: Include current date in system prompt (bool)\n        templates:\n            Dictionary mapping template types to Jinja template strings.\n            Valid keys: \"task\", \"response\", \"context\"\n            !!! example\n                templates={\n                    \"task\": \"Who was {{person}}?\",\n                    \"response\": \"{{final_answer}}\",\n                    \"context\": \"Context: {{context}}\"\n                }\n\n            Template descriptions:\n            - task: Formats the task/prompt sent to the model\n            - response: Formats the model's response\n            - context: Formats context_inputs (does NOT apply to context_cache)\n        context_cache:\n            A fixed context.\n        prefilling:\n            Forces an initial message from the model. From that message it\n            will continue its response from there.\n        generation_schema:\n            Schema that defines how the output should be structured.\n        typed_parser:\n            Converts the model raw output into a typed-dict. Supported parser:\n            `typed_xml`.\n        response_mode:\n            What the response should be.\n            * `plain_response` (default): Returns the final agent response directly.\n            * other: Write on field in Message object.\n        tools:\n            A list of callable objects.\n        mcp_servers:\n            List of MCP (Model Context Protocol) server configurations.\n            Each config should contain:\n            - name: Namespace for tools from this server\n            - transport: \"stdio\" or \"http\"\n            - For stdio: command, args, cwd, env\n            - For http: base_url, headers\n            - Optional: include_tools, exclude_tools, tool_config\n            !!! example\n                mcp_servers=[{\n                    \"name\": \"fs\",\n                    \"transport\": \"stdio\",\n                    \"command\": \"npx\",\n                    \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\"],\n                    \"include_tools\": [\"read_file\", \"write_file\"],\n                    \"tool_config\": {\"read_file\": {\"inject_vars\": [\"context\"]}}\n                }]\n        fixed_messages:\n            A fixed list of chats in ChatML format.\n        signature:\n            A DSPy-based signature. A signature creates a task_template,\n            a generation_schema, instructions and examples (both if passed).\n            Can be combined with standard generation_schemas like `ReAct` and\n            `ChainOfThought`. Can also be combined with `typed_parser`.\n        description:\n            The Agent description. It's useful when using an agent-as-a-tool.\n        annotations\n            Define the input and output annotations to use the agent-as-a-function.\n        \"\"\"\n        if annotations is None:\n            annotations = {\"message\": str, \"return\": str}\n\n        super().__init__()\n        self.set_name(name)\n        self.set_description(description)\n        self.set_annotations(annotations)\n        self._set_config(config)\n\n        stream = config.get(\"stream\", False) if config else False\n\n        if stream is True:\n            if generation_schema is not None:\n                raise ValueError(\"`generation_schema` is not `stream=True` compatible\")\n\n            if guardrails is not None and \"output\" in guardrails:\n                raise ValueError(\"`guardrails['output']` is not `stream=True` compatible\")\n\n            if templates is not None and templates.get(\"response\") is not None:\n                raise ValueError(\"`templates['response']` is not `stream=True` compatible\")\n\n            if typed_parser is not None:\n                raise ValueError(\"`typed_parser` is not `stream=True` compatible\")\n\n        self._set_context_cache(context_cache)\n        self._set_fixed_messages(fixed_messages)\n        self._set_guardrails(guardrails)\n        self._set_message_fields(message_fields)\n        self._set_model(model)\n        self._set_prefilling(prefilling)\n        self._set_system_extra_message(system_extra_message)\n        self._set_response_mode(response_mode)\n        self._set_templates(templates)\n        self._set_tools(tools, mcp_servers)\n\n        if signature is not None:\n            signature_params = dotdict(\n                signature=signature,\n                examples=examples,\n                instructions=instructions,\n                system_message=system_message,\n                typed_parser=typed_parser\n            )\n            if generation_schema is not None:\n                signature_params.generation_schema = generation_schema\n            self._set_signature(**signature_params)\n        else:\n            self._set_typed_parser(typed_parser)\n            self._set_examples(examples)\n            self._set_generation_schema(generation_schema)\n            self._set_expected_output(expected_output)\n            self._set_instructions(instructions)\n            self._set_system_message(system_message)\n\n    def forward(\n        self, message: Optional[Union[str, Mapping[str, Any], Message]] = None, **kwargs\n    ) -&gt; Union[str, Mapping[str, None], ModelStreamResponse, Message]:\n        \"\"\"Execute the agent with the given message.\n\n        Args:\n            message: The input message, which can be:\n                - str: Direct task input (used as task_inputs)\n                - Message: Message object with fields mapped via message_fields\n                - dict: Task inputs as a dictionary\n                - None: When using task_template without dynamic inputs\n            **kwargs: Runtime overrides for message_fields. Can include:\n                - task_inputs: Override field path or direct value\n                - task_multimodal_inputs: Override multimodal inputs\n                - task_messages: Override chat messages\n                - context_inputs: Override context\n                - model_preference: Override model preference\n                - vars: Override template/tool variables\n\n        Returns:\n            Agent response (str, Message, or ModelStreamResponse depending on configuration)\n        \"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = self._execute_model(prefilling=self.prefilling, **inputs)\n        response = self._process_model_response(message, model_response, **inputs)\n        return response\n\n    async def aforward(\n        self, message: Optional[Union[str, Mapping[str, Any], Message]] = None, **kwargs\n    ) -&gt; Union[str, Mapping[str, None], ModelStreamResponse, Message]:\n        \"\"\"Async version of forward.\"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = await self._aexecute_model(prefilling=self.prefilling, **inputs)\n        response = await self._aprocess_model_response(message, model_response, **inputs)\n        return response\n\n    def _execute_model(\n        self,\n        model_state: List[Mapping[str, Any]],\n        vars: Mapping[str, Any],\n        prefilling: Optional[str] = None,\n        model_preference: Optional[str] = None,\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        model_execution_params = self._prepare_model_execution(\n            model_state=model_state, prefilling=prefilling,\n            model_preference=model_preference, vars=vars\n        )\n        if self.guardrails.get(\"input\"):\n            self._execute_input_guardrail(model_execution_params)\n        if self.config.get(\"verbose\", False):\n            cprint(f\"[{self.name}][call_model]\", bc=\"br1\", ls=\"b\")\n        model_response = self.lm(**model_execution_params)\n        return model_response\n\n    async def _aexecute_model(\n        self,\n        model_state: List[Mapping[str, Any]],\n        vars: Mapping[str, Any],\n        prefilling: Optional[str] = None,\n        model_preference: Optional[str] = None,\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        model_execution_params = self._prepare_model_execution(\n            model_state=model_state, prefilling=prefilling,\n            model_preference=model_preference, vars=vars\n        )\n        if self.guardrails.get(\"input\"):\n            await self._aexecute_input_guardrail(model_execution_params)\n        if self.config.get(\"verbose\", False):\n            cprint(f\"[{self.name}][call_model]\", bc=\"br1\", ls=\"b\")\n        model_response = await self.lm.acall(**model_execution_params)\n        return model_response\n\n    def _prepare_model_execution(\n        self,\n        model_state: List[Mapping[str, Any]],\n        vars: Mapping[str, Any],\n        prefilling: Optional[str] = None,\n        model_preference: Optional[str] = None,\n    ) -&gt; Mapping[str, Any]:\n        # model_state, prefilling, model_preference, vars\n        agent_state = []\n\n        if self.fixed_messages:\n            agent_state.extend(self.fixed_messages)\n\n        agent_state.extend(model_state)\n\n        system_prompt = self._get_system_prompt(vars)\n\n        tool_schemas = self.tool_library.get_tool_json_schemas()\n        if not tool_schemas:\n            tool_schemas = None\n\n        tool_choice = self.config.get(\"tool_choice\")\n\n        if is_subclass_of(self.generation_schema, ToolFlowControl) and tool_schemas:\n            tools_template = self.generation_schema.tools_template\n            inputs = {\"tool_schemas\": tool_schemas, \"tool_choice\": tool_choice}\n            flow_control_tools = self._format_template(inputs, tools_template)\n            if system_prompt:\n                system_prompt = flow_control_tools + \"\\n\\n\" + system_prompt\n            else:\n                system_prompt = flow_control_tools\n            tool_schemas = None  # Disable tool_schemas to controlflow preference\n            tool_choice = None  # Disable tool_choice to controlflow preference\n\n        model_execution_params = dotdict(\n            messages=agent_state,\n            system_prompt=system_prompt or None,\n            prefilling=prefilling,\n            stream=self.config.get(\"stream\", False),\n            tool_schemas=tool_schemas,\n            tool_choice=tool_choice,\n            generation_schema=self.generation_schema,\n            typed_parser=self.typed_parser,\n        )\n\n        if model_preference:\n            model_execution_params.model_preference = model_preference\n\n        return model_execution_params\n\n    def _prepare_input_guardrail_execution(\n        self, model_execution_params: Mapping[str, Any]\n    ) -&gt; Mapping[str, Any]:\n        messages = model_execution_params.get(\"messages\")\n        last_message = messages[-1]\n        if isinstance(last_message.get(\"content\"), list):\n            if last_message.get(\"content\")[0][\"type\"] == \"image_url\":\n                data = [last_message]\n            else: # audio, file\n                data = last_message.get(\"content\")[-1]  # text input\n        else:\n            data = last_message.get(\"content\")\n        guardrail_params = {\"data\": data}\n        return guardrail_params\n\n    def _process_model_response(\n        self,\n        message: Union[str, Mapping[str, str], Message],\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        model_state: List[Mapping[str, Any]],\n        vars: Mapping[str, Any],\n        model_preference: Optional[str] = None,\n    ) -&gt; Union[str, Mapping[str, str], Message, ModelStreamResponse]:\n        if \"tool_call\" in model_response.response_type:\n            model_response, model_state = self._process_tool_call_response(\n                model_response, model_state, vars, model_preference\n            )\n        elif is_subclass_of(self.generation_schema, ToolFlowControl):\n            model_response, model_state = self._process_tool_flow_control_response(\n                model_response, model_state, vars, model_preference\n            )\n\n        if isinstance(model_response, (ModelResponse, ModelStreamResponse)):\n            raw_response = self._extract_raw_response(model_response)\n            response_type = model_response.response_type\n        else:  # returns tool result as response or tool call as response\n            raw_response = model_response\n            response_type = \"tool_responses\"\n\n        if response_type in self._supported_outputs:\n            response = self._prepare_response(\n                raw_response, response_type, model_state, message, vars\n            )\n            return response\n        else:\n            raise ValueError(f\"Unsupported `response_type={response_type}`\")\n\n    async def _aprocess_model_response(\n        self,\n        message: Union[str, Mapping[str, str], Message],\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        model_state: List[Mapping[str, Any]],\n        vars: Mapping[str, Any],\n        model_preference: Optional[str] = None,\n    ) -&gt; Union[str, Mapping[str, str], Message, ModelStreamResponse]:\n        if \"tool_call\" in model_response.response_type:\n            model_response, model_state = await self._aprocess_tool_call_response(\n                model_response, model_state, vars, model_preference\n            )\n        elif is_subclass_of(self.generation_schema, ToolFlowControl):\n            model_response, model_state = await self._aprocess_tool_flow_control_response(\n                model_response, model_state, vars, model_preference\n            )\n\n        if isinstance(model_response, (ModelResponse, ModelStreamResponse)):\n            raw_response = self._extract_raw_response(model_response)\n            response_type = model_response.response_type\n        else:  # returns tool result as response or tool call as response\n            raw_response = model_response\n            response_type = \"tool_responses\"\n\n        if response_type in self._supported_outputs:\n            response = await self._aprepare_response(\n                raw_response, response_type, model_state, message, vars\n            )\n            return response\n        else:\n            raise ValueError(f\"Unsupported `response_type={response_type}`\")\n\n    def _process_tool_flow_control_response(\n        self,\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        model_state: Mapping[str, Any],\n        vars: Mapping[str, Any],\n        model_preference: Optional[str] = None,\n    ) -&gt; Tuple[Union[str, Mapping[str, Any], ModelStreamResponse], Mapping[str, Any]]:\n        \"\"\"Handle the fields returned by `ReAct`. If the fields are different,\n        you must rewrite this function.\n        \"\"\"\n        while True:\n            raw_response = self._extract_raw_response(model_response)\n\n            if getattr(raw_response, \"final_answer\", None):\n                return model_response, model_state\n\n            if getattr(raw_response, \"current_step\", None):\n                step = raw_response.current_step\n                actions = step.actions\n                reasoning = step.thought\n\n                if self.config.get(\"verbose\", False):\n                    repr = f\"[{self.name}][tool_calls_reasoning] {reasoning}\"\n                    cprint(repr, bc=\"br2\", ls=\"b\")\n\n                for act in actions:\n                    act.id = str(uuid4())  # Add tool_id\n\n                tool_callings = [(act.id, act.name, act.arguments) for act in actions]\n                tool_results = self._process_tool_call(tool_callings, model_state, vars)\n\n                if tool_results.return_directly:\n                    tool_calls = tool_results.to_dict().pop(\"return_directly\")\n                    tool_calls[\"reasoning\"] = reasoning\n                    tool_responses = dotdict(tool_responses=tool_calls)\n                    # TODO converter tool calls em tool call msgs\n                    return tool_responses, model_state\n\n                for act in actions:  # Add results\n                    result = tool_results.get_by_id(act.id).result\n                    error = tool_results.get_by_id(act.id).error\n                    act.result = result or error\n\n                # Compact steps history\n                if model_state and model_state[-1].get(\"role\") == \"assistant\":\n                    last_react_msg = model_state[-1].get(\"content\")\n                    react_state = msgspec.json.decode(last_react_msg)\n                    react_state.append(raw_response)\n                    model_state[-1] = ChatBlock.assist(react_state)\n                else:\n                    react_state = [raw_response]\n                    model_state.append(ChatBlock.assist(react_state))\n\n            model_response = self._execute_model(\n                model_state=model_state,\n                model_preference=model_preference,\n                vars=vars\n            )\n\n    async def _aprocess_tool_flow_control_response(\n        self,\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        model_state: Mapping[str, Any],\n        vars: Mapping[str, Any],\n        model_preference: Optional[str] = None,\n    ) -&gt; Tuple[Union[str, Mapping[str, Any], ModelStreamResponse], Mapping[str, Any]]:\n        \"\"\"Async version of _process_tool_flow_control_response.\n        Handle the fields returned by `ReAct`. If the fields are different,\n        you must rewrite this function.\n        \"\"\"\n        while True:\n            raw_response = self._extract_raw_response(model_response)\n\n            if getattr(raw_response, \"final_answer\", None):\n                return model_response, model_state\n\n            if getattr(raw_response, \"current_step\", None):\n                step = raw_response.current_step\n                actions = step.actions\n                reasoning = step.thought\n\n                if self.config.get(\"verbose\", False):\n                    repr = f\"[{self.name}][tool_calls_reasoning] {reasoning}\"\n                    cprint(repr, bc=\"br2\", ls=\"b\")\n\n                for act in actions:\n                    act.id = str(uuid4())  # Add tool_id\n\n                tool_callings = [(act.id, act.name, act.arguments) for act in actions]\n                tool_results = await self._aprocess_tool_call(tool_callings, model_state, vars)\n\n                if tool_results.return_directly:\n                    tool_calls = tool_results.to_dict().pop(\"return_directly\")\n                    tool_calls[\"reasoning\"] = reasoning\n                    tool_responses = dotdict(tool_responses=tool_calls)\n                    # TODO converter tool calls em tool call msgs\n                    return tool_responses, model_state\n\n                for act in actions:  # Add results\n                    result = tool_results.get_by_id(act.id).result\n                    error = tool_results.get_by_id(act.id).error\n                    act.result = result or error\n\n                # Compact steps history\n                if model_state and model_state[-1].get(\"role\") == \"assistant\":\n                    last_react_msg = model_state[-1].get(\"content\")\n                    react_state = msgspec.json.decode(last_react_msg)\n                    react_state.append(raw_response)\n                    model_state[-1] = ChatBlock.assist(react_state)\n                else:\n                    react_state = [raw_response]\n                    model_state.append(ChatBlock.assist(react_state))\n\n            model_response = await self._aexecute_model(\n                model_state=model_state,\n                model_preference=model_preference,\n                vars=vars\n            )\n\n    def _process_tool_call_response(\n        self,\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        model_state: Mapping[str, Any],\n        vars: Mapping[str, Any],\n        model_preference: Optional[str] = None\n    ) -&gt; Tuple[Union[str, Mapping[str, Any], ModelStreamResponse], Mapping[str, Any]]:\n        \"\"\"ToolCall example: [{'role': 'assistant', 'tool_responses': [{'id': 'call_1YL',\n        'type': 'function', 'function': {'arguments': '{\"order_id\":\"order_12345\"}',\n        'name': 'get_delivery_date'}}]}, {'role': 'tool', 'tool_call_id': 'call_HA',\n        'content': '2024-10-15'}].\n        \"\"\"\n        while True:\n            if model_response.response_type == \"tool_call\":\n                raw_response = model_response.data\n                reasoning = raw_response.reasoning\n\n                if self.config.get(\"verbose\", False):\n                    if reasoning:\n                        repr = f\"[{self.name}][tool_calls_reasoning] {reasoning}\"\n                        cprint(repr, bc=\"br2\", ls=\"b\")\n\n                tool_callings = raw_response.get_calls()\n                tool_results = self._process_tool_call(tool_callings, model_state, vars)\n\n                if tool_results.return_directly:\n                    tool_calls = tool_results.to_dict()\n                    tool_calls.pop(\"return_directly\")\n                    tool_calls[\"reasoning\"] = reasoning\n                    tool_responses = dotdict(tool_responses=tool_calls)\n                    return tool_responses, model_state\n\n                id_results = {\n                    call.id: call.result or call.error\n                    for call in tool_results.tool_calls\n                }\n                raw_response.insert_results(id_results)\n                tool_responses_message = raw_response.get_messages()\n                model_state.extend(tool_responses_message)\n            else:\n                return model_response, model_state\n\n            model_response = self._execute_model(\n                model_state=model_state,\n                model_preference=model_preference,\n                vars=vars\n            )\n\n    async def _aprocess_tool_call_response(\n        self,\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        model_state: Mapping[str, Any],\n        vars: Mapping[str, Any],\n        model_preference: Optional[str] = None\n    ) -&gt; Tuple[Union[str, Mapping[str, Any], ModelStreamResponse], Mapping[str, Any]]:\n        \"\"\"Async version of _process_tool_call_response.\n        ToolCall example: [{'role': 'assistant', 'tool_responses': [{'id': 'call_1YL',\n        'type': 'function', 'function': {'arguments': '{\"order_id\":\"order_12345\"}',\n        'name': 'get_delivery_date'}}]}, {'role': 'tool', 'tool_call_id': 'call_HA',\n        'content': '2024-10-15'}].\n        \"\"\"\n        while True:\n            if model_response.response_type == \"tool_call\":\n                raw_response = model_response.data\n                reasoning = raw_response.reasoning\n\n                if self.config.get(\"verbose\", False):\n                    if reasoning:\n                        repr = f\"[{self.name}][tool_calls_reasoning] {reasoning}\"\n                        cprint(repr, bc=\"br2\", ls=\"b\")\n\n                tool_callings = raw_response.get_calls()\n                tool_results = await self._aprocess_tool_call(tool_callings, model_state, vars)\n\n                if tool_results.return_directly:\n                    tool_calls = tool_results.to_dict()\n                    tool_calls.pop(\"return_directly\")\n                    tool_calls[\"reasoning\"] = reasoning\n                    tool_responses = dotdict(tool_responses=tool_calls)\n                    return tool_responses, model_state\n\n                id_results = {\n                    call.id: call.result or call.error\n                    for call in tool_results.tool_calls\n                }\n                raw_response.insert_results(id_results)\n                tool_responses_message = raw_response.get_messages()\n                model_state.extend(tool_responses_message)\n            else:\n                return model_response, model_state\n\n            model_response = await self._aexecute_model(\n                model_state=model_state,\n                model_preference=model_preference,\n                vars=vars\n            )\n\n    def _process_tool_call(\n        self,\n        tool_callings: Mapping[str, Any],\n        model_state: List[Mapping[str, Any]],\n        vars: Mapping[str, Any],\n    ) -&gt; ToolResponses:\n        if self.config.get(\"verbose\", False):\n            for call in tool_callings:\n                repr = f\"[{self.name}][tool_call] {call[1]}: {call[2]}\"\n                cprint(repr, bc=\"br2\", ls=\"b\")\n        tool_results = self.tool_library(\n            tool_callings=tool_callings,\n            model_state=model_state,\n            vars=vars,\n        )\n        if self.config.get(\"verbose\", False):\n            repr = f\"[{self.name}][tool_responses]\"\n            if tool_results.return_directly:\n                repr += \" return directly\"\n            cprint(repr, bc=\"br1\", ls=\"b\")\n            for call in tool_results.tool_calls:\n                result = call.result or call.error or \"\"\n                repr = f\"[{self.name}][tool_response] {call.name}: {result}\"\n                cprint(repr, ls=\"b\")\n        return tool_results\n\n    async def _aprocess_tool_call(\n        self,\n        tool_callings: Mapping[str, Any],\n        model_state: List[Mapping[str, Any]],\n        vars: Mapping[str, Any],\n    ) -&gt; ToolResponses:\n        \"\"\"Async version of _process_tool_call.\"\"\"\n        if self.config.get(\"verbose\", False):\n            for call in tool_callings:\n                repr = f\"[{self.name}][tool_call] {call[1]}: {call[2]}\"\n                cprint(repr, bc=\"br2\", ls=\"b\")\n        tool_results = await self.tool_library.acall(\n            tool_callings=tool_callings,\n            model_state=model_state,\n            vars=vars,\n        )\n        if self.config.get(\"verbose\", False):\n            repr = f\"[{self.name}][tool_responses]\"\n            if tool_results.return_directly:\n                repr += \" return directly\"\n            cprint(repr, bc=\"br1\", ls=\"b\")\n            for call in tool_results.tool_calls:\n                result = call.result or call.error or \"\"\n                repr = f\"[{self.name}][tool_response] {call.name}: {result}\"\n                cprint(repr, ls=\"b\")\n        return tool_results\n\n    def _prepare_response(\n        self,\n        raw_response: Union[str, Mapping[str, Any], ModelStreamResponse],\n        response_type: str,\n        model_state: List[Mapping[str, Any]],\n        message: Union[str, Mapping[str, Any], Message],\n        vars: Mapping[str, Any],\n    ) -&gt; Union[str, Mapping[str, Any], ModelStreamResponse]:\n        formatted_response = None\n        if not isinstance(raw_response, ModelStreamResponse):\n            if response_type == \"text_generation\" or \"structured\" in response_type:\n                if self.config.get(\"verbose\", False):\n                    cprint(f\"[{self.name}][response] {raw_response}\", bc=\"y\", ls=\"b\")\n                if self.guardrails.get(\"output\"):\n                    self._execute_output_guardrail(raw_response)\n                if self.templates.get(\"response\"):\n                    if isinstance(raw_response, str):\n                        pre_response = self._format_response_template(vars)\n                        formatted_response = self._format_template(\n                            raw_response, pre_response\n                        )\n                    elif isinstance(raw_response, dict):\n                        raw_response.update(vars)\n                        formatted_response = self._format_response_template(\n                            raw_response\n                        )\n\n        response = formatted_response or raw_response\n        if self.config.get(\"return_model_state\", False):\n            if response_type == \"tool_responses\":\n                response.model_state = model_state\n            else:\n                response = dotdict(agent_response=response, model_state=model_state)\n        return self._define_response_mode(response, message)\n\n    async def _aprepare_response(\n        self,\n        raw_response: Union[str, Mapping[str, Any], ModelStreamResponse],\n        response_type: str,\n        model_state: List[Mapping[str, Any]],\n        message: Union[str, Mapping[str, Any], Message],\n        vars: Mapping[str, Any],\n    ) -&gt; Union[str, Mapping[str, Any], ModelStreamResponse]:\n        \"\"\"Async version of _prepare_response with async output guardrail support.\"\"\"\n        formatted_response = None\n        if not isinstance(raw_response, ModelStreamResponse):\n            if response_type == \"text_generation\" or \"structured\" in response_type:\n                if self.config.get(\"verbose\", False):\n                    cprint(f\"[{self.name}][response] {raw_response}\", bc=\"y\", ls=\"b\")\n                if self.guardrails.get(\"output\"):\n                    await self._aexecute_output_guardrail(raw_response)\n                if self.templates.get(\"response\"):\n                    if isinstance(raw_response, str):\n                        pre_response = self._format_response_template(vars)\n                        formatted_response = self._format_template(\n                            raw_response, pre_response\n                        )\n                    elif isinstance(raw_response, dict):\n                        raw_response.update(vars)\n                        formatted_response = self._format_response_template(\n                            raw_response\n                        )\n\n        response = formatted_response or raw_response\n        if self.config.get(\"return_model_state\", False):\n            if response_type == \"tool_responses\":\n                response.model_state = model_state\n            else:\n                response = dotdict(model_response=response, model_state=model_state)\n        return self._define_response_mode(response, message)\n\n    def _prepare_output_guardrail_execution(\n        self, model_response: Union[str, Mapping[str, Any]]\n    ) -&gt; Mapping[str, Any]:\n        if isinstance(model_response, str):\n            data = model_response\n        else:\n            data = str(model_response)\n        guardrail_params = {\"data\": data}\n        return guardrail_params\n\n    def _prepare_task(\n        self, message: Union[str, Message, Mapping[str, str]], **kwargs\n    ) -&gt; Mapping[str, Any]:\n        \"\"\"Prepare model input in ChatML format and execution params.\"\"\"\n        vars = kwargs.pop(\"vars\", {})\n        if (\n            not vars\n            and isinstance(message, Message)\n            and self.vars is not None\n        ):\n            vars = message.get(self.vars, {})\n\n        task_messages = kwargs.pop(\"task_messages\", None)\n        if (\n            task_messages is None\n            and isinstance(message, Message)\n            and self.vars is not None\n        ):\n            task_messages = self._get_content_from_message(self.task_messages, message)\n\n        content = self._process_task_inputs(message, vars=vars, **kwargs)\n\n        if content is None and task_messages is None:\n            raise ValueError(\"No data was detected to make the model input\")\n\n        if content is not None:            \n            chat_content = [ChatBlock.user(content)]\n            if task_messages is None:\n                model_state = chat_content\n            else:\n                task_messages.extend(chat_content)\n                model_state = task_messages\n        else:\n            model_state = task_messages\n\n        model_preference = kwargs.pop(\"model_preference\", None)\n        if model_preference is None and isinstance(message, Message):\n            model_preference = self.get_model_preference_from_message(message)\n\n        return {\n            \"model_state\": model_state,\n            \"model_preference\": model_preference,\n            \"vars\": vars,\n        }\n\n    def _process_task_inputs(\n        self,\n        message: Union[str, Message, Mapping[str, str]],\n        vars: Mapping[str, Any],\n        **kwargs\n    ) -&gt; Optional[Union[str, Mapping[str, Any]]]:\n        content = \"\"\n\n        context_content = self._context_manager(message, vars=vars, **kwargs)\n        if context_content:\n            content += context_content\n\n        if isinstance(message, Message):\n            task_inputs = self._extract_message_values(self.task_inputs, message)\n        else:\n            task_inputs = message\n\n        if task_inputs is None and self.templates.get(\"task\") is None:\n            return None\n\n        if self.templates.get(\"task\"):\n            if task_inputs:\n                if isinstance(task_inputs, str):\n                    pre_task = self._format_task_template(vars)\n                    task_content = self._format_template(task_inputs, pre_task)\n                elif isinstance(task_inputs, dict):\n                    task_inputs.update(vars)\n                    task_content = self._format_task_template(task_inputs)\n            # It's possible to use `task_template` as the default task message\n            # if no `task_inputs` is selected. This can be useful for multimodal\n            # models that require a text message to be sent along with the data\n            else:\n                if vars:\n                    task_content = self._format_task_template(vars)\n                else:\n                    task_content = self.templates.get(\"task\")\n        else:\n            task_content = task_inputs\n            if isinstance(task_content, Mapping): # dict -&gt; str\n                task_content = \"\\n\".join(f\"{k}: {v}\" for k, v in task_content.items())\n\n        task_content = apply_xml_tags(\"task\", task_content)\n        content += task_content\n        content = content.strip() # Remove whitespace\n\n        multimodal_content = self._process_task_multimodal_inputs(message, **kwargs)\n        if multimodal_content:            \n            multimodal_content.append(ChatBlock.text(content))\n            return multimodal_content\n        return content\n\n    def _context_manager( # noqa: C901\n        self,\n        message: Union[str, Message, Mapping[str, str]],\n        vars: Mapping[str, Any],\n        **kwargs\n    ) -&gt; Optional[str]:\n        \"\"\"Mount context.\"\"\"\n        context_content = \"\"\n\n        if self.context_cache:  # Fixed Context Cache\n            context_content += self.context_cache\n\n        context_inputs = None\n        runtime_context_inputs = kwargs.pop(\"context_inputs\", None)\n        if runtime_context_inputs is not None:\n            context_inputs = runtime_context_inputs\n        elif isinstance(message, Message):\n            context_inputs = self._extract_message_values(self.context_inputs, message)\n\n        if context_inputs is not None:\n            if self.templates.get(\"context\"):\n                if isinstance(context_inputs, Mapping):                \n                    context_inputs.update(vars)\n                    msg_context = self._format_template(\n                        context_inputs, self.templates.get(\"context\")\n                    )\n                else:\n                    pre_msg_context = self._format_template(\n                        vars, self.templates.get(\"context\")\n                    )\n                    msg_context = self._format_template(\n                        context_inputs, pre_msg_context\n                    )                                    \n            elif isinstance(context_inputs, str):\n                msg_context = context_inputs\n            elif isinstance(context_inputs, list):\n                msg_context = \" \".join(\n                    str(v) for v in context_inputs if v is not None\n                )\n            elif isinstance(context_inputs, dict):\n                msg_context = \"\\n\".join(\n                    f\"{k}: {v if not isinstance(v, list) else ', '.join(v)}\"\n                    for k, v in context_inputs.items()\n                )\n            context_content += msg_context\n\n        if context_content:\n            if vars:\n                context_content = self._format_template(vars, context_content)\n            return apply_xml_tags(\"context\", context_content) + \"\\n\\n\"\n        return None\n\n    def _process_task_multimodal_inputs(\n        self, message: Union[str, Message, Mapping[str, str]], **kwargs\n    ) -&gt; Optional[List[Mapping[str, Any]]]:\n        \"\"\"Processes multimodal inputs (image, audio, video, file) via kwargs or message.\n        Returns a list of multimodal content in ChatML format.\n        \"\"\"\n        multimodal_paths = None\n        task_multimodal_inputs = kwargs.get(\"task_multimodal_inputs\", None)\n        if task_multimodal_inputs is not None:\n            multimodal_paths = task_multimodal_inputs\n        elif isinstance(message, Message) and self.task_multimodal_inputs is not None:\n            multimodal_paths = self._extract_message_values(\n                self.task_multimodal_inputs, message\n            )\n\n        if multimodal_paths is None:\n            return None\n\n        content = []\n\n        formatters = {\n            \"image\": self._format_image_input,\n            \"audio\": self._format_audio_input,\n            \"video\": self._format_video_input,\n            \"file\": self._format_file_input,\n        }\n\n        for media_type, formatter in formatters.items():\n            media_sources = multimodal_paths.get(media_type, [])\n            if not isinstance(media_sources, list):\n                media_sources = [media_sources]\n            for media_source in media_sources:\n                if media_source is not None:\n                    formatted_input = formatter(media_source)\n                    if formatted_input:\n                        content.append(formatted_input)\n\n        return content\n\n    def _format_image_input(self, image_source: str) -&gt; Optional[Mapping[str, Any]]:\n        \"\"\"Formats the image input for the model.\"\"\"\n        encoded_image = self._prepare_data_uri(image_source, force_encode=False)\n\n        if not encoded_image:\n            return None\n\n        if not encoded_image.startswith(\"http\"):\n            # Try to guess from the original source\n            mime_type = get_mime_type(image_source)\n            if not mime_type.startswith(\"image/\"):\n                mime_type = \"image/jpeg\"  # Fallback\n            encoded_image = f\"data:{mime_type};base64,{encoded_image}\"\n\n        return ChatBlock.image(encoded_image, **self.config.get(\"image_block_kwargs\", {}))\n\n    def _format_video_input(self, video_source: str) -&gt; Optional[Mapping[str, Any]]:\n        \"\"\"Formats the video input for the model.\"\"\"\n        # Check if it's a URL\n        if video_source.startswith(\"http://\") or video_source.startswith(\"https://\"):\n            return ChatBlock.video(video_source, **self.config.get(\"video_block_kwargs\", {}))\n\n        # Otherwise, encode as base64\n        encoded_video = self._prepare_data_uri(video_source, force_encode=True)\n\n        if not encoded_video:\n            return None\n\n        # Get MIME type or use mp4 as fallback\n        mime_type = get_mime_type(video_source)\n        if not mime_type.startswith(\"video/\"):\n            mime_type = \"video/mp4\"  # Fallback\n\n        video_data_uri = f\"data:{mime_type};base64,{encoded_video}\"\n\n        return ChatBlock.video(video_data_uri, **self.config.get(\"video_block_kwargs\", {}))\n\n    def _format_audio_input(self, audio_source: str) -&gt; Optional[Mapping[str, Any]]:\n        \"\"\"Formats the audio input for the model.\"\"\"\n        base64_audio = self._prepare_data_uri(audio_source, force_encode=True)\n\n        if not base64_audio:\n            return None\n\n        audio_format_suffix = Path(audio_source).suffix.lstrip(\".\")\n        mime_type = get_mime_type(audio_source)\n        if not mime_type.startswith(\"audio/\"):\n            # If MIME type is not audio, use suffix or fallback\n            audio_format_for_uri = (\n                audio_format_suffix if audio_format_suffix else \"mpeg\"\n            )  # fallback\n            mime_type = f\"audio/{audio_format_for_uri}\"\n\n        # Use suffix like 'format' if available, otherwise extract from mime type\n        audio_format = (\n            audio_format_suffix if audio_format_suffix else mime_type.split(\"/\")[-1]\n        )        \n\n        return ChatBlock.audio(base64_audio, audio_format)\n\n    def _format_file_input(self, file_source: str) -&gt; Optional[Mapping[str, Any]]:\n        \"\"\"Formats the file input for the model.\"\"\"\n        base64_file = self._prepare_data_uri(file_source, force_encode=True)\n\n        if not base64_file:\n            return None\n\n        filename = get_filename(file_source)\n        mime_type = get_mime_type(file_source)\n\n        if mime_type == \"application/octet-stream\" and filename.lower().endswith(\n            \".pdf\"\n        ):\n            mime_type = \"application/pdf\"\n\n        file_data_uri = f\"data:{mime_type};base64,{base64_file}\"\n\n        return ChatBlock.file(filename, file_data_uri)\n\n    def inspect_model_execution_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n        \"\"\"Debug model input parameters.\"\"\"\n        inputs = self._prepare_task(*args, **kwargs)\n        model_execution_params = self._prepare_model_execution(\n            prefilling=self.prefilling, **inputs\n        )\n        return model_execution_params\n\n    def _set_context_inputs(\n        self, context_inputs: Optional[Union[str, List[str]]] = None\n    ):\n        if isinstance(context_inputs, (str, list)) or context_inputs is None:\n            if isinstance(context_inputs, str) and context_inputs == \"\":\n                raise ValueError(\n                    \"`context_inputs` requires a string not empty\"\n                    f\"given `{context_inputs}`\"\n                )\n            if isinstance(context_inputs, list) and not context_inputs:\n                raise ValueError(\n                    \"`context_inputs` requires a list not empty\"\n                    f\"given `{context_inputs}`\"\n                )\n            self.register_buffer(\"context_inputs\", context_inputs)\n        else:\n            raise TypeError(\n                \"`context_inputs` requires a string, list or None\"\n                f\"given `{type(context_inputs)}`\"\n            )\n\n    def _set_context_cache(self, context_cache: Optional[str] = None):\n        if isinstance(context_cache, str) or context_cache is None:\n            self.register_buffer(\"context_cache\", context_cache)\n        else:\n            raise TypeError(\n                \"`context_cache` requires a string or None\"\n                f\"given `{type(context_cache)}`\"\n            )\n\n\n    def _set_prefilling(self, prefilling: Optional[str] = None):\n        if isinstance(prefilling, str) or prefilling is None:\n            self.register_buffer(\"prefilling\", prefilling)\n        else:\n            raise TypeError(\n                f\"`prefilling` requires a string or Nonegiven `{type(prefilling)}`\"\n            )\n\n    def _set_tools(\n        self,\n        tools: Optional[List[Callable]] = None,\n        mcp_servers: Optional[List[Mapping[str, Any]]] = None\n    ):\n        self.tool_library = ToolLibrary(\n            self.get_module_name(),\n            tools or [],\n            mcp_servers=mcp_servers\n        )\n\n    def _set_fixed_messages(\n        self, fixed_messages: Optional[List[Mapping[str, Any]]] = None\n    ):\n        if (\n            isinstance(fixed_messages, list)\n            and all(dict(obj) for obj in fixed_messages)\n        ) or fixed_messages is None:\n            self.register_buffer(\"fixed_messages\", fixed_messages)\n        else:\n            raise TypeError(\n                \"`fixed_messages` need be a list of dict or None\"\n                f\"given `{type(fixed_messages)}`\"\n            )\n\n    def _set_generation_schema(\n        self, generation_schema: Optional[msgspec.Struct] = None\n    ):\n        if (\n            generation_schema is None \n            or\n            is_subclass_of(generation_schema, msgspec.Struct)\n        ):\n            self.register_buffer(\"generation_schema\", generation_schema)\n        else:\n            raise TypeError(\n                \"`generation_schema` need be a `msgspec.Struct` or None \"\n                f\"given `{type(generation_schema)}`\"\n            )\n\n    def _set_model(self, model: Union[ChatCompletionModel, ModelGateway, LM]):\n        if isinstance(model, LM): # If already LM, use directly\n            self.lm = model\n        else: # LM will validate model type\n            self.lm = LM(model)\n\n    @property\n    def model(self):\n        \"\"\"Access underlying model for convenience.\n\n        Returns:\n            The wrapped model instance\n        \"\"\"\n        return self.lm.model\n\n    @model.setter\n    def model(self, value: Union[ChatCompletionModel, ModelGateway, LM]):\n        \"\"\"Update the agent's model.\n\n        Args:\n            value: New model (can be Model or LM)\n        \"\"\"\n        self._set_model(value)\n\n    def _set_system_message(self, system_message: Optional[str] = None):\n        if isinstance(system_message, str) or system_message is None:\n            if (\n                hasattr(self.generation_schema, \"system_message\")\n                and \n                self.generation_schema.system_message is not None\n            ):\n                if system_message is None:\n                    system_message = self.generation_schema.system_message\n                else:\n                    system_message = self.generation_schema.system_message + system_message\n            self.system_message = Parameter(system_message, PromptSpec.SYSTEM_MESSAGE)\n        else:\n            raise TypeError(\n                \"`system_message` requires a string or None \"\n                f\"given `{type(system_message)}`\"\n            )\n\n    def _set_instructions(self, instructions: Optional[str] = None):\n        if isinstance(instructions, str) or instructions is None:\n            typed_parser_cls = typed_parser_registry.get(self.typed_parser, None)\n            if typed_parser_cls is not None:\n                instructions = self._format_template(\n                    {\"instructions\": instructions}, typed_parser_cls.template\n                )\n            self.instructions = Parameter(instructions, PromptSpec.INSTRUCTIONS)\n        else:\n            raise TypeError(\n                f\"`instructions` requires a string or None given `{type(instructions)}`\"\n            )\n\n    def _set_expected_output(self, expected_output: Optional[str] = None):\n        if isinstance(expected_output, str) or expected_output is None: # TODO\n            expected_output_temp = \"\"\n            if expected_output:\n               expected_output_temp += expected_output\n            typed_parser_cls = typed_parser_registry.get(self.typed_parser, None)\n            if typed_parser_cls is not None:  # Schema as expected output\n                response_format = response_format_from_msgspec_struct(\n                    self.generation_schema\n                )\n                schema = typed_parser_cls.schema_from_response_format(response_format)\n                content = {\"expected_outputs\": schema}\n                rendered = self._format_template(content, EXPECTED_OUTPUTS_TEMPLATE)\n                expected_output_temp += rendered\n            self.expected_output = Parameter(\n                expected_output_temp or None, PromptSpec.EXPECTED_OUTPUT\n            )            \n        else:\n            raise TypeError(\n                \"`expected_output` requires a string or None \"\n                f\"given `{type(expected_output)}`\"\n            )\n\n    def _set_examples(\n        self,\n        examples: Optional[Union[str, List[Union[Example, Mapping[str, Any]]]]] = None\n    ):\n        if isinstance(examples, (str, list)) or examples is None:\n            if isinstance(examples, list):\n                typed_parser_cls = typed_parser_registry.get(self.typed_parser, None)\n                collection = ExampleCollection(examples)\n                if typed_parser_cls is not None:\n                    T = typed_parser_cls.encode\n                else:\n                    T = msgspec_dumps\n                examples = collection.get_formatted(T, T)\n            self.examples = Parameter(examples, PromptSpec.EXAMPLES)\n        else:\n            raise TypeError(\n                f\"`examples` requires a List[Example] or None given `{type(examples)}`\"\n            )\n\n    def _set_task_messages(self, task_messages: Optional[str] = None):\n        if isinstance(task_messages, str) or task_messages is None:\n            self.register_buffer(\"task_messages\", task_messages)\n        else:\n            raise TypeError(\n                \"`task_messages` requires a string or None \"\n                f\"given `{type(task_messages)}`\"\n            )\n\n    def _set_config(self, config: Optional[Dict[str, Any]] = None):\n        \"\"\"Set agent configuration.\n\n        Args:\n            config:\n                Dictionary with configuration options.\n                Valid keys: \"verbose\", \"return_model_state\", \"tool_choice\",\n                \"stream\", \"image_block_kwargs\", \"video_block_kwargs\", \"include_date\"\n\n        Raises:\n            TypeError:\n                If config is not a dict or None.\n            ValueError:\n                If invalid keys are provided.\n        \"\"\"\n        # Define valid keys for Agent\n        valid_keys = {\n            \"verbose\", \"return_model_state\", \"tool_choice\",\n            \"stream\", \"image_block_kwargs\", \"video_block_kwargs\", \"include_date\",\n            \"execution\"  # Added for execution settings\n        }\n\n        if config is None:\n            self.register_buffer(\"config\", {})\n            return\n\n        if not isinstance(config, dict):\n            raise TypeError(\n                f\"`config` must be a dict or None, given `{type(config)}`\"\n            )\n\n        invalid_keys = set(config.keys()) - valid_keys\n        if invalid_keys:\n            raise ValueError(\n                f\"Invalid config keys: {invalid_keys}. \"\n                f\"Valid keys are: {valid_keys}\"\n            )\n\n        if \"image_block_kwargs\" in config:\n            if not isinstance(config[\"image_block_kwargs\"], dict):\n                raise TypeError(\n                    f\"`image_block_kwargs` must be a dict, \"\n                    f\"given `{type(config['image_block_kwargs'])}`\"\n                )\n\n        if \"video_block_kwargs\" in config:\n            if not isinstance(config[\"video_block_kwargs\"], dict):\n                raise TypeError(\n                    f\"`video_block_kwargs` must be a dict, \"\n                    f\"given `{type(config['video_block_kwargs'])}`\"\n                )\n\n        self.register_buffer(\"config\", config.copy())\n\n    def _set_system_extra_message(self, system_extra_message: Optional[str] = None):\n        if isinstance(system_extra_message, str) or system_extra_message is None:\n            self.register_buffer(\"system_extra_message\", system_extra_message)\n        else:\n            raise TypeError(\n                \"`system_extra_message` requires a string or None \"\n                f\"given `{type(system_extra_message)}`\"\n            )\n\n    def _set_vars(self, vars: Optional[str] = None):\n        if isinstance(vars, str) or vars is None:\n            self.register_buffer(\"vars\", vars)\n        else:\n            raise TypeError(\n                \"`vars` requires a string or None \"\n                f\"given `{type(vars)}`\"\n            )\n\n    def _set_message_fields(\n        self, message_fields: Optional[Dict[str, Any]] = None\n    ):\n        \"\"\"Set message field mappings for Agent.\n\n        Args:\n            message_fields: Dictionary mapping field names to their values.\n                Valid keys: \"task_inputs\", \"task_multimodal_inputs\", \"task_messages\",\n                \"context_inputs\", \"model_preference\", \"vars\"\n\n        Raises:\n            TypeError: If message_fields is not a dict or None\n            ValueError: If invalid keys are provided\n        \"\"\"\n        # Define valid keys for Agent class\n        valid_keys = {\n            \"task_inputs\", \"task_multimodal_inputs\", \"task_messages\",\n            \"context_inputs\", \"model_preference\", \"vars\"\n        }\n\n        if message_fields is None:\n            # Set all fields to None\n            self._set_task_inputs(None)\n            self._set_task_multimodal_inputs(None)\n            self._set_model_preference(None)\n            self._set_context_inputs(None)\n            self._set_task_messages(None)\n            self._set_vars(None)\n            return\n\n        if not isinstance(message_fields, dict):\n            raise TypeError(\n                f\"`message_fields` must be a dict or None, given `{type(message_fields)}`\"\n            )\n\n        # Validate keys\n        invalid_keys = set(message_fields.keys()) - valid_keys\n        if invalid_keys:\n            raise ValueError(\n                f\"Invalid message_fields keys: {invalid_keys}. \"\n                f\"Valid keys are: {valid_keys}\"\n            )\n\n        # Set each field using its setter, defaulting to None if not provided\n        self._set_task_inputs(message_fields.get(\"task_inputs\"))\n        self._set_task_multimodal_inputs(message_fields.get(\"task_multimodal_inputs\"))\n        self._set_model_preference(message_fields.get(\"model_preference\"))\n        self._set_context_inputs(message_fields.get(\"context_inputs\"))\n        self._set_task_messages(message_fields.get(\"task_messages\"))\n        self._set_vars(message_fields.get(\"vars\"))\n\n    def _set_typed_parser(self, typed_parser: Optional[str] = None):\n        if isinstance(typed_parser, str) or typed_parser is None:\n            if (\n                isinstance(typed_parser, str)\n                and \n                typed_parser not in typed_parser_registry\n            ):\n                raise ValueError(\n                    f\"`typed_parser` supports only `{typed_parser_registry.keys()}`\"\n                    f\" given `{typed_parser}`\"\n                )\n            self.register_buffer(\"typed_parser\", typed_parser)\n        else:\n            raise TypeError(f\"`typed_parser` requires a str given `{type(typed_parser)}`\")\n\n    def _set_signature(\n        self,\n        *,\n        signature: Optional[Union[str, Signature]] = None,\n        examples: Optional[List[Example]] = None,\n        generation_schema: Optional[msgspec.Struct] = None,\n        instructions: Optional[str] = None,\n        system_message: Optional[str] = None,\n        typed_parser: Optional[str] = None,\n    ):\n        if signature is not None:\n\n            typed_parser_cls = typed_parser_registry.get(typed_parser, None)\n\n            examples = examples or []\n            output_descriptions = None\n            signature_instructions = None\n\n            if isinstance(signature, str):\n                input_str_signature, output_str_signature = signature.split(\"-&gt;\")\n                inputs_info = StructFactory._parse_annotations(input_str_signature)\n                outputs_info = StructFactory._parse_annotations(output_str_signature)\n            elif issubclass(signature, Signature):\n                output_str_signature = signature.get_str_signature().split(\"-&gt;\")[-1]                \n                inputs_info = signature.get_inputs_info()\n                outputs_info = signature.get_outputs_info()\n                output_descriptions = signature.get_output_descriptions()\n                signature_instructions = signature.get_instructions()\n                signature_examples = SignatureFactory.get_examples_from_signature(\n                    signature\n                )\n                if signature_examples:\n                    examples.extend(signature_examples)\n            else:\n                raise TypeError(\n                    \"`signature` requires a string, `Signature` or None \"\n                    f\"given `{type(signature)}`\"\n                )\n\n            # typed_parser\n            self._set_typed_parser(typed_parser)\n\n            # task template - add to templates dict, overriding if present\n            task_template = SignatureFactory.get_task_template_from_signature(\n                inputs_info\n            )\n            self.templates[\"task\"] = task_template\n\n            # instructions\n            self._set_instructions(instructions or signature_instructions)\n\n            # generation schema\n            signature_output_struct = StructFactory.from_signature(\n                output_str_signature, \"Outputs\", output_descriptions\n            )\n            fused_output_struct = None\n            if generation_schema is not None:\n                signature_as_type = cast(Type[msgspec.Struct], signature_output_struct)\n                if is_optional_field(generation_schema, \"final_answer\"):\n                    signature_as_type = Optional[signature_output_struct] # type: ignore\n                class Output(generation_schema):  \n                    final_answer: signature_as_type # type: ignore\n                fused_output_struct = Output                \n            self._set_generation_schema(fused_output_struct or signature_output_struct)\n\n            # system message\n            self._set_system_message(system_message)\n\n            # expected output\n            expected_output = SignatureFactory.get_expected_output_from_signature(\n                inputs_info, outputs_info, typed_parser_cls\n            )   \n            self._set_expected_output(expected_output)\n\n            # examples\n            self._set_examples(examples)\n\n    def _get_system_prompt(\n        self, vars: Optional[Mapping[str, Any]] = None\n    ) -&gt; str:\n        \"\"\"Render the system prompt using the Jinja template.\n        Returns an empty string if no segments are provided.\n        \"\"\"\n        template_inputs = dotdict(\n            system_message=self.system_message.data,\n            instructions=self.instructions.data,\n            expected_output=self.expected_output.data,\n            examples=self.examples.data,\n            system_extra_message=self.system_extra_message,\n        )\n\n        if self.config.get(\"include_date\", False):\n            now = datetime.now(tz=timezone.utc)\n            template_inputs.current_date = now.strftime(\"%m/%d/%Y\")\n\n        system_prompt = self._format_template(template_inputs, SYSTEM_PROMPT_TEMPLATE)\n\n        if vars:  # Runtime inputs to system template\n            system_prompt = self._format_template(vars, system_prompt)\n        return system_prompt\n</code></pre>"},{"location":"api-reference/nn/modules/agent/#msgflux.nn.modules.agent.Agent.model","title":"model  <code>property</code> <code>writable</code>","text":"<pre><code>model\n</code></pre> <p>Access underlying model for convenience.</p> <p>Returns:</p> Type Description <p>The wrapped model instance</p>"},{"location":"api-reference/nn/modules/agent/#msgflux.nn.modules.agent.Agent.__init__","title":"__init__","text":"<pre><code>__init__(\n    name,\n    model,\n    *,\n    system_message=None,\n    instructions=None,\n    expected_output=None,\n    examples=None,\n    system_extra_message=None,\n    guardrails=None,\n    message_fields=None,\n    config=None,\n    templates=None,\n    context_cache=None,\n    prefilling=None,\n    generation_schema=None,\n    typed_parser=None,\n    response_mode=\"plain_response\",\n    tools=None,\n    mcp_servers=None,\n    fixed_messages=None,\n    signature=None,\n    description=None,\n    annotations=None,\n)\n</code></pre> <p>name:     Agent name in snake case format. model:     Chat Completation Model client. system_message:     The Agent behaviour. instructions:     What the Agent should do. expected_output:     What the response should be like. examples:     Examples of inputs, reasoning and outputs. system_extra_message:     An extra message in system prompt. guardrails:     Dictionary mapping guardrail types to callables.     Valid keys: \"input\", \"output\"     !!! example         guardrails={\"input\": input_checker, \"output\": output_checker} message_fields:     Dictionary mapping Message field names to their paths in the Message object.     Valid keys: \"task_inputs\", \"task_multimodal_inputs\", \"task_messages\",     \"context_inputs\", \"model_preference\", \"vars\"     !!! example         message_fields={             \"task_inputs\": \"input.user\",             \"task_multimodal_inputs\": {\"audio\": \"audio.user\"},             \"task_messages\": \"messages.history\",             \"context_inputs\": \"context.data\",             \"model_preference\": \"model.preference\",             \"vars\": \"vars.data\"         }</p> <pre><code>Field descriptions:\n- task_inputs: Field path for task input (str, dict, or tuple)\n- task_multimodal_inputs: Map datatype (image, video, audio, file) to field paths\n- task_messages: Field path for list of chats in ChatML format\n- context_inputs: Field path for context (str or list of str)\n- model_preference: Field path for model preference (str, only valid with ModelGateway)\n- vars: Field path for inputs to templates and tools (str)\n</code></pre> <p>config:     Dictionary with configuration options.     Valid keys: \"verbose\", \"return_model_state\", \"tool_choice\",     \"stream\", \"image_block_kwargs\", \"video_block_kwargs\", \"include_date\"     !!! example         config={             \"verbose\": True,             \"return_model_state\": False,             \"tool_choice\": \"auto\",             \"stream\": False,             \"image_block_kwargs\": {\"detail\": \"high\"},             \"video_block_kwargs\": {\"format\": \"mp4\"},             \"include_date\": False         }</p> <pre><code>Configuration options:\n- verbose: Print model output and tool calls to console (bool)\n- return_model_state: Return dict with model_state and response (bool)\n- tool_choice: Control tool selection (\"auto\", \"required\", or function name)\n- stream: Transmit response on-the-fly (bool)\n- image_block_kwargs: Dict of kwargs to pass to ChatBlock.image (e.g., {\"detail\": \"high\"})\n- video_block_kwargs: Dict of kwargs to pass to ChatBlock.video (e.g., {\"format\": \"mp4\"})\n- include_date: Include current date in system prompt (bool)\n</code></pre> <p>templates:     Dictionary mapping template types to Jinja template strings.     Valid keys: \"task\", \"response\", \"context\"     !!! example         templates={             \"task\": \"Who was {{person}}?\",             \"response\": \"{{final_answer}}\",             \"context\": \"Context: {{context}}\"         }</p> <pre><code>Template descriptions:\n- task: Formats the task/prompt sent to the model\n- response: Formats the model's response\n- context: Formats context_inputs (does NOT apply to context_cache)\n</code></pre> <p>context_cache:     A fixed context. prefilling:     Forces an initial message from the model. From that message it     will continue its response from there. generation_schema:     Schema that defines how the output should be structured. typed_parser:     Converts the model raw output into a typed-dict. Supported parser:     <code>typed_xml</code>. response_mode:     What the response should be.     * <code>plain_response</code> (default): Returns the final agent response directly.     * other: Write on field in Message object. tools:     A list of callable objects. mcp_servers:     List of MCP (Model Context Protocol) server configurations.     Each config should contain:     - name: Namespace for tools from this server     - transport: \"stdio\" or \"http\"     - For stdio: command, args, cwd, env     - For http: base_url, headers     - Optional: include_tools, exclude_tools, tool_config     !!! example         mcp_servers=[{             \"name\": \"fs\",             \"transport\": \"stdio\",             \"command\": \"npx\",             \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\"],             \"include_tools\": [\"read_file\", \"write_file\"],             \"tool_config\": {\"read_file\": {\"inject_vars\": [\"context\"]}}         }] fixed_messages:     A fixed list of chats in ChatML format. signature:     A DSPy-based signature. A signature creates a task_template,     a generation_schema, instructions and examples (both if passed).     Can be combined with standard generation_schemas like <code>ReAct</code> and     <code>ChainOfThought</code>. Can also be combined with <code>typed_parser</code>. description:     The Agent description. It's useful when using an agent-as-a-tool. annotations     Define the input and output annotations to use the agent-as-a-function.</p> Source code in <code>src/msgflux/nn/modules/agent.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    model: Union[ChatCompletionModel, ModelGateway, LM],\n    *,\n    system_message: Optional[str] = None,\n    instructions: Optional[str] = None,\n    expected_output: Optional[str] = None,\n    examples: Optional[Union[str, List[Union[Example, Mapping[str, Any]]]]] = None,\n    system_extra_message: Optional[str] = None,\n    guardrails: Optional[Dict[str, Callable]] = None,\n    message_fields: Optional[Dict[str, Any]] = None,\n    config: Optional[Dict[str, Any]] = None,\n    templates: Optional[Dict[str, str]] = None,\n    context_cache: Optional[str] = None,\n    prefilling: Optional[str] = None,\n    generation_schema: Optional[msgspec.Struct] = None,\n    typed_parser: Optional[str] = None,\n    response_mode: Optional[str] = \"plain_response\",\n    tools: Optional[List[Callable]] = None,\n    mcp_servers: Optional[List[Mapping[str, Any]]] = None,\n    fixed_messages: Optional[List[Mapping[str, Any]]] = None,\n    signature: Optional[Union[str, Signature]] = None,\n    description: Optional[str] = None,\n    annotations: Optional[Mapping[str, type]] = None,\n):\n    \"\"\"Args:\n    name:\n        Agent name in snake case format.\n    model:\n        Chat Completation Model client.\n    system_message:\n        The Agent behaviour.\n    instructions:\n        What the Agent should do.\n    expected_output:\n        What the response should be like.\n    examples:\n        Examples of inputs, reasoning and outputs.\n    system_extra_message:\n        An extra message in system prompt.\n    guardrails:\n        Dictionary mapping guardrail types to callables.\n        Valid keys: \"input\", \"output\"\n        !!! example\n            guardrails={\"input\": input_checker, \"output\": output_checker}\n    message_fields:\n        Dictionary mapping Message field names to their paths in the Message object.\n        Valid keys: \"task_inputs\", \"task_multimodal_inputs\", \"task_messages\",\n        \"context_inputs\", \"model_preference\", \"vars\"\n        !!! example\n            message_fields={\n                \"task_inputs\": \"input.user\",\n                \"task_multimodal_inputs\": {\"audio\": \"audio.user\"},\n                \"task_messages\": \"messages.history\",\n                \"context_inputs\": \"context.data\",\n                \"model_preference\": \"model.preference\",\n                \"vars\": \"vars.data\"\n            }\n\n        Field descriptions:\n        - task_inputs: Field path for task input (str, dict, or tuple)\n        - task_multimodal_inputs: Map datatype (image, video, audio, file) to field paths\n        - task_messages: Field path for list of chats in ChatML format\n        - context_inputs: Field path for context (str or list of str)\n        - model_preference: Field path for model preference (str, only valid with ModelGateway)\n        - vars: Field path for inputs to templates and tools (str)\n    config:\n        Dictionary with configuration options.\n        Valid keys: \"verbose\", \"return_model_state\", \"tool_choice\",\n        \"stream\", \"image_block_kwargs\", \"video_block_kwargs\", \"include_date\"\n        !!! example\n            config={\n                \"verbose\": True,\n                \"return_model_state\": False,\n                \"tool_choice\": \"auto\",\n                \"stream\": False,\n                \"image_block_kwargs\": {\"detail\": \"high\"},\n                \"video_block_kwargs\": {\"format\": \"mp4\"},\n                \"include_date\": False\n            }\n\n        Configuration options:\n        - verbose: Print model output and tool calls to console (bool)\n        - return_model_state: Return dict with model_state and response (bool)\n        - tool_choice: Control tool selection (\"auto\", \"required\", or function name)\n        - stream: Transmit response on-the-fly (bool)\n        - image_block_kwargs: Dict of kwargs to pass to ChatBlock.image (e.g., {\"detail\": \"high\"})\n        - video_block_kwargs: Dict of kwargs to pass to ChatBlock.video (e.g., {\"format\": \"mp4\"})\n        - include_date: Include current date in system prompt (bool)\n    templates:\n        Dictionary mapping template types to Jinja template strings.\n        Valid keys: \"task\", \"response\", \"context\"\n        !!! example\n            templates={\n                \"task\": \"Who was {{person}}?\",\n                \"response\": \"{{final_answer}}\",\n                \"context\": \"Context: {{context}}\"\n            }\n\n        Template descriptions:\n        - task: Formats the task/prompt sent to the model\n        - response: Formats the model's response\n        - context: Formats context_inputs (does NOT apply to context_cache)\n    context_cache:\n        A fixed context.\n    prefilling:\n        Forces an initial message from the model. From that message it\n        will continue its response from there.\n    generation_schema:\n        Schema that defines how the output should be structured.\n    typed_parser:\n        Converts the model raw output into a typed-dict. Supported parser:\n        `typed_xml`.\n    response_mode:\n        What the response should be.\n        * `plain_response` (default): Returns the final agent response directly.\n        * other: Write on field in Message object.\n    tools:\n        A list of callable objects.\n    mcp_servers:\n        List of MCP (Model Context Protocol) server configurations.\n        Each config should contain:\n        - name: Namespace for tools from this server\n        - transport: \"stdio\" or \"http\"\n        - For stdio: command, args, cwd, env\n        - For http: base_url, headers\n        - Optional: include_tools, exclude_tools, tool_config\n        !!! example\n            mcp_servers=[{\n                \"name\": \"fs\",\n                \"transport\": \"stdio\",\n                \"command\": \"npx\",\n                \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\"],\n                \"include_tools\": [\"read_file\", \"write_file\"],\n                \"tool_config\": {\"read_file\": {\"inject_vars\": [\"context\"]}}\n            }]\n    fixed_messages:\n        A fixed list of chats in ChatML format.\n    signature:\n        A DSPy-based signature. A signature creates a task_template,\n        a generation_schema, instructions and examples (both if passed).\n        Can be combined with standard generation_schemas like `ReAct` and\n        `ChainOfThought`. Can also be combined with `typed_parser`.\n    description:\n        The Agent description. It's useful when using an agent-as-a-tool.\n    annotations\n        Define the input and output annotations to use the agent-as-a-function.\n    \"\"\"\n    if annotations is None:\n        annotations = {\"message\": str, \"return\": str}\n\n    super().__init__()\n    self.set_name(name)\n    self.set_description(description)\n    self.set_annotations(annotations)\n    self._set_config(config)\n\n    stream = config.get(\"stream\", False) if config else False\n\n    if stream is True:\n        if generation_schema is not None:\n            raise ValueError(\"`generation_schema` is not `stream=True` compatible\")\n\n        if guardrails is not None and \"output\" in guardrails:\n            raise ValueError(\"`guardrails['output']` is not `stream=True` compatible\")\n\n        if templates is not None and templates.get(\"response\") is not None:\n            raise ValueError(\"`templates['response']` is not `stream=True` compatible\")\n\n        if typed_parser is not None:\n            raise ValueError(\"`typed_parser` is not `stream=True` compatible\")\n\n    self._set_context_cache(context_cache)\n    self._set_fixed_messages(fixed_messages)\n    self._set_guardrails(guardrails)\n    self._set_message_fields(message_fields)\n    self._set_model(model)\n    self._set_prefilling(prefilling)\n    self._set_system_extra_message(system_extra_message)\n    self._set_response_mode(response_mode)\n    self._set_templates(templates)\n    self._set_tools(tools, mcp_servers)\n\n    if signature is not None:\n        signature_params = dotdict(\n            signature=signature,\n            examples=examples,\n            instructions=instructions,\n            system_message=system_message,\n            typed_parser=typed_parser\n        )\n        if generation_schema is not None:\n            signature_params.generation_schema = generation_schema\n        self._set_signature(**signature_params)\n    else:\n        self._set_typed_parser(typed_parser)\n        self._set_examples(examples)\n        self._set_generation_schema(generation_schema)\n        self._set_expected_output(expected_output)\n        self._set_instructions(instructions)\n        self._set_system_message(system_message)\n</code></pre>"},{"location":"api-reference/nn/modules/agent/#msgflux.nn.modules.agent.Agent.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(message=None, **kwargs)\n</code></pre> <p>Async version of forward.</p> Source code in <code>src/msgflux/nn/modules/agent.py</code> <pre><code>async def aforward(\n    self, message: Optional[Union[str, Mapping[str, Any], Message]] = None, **kwargs\n) -&gt; Union[str, Mapping[str, None], ModelStreamResponse, Message]:\n    \"\"\"Async version of forward.\"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = await self._aexecute_model(prefilling=self.prefilling, **inputs)\n    response = await self._aprocess_model_response(message, model_response, **inputs)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/agent/#msgflux.nn.modules.agent.Agent.forward","title":"forward","text":"<pre><code>forward(message=None, **kwargs)\n</code></pre> <p>Execute the agent with the given message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Optional[Union[str, Mapping[str, Any], Message]]</code> <p>The input message, which can be: - str: Direct task input (used as task_inputs) - Message: Message object with fields mapped via message_fields - dict: Task inputs as a dictionary - None: When using task_template without dynamic inputs</p> <code>None</code> <code>**kwargs</code> <p>Runtime overrides for message_fields. Can include: - task_inputs: Override field path or direct value - task_multimodal_inputs: Override multimodal inputs - task_messages: Override chat messages - context_inputs: Override context - model_preference: Override model preference - vars: Override template/tool variables</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, Mapping[str, None], ModelStreamResponse, Message]</code> <p>Agent response (str, Message, or ModelStreamResponse depending on configuration)</p> Source code in <code>src/msgflux/nn/modules/agent.py</code> <pre><code>def forward(\n    self, message: Optional[Union[str, Mapping[str, Any], Message]] = None, **kwargs\n) -&gt; Union[str, Mapping[str, None], ModelStreamResponse, Message]:\n    \"\"\"Execute the agent with the given message.\n\n    Args:\n        message: The input message, which can be:\n            - str: Direct task input (used as task_inputs)\n            - Message: Message object with fields mapped via message_fields\n            - dict: Task inputs as a dictionary\n            - None: When using task_template without dynamic inputs\n        **kwargs: Runtime overrides for message_fields. Can include:\n            - task_inputs: Override field path or direct value\n            - task_multimodal_inputs: Override multimodal inputs\n            - task_messages: Override chat messages\n            - context_inputs: Override context\n            - model_preference: Override model preference\n            - vars: Override template/tool variables\n\n    Returns:\n        Agent response (str, Message, or ModelStreamResponse depending on configuration)\n    \"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = self._execute_model(prefilling=self.prefilling, **inputs)\n    response = self._process_model_response(message, model_response, **inputs)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/agent/#msgflux.nn.modules.agent.Agent.inspect_model_execution_params","title":"inspect_model_execution_params","text":"<pre><code>inspect_model_execution_params(*args, **kwargs)\n</code></pre> <p>Debug model input parameters.</p> Source code in <code>src/msgflux/nn/modules/agent.py</code> <pre><code>def inspect_model_execution_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n    \"\"\"Debug model input parameters.\"\"\"\n    inputs = self._prepare_task(*args, **kwargs)\n    model_execution_params = self._prepare_model_execution(\n        prefilling=self.prefilling, **inputs\n    )\n    return model_execution_params\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/","title":"Module dict","text":""},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict","title":"ModuleDict","text":"<p>               Bases: <code>Module</code>, <code>Mapping</code></p> <p>Holds submodules in a dictionary.</p> <p><code>msgflux.nn.ModuleDict</code> can be indexed like a regular Python dictionary, but modules it contains are properly registered, and will be visible by all <code>msgflux.nn.Module</code> methods.</p> <p><code>msgflux.nn.ModuleDict</code> is an ordered dictionary that respects:</p> <pre><code>* the order of insertion, and\n\n* in `msgflux.nn.ModuleDict.update` the order of the merged\n`OrderedDict`, `dict` (started from Python 3.6) or another\n`msgflux.nn.ModuleDict` (the argument to `msgflux.nn.ModuleDict.update`).\n</code></pre> <p>Note that <code>msgflux.nn.ModuleDict.update</code> with other unordered mapping types (e.g., Python's plain <code>dict</code> before Python version 3.6) does not preserve the order of the merged mapping.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>class ModuleDict(Module, container_abcs.Mapping):\n    \"\"\"Holds submodules in a dictionary.\n\n    `msgflux.nn.ModuleDict` can be indexed like a regular Python dictionary,\n    but modules it contains are properly registered, and will be visible by all\n    `msgflux.nn.Module` methods.\n\n    `msgflux.nn.ModuleDict` is an **ordered** dictionary that respects:\n\n        * the order of insertion, and\n\n        * in `msgflux.nn.ModuleDict.update` the order of the merged\n        `OrderedDict`, `dict` (started from Python 3.6) or another\n        `msgflux.nn.ModuleDict` (the argument to `msgflux.nn.ModuleDict.update`).\n\n    Note that `msgflux.nn.ModuleDict.update` with other unordered mapping\n    types (e.g., Python's plain `dict` before Python version 3.6) does not\n    preserve the order of the merged mapping.\n    \"\"\"\n\n    _modules: Dict[str, Module]  # type: ignore[assignment]\n\n    def __init__(self, modules: Optional[Mapping[str, Module]] = None) -&gt; None:\n        \"\"\"Args:\n            modules (iterable, optional): a mapping (dictionary) of (string: module)\n                or an iterable of key-value pairs of type (string, module).\n\n        !!! example\n            ```python\n            import random\n            import msgflux.nn as nn\n\n            class ExpertSales(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n                def forward(self, msg: str):\n                    return msg + self.response\n\n            class ExpertSupport(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.register_buffer(\"response\", \"Hi, call 190\")\n\n                def forward(self, msg: str):\n                    return msg + self.response\n\n            def draw_choice(choices: list[str]) -&gt; str:\n                return random.choice(choices)\n\n            class Router(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.choices = nn.ModuleDict({\n                        \"sales\": ExpertSales(),\n                        \"support\": ExpertSupport()\n                    })\n\n                def forward(self, msg: str) -&gt; str:\n                    choice = draw_choice(list(self.choices.keys()))\n                    msg = self.choices[choice](msg)\n                    return msg\n\n            router = Router()\n            router(\"I need help with my tv.\")\n            ```\n        \"\"\"\n        super().__init__()\n        if modules is not None:\n            self.update(modules)\n\n    def __getitem__(self, key: str) -&gt; Module:\n        return self._modules[key]\n\n    def __setitem__(self, key: str, module: Module) -&gt; None:\n        self.add_module(key, module)\n\n    def __delitem__(self, key: str) -&gt; None:\n        del self._modules[key]\n\n    def __len__(self) -&gt; int:\n        return len(self._modules)\n\n    def __iter__(self) -&gt; Iterator[str]:\n        return iter(self._modules)\n\n    def __contains__(self, key: str) -&gt; bool:\n        return key in self._modules\n\n    def clear(self) -&gt; None:\n        \"\"\"Remove all items from the ModuleDict.\"\"\"\n        self._modules.clear()\n\n    def pop(self, key: str) -&gt; Module:\n        \"\"\"Remove key from the ModuleDict and return its module.\n\n        Args:\n            key:\n                key to pop from the ModuleDict.\n        \"\"\"\n        v = self[key]\n        del self[key]\n        return v\n\n    def keys(self) -&gt; Iterable[str]:\n        \"\"\"Return an iterable of the ModuleDict keys.\"\"\"\n        return self._modules.keys()\n\n    def items(self) -&gt; Iterable[Tuple[str, Module]]:\n        \"\"\"Return an iterable of the ModuleDict key/value pairs.\"\"\"\n        return self._modules.items()\n\n    def values(self) -&gt; Iterable[Module]:\n        \"\"\"Return an iterable of the ModuleDict values.\"\"\"\n        return self._modules.values()\n\n    def update(self, modules: Mapping[str, Module]) -&gt; None:\n        \"\"\"Update the class **msgflux.nn.ModuleDict** with\n        key-value pairs from a mapping, overwriting existing keys.\n\n        !!! note\n\n            If `modules` is an `OrderedDict`, a `msgflux.nn.ModuleDict`, or\n            an iterable of key-value pairs, the order of new elements in\n            it is preserved.\n\n        Args:\n            modules:\n                A mapping (dictionary) from string to `msgflux.nn.Module`,\n                or an iterable of key-value pairs of type (string, `msgflux.nn.Module`).\n        \"\"\"\n        if not isinstance(modules, container_abcs.Iterable):\n            raise TypeError(\n                \"ModuleDict.update should be called with an \"\n                \"iterable of key/value pairs, but got \" + type(modules).__name__\n            )\n\n        if isinstance(modules, (OrderedDict, ModuleDict, container_abcs.Mapping)):\n            for key, module in modules.items():\n                self[key] = module\n        else:\n            # modules here can be a list with two items\n            for j, m in enumerate(modules):\n                if not isinstance(m, container_abcs.Iterable):\n                    raise TypeError(\n                        \"ModuleDict update sequence element \"\n                        \"#\" + str(j) + \" should be Iterable; is\" + type(m).__name__\n                    )\n                if not len(m) == 2:\n                    raise ValueError(\n                        \"ModuleDict update sequence element \"\n                        \"#\" + str(j) + \" has length \" + str(len(m)) + \"; 2 is required\"\n                    )\n                # modules can be Mapping (what it's typed at),\n                # or a list: [(name1, module1), (name2, module2)]\n                # that's too cumbersome to type correctly with overloads,\n                # so we add an ignore here\n                self[m[0]] = m[1]  # type: ignore[assignment]islice\n\n    def get(self, key: str, default: Optional[Module] = None) -&gt; Optional[Module]:\n        \"\"\"Return the module for the given key if it exists,\n        else return the default value.\n\n        Args:\n            key:\n                The key to look up in the ModuleDict.\n            default:\n                The value to return if the key is not found. Defaults to None.\n\n        Returns:\n            The module associated with the key, or the default\n            value if the key is not found.\n        \"\"\"\n        return self._modules.get(key, default)\n\n    def set(self, key: str, module: Module) -&gt; None:\n        \"\"\"Set a single key-value pair in the ModuleDict.\n\n        Args:\n            key:\n                The key to set in the ModuleDict.\n            module:\n                The module to associate with the key.\n\n        !!! note\n            This method registers the module using `add_module` to ensure\n            proper registration and preserves the order of insertion,\n            consistent with `ModuleDict` behavior.\n        \"\"\"\n        self.add_module(key, module)\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.__contains__","title":"__contains__","text":"<pre><code>__contains__(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __contains__(self, key: str) -&gt; bool:\n    return key in self._modules\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __delitem__(self, key: str) -&gt; None:\n    del self._modules[key]\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Module:\n    return self._modules[key]\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.__init__","title":"__init__","text":"<pre><code>__init__(modules=None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>iterable</code> <p>a mapping (dictionary) of (string: module) or an iterable of key-value pairs of type (string, module).</p> <code>None</code> <p>Example</p> <pre><code>import random\nimport msgflux.nn as nn\n\nclass ExpertSales(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass ExpertSupport(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, call 190\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\ndef draw_choice(choices: list[str]) -&gt; str:\n    return random.choice(choices)\n\nclass Router(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.choices = nn.ModuleDict({\n            \"sales\": ExpertSales(),\n            \"support\": ExpertSupport()\n        })\n\n    def forward(self, msg: str) -&gt; str:\n        choice = draw_choice(list(self.choices.keys()))\n        msg = self.choices[choice](msg)\n        return msg\n\nrouter = Router()\nrouter(\"I need help with my tv.\")\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __init__(self, modules: Optional[Mapping[str, Module]] = None) -&gt; None:\n    \"\"\"Args:\n        modules (iterable, optional): a mapping (dictionary) of (string: module)\n            or an iterable of key-value pairs of type (string, module).\n\n    !!! example\n        ```python\n        import random\n        import msgflux.nn as nn\n\n        class ExpertSales(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        class ExpertSupport(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, call 190\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        def draw_choice(choices: list[str]) -&gt; str:\n            return random.choice(choices)\n\n        class Router(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.choices = nn.ModuleDict({\n                    \"sales\": ExpertSales(),\n                    \"support\": ExpertSupport()\n                })\n\n            def forward(self, msg: str) -&gt; str:\n                choice = draw_choice(list(self.choices.keys()))\n                msg = self.choices[choice](msg)\n                return msg\n\n        router = Router()\n        router(\"I need help with my tv.\")\n        ```\n    \"\"\"\n    super().__init__()\n    if modules is not None:\n        self.update(modules)\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iter__(self) -&gt; Iterator[str]:\n    return iter(self._modules)\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._modules)\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key, module)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __setitem__(self, key: str, module: Module) -&gt; None:\n    self.add_module(key, module)\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Remove all items from the ModuleDict.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Remove all items from the ModuleDict.\"\"\"\n    self._modules.clear()\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.get","title":"get","text":"<pre><code>get(key, default=None)\n</code></pre> <p>Return the module for the given key if it exists, else return the default value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to look up in the ModuleDict.</p> required <code>default</code> <code>Optional[Module]</code> <p>The value to return if the key is not found. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Module]</code> <p>The module associated with the key, or the default</p> <code>Optional[Module]</code> <p>value if the key is not found.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def get(self, key: str, default: Optional[Module] = None) -&gt; Optional[Module]:\n    \"\"\"Return the module for the given key if it exists,\n    else return the default value.\n\n    Args:\n        key:\n            The key to look up in the ModuleDict.\n        default:\n            The value to return if the key is not found. Defaults to None.\n\n    Returns:\n        The module associated with the key, or the default\n        value if the key is not found.\n    \"\"\"\n    return self._modules.get(key, default)\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.items","title":"items","text":"<pre><code>items()\n</code></pre> <p>Return an iterable of the ModuleDict key/value pairs.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def items(self) -&gt; Iterable[Tuple[str, Module]]:\n    \"\"\"Return an iterable of the ModuleDict key/value pairs.\"\"\"\n    return self._modules.items()\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.keys","title":"keys","text":"<pre><code>keys()\n</code></pre> <p>Return an iterable of the ModuleDict keys.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def keys(self) -&gt; Iterable[str]:\n    \"\"\"Return an iterable of the ModuleDict keys.\"\"\"\n    return self._modules.keys()\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.pop","title":"pop","text":"<pre><code>pop(key)\n</code></pre> <p>Remove key from the ModuleDict and return its module.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key to pop from the ModuleDict.</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def pop(self, key: str) -&gt; Module:\n    \"\"\"Remove key from the ModuleDict and return its module.\n\n    Args:\n        key:\n            key to pop from the ModuleDict.\n    \"\"\"\n    v = self[key]\n    del self[key]\n    return v\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.set","title":"set","text":"<pre><code>set(key, module)\n</code></pre> <p>Set a single key-value pair in the ModuleDict.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set in the ModuleDict.</p> required <code>module</code> <code>Module</code> <p>The module to associate with the key.</p> required <p>Note</p> <p>This method registers the module using <code>add_module</code> to ensure proper registration and preserves the order of insertion, consistent with <code>ModuleDict</code> behavior.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def set(self, key: str, module: Module) -&gt; None:\n    \"\"\"Set a single key-value pair in the ModuleDict.\n\n    Args:\n        key:\n            The key to set in the ModuleDict.\n        module:\n            The module to associate with the key.\n\n    !!! note\n        This method registers the module using `add_module` to ensure\n        proper registration and preserves the order of insertion,\n        consistent with `ModuleDict` behavior.\n    \"\"\"\n    self.add_module(key, module)\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.update","title":"update","text":"<pre><code>update(modules)\n</code></pre> <p>Update the class msgflux.nn.ModuleDict with key-value pairs from a mapping, overwriting existing keys.</p> <p>Note</p> <p>If <code>modules</code> is an <code>OrderedDict</code>, a <code>msgflux.nn.ModuleDict</code>, or an iterable of key-value pairs, the order of new elements in it is preserved.</p> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>Mapping[str, Module]</code> <p>A mapping (dictionary) from string to <code>msgflux.nn.Module</code>, or an iterable of key-value pairs of type (string, <code>msgflux.nn.Module</code>).</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def update(self, modules: Mapping[str, Module]) -&gt; None:\n    \"\"\"Update the class **msgflux.nn.ModuleDict** with\n    key-value pairs from a mapping, overwriting existing keys.\n\n    !!! note\n\n        If `modules` is an `OrderedDict`, a `msgflux.nn.ModuleDict`, or\n        an iterable of key-value pairs, the order of new elements in\n        it is preserved.\n\n    Args:\n        modules:\n            A mapping (dictionary) from string to `msgflux.nn.Module`,\n            or an iterable of key-value pairs of type (string, `msgflux.nn.Module`).\n    \"\"\"\n    if not isinstance(modules, container_abcs.Iterable):\n        raise TypeError(\n            \"ModuleDict.update should be called with an \"\n            \"iterable of key/value pairs, but got \" + type(modules).__name__\n        )\n\n    if isinstance(modules, (OrderedDict, ModuleDict, container_abcs.Mapping)):\n        for key, module in modules.items():\n            self[key] = module\n    else:\n        # modules here can be a list with two items\n        for j, m in enumerate(modules):\n            if not isinstance(m, container_abcs.Iterable):\n                raise TypeError(\n                    \"ModuleDict update sequence element \"\n                    \"#\" + str(j) + \" should be Iterable; is\" + type(m).__name__\n                )\n            if not len(m) == 2:\n                raise ValueError(\n                    \"ModuleDict update sequence element \"\n                    \"#\" + str(j) + \" has length \" + str(len(m)) + \"; 2 is required\"\n                )\n            # modules can be Mapping (what it's typed at),\n            # or a list: [(name1, module1), (name2, module2)]\n            # that's too cumbersome to type correctly with overloads,\n            # so we add an ignore here\n            self[m[0]] = m[1]  # type: ignore[assignment]islice\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.values","title":"values","text":"<pre><code>values()\n</code></pre> <p>Return an iterable of the ModuleDict values.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def values(self) -&gt; Iterable[Module]:\n    \"\"\"Return an iterable of the ModuleDict values.\"\"\"\n    return self._modules.values()\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/","title":"Module list","text":""},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList","title":"ModuleList","text":"<p>               Bases: <code>Module</code></p> <p>Holds submodules in a list.</p> <p><code>msgflux.nn.ModuleList</code> can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all <code>msgflux.nn.Module</code> methods.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>class ModuleList(Module):\n    \"\"\"Holds submodules in a list.\n\n    `msgflux.nn.ModuleList` can be indexed like a regular Python list, but\n    modules it contains are properly registered, and will be visible by all\n    `msgflux.nn.Module` methods.\n    \"\"\"\n\n    _modules: Dict[str, Module]\n\n    def __init__(self, modules: Optional[Iterable[Module]] = None) -&gt; None:\n        \"\"\"Args:\n            modules (iterable, optional):\n                An iterable of modules to add.\n\n        !!! example\n\n            ```python\n\n            class ExpertSales(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n                def forward(self, msg: str):\n                    return msg + self.response\n\n            class ExpertSupport(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.register_buffer(\"response\", \"Hi, call 190\")\n\n                def forward(self, msg: str):\n                    return msg + self.response\n\n            class Expert(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.experts = nn.ModuleList([ExpertSales(), ExpertSupport()])\n\n                def forward(self, msg: str) -&gt; str:\n                    # ModuleList can act as an iterable, or be indexed using ints\n                    for i, l in enumerate(self.experts):\n                        msg = self.experts[i](msg)\n                    return msg\n\n            expert = Expert()\n            expert(\"I need help with my tv.\")\n            ```\n        \"\"\"\n        super().__init__()\n        if modules is not None:\n            self += modules\n\n    def _get_abs_string_index(self, idx):\n        \"\"\"Get the absolute index for the list of modules.\"\"\"\n        idx = operator.index(idx)\n        if not (-len(self) &lt;= idx &lt; len(self)):\n            raise IndexError(f\"index {idx} is out of range\")\n        if idx &lt; 0:\n            idx += len(self)\n        return str(idx)\n\n    def __getitem__(self, idx: Union[int, slice]) -&gt; Union[Module, \"ModuleList\"]:\n        if isinstance(idx, slice):\n            return self.__class__(list(self._modules.values())[idx])\n        else:\n            return self._modules[self._get_abs_string_index(idx)]\n\n    def __setitem__(self, idx: int, module: Module) -&gt; None:\n        idx = self._get_abs_string_index(idx)\n        return setattr(self, str(idx), module)\n\n    def __delitem__(self, idx: Union[int, slice]) -&gt; None:\n        if isinstance(idx, slice):\n            for k in range(len(self._modules))[idx]:\n                delattr(self, str(k))\n        else:\n            delattr(self, self._get_abs_string_index(idx))\n        # To preserve numbering, self._modules is being\n        # reconstructed with modules after deletion\n        str_indices = [str(i) for i in range(len(self._modules))]\n        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n\n    def __len__(self) -&gt; int:\n        return len(self._modules)\n\n    def __iter__(self) -&gt; Iterator[Module]:\n        return iter(self._modules.values())\n\n    def __iadd__(self, modules: Iterable[Module]) -&gt; Self:\n        return self.extend(modules)\n\n    def __add__(self, other: Iterable[Module]) -&gt; \"ModuleList\":\n        combined = ModuleList()\n        for i, module in enumerate(chain(self, other)):\n            combined.add_module(str(i), module)\n        return combined\n\n    def __repr__(self):\n        \"\"\"Return a custom repr for ModuleList that compresses\n        repeated module representations.\n        \"\"\"\n        list_of_reprs = [repr(item) for item in self]\n        if len(list_of_reprs) == 0:\n            return self._get_name() + \"()\"\n\n        start_end_indices = [[0, 0]]\n        repeated_blocks = [list_of_reprs[0]]\n        for i, r in enumerate(list_of_reprs[1:], 1):\n            if r == repeated_blocks[-1]:\n                start_end_indices[-1][1] += 1\n                continue\n\n            start_end_indices.append([i, i])\n            repeated_blocks.append(r)\n\n        lines = []\n        main_str = self._get_name() + \"(\"\n        for (start_id, end_id), b in zip(start_end_indices, repeated_blocks):\n            local_repr = f\"({start_id}): {b}\"  # default repr\n\n            if start_id != end_id:\n                n = end_id - start_id + 1\n                local_repr = f\"({start_id}-{end_id}): {n} x {b}\"\n\n            local_repr = _addindent(local_repr, 2)\n            lines.append(local_repr)\n\n        main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n        main_str += \")\"\n        return main_str\n\n    def __dir__(self):\n        keys = super().__dir__()\n        keys = [key for key in keys if not key.isdigit()]\n        return keys\n\n    def insert(self, index: int, module: Module) -&gt; None:\n        \"\"\"Insert a given module before a given index in the list.\n\n        Args:\n            index (int): index to insert.\n            module (nn.Module): module to insert\n        \"\"\"\n        for i in range(len(self._modules), index, -1):\n            self._modules[str(i)] = self._modules[str(i - 1)]\n        self._modules[str(index)] = module\n\n    def append(self, module: Module) -&gt; \"ModuleList\":\n        \"\"\"Append a given module to the end of the list.\n\n        Args:\n            module (nn.Module): module to append\n        \"\"\"\n        self.add_module(str(len(self)), module)\n        return self\n\n    def pop(self, key: Union[int, slice]) -&gt; Module:\n        v = self[key]\n        del self[key]\n        return v\n\n    def extend(self, modules: Iterable[Module]) -&gt; Self:\n        \"\"\"Append modules from a Python iterable to the end of the list.\n\n        Args:\n            modules (iterable): iterable of modules to append\n        \"\"\"\n        if not isinstance(modules, container_abcs.Iterable):\n            raise TypeError(\n                \"ModuleList.extend should be called with an \"\n                \"iterable, but got \" + type(modules).__name__\n            )\n        offset = len(self)\n        for i, module in enumerate(modules):\n            self.add_module(str(offset + i), module)\n        return self\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __add__(self, other: Iterable[Module]) -&gt; \"ModuleList\":\n    combined = ModuleList()\n    for i, module in enumerate(chain(self, other)):\n        combined.add_module(str(i), module)\n    return combined\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(idx)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __delitem__(self, idx: Union[int, slice]) -&gt; None:\n    if isinstance(idx, slice):\n        for k in range(len(self._modules))[idx]:\n            delattr(self, str(k))\n    else:\n        delattr(self, self._get_abs_string_index(idx))\n    # To preserve numbering, self._modules is being\n    # reconstructed with modules after deletion\n    str_indices = [str(i) for i in range(len(self._modules))]\n    self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__dir__","title":"__dir__","text":"<pre><code>__dir__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __dir__(self):\n    keys = super().__dir__()\n    keys = [key for key in keys if not key.isdigit()]\n    return keys\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __getitem__(self, idx: Union[int, slice]) -&gt; Union[Module, \"ModuleList\"]:\n    if isinstance(idx, slice):\n        return self.__class__(list(self._modules.values())[idx])\n    else:\n        return self._modules[self._get_abs_string_index(idx)]\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__iadd__","title":"__iadd__","text":"<pre><code>__iadd__(modules)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iadd__(self, modules: Iterable[Module]) -&gt; Self:\n    return self.extend(modules)\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__init__","title":"__init__","text":"<pre><code>__init__(modules=None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>iterable</code> <p>An iterable of modules to add.</p> <code>None</code> <p>Example</p> <pre><code>class ExpertSales(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass ExpertSupport(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, call 190\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass Expert(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.experts = nn.ModuleList([ExpertSales(), ExpertSupport()])\n\n    def forward(self, msg: str) -&gt; str:\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.experts):\n            msg = self.experts[i](msg)\n        return msg\n\nexpert = Expert()\nexpert(\"I need help with my tv.\")\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __init__(self, modules: Optional[Iterable[Module]] = None) -&gt; None:\n    \"\"\"Args:\n        modules (iterable, optional):\n            An iterable of modules to add.\n\n    !!! example\n\n        ```python\n\n        class ExpertSales(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        class ExpertSupport(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, call 190\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        class Expert(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.experts = nn.ModuleList([ExpertSales(), ExpertSupport()])\n\n            def forward(self, msg: str) -&gt; str:\n                # ModuleList can act as an iterable, or be indexed using ints\n                for i, l in enumerate(self.experts):\n                    msg = self.experts[i](msg)\n                return msg\n\n        expert = Expert()\n        expert(\"I need help with my tv.\")\n        ```\n    \"\"\"\n    super().__init__()\n    if modules is not None:\n        self += modules\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Module]:\n    return iter(self._modules.values())\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._modules)\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Return a custom repr for ModuleList that compresses repeated module representations.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a custom repr for ModuleList that compresses\n    repeated module representations.\n    \"\"\"\n    list_of_reprs = [repr(item) for item in self]\n    if len(list_of_reprs) == 0:\n        return self._get_name() + \"()\"\n\n    start_end_indices = [[0, 0]]\n    repeated_blocks = [list_of_reprs[0]]\n    for i, r in enumerate(list_of_reprs[1:], 1):\n        if r == repeated_blocks[-1]:\n            start_end_indices[-1][1] += 1\n            continue\n\n        start_end_indices.append([i, i])\n        repeated_blocks.append(r)\n\n    lines = []\n    main_str = self._get_name() + \"(\"\n    for (start_id, end_id), b in zip(start_end_indices, repeated_blocks):\n        local_repr = f\"({start_id}): {b}\"  # default repr\n\n        if start_id != end_id:\n            n = end_id - start_id + 1\n            local_repr = f\"({start_id}-{end_id}): {n} x {b}\"\n\n        local_repr = _addindent(local_repr, 2)\n        lines.append(local_repr)\n\n    main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n    main_str += \")\"\n    return main_str\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(idx, module)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __setitem__(self, idx: int, module: Module) -&gt; None:\n    idx = self._get_abs_string_index(idx)\n    return setattr(self, str(idx), module)\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.append","title":"append","text":"<pre><code>append(module)\n</code></pre> <p>Append a given module to the end of the list.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module to append</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def append(self, module: Module) -&gt; \"ModuleList\":\n    \"\"\"Append a given module to the end of the list.\n\n    Args:\n        module (nn.Module): module to append\n    \"\"\"\n    self.add_module(str(len(self)), module)\n    return self\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.extend","title":"extend","text":"<pre><code>extend(modules)\n</code></pre> <p>Append modules from a Python iterable to the end of the list.</p> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>iterable</code> <p>iterable of modules to append</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def extend(self, modules: Iterable[Module]) -&gt; Self:\n    \"\"\"Append modules from a Python iterable to the end of the list.\n\n    Args:\n        modules (iterable): iterable of modules to append\n    \"\"\"\n    if not isinstance(modules, container_abcs.Iterable):\n        raise TypeError(\n            \"ModuleList.extend should be called with an \"\n            \"iterable, but got \" + type(modules).__name__\n        )\n    offset = len(self)\n    for i, module in enumerate(modules):\n        self.add_module(str(offset + i), module)\n    return self\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.insert","title":"insert","text":"<pre><code>insert(index, module)\n</code></pre> <p>Insert a given module before a given index in the list.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>index to insert.</p> required <code>module</code> <code>Module</code> <p>module to insert</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def insert(self, index: int, module: Module) -&gt; None:\n    \"\"\"Insert a given module before a given index in the list.\n\n    Args:\n        index (int): index to insert.\n        module (nn.Module): module to insert\n    \"\"\"\n    for i in range(len(self._modules), index, -1):\n        self._modules[str(i)] = self._modules[str(i - 1)]\n    self._modules[str(index)] = module\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.pop","title":"pop","text":"<pre><code>pop(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def pop(self, key: Union[int, slice]) -&gt; Module:\n    v = self[key]\n    del self[key]\n    return v\n</code></pre>"},{"location":"api-reference/nn/modules/predictor/","title":"Predictor","text":""},{"location":"api-reference/nn/modules/predictor/#msgflux.nn.modules.predictor.Predictor","title":"Predictor","text":"<p>               Bases: <code>Module</code></p> <p>Predictor is a generic Module type that uses Classifier, Regressors, Detectors and Segmenters to generate insights above data.</p> Source code in <code>src/msgflux/nn/modules/predictor.py</code> <pre><code>class Predictor(Module):\n    \"\"\"Predictor is a generic Module type that uses Classifier, Regressors,\n    Detectors and Segmenters to generate insights above data.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Union[BaseModel, ModelGateway],\n        *,\n        message_fields: Optional[Dict[str, Any]] = None,\n        response_mode: Optional[str] = \"plain_response\",\n        response_template: Optional[str] = None,\n        config: Optional[Dict[str, Any]] = None,\n        name: Optional[str] = None,        \n    ):\n        \"\"\"Args:\n        model:\n            Predictor Model client.\n        message_fields:\n            Dictionary mapping Message field names to their paths in the Message object.\n            Valid keys: \"task_inputs\", \"model_preference\"\n            !!! example\n                message_fields={\n                    \"task_inputs\": \"data.input\",\n                    \"model_preference\": \"model.preference\"\n                }\n\n            Field descriptions:\n            - task_inputs: Field path for task input (str)\n            - model_preference: Field path for model preference (str, only valid with ModelGateway)\n        response_mode:\n            What the response should be.\n            * `plain_response` (default): Returns the final agent response directly.\n            * other: Write on field in Message object.\n        response_template:\n            A Jinja template to format response.\n        config:\n            Dictionary with configuration options. Accepts any keys without validation.\n            All parameters will be passed directly to model execution.\n            !!! example\n                config={\"temperature\": 0.7, \"top_k\": 50}\n        name:\n            Predictor name in snake case format.                \n        \"\"\"\n        super().__init__()\n        self._set_model(model)\n        self._set_message_fields(message_fields)\n        self._set_response_mode(response_mode)\n        self._set_response_template(response_template)\n        self._set_config(config)\n        if name:\n            self.set_name(name)\n\n    def forward(self, message: Union[Any, Message], **kwargs) -&gt; Any:\n        \"\"\"Execute the predictor with the given message.\n\n        Args:\n            message: The input message, which can be:\n                - Any: Direct data input for prediction (text, image, audio, etc.)\n                - Message: Message object with fields mapped via message_fields\n            **kwargs: Runtime overrides for message_fields. Can include:\n                - task_inputs: Override field path or direct value\n                - model_preference: Override model preference\n\n        Returns:\n            Prediction results (type depends on model and response_mode)\n        \"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = self._execute_model(**inputs)\n        response = self._process_model_response(model_response, message)\n        return response\n\n    async def aforward(self, message: Union[Any, Message], **kwargs) -&gt; Any:\n        \"\"\"Async version of forward. Execute the predictor asynchronously.\"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = await self._aexecute_model(**inputs)\n        response = self._process_model_response(model_response, message)\n        return response\n\n    def _execute_model(\n        self, data: Any, model_preference: Optional[str] = None\n    ) -&gt; ModelResponse:\n        model_execution_params = self._prepare_model_execution(data, model_preference)\n        model_response = self.model(**model_execution_params)\n        return model_response\n\n    async def _aexecute_model(\n        self, data: Any, model_preference: Optional[str] = None\n    ) -&gt; ModelResponse:\n        model_execution_params = self._prepare_model_execution(data, model_preference)\n        model_response = await self.model.acall(**model_execution_params)\n        return model_response\n\n    def _prepare_model_execution(\n        self, data: Any, model_preference: Optional[str] = None\n    ) -&gt; Dict[str, Any]:\n        model_execution_params = dotdict(self.config) if self.config else dotdict()\n        model_execution_params.data = data\n        if model_preference:\n            model_execution_params.model_preference = model_preference\n        return model_execution_params\n\n    def _process_model_response(\n        self, model_response: ModelResponse, message: Union[Any, Message]\n    ) -&gt; Any:\n        if model_response.response_type == \"audio_generation\":\n            raw_response = self._extract_raw_response(model_response)\n            response = self._prepare_response(raw_response, message)\n            return response\n        else:\n            raise ValueError(\n                f\"Unsupported model response type `{model_response.response_type}`\"\n            )\n\n    def _prepare_task(self, message: Union[Any, Message], **kwargs) -&gt; Dict[str, Any]:\n        inputs = dotdict()\n\n        if isinstance(message, Message):\n            data = self._extract_message_values(self.task_inputs, message)\n        else:\n            data = message\n\n        inputs.data = data\n\n        model_preference = kwargs.pop(\"model_preference\", None)\n        if model_preference is None and isinstance(message, Message):\n            model_preference = self.get_model_preference_from_message(message)\n\n        if model_preference:\n            inputs.model_preference = model_preference\n\n        return inputs\n\n    def inspect_model_execution_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n        \"\"\"Debug model input parameters.\"\"\"\n        inputs = self._prepare_task(*args, **kwargs)\n        model_execution_params = self._prepare_model_execution(**inputs)\n        return model_execution_params\n\n    def _set_model(self, model: Union[BaseModel, ModelGateway]):\n        if isinstance(model, (BaseModel, ModelGateway)):\n            self.register_buffer(\"model\", model)\n        else:\n            raise TypeError(\n                f\"`model` need be a `BaseModel` model, given `{type(model)}`\"\n            )\n\n    def _set_config(self, config: Optional[Dict[str, Any]] = None):\n        if config is None:\n            self.register_buffer(\"config\", {})\n            return\n\n        if not isinstance(config, dict):\n            raise TypeError(\n                f\"`config` must be a dict or None, given `{type(config)}`\"\n            )\n\n        self.register_buffer(\"config\", config.copy())\n</code></pre>"},{"location":"api-reference/nn/modules/predictor/#msgflux.nn.modules.predictor.Predictor.__init__","title":"__init__","text":"<pre><code>__init__(\n    model,\n    *,\n    message_fields=None,\n    response_mode=\"plain_response\",\n    response_template=None,\n    config=None,\n    name=None,\n)\n</code></pre> <p>model:     Predictor Model client. message_fields:     Dictionary mapping Message field names to their paths in the Message object.     Valid keys: \"task_inputs\", \"model_preference\"     !!! example         message_fields={             \"task_inputs\": \"data.input\",             \"model_preference\": \"model.preference\"         }</p> <pre><code>Field descriptions:\n- task_inputs: Field path for task input (str)\n- model_preference: Field path for model preference (str, only valid with ModelGateway)\n</code></pre> <p>response_mode:     What the response should be.     * <code>plain_response</code> (default): Returns the final agent response directly.     * other: Write on field in Message object. response_template:     A Jinja template to format response. config:     Dictionary with configuration options. Accepts any keys without validation.     All parameters will be passed directly to model execution.     !!! example         config={\"temperature\": 0.7, \"top_k\": 50} name:     Predictor name in snake case format.</p> Source code in <code>src/msgflux/nn/modules/predictor.py</code> <pre><code>def __init__(\n    self,\n    model: Union[BaseModel, ModelGateway],\n    *,\n    message_fields: Optional[Dict[str, Any]] = None,\n    response_mode: Optional[str] = \"plain_response\",\n    response_template: Optional[str] = None,\n    config: Optional[Dict[str, Any]] = None,\n    name: Optional[str] = None,        \n):\n    \"\"\"Args:\n    model:\n        Predictor Model client.\n    message_fields:\n        Dictionary mapping Message field names to their paths in the Message object.\n        Valid keys: \"task_inputs\", \"model_preference\"\n        !!! example\n            message_fields={\n                \"task_inputs\": \"data.input\",\n                \"model_preference\": \"model.preference\"\n            }\n\n        Field descriptions:\n        - task_inputs: Field path for task input (str)\n        - model_preference: Field path for model preference (str, only valid with ModelGateway)\n    response_mode:\n        What the response should be.\n        * `plain_response` (default): Returns the final agent response directly.\n        * other: Write on field in Message object.\n    response_template:\n        A Jinja template to format response.\n    config:\n        Dictionary with configuration options. Accepts any keys without validation.\n        All parameters will be passed directly to model execution.\n        !!! example\n            config={\"temperature\": 0.7, \"top_k\": 50}\n    name:\n        Predictor name in snake case format.                \n    \"\"\"\n    super().__init__()\n    self._set_model(model)\n    self._set_message_fields(message_fields)\n    self._set_response_mode(response_mode)\n    self._set_response_template(response_template)\n    self._set_config(config)\n    if name:\n        self.set_name(name)\n</code></pre>"},{"location":"api-reference/nn/modules/predictor/#msgflux.nn.modules.predictor.Predictor.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(message, **kwargs)\n</code></pre> <p>Async version of forward. Execute the predictor asynchronously.</p> Source code in <code>src/msgflux/nn/modules/predictor.py</code> <pre><code>async def aforward(self, message: Union[Any, Message], **kwargs) -&gt; Any:\n    \"\"\"Async version of forward. Execute the predictor asynchronously.\"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = await self._aexecute_model(**inputs)\n    response = self._process_model_response(model_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/predictor/#msgflux.nn.modules.predictor.Predictor.forward","title":"forward","text":"<pre><code>forward(message, **kwargs)\n</code></pre> <p>Execute the predictor with the given message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Union[Any, Message]</code> <p>The input message, which can be: - Any: Direct data input for prediction (text, image, audio, etc.) - Message: Message object with fields mapped via message_fields</p> required <code>**kwargs</code> <p>Runtime overrides for message_fields. Can include: - task_inputs: Override field path or direct value - model_preference: Override model preference</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Prediction results (type depends on model and response_mode)</p> Source code in <code>src/msgflux/nn/modules/predictor.py</code> <pre><code>def forward(self, message: Union[Any, Message], **kwargs) -&gt; Any:\n    \"\"\"Execute the predictor with the given message.\n\n    Args:\n        message: The input message, which can be:\n            - Any: Direct data input for prediction (text, image, audio, etc.)\n            - Message: Message object with fields mapped via message_fields\n        **kwargs: Runtime overrides for message_fields. Can include:\n            - task_inputs: Override field path or direct value\n            - model_preference: Override model preference\n\n    Returns:\n        Prediction results (type depends on model and response_mode)\n    \"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = self._execute_model(**inputs)\n    response = self._process_model_response(model_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/predictor/#msgflux.nn.modules.predictor.Predictor.inspect_model_execution_params","title":"inspect_model_execution_params","text":"<pre><code>inspect_model_execution_params(*args, **kwargs)\n</code></pre> <p>Debug model input parameters.</p> Source code in <code>src/msgflux/nn/modules/predictor.py</code> <pre><code>def inspect_model_execution_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n    \"\"\"Debug model input parameters.\"\"\"\n    inputs = self._prepare_task(*args, **kwargs)\n    model_execution_params = self._prepare_model_execution(**inputs)\n    return model_execution_params\n</code></pre>"},{"location":"api-reference/nn/modules/retriever/","title":"Retriever","text":""},{"location":"api-reference/nn/modules/retriever/#msgflux.nn.modules.retriever.Retriever","title":"Retriever","text":"<p>               Bases: <code>Module</code></p> <p>Retriever is a Module type that uses information retrivers.</p> Source code in <code>src/msgflux/nn/modules/retriever.py</code> <pre><code>class Retriever(Module):\n    \"\"\"Retriever is a Module type that uses information retrivers.\"\"\"\n\n    def __init__(\n        self,\n        retriever: RETRIVERS,\n        *,\n        model: Optional[Union[EMBEDDER_MODELS, Embedder]] = None,\n        message_fields: Optional[Dict[str, Any]] = None,\n        response_mode: Optional[str] = \"plain_response\",\n        templates: Optional[Dict[str, str]] = None,\n        config: Optional[Dict[str, Any]] = None,\n        name: Optional[str] = None,\n    ):\n        \"\"\"Args:\n        retriever:\n            Retriever client.\n        model:\n            Embedding model for converting queries to embeddings. Can be either:\n            - Embedder: Custom Embedder instance (for advanced usage with hooks)\n            - EmbedderModel/ModelGateway: Will be auto-wrapped in Embedder\n            Optional - only needed for semantic retrieval.\n        message_fields:\n            Dictionary mapping Message field names to their paths in the Message object.\n            Valid keys: \"task_inputs\"\n            !!! example\n                message_fields={\"task_inputs\": \"query.user\"}\n\n            Field description:\n            - task_inputs: Field path for query input (str or dict)\n        response_mode:\n            What the response should be.\n            * `plain_response` (default): Returns the final agent response directly.\n            * other: Write on field in Message object.\n        templates:\n            Dictionary mapping template types to Jinja template strings.\n            Valid keys: \"response\"\n            !!! example\n                templates={\"response\": \"Results: {{ content }}\"}\n        config:\n            Dictionary with configuration options. Accepts any keys without validation.\n            Common options: \"top_k\", \"threshold\", \"return_score\", \"dict_key\"\n            !!! example\n                config={\n                    \"top_k\": 4,\n                    \"threshold\": 0.0,\n                    \"return_score\": False,\n                    \"dict_key\": \"name\"\n                }\n\n            Configuration options:\n            - top_k: Maximum return of similar points (int)\n            - threshold: Retriever threshold (float)\n            - return_score: If True, return similarity score (bool)\n            - dict_key: Help to extract a value from task_inputs if dict (str)\n        name:\n            Retriever name in snake case format.            \n        \"\"\"\n        super().__init__()\n        self._set_retriever(retriever)\n        self._set_model(model)\n        self._set_message_fields(message_fields)\n        self._set_response_mode(response_mode)\n        self._set_templates(templates)\n        self._set_config(config)\n        if name:\n            self.set_name(name)\n\n    def forward(\n        self, message: Union[str, List[str], List[Dict[str, Any]], Message], **kwargs\n    ) -&gt; Union[str, Dict[str, str], Message]:\n        \"\"\"Execute the retriever with the given message.\n\n        Args:\n            message: The input message, which can be:\n                - str: Direct query string for retrieval\n                - List[str]: List of query strings\n                - List[Dict[str, Any]]: List of query dictionaries\n                - Message: Message object with fields mapped via message_fields\n            **kwargs: Runtime overrides for message_fields. Can include:\n                - task_inputs: Override field path or direct value\n\n        Returns:\n            Retrieved results (str, dict, or Message depending on response_mode)\n        \"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        retriever_response = self._execute_retriever(**inputs)\n        response = self._prepare_response(retriever_response, message)\n        return response\n\n    async def aforward(\n        self, message: Union[str, List[str], List[Dict[str, Any]], Message], **kwargs\n    ) -&gt; Union[str, Dict[str, str], Message]:\n        \"\"\"Async version of forward. Execute the retriever asynchronously.\"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        retriever_response = await self._aexecute_retriever(**inputs)\n        response = self._prepare_response(retriever_response, message)\n        return response\n\n    def _execute_retriever(\n        self, queries: List[str], model_preference: Optional[str] = None\n    ) -&gt; List[Dict[str, Any]]:\n        queries_embed = None\n        if self.embedder:\n            queries_embed = self.embedder(queries, model_preference=model_preference)\n            # Ensure list format\n            if not isinstance(queries_embed[0], list):\n                queries_embed = [queries_embed]\n\n        retriever_execution_params = self._prepare_retriever_execution(\n            queries_embed or queries\n        )\n        retriever_response = self.retriever(**retriever_execution_params)\n\n        results = []\n\n        for query, query_results in zip(queries, retriever_response):\n            formatted_result = {\n                \"results\": [\n                    {\"data\": item.get(\"data\", None), \"score\": item.get(\"score\", None)}\n                    for item in query_results\n                ],\n            }\n            if isinstance(query, str):\n                formatted_result[\"query\"] = query\n            results.append(formatted_result)\n\n        return results\n\n    async def _aexecute_retriever(\n        self, queries: List[str], model_preference: Optional[str] = None\n    ) -&gt; List[Dict[str, Any]]:\n        queries_embed = None\n        if self.embedder:\n            queries_embed = await self.embedder.aforward(queries, model_preference=model_preference)\n            # Ensure list format\n            if not isinstance(queries_embed[0], list):\n                queries_embed = [queries_embed]\n\n        retriever_execution_params = self._prepare_retriever_execution(\n            queries_embed or queries\n        )\n        retriever_response = self.retriever(**retriever_execution_params)\n\n        results = []\n\n        for query, query_results in zip(queries, retriever_response):\n            formatted_result = {\n                \"results\": [\n                    {\"data\": item.get(\"data\", None), \"score\": item.get(\"score\", None)}\n                    for item in query_results\n                ],\n            }\n            if isinstance(query, str):\n                formatted_result[\"query\"] = query\n            results.append(formatted_result)\n\n        return results\n\n    def _prepare_retriever_execution(\n        self, queries: List[Union[str, List[float]]]\n    ) -&gt; Dict[str, Any]:\n        retriever_execution_params = dotdict(\n            queries=queries,\n            top_k=self.config.get(\"top_k\", 4),\n            return_score=self.config.get(\"return_score\", False)\n        )\n        threshold = self.config.get(\"threshold\")\n        if threshold:\n            retriever_execution_params.threshold = threshold\n        return retriever_execution_params\n\n\n    def _prepare_task(\n        self, message: Union[str, List[str], List[Dict[str, Any]], Message], **kwargs\n    ) -&gt; List[str]:\n        if isinstance(message, Message):\n            queries = self._extract_message_values(self.task_inputs, message)\n        else:\n            queries = message\n\n        if isinstance(queries, str):\n            queries = [queries]\n        elif isinstance(queries, list):\n            if isinstance(queries[0], dict):\n                queries = self._process_list_of_dict_inputs(queries)\n\n        model_preference = kwargs.pop(\"model_preference\", None)\n        if model_preference is None and isinstance(message, Message):\n            model_preference = self.get_model_preference_from_message(message)\n\n        return {\"queries\": queries, \"model_preference\": model_preference}\n\n    def _process_list_of_dict_inputs(self, queries: List[Dict[str, Any]]) -&gt; List[str]:\n        \"\"\"Extract the query value from a dict.\"\"\"\n        dict_key = self.config.get(\"dict_key\")\n        if dict_key:\n            queries_list = [data[dict_key] for data in queries]\n            return queries_list\n        else:\n            raise AttributeError(\n                \"message that contain `List[Dict[str, Any]]` \"\n                \"require a `dict_key` to select the key for retrieval\"\n            )\n\n    def inspect_embedder_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n        \"\"\"Debug embedder input parameters.\n\n        Returns the parameters that would be passed to the embedder module.\n        \"\"\"\n        if self.embedder:\n            inputs = self._prepare_task(*args, **kwargs)\n            return {\"queries\": inputs[\"queries\"], \"model_preference\": inputs.get(\"model_preference\")}\n        return {}\n\n    def _set_retriever(self, retriever: RETRIVERS):\n        if isinstance(\n            retriever, (WebRetriever, LexicalRetriever, SemanticRetriever, VectorDB)\n        ):\n            self.register_buffer(\"retriever\", retriever)\n        else:\n            raise TypeError(\n                \"`retriever` requires `HybridRetriever`, `LexicalRetriever`, \"\n                f\"`SemanticRetriever` or `VectorDB` instance given `{type(retriever)}`\"\n            )\n\n    def _set_model(self, model: Optional[Union[EMBEDDER_MODELS, Embedder]] = None):\n        if model is None:\n            self.embedder = None\n            return\n\n        if isinstance(model, Embedder): # If already Embedder, use directly\n            self.embedder = model\n        else: # Auto-wrap in Embedder\n            self.embedder = Embedder(model=model)\n\n    @property\n    def model(self):\n        \"\"\"Access underlying model for convenience.\n\n        Returns:\n            The wrapped model instance, or None if no embedder\n        \"\"\"\n        if self.embedder is None:\n            return None\n        return self.embedder.model\n\n    @model.setter\n    def model(self, value: Optional[Union[EMBEDDER_MODELS, Embedder]]):\n        \"\"\"Update the retriever's model.\"\"\"\n        self._set_model(value)\n\n    def _set_config(self, config: Optional[Dict[str, Any]] = None):\n        if config is None:\n            self.register_buffer(\"config\", {})\n            return\n\n        if not isinstance(config, dict):\n            raise TypeError(\n                f\"`config` must be a dict or None, given `{type(config)}`\"\n            )\n\n        self.register_buffer(\"config\", config.copy())\n</code></pre>"},{"location":"api-reference/nn/modules/retriever/#msgflux.nn.modules.retriever.Retriever.model","title":"model  <code>property</code> <code>writable</code>","text":"<pre><code>model\n</code></pre> <p>Access underlying model for convenience.</p> <p>Returns:</p> Type Description <p>The wrapped model instance, or None if no embedder</p>"},{"location":"api-reference/nn/modules/retriever/#msgflux.nn.modules.retriever.Retriever.__init__","title":"__init__","text":"<pre><code>__init__(\n    retriever,\n    *,\n    model=None,\n    message_fields=None,\n    response_mode=\"plain_response\",\n    templates=None,\n    config=None,\n    name=None,\n)\n</code></pre> <p>retriever:     Retriever client. model:     Embedding model for converting queries to embeddings. Can be either:     - Embedder: Custom Embedder instance (for advanced usage with hooks)     - EmbedderModel/ModelGateway: Will be auto-wrapped in Embedder     Optional - only needed for semantic retrieval. message_fields:     Dictionary mapping Message field names to their paths in the Message object.     Valid keys: \"task_inputs\"     !!! example         message_fields={\"task_inputs\": \"query.user\"}</p> <pre><code>Field description:\n- task_inputs: Field path for query input (str or dict)\n</code></pre> <p>response_mode:     What the response should be.     * <code>plain_response</code> (default): Returns the final agent response directly.     * other: Write on field in Message object. templates:     Dictionary mapping template types to Jinja template strings.     Valid keys: \"response\"     !!! example         templates={\"response\": \"Results: {{ content }}\"} config:     Dictionary with configuration options. Accepts any keys without validation.     Common options: \"top_k\", \"threshold\", \"return_score\", \"dict_key\"     !!! example         config={             \"top_k\": 4,             \"threshold\": 0.0,             \"return_score\": False,             \"dict_key\": \"name\"         }</p> <pre><code>Configuration options:\n- top_k: Maximum return of similar points (int)\n- threshold: Retriever threshold (float)\n- return_score: If True, return similarity score (bool)\n- dict_key: Help to extract a value from task_inputs if dict (str)\n</code></pre> <p>name:     Retriever name in snake case format.</p> Source code in <code>src/msgflux/nn/modules/retriever.py</code> <pre><code>def __init__(\n    self,\n    retriever: RETRIVERS,\n    *,\n    model: Optional[Union[EMBEDDER_MODELS, Embedder]] = None,\n    message_fields: Optional[Dict[str, Any]] = None,\n    response_mode: Optional[str] = \"plain_response\",\n    templates: Optional[Dict[str, str]] = None,\n    config: Optional[Dict[str, Any]] = None,\n    name: Optional[str] = None,\n):\n    \"\"\"Args:\n    retriever:\n        Retriever client.\n    model:\n        Embedding model for converting queries to embeddings. Can be either:\n        - Embedder: Custom Embedder instance (for advanced usage with hooks)\n        - EmbedderModel/ModelGateway: Will be auto-wrapped in Embedder\n        Optional - only needed for semantic retrieval.\n    message_fields:\n        Dictionary mapping Message field names to their paths in the Message object.\n        Valid keys: \"task_inputs\"\n        !!! example\n            message_fields={\"task_inputs\": \"query.user\"}\n\n        Field description:\n        - task_inputs: Field path for query input (str or dict)\n    response_mode:\n        What the response should be.\n        * `plain_response` (default): Returns the final agent response directly.\n        * other: Write on field in Message object.\n    templates:\n        Dictionary mapping template types to Jinja template strings.\n        Valid keys: \"response\"\n        !!! example\n            templates={\"response\": \"Results: {{ content }}\"}\n    config:\n        Dictionary with configuration options. Accepts any keys without validation.\n        Common options: \"top_k\", \"threshold\", \"return_score\", \"dict_key\"\n        !!! example\n            config={\n                \"top_k\": 4,\n                \"threshold\": 0.0,\n                \"return_score\": False,\n                \"dict_key\": \"name\"\n            }\n\n        Configuration options:\n        - top_k: Maximum return of similar points (int)\n        - threshold: Retriever threshold (float)\n        - return_score: If True, return similarity score (bool)\n        - dict_key: Help to extract a value from task_inputs if dict (str)\n    name:\n        Retriever name in snake case format.            \n    \"\"\"\n    super().__init__()\n    self._set_retriever(retriever)\n    self._set_model(model)\n    self._set_message_fields(message_fields)\n    self._set_response_mode(response_mode)\n    self._set_templates(templates)\n    self._set_config(config)\n    if name:\n        self.set_name(name)\n</code></pre>"},{"location":"api-reference/nn/modules/retriever/#msgflux.nn.modules.retriever.Retriever.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(message, **kwargs)\n</code></pre> <p>Async version of forward. Execute the retriever asynchronously.</p> Source code in <code>src/msgflux/nn/modules/retriever.py</code> <pre><code>async def aforward(\n    self, message: Union[str, List[str], List[Dict[str, Any]], Message], **kwargs\n) -&gt; Union[str, Dict[str, str], Message]:\n    \"\"\"Async version of forward. Execute the retriever asynchronously.\"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    retriever_response = await self._aexecute_retriever(**inputs)\n    response = self._prepare_response(retriever_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/retriever/#msgflux.nn.modules.retriever.Retriever.forward","title":"forward","text":"<pre><code>forward(message, **kwargs)\n</code></pre> <p>Execute the retriever with the given message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Union[str, List[str], List[Dict[str, Any]], Message]</code> <p>The input message, which can be: - str: Direct query string for retrieval - List[str]: List of query strings - List[Dict[str, Any]]: List of query dictionaries - Message: Message object with fields mapped via message_fields</p> required <code>**kwargs</code> <p>Runtime overrides for message_fields. Can include: - task_inputs: Override field path or direct value</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, Dict[str, str], Message]</code> <p>Retrieved results (str, dict, or Message depending on response_mode)</p> Source code in <code>src/msgflux/nn/modules/retriever.py</code> <pre><code>def forward(\n    self, message: Union[str, List[str], List[Dict[str, Any]], Message], **kwargs\n) -&gt; Union[str, Dict[str, str], Message]:\n    \"\"\"Execute the retriever with the given message.\n\n    Args:\n        message: The input message, which can be:\n            - str: Direct query string for retrieval\n            - List[str]: List of query strings\n            - List[Dict[str, Any]]: List of query dictionaries\n            - Message: Message object with fields mapped via message_fields\n        **kwargs: Runtime overrides for message_fields. Can include:\n            - task_inputs: Override field path or direct value\n\n    Returns:\n        Retrieved results (str, dict, or Message depending on response_mode)\n    \"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    retriever_response = self._execute_retriever(**inputs)\n    response = self._prepare_response(retriever_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/retriever/#msgflux.nn.modules.retriever.Retriever.inspect_embedder_params","title":"inspect_embedder_params","text":"<pre><code>inspect_embedder_params(*args, **kwargs)\n</code></pre> <p>Debug embedder input parameters.</p> <p>Returns the parameters that would be passed to the embedder module.</p> Source code in <code>src/msgflux/nn/modules/retriever.py</code> <pre><code>def inspect_embedder_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n    \"\"\"Debug embedder input parameters.\n\n    Returns the parameters that would be passed to the embedder module.\n    \"\"\"\n    if self.embedder:\n        inputs = self._prepare_task(*args, **kwargs)\n        return {\"queries\": inputs[\"queries\"], \"model_preference\": inputs.get(\"model_preference\")}\n    return {}\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/","title":"Sequential","text":""},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential","title":"Sequential","text":"<p>               Bases: <code>Module</code></p> <p>A sequential container.</p> <p>Modules will be added to it in the order they are passed in the constructor. Alternatively, an <code>OrderedDict</code> of modules can be passed in. The <code>forward()</code> method of <code>Sequential</code> accepts any input and forwards it to the first module it contains. It then \"chains\" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.</p> <p>The value a <code>Sequential</code> provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the <code>Sequential</code> applies to each of the modules it stores (which are each a registered submodule of the <code>Sequential</code>).</p> <p>What's the difference between a <code>Sequential</code> and a <code>msgflux.nn.ModuleList</code>? A <code>ModuleList</code> is exactly what it sounds like--a list for storing <code>Module</code>s! On the other hand, the layers in a <code>Sequential</code> are connected in a cascading way.</p> <p>Example</p> <pre><code>from collections import OrderedDict\nimport msgflux.nn as nn\nclass ExpertSales(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass ExpertSupport(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, call 190\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\n# Using Sequential to create a small workflow. When **expert** is run,\n# input will first be passed to **ExpertSales**. The output of\n# **ExpertSales** will be used as the input to the first\n# **ExpertSupport**; Finally, the output of\n# **ExpertSupport** will be the experts response.\nexperts = nn.Sequential(ExpertSales(), ExpertSupport())\nexperts(\"I need help with my tv.\")\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nexperts_dict = nn.Sequential(OrderedDict([\n    (\"expert_sales\", ExpertSales()),\n    (\"expert_support\", ExpertSupport())\n]))\nexperts_dict(\"I need help with my tv.\")\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>class Sequential(Module):\n    \"\"\"A sequential container.\n\n    Modules will be added to it in the order they are passed in the\n    constructor. Alternatively, an `OrderedDict` of modules can be\n    passed in. The `forward()` method of `Sequential` accepts any\n    input and forwards it to the first module it contains. It then\n    \"chains\" outputs to inputs sequentially for each subsequent module,\n    finally returning the output of the last module.\n\n    The value a `Sequential` provides over manually calling a sequence\n    of modules is that it allows treating the whole container as a\n    single module, such that performing a transformation on the\n    `Sequential` applies to each of the modules it stores (which are\n    each a registered submodule of the `Sequential`).\n\n    What's the difference between a `Sequential` and a\n    `msgflux.nn.ModuleList`? A `ModuleList` is exactly what it\n    sounds like--a list for storing `Module`s! On the other hand,\n    the layers in a `Sequential` are connected in a cascading way.\n\n    !!! example\n        ```python\n        from collections import OrderedDict\n        import msgflux.nn as nn\n        class ExpertSales(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        class ExpertSupport(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, call 190\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        # Using Sequential to create a small workflow. When **expert** is run,\n        # input will first be passed to **ExpertSales**. The output of\n        # **ExpertSales** will be used as the input to the first\n        # **ExpertSupport**; Finally, the output of\n        # **ExpertSupport** will be the experts response.\n        experts = nn.Sequential(ExpertSales(), ExpertSupport())\n        experts(\"I need help with my tv.\")\n\n        # Using Sequential with OrderedDict. This is functionally the\n        # same as the above code\n        experts_dict = nn.Sequential(OrderedDict([\n            (\"expert_sales\", ExpertSales()),\n            (\"expert_support\", ExpertSupport())\n        ]))\n        experts_dict(\"I need help with my tv.\")\n        ```\n    \"\"\"\n\n    _modules: Dict[str, Module] = OrderedDict()\n\n    def __init__(self, *args: Union[Module, OrderedDict[str, Module]]):\n        super().__init__()\n        if len(args) == 1 and isinstance(args[0], OrderedDict):\n            for key, module in args[0].items():\n                self.add_module(key, module)\n        else:\n            for idx, module in enumerate(args):\n                self.add_module(str(idx), module)\n\n    def forward(self, *args, **kwargs) -&gt; Any:\n        modules_iter = iter(self._modules.values())\n        first_module = next(modules_iter)\n        output = first_module(*args, **kwargs)\n\n        for module in modules_iter:\n            output = module(output)\n\n        return output\n\n    async def aforward(self, *args, **kwargs) -&gt; Any:\n        \"\"\"Async version of forward. Executes modules sequentially with async support.\"\"\"\n        modules_iter = iter(self._modules.values())\n        first_module = next(modules_iter)\n\n        # Check for acall method first, then coroutine function\n        if hasattr(first_module, \"acall\"):\n            output = await first_module.acall(*args, **kwargs)\n        elif asyncio.iscoroutinefunction(first_module):\n            output = await first_module(*args, **kwargs)\n        else:\n            # Fallback to sync call\n            output = first_module(*args, **kwargs)\n\n        for module in modules_iter:\n            # Check for acall method first, then coroutine function\n            if hasattr(module, 'acall'):\n                output = await module.acall(output)\n            elif asyncio.iscoroutinefunction(module):\n                output = await module(output)\n            else:\n                # Fallback to sync call\n                output = module(output)\n\n        return output\n\n    def _get_mermaid(\n        self,\n        title: Optional[str] = None,\n        orientation: Optional[str] = \"TD\",\n    ) -&gt; str:\n        mermaid_code = [\n            \"%%{\",\n            \"        init: {\",\n            \"            'theme': 'base',\",\n            \"            'themeVariables': {\",\n            \"            'primaryColor': '#E9E7E7',\",\n            \"            'primaryTextColor': '#000000',\",\n            \"            'primaryBorderColor': '#C0000',\",\n            \"            'lineColor': '#F8B229',\",\n            \"            'secondaryColor': '#91939C',\",\n            \"            'tertiaryColor': '#fff'\",\n            \"            }\",\n            \"        }\",\n            \"    }%%\",\n            f\"flowchart {orientation}\",\n        ]\n\n        if title:\n            mermaid_code.insert(0, f\"%% Title: {title}\")\n\n        mermaid_code.append(\"subgraph PARAMETERS\")\n        mermaid_code.append(\"direction LR\")\n        mermaid_code.append(\"param_msg([**msg**])\")\n        mermaid_code.append(\"end\")\n\n        first_node = None\n        for i, module_name in enumerate(self._modules.keys()):\n            node_id = f\"node_{i}\"\n            if i == 0:\n                first_node = node_id\n            mermaid_code.append(f\"{node_id}[ **msg = {module_name}\ufe59msg\ufe5a** ]\")\n\n        for i in range(len(self._modules) - 1):\n            mermaid_code.append(f\"node_{i} --&gt; node_{i + 1}\")\n\n        last_node = f\"node_{len(self._modules) - 1}\"\n        mermaid_code.append(f\"{last_node} --&gt; node_return\")\n        mermaid_code.append(\"node_return([**return msg**])\")\n\n        if first_node:\n            mermaid_code.append(f\"PARAMETERS --&gt; {first_node}\")\n\n        mermaid_code.append(\"%% Styles\")\n        mermaid_code.append(\n            \"classDef terminal fill:#FFF4DD,stroke:#333,stroke-width:2px;\"\n        )\n        mermaid_code.append(\n            \"classDef parameter fill:#FFF4DD,stroke:#333,stroke-width:px;\"\n        )\n\n        for i in range(len(self._modules)):\n            mermaid_code.append(f\"class node_{i} default;\")\n        mermaid_code.append(\"class node_return terminal;\")\n\n        mermaid_code.append(\"class param_msg parameter;\")\n\n        return \"\\n\".join(mermaid_code)\n\n    def _get_item_by_idx(self, iterator, idx) -&gt; T:\n        \"\"\"Get the idx-th item of the iterator.\"\"\"\n        size = len(self)\n        idx = operator.index(idx)\n        if not -size &lt;= idx &lt; size:\n            raise IndexError(f\"index {idx} is out of range\")\n        idx %= size\n        return next(islice(iterator, idx, None))\n\n    def __getitem__(self, idx: Union[slice, int]) -&gt; Union[\"Sequential\", T]:\n        if isinstance(idx, slice):\n            return self.__class__(OrderedDict(list(self._modules.items())[idx]))\n        else:\n            return self._get_item_by_idx(self._modules.values(), idx)\n\n    def __setitem__(self, idx: int, module: Module) -&gt; None:\n        key: str = self._get_item_by_idx(self._modules.keys(), idx)\n        return setattr(self, key, module)\n\n    def __delitem__(self, idx: Union[slice, int]) -&gt; None:\n        if isinstance(idx, slice):\n            for key in list(self._modules.keys())[idx]:\n                delattr(self, key)\n        else:\n            key = self._get_item_by_idx(self._modules.keys(), idx)\n            delattr(self, key)\n        # To preserve numbering\n        str_indices = [str(i) for i in range(len(self._modules))]\n        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n\n    def __len__(self) -&gt; int:\n        return len(self._modules)\n\n    def __add__(self, other) -&gt; \"Sequential\":\n        if isinstance(other, Sequential):\n            ret = Sequential()\n            for layer in self:\n                ret.append(layer)\n            for layer in other:\n                ret.append(layer)\n            return ret\n        else:\n            raise ValueError(\n                \"add operator supports only objects \"\n                f\"of Sequential class, but {type(other)!s} is given.\"\n            )\n\n    def pop(self, key: Union[int, slice]) -&gt; Module:\n        v = self[key]\n        del self[key]\n        return v\n\n    def __iadd__(self, other) -&gt; Self:\n        if isinstance(other, Sequential):\n            offset = len(self)\n            for i, module in enumerate(other):\n                self.add_module(str(i + offset), module)\n            return self\n        else:\n            raise ValueError(\n                \"add operator supports only objects \"\n                f\"of Sequential class, but {type(other)!s} is given.\"\n            )\n\n    def __dir__(self):\n        keys = super().__dir__()\n        keys = [key for key in keys if not key.isdigit()]\n        return keys\n\n    def __iter__(self) -&gt; Iterator[Module]:\n        return iter(self._modules.values())\n\n    def append(self, module: Module) -&gt; \"Sequential\":\n        r\"\"\"Append a given module to the end.\n\n        Args:\n            module (nn.Module): module to append\n        \"\"\"\n        self.add_module(str(len(self)), module)\n        return self\n\n    def insert(self, index: int, module: Module) -&gt; \"Sequential\":\n        if not isinstance(module, Module):\n            raise AssertionError(f\"module should be of type: {Module}\")\n        n = len(self._modules)\n        if not (-n &lt;= index &lt;= n):\n            raise IndexError(f\"Index out of range: {index}\")\n        if index &lt; 0:\n            index += n\n        for i in range(n, index, -1):\n            self._modules[str(i)] = self._modules[str(i - 1)]\n        self._modules[str(index)] = module\n        return self\n\n    def extend(self, sequential) -&gt; \"Sequential\":\n        for layer in sequential:\n            self.append(layer)\n        return self\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __add__(self, other) -&gt; \"Sequential\":\n    if isinstance(other, Sequential):\n        ret = Sequential()\n        for layer in self:\n            ret.append(layer)\n        for layer in other:\n            ret.append(layer)\n        return ret\n    else:\n        raise ValueError(\n            \"add operator supports only objects \"\n            f\"of Sequential class, but {type(other)!s} is given.\"\n        )\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(idx)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __delitem__(self, idx: Union[slice, int]) -&gt; None:\n    if isinstance(idx, slice):\n        for key in list(self._modules.keys())[idx]:\n            delattr(self, key)\n    else:\n        key = self._get_item_by_idx(self._modules.keys(), idx)\n        delattr(self, key)\n    # To preserve numbering\n    str_indices = [str(i) for i in range(len(self._modules))]\n    self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__dir__","title":"__dir__","text":"<pre><code>__dir__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __dir__(self):\n    keys = super().__dir__()\n    keys = [key for key in keys if not key.isdigit()]\n    return keys\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __getitem__(self, idx: Union[slice, int]) -&gt; Union[\"Sequential\", T]:\n    if isinstance(idx, slice):\n        return self.__class__(OrderedDict(list(self._modules.items())[idx]))\n    else:\n        return self._get_item_by_idx(self._modules.values(), idx)\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__iadd__","title":"__iadd__","text":"<pre><code>__iadd__(other)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iadd__(self, other) -&gt; Self:\n    if isinstance(other, Sequential):\n        offset = len(self)\n        for i, module in enumerate(other):\n            self.add_module(str(i + offset), module)\n        return self\n    else:\n        raise ValueError(\n            \"add operator supports only objects \"\n            f\"of Sequential class, but {type(other)!s} is given.\"\n        )\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__init__","title":"__init__","text":"<pre><code>__init__(*args)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __init__(self, *args: Union[Module, OrderedDict[str, Module]]):\n    super().__init__()\n    if len(args) == 1 and isinstance(args[0], OrderedDict):\n        for key, module in args[0].items():\n            self.add_module(key, module)\n    else:\n        for idx, module in enumerate(args):\n            self.add_module(str(idx), module)\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Module]:\n    return iter(self._modules.values())\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._modules)\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(idx, module)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __setitem__(self, idx: int, module: Module) -&gt; None:\n    key: str = self._get_item_by_idx(self._modules.keys(), idx)\n    return setattr(self, key, module)\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(*args, **kwargs)\n</code></pre> <p>Async version of forward. Executes modules sequentially with async support.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>async def aforward(self, *args, **kwargs) -&gt; Any:\n    \"\"\"Async version of forward. Executes modules sequentially with async support.\"\"\"\n    modules_iter = iter(self._modules.values())\n    first_module = next(modules_iter)\n\n    # Check for acall method first, then coroutine function\n    if hasattr(first_module, \"acall\"):\n        output = await first_module.acall(*args, **kwargs)\n    elif asyncio.iscoroutinefunction(first_module):\n        output = await first_module(*args, **kwargs)\n    else:\n        # Fallback to sync call\n        output = first_module(*args, **kwargs)\n\n    for module in modules_iter:\n        # Check for acall method first, then coroutine function\n        if hasattr(module, 'acall'):\n            output = await module.acall(output)\n        elif asyncio.iscoroutinefunction(module):\n            output = await module(output)\n        else:\n            # Fallback to sync call\n            output = module(output)\n\n    return output\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.append","title":"append","text":"<pre><code>append(module)\n</code></pre> <p>Append a given module to the end.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module to append</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def append(self, module: Module) -&gt; \"Sequential\":\n    r\"\"\"Append a given module to the end.\n\n    Args:\n        module (nn.Module): module to append\n    \"\"\"\n    self.add_module(str(len(self)), module)\n    return self\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.extend","title":"extend","text":"<pre><code>extend(sequential)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def extend(self, sequential) -&gt; \"Sequential\":\n    for layer in sequential:\n        self.append(layer)\n    return self\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; Any:\n    modules_iter = iter(self._modules.values())\n    first_module = next(modules_iter)\n    output = first_module(*args, **kwargs)\n\n    for module in modules_iter:\n        output = module(output)\n\n    return output\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.insert","title":"insert","text":"<pre><code>insert(index, module)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def insert(self, index: int, module: Module) -&gt; \"Sequential\":\n    if not isinstance(module, Module):\n        raise AssertionError(f\"module should be of type: {Module}\")\n    n = len(self._modules)\n    if not (-n &lt;= index &lt;= n):\n        raise IndexError(f\"Index out of range: {index}\")\n    if index &lt; 0:\n        index += n\n    for i in range(n, index, -1):\n        self._modules[str(i)] = self._modules[str(i - 1)]\n    self._modules[str(index)] = module\n    return self\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.pop","title":"pop","text":"<pre><code>pop(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def pop(self, key: Union[int, slice]) -&gt; Module:\n    v = self[key]\n    del self[key]\n    return v\n</code></pre>"},{"location":"api-reference/nn/modules/speaker/","title":"Speaker","text":""},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker","title":"Speaker","text":"<p>               Bases: <code>Module</code></p> <p>Speaker is a Module type that uses language models to transform text in speak.</p> Source code in <code>src/msgflux/nn/modules/speaker.py</code> <pre><code>class Speaker(Module):\n    \"\"\"Speaker is a Module type that uses language models to transform text in speak.\"\"\"\n\n    def __init__(\n        self,\n        model: Union[TextToSpeechModel, ModelGateway],\n        *,\n        guardrails: Optional[Dict[str, Callable]] = None,\n        message_fields: Optional[Dict[str, Any]] = None,\n        response_mode: Optional[str] = \"plain_response\",\n        response_format: Optional[\n            Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"]\n        ] = \"opus\",\n        prompt: Optional[str] = None,\n        config: Optional[Dict[str, Any]] = None,\n        name: Optional[str] = None,  \n    ):\n        \"\"\"Args:\n        model:\n            Transcriber Model client.\n        guardrails:\n            Dictionary mapping guardrail types to callables.\n            Valid keys: \"input\" (output not supported for Speaker).\n            !!! example\n                guardrails={\"input\": input_checker}\n        message_fields:\n            Dictionary mapping Message field names to their paths in the Message object.\n            Valid keys: \"task_inputs\" (other fields not supported for Speaker).\n            !!! example\n                message_fields={\"task_inputs\": \"input.user\"}\n\n            Field description:\n            - task_inputs: Field path for task input (str)\n        response_mode:\n            What the response should be.\n            * `plain_response` (default): Returns the final agent response directly.\n            * other: Write on field in Message object.\n        response_format:\n            The format to audio in.\n        prompt:\n            Useful for instructing the model to follow some speak generation pattern.\n        config:\n            Dictionary with configuration options. Accepts any keys without validation.\n            Common option: \"stream\"\n            !!! example\n                config={\"stream\": False}\n\n            Configuration option:\n            - stream: Transmit response on-the-fly (bool)\n        name:\n            Transcriber name in snake case format.            \n        \"\"\"\n        super().__init__()\n        self._set_guardrails(guardrails)\n        self._set_model(model)\n        self._set_prompt(prompt)\n        self._set_response_format(response_format)\n        self._set_response_mode(response_mode)\n        self._set_message_fields(message_fields)\n        self._set_config(config)\n        if name:\n            self.set_name(name)\n\n    def forward(\n        self, message: Union[str, Message], **kwargs\n    ) -&gt; Union[bytes, ModelStreamResponse]:\n        \"\"\"Execute the speaker with the given message.\n\n        Args:\n            message: The input message, which can be:\n                - str: Direct text input to convert to speech\n                - Message: Message object with fields mapped via message_fields\n            **kwargs: Runtime overrides for message_fields. Can include:\n                - task_inputs: Override field path or direct value\n\n        Returns:\n            Audio bytes or ModelStreamResponse if stream=True\n\n        Examples:\n            # Direct string input\n            speaker(\"Hello world\")\n\n            # Using Message object with message_fields\n            msg = Message(text=\"Hello world\")\n            speaker(msg)\n\n            # Runtime override\n            speaker(msg, task_inputs=\"custom.path\")\n        \"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = self._execute_model(**inputs)\n        response = self._process_model_response(model_response, message)\n        return response\n\n    async def aforward(\n        self, message: Union[str, Message], **kwargs\n    ) -&gt; Union[bytes, ModelStreamResponse]:\n        \"\"\"Async version of forward. Execute the speaker asynchronously.\"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = await self._aexecute_model(**inputs)\n        response = self._process_model_response(model_response, message)\n        return response\n\n    def _execute_model(\n        self, data: str, model_preference: Optional[str] = None\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        model_execution_params = self._prepare_model_execution(data, model_preference)\n        if self.guardrails.get(\"input\"):\n            self._execute_input_guardrail(model_execution_params)\n        model_response = self.model(**model_execution_params)\n        return model_response\n\n    async def _aexecute_model(\n        self, data: str, model_preference: Optional[str] = None\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        model_execution_params = self._prepare_model_execution(data, model_preference)\n        if self.guardrails.get(\"input\"):\n            await self._aexecute_input_guardrail(model_execution_params)\n        model_response = await self.model.acall(**model_execution_params)\n        return model_response\n\n    def _prepare_model_execution(\n        self, data: str, model_preference: Optional[str] = None\n    ) -&gt; Dict[str, Union[str, bool]]:\n        model_execution_params = dotdict(\n            data=data, response_format=self.response_format, prompt=self.prompt\n        )\n        stream = self.config.get(\"stream\", False)\n        if stream:\n            model_execution_params.stream = stream\n        if isinstance(self.model, ModelGateway) and model_preference is not None:\n            model_execution_params.model_preference = model_preference\n        return model_execution_params\n\n    def _prepare_guardrail_execution(\n        self, model_execution_params: Dict[str, Union[str, bool]]\n    ) -&gt; Dict[str, str]:\n        guardrail_params = {\"data\": model_execution_params.data}\n        return guardrail_params\n\n    def _process_model_response(\n        self,\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        message: Union[str, Message],\n    ) -&gt; Union[bytes, Message, ModelStreamResponse]:\n        if model_response.response_type == \"audio_generation\":\n            raw_response = self._extract_raw_response(model_response)\n            response = self._prepare_response(raw_response, message)\n            return response\n        else:\n            raise ValueError(\n                f\"Unsupported model response type `{model_response.response_type}`\"\n            )\n\n    def _prepare_task(self, message: Union[str, Message], **kwargs) -&gt; Dict[str, str]:\n        if isinstance(message, Message):\n            data = self._extract_message_values(self.task_inputs, message)\n            if data is None:\n                raise ValueError(f\"No text found in paths: `{self.task_inputs}`\")\n        elif isinstance(message, str):\n            data = message\n        else:\n            raise ValueError(f\"Unsupported message type: `{type(message)}`\")\n\n        model_preference = kwargs.pop(\"model_preference\", None)\n        if model_preference is None and isinstance(message, Message):\n            model_preference = self.get_model_preference_from_message(message)\n\n        return {\"data\": data, \"model_preference\": model_preference}\n\n    def inspect_model_execution_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n        \"\"\"Debug model input parameters.\"\"\"\n        inputs = self._prepare_task(*args, **kwargs)\n        model_execution_params = self._prepare_model_execution(**inputs)\n        return model_execution_params\n\n    def _set_model(self, model: Union[TextToSpeechModel, ModelGateway]):\n        if model.model_type == \"text_to_speech\":\n            self.register_buffer(\"model\", model)\n        else:\n            raise TypeError(\n                f\"`model` need be a `text_to_speech` model, given `{type(model)}`\"\n            )\n\n    def _set_response_format(self, response_format: str):\n        supported_formats = [\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"]\n        if isinstance(response_format, str):\n            if response_format in supported_formats:\n                self.register_buffer(\"response_format\", response_format)\n            else:\n                raise ValueError(\n                    f\"`response_format` can be `{supported_formats}` \"\n                    f\"given `{response_format}\"\n                )\n        else:\n            raise TypeError(\n                f\"`response_format` need be a str or given `{type(response_format)}\"\n            )\n\n    def _set_config(self, config: Optional[Dict[str, Any]] = None):\n        if config is None:\n            self.register_buffer(\"config\", {})\n            return\n\n        if not isinstance(config, dict):\n            raise TypeError(\n                f\"`config` must be a dict or None, given `{type(config)}`\"\n            )\n\n        self.register_buffer(\"config\", config.copy())\n</code></pre>"},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker.__init__","title":"__init__","text":"<pre><code>__init__(\n    model,\n    *,\n    guardrails=None,\n    message_fields=None,\n    response_mode=\"plain_response\",\n    response_format=\"opus\",\n    prompt=None,\n    config=None,\n    name=None,\n)\n</code></pre> <p>model:     Transcriber Model client. guardrails:     Dictionary mapping guardrail types to callables.     Valid keys: \"input\" (output not supported for Speaker).     !!! example         guardrails={\"input\": input_checker} message_fields:     Dictionary mapping Message field names to their paths in the Message object.     Valid keys: \"task_inputs\" (other fields not supported for Speaker).     !!! example         message_fields={\"task_inputs\": \"input.user\"}</p> <pre><code>Field description:\n- task_inputs: Field path for task input (str)\n</code></pre> <p>response_mode:     What the response should be.     * <code>plain_response</code> (default): Returns the final agent response directly.     * other: Write on field in Message object. response_format:     The format to audio in. prompt:     Useful for instructing the model to follow some speak generation pattern. config:     Dictionary with configuration options. Accepts any keys without validation.     Common option: \"stream\"     !!! example         config={\"stream\": False}</p> <pre><code>Configuration option:\n- stream: Transmit response on-the-fly (bool)\n</code></pre> <p>name:     Transcriber name in snake case format.</p> Source code in <code>src/msgflux/nn/modules/speaker.py</code> <pre><code>def __init__(\n    self,\n    model: Union[TextToSpeechModel, ModelGateway],\n    *,\n    guardrails: Optional[Dict[str, Callable]] = None,\n    message_fields: Optional[Dict[str, Any]] = None,\n    response_mode: Optional[str] = \"plain_response\",\n    response_format: Optional[\n        Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"]\n    ] = \"opus\",\n    prompt: Optional[str] = None,\n    config: Optional[Dict[str, Any]] = None,\n    name: Optional[str] = None,  \n):\n    \"\"\"Args:\n    model:\n        Transcriber Model client.\n    guardrails:\n        Dictionary mapping guardrail types to callables.\n        Valid keys: \"input\" (output not supported for Speaker).\n        !!! example\n            guardrails={\"input\": input_checker}\n    message_fields:\n        Dictionary mapping Message field names to their paths in the Message object.\n        Valid keys: \"task_inputs\" (other fields not supported for Speaker).\n        !!! example\n            message_fields={\"task_inputs\": \"input.user\"}\n\n        Field description:\n        - task_inputs: Field path for task input (str)\n    response_mode:\n        What the response should be.\n        * `plain_response` (default): Returns the final agent response directly.\n        * other: Write on field in Message object.\n    response_format:\n        The format to audio in.\n    prompt:\n        Useful for instructing the model to follow some speak generation pattern.\n    config:\n        Dictionary with configuration options. Accepts any keys without validation.\n        Common option: \"stream\"\n        !!! example\n            config={\"stream\": False}\n\n        Configuration option:\n        - stream: Transmit response on-the-fly (bool)\n    name:\n        Transcriber name in snake case format.            \n    \"\"\"\n    super().__init__()\n    self._set_guardrails(guardrails)\n    self._set_model(model)\n    self._set_prompt(prompt)\n    self._set_response_format(response_format)\n    self._set_response_mode(response_mode)\n    self._set_message_fields(message_fields)\n    self._set_config(config)\n    if name:\n        self.set_name(name)\n</code></pre>"},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(message, **kwargs)\n</code></pre> <p>Async version of forward. Execute the speaker asynchronously.</p> Source code in <code>src/msgflux/nn/modules/speaker.py</code> <pre><code>async def aforward(\n    self, message: Union[str, Message], **kwargs\n) -&gt; Union[bytes, ModelStreamResponse]:\n    \"\"\"Async version of forward. Execute the speaker asynchronously.\"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = await self._aexecute_model(**inputs)\n    response = self._process_model_response(model_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker.forward","title":"forward","text":"<pre><code>forward(message, **kwargs)\n</code></pre> <p>Execute the speaker with the given message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Union[str, Message]</code> <p>The input message, which can be: - str: Direct text input to convert to speech - Message: Message object with fields mapped via message_fields</p> required <code>**kwargs</code> <p>Runtime overrides for message_fields. Can include: - task_inputs: Override field path or direct value</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[bytes, ModelStreamResponse]</code> <p>Audio bytes or ModelStreamResponse if stream=True</p> <p>Examples:</p>"},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker.forward--direct-string-input","title":"Direct string input","text":"<p>speaker(\"Hello world\")</p>"},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker.forward--using-message-object-with-message_fields","title":"Using Message object with message_fields","text":"<p>msg = Message(text=\"Hello world\") speaker(msg)</p>"},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker.forward--runtime-override","title":"Runtime override","text":"<p>speaker(msg, task_inputs=\"custom.path\")</p> Source code in <code>src/msgflux/nn/modules/speaker.py</code> <pre><code>def forward(\n    self, message: Union[str, Message], **kwargs\n) -&gt; Union[bytes, ModelStreamResponse]:\n    \"\"\"Execute the speaker with the given message.\n\n    Args:\n        message: The input message, which can be:\n            - str: Direct text input to convert to speech\n            - Message: Message object with fields mapped via message_fields\n        **kwargs: Runtime overrides for message_fields. Can include:\n            - task_inputs: Override field path or direct value\n\n    Returns:\n        Audio bytes or ModelStreamResponse if stream=True\n\n    Examples:\n        # Direct string input\n        speaker(\"Hello world\")\n\n        # Using Message object with message_fields\n        msg = Message(text=\"Hello world\")\n        speaker(msg)\n\n        # Runtime override\n        speaker(msg, task_inputs=\"custom.path\")\n    \"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = self._execute_model(**inputs)\n    response = self._process_model_response(model_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker.inspect_model_execution_params","title":"inspect_model_execution_params","text":"<pre><code>inspect_model_execution_params(*args, **kwargs)\n</code></pre> <p>Debug model input parameters.</p> Source code in <code>src/msgflux/nn/modules/speaker.py</code> <pre><code>def inspect_model_execution_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n    \"\"\"Debug model input parameters.\"\"\"\n    inputs = self._prepare_task(*args, **kwargs)\n    model_execution_params = self._prepare_model_execution(**inputs)\n    return model_execution_params\n</code></pre>"},{"location":"api-reference/nn/modules/tool/","title":"Tool","text":""},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.Tool","title":"Tool","text":"<p>               Bases: <code>Module</code></p> <p>Tool is Module type that provide a json schema to tools.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>class Tool(Module):\n    \"\"\"Tool is Module type that provide a json schema to tools.\"\"\"\n\n    def get_json_schema(self):\n        return generate_tool_json_schema(self)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.Tool.get_json_schema","title":"get_json_schema","text":"<pre><code>get_json_schema()\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def get_json_schema(self):\n    return generate_tool_json_schema(self)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.LocalTool","title":"LocalTool","text":"<p>               Bases: <code>Tool</code></p> <p>Local tool implementation.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>class LocalTool(Tool):\n    \"\"\"Local tool implementation.\"\"\"\n    def __init__(\n        self,\n        name: str,\n        description: str,\n        annotations: Dict[str, Any],\n        tool_config: Dict[str, Any],\n        impl: Callable,\n    ):\n        super().__init__()\n        self.set_name(name)\n        self.set_description(description)\n        self.set_annotations(annotations)\n        self.register_buffer(\"tool_config\", tool_config)\n        self.impl = impl  # Not a buffer for now\n\n    @tool_retry\n    @set_tool_attributes(execution_type=\"local\")\n    def forward(self, **kwargs):\n        if inspect.iscoroutinefunction(self.impl):\n            return F.wait_for(self.impl, **kwargs)\n        return self.impl(**kwargs)\n\n    @tool_retry\n    @aset_tool_attributes(execution_type=\"local\")\n    async def aforward(self, *args, **kwargs):\n        if hasattr(self.impl, \"acall\"):\n            return await self.impl.acall(*args, **kwargs)\n        elif inspect.iscoroutinefunction(self.impl):\n            return await self.impl(*args, **kwargs)\n        # Fall back to sync call in executor to avoid blocking event loop\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(None, lambda: self.impl(*args, **kwargs))\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.LocalTool.impl","title":"impl  <code>instance-attribute</code>","text":"<pre><code>impl = impl\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.LocalTool.__init__","title":"__init__","text":"<pre><code>__init__(name, description, annotations, tool_config, impl)\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    description: str,\n    annotations: Dict[str, Any],\n    tool_config: Dict[str, Any],\n    impl: Callable,\n):\n    super().__init__()\n    self.set_name(name)\n    self.set_description(description)\n    self.set_annotations(annotations)\n    self.register_buffer(\"tool_config\", tool_config)\n    self.impl = impl  # Not a buffer for now\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.LocalTool.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(*args, **kwargs)\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>@tool_retry\n@aset_tool_attributes(execution_type=\"local\")\nasync def aforward(self, *args, **kwargs):\n    if hasattr(self.impl, \"acall\"):\n        return await self.impl.acall(*args, **kwargs)\n    elif inspect.iscoroutinefunction(self.impl):\n        return await self.impl(*args, **kwargs)\n    # Fall back to sync call in executor to avoid blocking event loop\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(None, lambda: self.impl(*args, **kwargs))\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.LocalTool.forward","title":"forward","text":"<pre><code>forward(**kwargs)\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>@tool_retry\n@set_tool_attributes(execution_type=\"local\")\ndef forward(self, **kwargs):\n    if inspect.iscoroutinefunction(self.impl):\n        return F.wait_for(self.impl, **kwargs)\n    return self.impl(**kwargs)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.MCPTool","title":"MCPTool","text":"<p>               Bases: <code>Tool</code></p> <p>MCP Tool Proxy - wraps remote MCP tool as a Tool object.</p> <p>This allows MCP tools to be treated exactly like local tools, enabling polymorphism and unified telemetry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Tool name (without namespace prefix)</p> required <code>mcp_client</code> <code>Any</code> <p>Connected MCP client</p> required <code>mcp_tool_info</code> <code>Any</code> <p>MCP tool metadata</p> required <code>namespace</code> <code>str</code> <p>MCP server namespace</p> required <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Optional tool configuration</p> <code>None</code> Example <p>mcp_tool = MCPTool( ...     name=\"read_file\", ...     mcp_client=client, ...     mcp_tool_info=tool_info, ...     namespace=\"filesystem\" ... ) result = mcp_tool(path=\"/file.txt\")</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>class MCPTool(Tool):\n    \"\"\"\n    MCP Tool Proxy - wraps remote MCP tool as a Tool object.\n\n    This allows MCP tools to be treated exactly like local tools,\n    enabling polymorphism and unified telemetry.\n\n    Args:\n        name: Tool name (without namespace prefix)\n        mcp_client: Connected MCP client\n        mcp_tool_info: MCP tool metadata\n        namespace: MCP server namespace\n        config: Optional tool configuration\n\n    Example:\n        &gt;&gt;&gt; mcp_tool = MCPTool(\n        ...     name=\"read_file\",\n        ...     mcp_client=client,\n        ...     mcp_tool_info=tool_info,\n        ...     namespace=\"filesystem\"\n        ... )\n        &gt;&gt;&gt; result = mcp_tool(path=\"/file.txt\")\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        mcp_client: Any,  # MCPClient type\n        mcp_tool_info: Any,  # MCPToolInfo type\n        namespace: str,\n        config: Optional[Dict[str, Any]] = None,\n    ):\n        super().__init__()\n\n        # Set full tool name with namespace\n        full_name = f\"{namespace}__{name}\"\n        self.set_name(full_name)\n\n        # Store MCP-specific data\n        self._mcp_client = mcp_client\n        self._mcp_tool_info = mcp_tool_info\n        self._namespace = namespace\n        self._mcp_tool_name = name\n\n        # Set description from MCP tool info\n        if hasattr(mcp_tool_info, \"description\"):\n            self.set_description(mcp_tool_info.description)\n\n        # Store config\n        self.register_buffer(\"tool_config\", config or {})\n\n    def get_json_schema(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert MCP tool schema to standard tool JSON schema.\"\"\"\n        return convert_mcp_schema_to_tool_schema(\n            self._mcp_tool_info,\n            self._namespace\n        )\n\n    @set_tool_attributes(execution_type=\"remote\", protocol=\"mcp\")\n    def forward(self, **kwargs) -&gt; Any:\n        \"\"\"Execute MCP tool call.\"\"\"\n        # Call MCP tool (wrap async in sync)\n        result = F.wait_for(self._mcp_client.call_tool, self._mcp_tool_name, kwargs)\n\n        # Handle errors\n        if result.isError:\n            error_text = extract_tool_result_text(result)\n            raise RuntimeError(f\"MCP tool error: {error_text}\")\n\n        # Extract and return result\n        return extract_tool_result_text(result)\n\n    @aset_tool_attributes(execution_type=\"remote\", protocol=\"mcp\")\n    async def aforward(self, **kwargs) -&gt; Any:\n        \"\"\"Execute MCP tool call asynchronously.\"\"\"\n        # Call MCP tool\n        result = await self._mcp_client.call_tool(self._mcp_tool_name, kwargs)\n\n        # Handle errors\n        if result.isError:\n            error_text = extract_tool_result_text(result)\n            raise RuntimeError(f\"MCP tool error: {error_text}\")\n\n        # Extract and return result\n        return extract_tool_result_text(result)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.MCPTool.__init__","title":"__init__","text":"<pre><code>__init__(\n    name, mcp_client, mcp_tool_info, namespace, config=None\n)\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    mcp_client: Any,  # MCPClient type\n    mcp_tool_info: Any,  # MCPToolInfo type\n    namespace: str,\n    config: Optional[Dict[str, Any]] = None,\n):\n    super().__init__()\n\n    # Set full tool name with namespace\n    full_name = f\"{namespace}__{name}\"\n    self.set_name(full_name)\n\n    # Store MCP-specific data\n    self._mcp_client = mcp_client\n    self._mcp_tool_info = mcp_tool_info\n    self._namespace = namespace\n    self._mcp_tool_name = name\n\n    # Set description from MCP tool info\n    if hasattr(mcp_tool_info, \"description\"):\n        self.set_description(mcp_tool_info.description)\n\n    # Store config\n    self.register_buffer(\"tool_config\", config or {})\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.MCPTool.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(**kwargs)\n</code></pre> <p>Execute MCP tool call asynchronously.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>@aset_tool_attributes(execution_type=\"remote\", protocol=\"mcp\")\nasync def aforward(self, **kwargs) -&gt; Any:\n    \"\"\"Execute MCP tool call asynchronously.\"\"\"\n    # Call MCP tool\n    result = await self._mcp_client.call_tool(self._mcp_tool_name, kwargs)\n\n    # Handle errors\n    if result.isError:\n        error_text = extract_tool_result_text(result)\n        raise RuntimeError(f\"MCP tool error: {error_text}\")\n\n    # Extract and return result\n    return extract_tool_result_text(result)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.MCPTool.forward","title":"forward","text":"<pre><code>forward(**kwargs)\n</code></pre> <p>Execute MCP tool call.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>@set_tool_attributes(execution_type=\"remote\", protocol=\"mcp\")\ndef forward(self, **kwargs) -&gt; Any:\n    \"\"\"Execute MCP tool call.\"\"\"\n    # Call MCP tool (wrap async in sync)\n    result = F.wait_for(self._mcp_client.call_tool, self._mcp_tool_name, kwargs)\n\n    # Handle errors\n    if result.isError:\n        error_text = extract_tool_result_text(result)\n        raise RuntimeError(f\"MCP tool error: {error_text}\")\n\n    # Extract and return result\n    return extract_tool_result_text(result)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.MCPTool.get_json_schema","title":"get_json_schema","text":"<pre><code>get_json_schema()\n</code></pre> <p>Convert MCP tool schema to standard tool JSON schema.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def get_json_schema(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert MCP tool schema to standard tool JSON schema.\"\"\"\n    return convert_mcp_schema_to_tool_schema(\n        self._mcp_tool_info,\n        self._namespace\n    )\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary","title":"ToolLibrary","text":"<p>               Bases: <code>Module</code></p> <p>ToolLibrary is a Module type that manage tool calls over the tool library.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>class ToolLibrary(Module):\n    \"\"\"ToolLibrary is a Module type that manage tool calls over the tool library.\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        tools: List[Callable],\n        special_tools: Optional[List[str]] = None,\n        mcp_servers: Optional[List[Dict[str, Any]]] = None,\n    ):\n        \"\"\"Args:\n        name:\n            Library name.\n        tools:\n            A list of callables.\n        special_tools:\n            Autonomy tools for the model.\n        mcp_servers:\n            List of MCP server configurations. Each config should contain:\n            - name: Namespace for tools from this server\n            - transport: \"stdio\" or \"http\"\n            - For stdio: command, args, cwd, env\n            - For http: base_url, headers\n            - Optional: include_tools, exclude_tools, tool_config\n        \"\"\"\n        super().__init__()\n        self.set_name(f\"{name}_tool_library\")\n        self.library = ModuleDict()\n        self.register_buffer(\"special_library\", [])\n        self.register_buffer(\"tool_configs\", {})\n        self.register_buffer(\"mcp_clients\", {})\n        for tool in tools:\n            self.add(tool)\n        if special_tools:\n            for special_tool in special_tools:\n                self.special_add(special_tool)\n        if mcp_servers:\n            self._initialize_mcp_clients(mcp_servers)\n\n    def add(self, tool: Union[str, Callable]):\n        \"\"\"Add a local tool in library.\"\"\"\n        if isinstance(tool, str):\n            if tool in self.special_library.keys():\n                raise ValueError(\n                    f\"The special tool name `{tool}` is already in special tool library\"\n                )\n            self.special_library.append(tool)\n        else:\n            name = (getattr(tool, \"name\", None) or getattr(tool, \"__name__\", None))\n            if name in self.library.keys():\n                raise ValueError(f\"The tool name `{name}` is already in tool library\")\n            if not isinstance(tool, Tool):\n                tool = _convert_module_to_nn_tool(tool)\n\n            # Store tool config (may be empty dict for local tools)\n            self.tool_configs[tool.name] = getattr(tool, 'tool_config', {})\n\n            self.library.update({tool.name: tool})\n\n    def remove(self, tool_name: str):\n        if tool_name in self.library.keys():\n            self.library.pop(tool_name)\n            self.tool_configs.pop(tool_name, None)\n        elif tool_name in self.special_library:\n            self.special_library.remove(tool_name)\n        else:\n            raise ValueError(f\"The tool name `{tool_name}` is not in tool library\")\n\n    def clear(self):\n        self.library.clear()\n        self.special_library.clear()\n        # TODO: clean mcp\n\n    def _initialize_mcp_clients(self, mcp_servers: List[Dict[str, Any]]):\n        \"\"\"Initialize MCP clients from server configurations.\"\"\"\n        for server_config in mcp_servers:\n            namespace = server_config.get(\"name\")\n            if not namespace:\n                raise ValueError(\"MCP server config must include 'name' field\")\n\n            transport_type = server_config.get(\"transport\", \"stdio\")\n\n            # Create client based on transport type\n            if transport_type == \"stdio\":\n                client = MCPClient.from_stdio(\n                    command=server_config.get(\"command\"),\n                    args=server_config.get(\"args\"),\n                    cwd=server_config.get(\"cwd\"),\n                    env=server_config.get(\"env\"),\n                    timeout=server_config.get(\"timeout\", 30.0)\n                )\n            elif transport_type == \"http\":\n                client = MCPClient.from_http(\n                    base_url=server_config.get(\"base_url\"),\n                    timeout=server_config.get(\"timeout\", 30.0),\n                    headers=server_config.get(\"headers\")\n                )\n            else:\n                raise ValueError(\n                    f\"Unknown transport type: {transport_type}. \"\n                    \"Supported types: 'stdio', 'http'\"\n                )\n\n            # Connect and list tools with error handling\n            try:\n                F.wait_for(client.connect)\n                all_tools = F.wait_for(client.list_tools, use_cache=False)\n\n                # Apply filters\n                include_tools = server_config.get(\"include_tools\")\n                exclude_tools = server_config.get(\"exclude_tools\")\n                filtered_tools = filter_tools(all_tools, include_tools, exclude_tools)\n\n                # Create MCPTool for each remote tool\n                tool_configs = server_config.get(\"tool_config\", {})\n                for mcp_tool_info in filtered_tools:\n                    tool_config = tool_configs.get(mcp_tool_info.name, {})\n\n                    # Create MCPTool instance\n                    mcp_tool = MCPTool(\n                        name=mcp_tool_info.name,\n                        mcp_client=client,\n                        mcp_tool_info=mcp_tool_info,\n                        namespace=namespace,\n                        config=tool_config\n                    )\n\n                    # Add to library (will have name like \"namespace__tool_name\")\n                    self.library.update({mcp_tool.name: mcp_tool})\n                    self.tool_configs[mcp_tool.name] = mcp_tool.tool_config\n\n                self.mcp_clients[namespace] = {\n                    \"client\": client,\n                    \"tools\": filtered_tools,\n                    \"tool_config\": tool_configs\n                }\n\n                logger.debug(\n                    f\"Successfully connected to MCP server `{namespace}` \"\n                    f\"with {len(filtered_tools)} tools\"\n                )\n            except Exception as e:\n                logger.error(\n                    f\"Failed to initialize MCP server '{namespace}': {str(e)}\",\n                    exc_info=True\n                )\n                # Continue with other servers instead of failing completely\n\n    def get_tools(self) -&gt; Iterator[Dict[str, Tool]]:\n        return self.library.items()\n\n    def get_tool_names(self) -&gt; List[str]:\n        \"\"\"Get names of all tools.\"\"\"\n        return list(self.library.keys())\n\n    def get_mcp_tool_names(self) -&gt; List[str]:\n        \"\"\"Get names of all MCP tools (with namespace).\"\"\"\n        tool_names = []\n        for namespace, mcp_data in self.mcp_clients.items():\n            for tool in mcp_data[\"tools\"]:\n                tool_names.append(f\"{namespace}__{tool.name}\")\n        return tool_names\n\n    def get_tool_json_schemas(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Returns a list of JSON schemas from local and MCP tools.\"\"\"\n        schemas = []\n\n        # Local tools\n        for tool_name in self.library:\n            schemas.append(self.library[tool_name].get_json_schema())\n\n        # MCP tools\n        if self.mcp_clients:\n            for namespace, mcp_data in self.mcp_clients.items():\n                for mcp_tool in mcp_data[\"tools\"]:\n                    schema = convert_mcp_schema_to_tool_schema(mcp_tool, namespace)\n                    schemas.append(schema)\n\n        return schemas\n\n    def forward(  # noqa: C901\n        self,\n        tool_callings: List[Tuple[str, str, Any]],\n        model_state: Optional[List[Dict[str, Any]]] = None,\n        vars: Optional[Mapping[str, Any]] = None,\n    ) -&gt; ToolResponses:\n        \"\"\"Executes tool calls with tool config logic.\n\n        Args:\n            tool_callings:\n                A list of tuples containing the tool id, name and parameters.\n                !!! example\n                    [('123121', 'tool_name1', {'parameter1': 'value1'}),\n                    ('322', 'tool_name2', '')]\n            model_state:\n                The current state of the Agent for the `handoff` functionality.\n            vars:\n                Extra kwargs to be used in tools.\n\n        Returns:\n            ToolResponses:\n                Structured object containing all tool call results.\n        \"\"\"\n        # TODO capturar no trace quando o modelo erra algo na tool\n        # TODO capturar no trace o tool config\n        if model_state is None:\n            model_state = {}\n\n        if vars is None:\n            vars = {}\n\n        prepared_calls = []\n        call_metadata = []\n        tool_calls: List[ToolCall] = []\n        return_directly = True if tool_callings else False\n\n        for tool_id, tool_name, tool_params in tool_callings:\n            if tool_name not in self.library:\n                tool_calls.append(\n                    ToolCall(\n                        id=tool_id,\n                        name=tool_name,\n                        parameters=tool_params,\n                        error=f\"Error: Tool `{tool_name}` not found.\"\n                    )\n                )\n                return_directly = False\n                continue\n\n            # Get tool\n            tool = self.library[tool_name]\n            config = self.tool_configs.get(tool_name, {})\n\n            # Handle inject_vars\n            inject_vars = config.get(\"inject_vars\", False)\n            if inject_vars:\n                if isinstance(inject_vars, list):\n                    for key in inject_vars:\n                        if key not in vars:\n                            raise ValueError(\n                                f\"The tool `{tool_name}` requires the injected \"\n                                f\"parameter `{key}`, but it was not found.\"\n                            )\n                        tool_params[key] = vars[key]\n                elif inject_vars is True:\n                    tool_params[\"vars\"] = vars\n\n            if config.get(\"background\", False):\n                return_directly = False\n                F.background_task(tool, **(tool_params or {}))\n                tool_calls.append(\n                    ToolCall(\n                        id=tool_id,\n                        name=tool_name,\n                        parameters=tool_params,\n                        result=f\"\"\"The `{tool_name}` tool was started in the background.\n                        This tool will not generate a return\"\"\"\n                    )\n                )\n                continue\n\n            if config.get(\"call_as_response\", False):  # return function call as response\n                tool_calls.append(\n                    ToolCall(id=tool_id, name=tool_name, parameters=tool_params)\n                )\n                return_directly = True\n                continue\n\n            if config.get(\"inject_model_state\", False):  # Add model_state\n                tool_params[\"task_messages\"] = model_state\n\n            if not config.get(\"return_direct\", False):\n                return_directly = False\n\n            tool_params = tool_params or {}\n            # Add tool_call_id for telemetry\n            tool_params[\"tool_call_id\"] = tool_id\n            prepared_calls.append(partial(tool, **tool_params))\n\n            call_metadata.append(\n                dotdict(id=tool_id, name=tool_name, config=config, params=tool_params)\n            )\n\n        if prepared_calls:\n            results = F.scatter_gather(prepared_calls)\n            for meta, result in zip(call_metadata, results):\n                if isinstance(meta.params, dict):\n                    parameters = meta.params.to_dict()\n                    parameters.pop(\"vars\", None)\n                    parameters.pop(\"tool_call_id\", None)\n                else:\n                    parameters = None\n                tool_calls.append(\n                    ToolCall(\n                        id=meta.id,\n                        name=meta.name,\n                        parameters=parameters,\n                        result=result,\n                    )\n                )\n\n        return ToolResponses(return_directly=return_directly, tool_calls=tool_calls)\n\n    async def aforward(  # noqa: C901\n        self,\n        tool_callings: List[Tuple[str, str, Any]],\n        model_state: Optional[List[Dict[str, Any]]] = None,\n        vars: Optional[Mapping[str, Any]] = None,\n    ) -&gt; ToolResponses:\n        \"\"\"Async version of forward. Executes tool calls with logic for `handoff`, `return_direct`.\n\n        Args:\n            tool_callings:\n                A list of tuples containing the tool id, name and parameters.\n                !!! example\n                    [('123121', 'tool_name1', {'parameter1': 'value1'}),\n                    ('322', 'tool_name2', '')]\n            model_state:\n                The current state of the Agent for the `handoff` functionality.\n            vars:\n                Extra kwargs to be used in tools.\n\n        Returns:\n            ToolResponses:\n                Structured object containing all tool call results.\n        \"\"\"\n        if model_state is None:\n            model_state = {}\n\n        if vars is None:\n            vars = {}\n\n        prepared_calls = []\n        call_metadata = []\n        tool_calls: List[ToolCall] = []\n        return_directly = True if tool_callings else False\n\n        for tool_id, tool_name, tool_params in tool_callings:\n            if tool_name not in self.library:\n                tool_calls.append(\n                    ToolCall(\n                        id=tool_id,\n                        name=tool_name,\n                        parameters=tool_params,\n                        error=f\"Error: Tool `{tool_name}` not found.\"\n                    )\n                )\n                return_directly = False\n                continue\n\n            # Get tool\n            tool = self.library[tool_name]\n            config = self.tool_configs.get(tool_name, {})\n\n            # Handle inject_vars\n            inject_vars = config.get(\"inject_vars\", False)\n            if inject_vars:\n                if isinstance(inject_vars, list):\n                    for key in inject_vars:\n                        if key not in vars:\n                            raise ValueError(\n                                f\"The tool `{tool_name}` requires the injected \"\n                                f\"parameter `{key}`, but it was not found.\"\n                            )\n                        tool_params[key] = vars[key]\n                elif inject_vars is True:\n                    tool_params[\"vars\"] = vars\n\n            if config.get(\"background\", False):\n                return_directly = False\n                await F.abackground_task(tool.acall, **(tool_params or {}))\n                tool_calls.append(\n                    ToolCall(\n                        id=tool_id,\n                        name=tool_name,\n                        parameters=tool_params,\n                        result=f\"\"\"The `{tool_name}` tool was started in the background.\n                        This tool will not generate a return\"\"\"\n                    )\n                )\n                continue\n\n            if config.get(\"call_as_response\", False):  # return function call as response\n                tool_calls.append(\n                    ToolCall(id=tool_id, name=tool_name, parameters=tool_params)\n                )\n                return_directly = True\n                continue\n\n            if config.get(\"inject_model_state\", False):  # Add model_state\n                tool_params[\"task_messages\"] = model_state\n\n            if not config.get(\"return_direct\", False):\n                return_directly = False\n\n            tool_params = tool_params or {}\n            # Add tool_call_id for telemetry\n            tool_params[\"tool_call_id\"] = tool_id\n            prepared_calls.append(partial(tool.acall, **tool_params))\n\n            call_metadata.append(\n                dotdict(id=tool_id, name=tool_name, config=config, params=tool_params)\n            )\n\n        if prepared_calls:\n            results = await F.ascatter_gather(prepared_calls)\n            for meta, result in zip(call_metadata, results):\n                if isinstance(meta.params, dict):\n                    parameters = meta.params.to_dict()\n                    parameters.pop(\"vars\", None)\n                    parameters.pop(\"tool_call_id\", None)\n                else:\n                    parameters = None\n                tool_calls.append(\n                    ToolCall(\n                        id=meta.id,\n                        name=meta.name,\n                        parameters=parameters,\n                        result=result,\n                    )\n                )\n\n        return ToolResponses(return_directly=return_directly, tool_calls=tool_calls)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.library","title":"library  <code>instance-attribute</code>","text":"<pre><code>library = ModuleDict()\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.__init__","title":"__init__","text":"<pre><code>__init__(name, tools, special_tools=None, mcp_servers=None)\n</code></pre> <p>name:     Library name. tools:     A list of callables. special_tools:     Autonomy tools for the model. mcp_servers:     List of MCP server configurations. Each config should contain:     - name: Namespace for tools from this server     - transport: \"stdio\" or \"http\"     - For stdio: command, args, cwd, env     - For http: base_url, headers     - Optional: include_tools, exclude_tools, tool_config</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    tools: List[Callable],\n    special_tools: Optional[List[str]] = None,\n    mcp_servers: Optional[List[Dict[str, Any]]] = None,\n):\n    \"\"\"Args:\n    name:\n        Library name.\n    tools:\n        A list of callables.\n    special_tools:\n        Autonomy tools for the model.\n    mcp_servers:\n        List of MCP server configurations. Each config should contain:\n        - name: Namespace for tools from this server\n        - transport: \"stdio\" or \"http\"\n        - For stdio: command, args, cwd, env\n        - For http: base_url, headers\n        - Optional: include_tools, exclude_tools, tool_config\n    \"\"\"\n    super().__init__()\n    self.set_name(f\"{name}_tool_library\")\n    self.library = ModuleDict()\n    self.register_buffer(\"special_library\", [])\n    self.register_buffer(\"tool_configs\", {})\n    self.register_buffer(\"mcp_clients\", {})\n    for tool in tools:\n        self.add(tool)\n    if special_tools:\n        for special_tool in special_tools:\n            self.special_add(special_tool)\n    if mcp_servers:\n        self._initialize_mcp_clients(mcp_servers)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.add","title":"add","text":"<pre><code>add(tool)\n</code></pre> <p>Add a local tool in library.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def add(self, tool: Union[str, Callable]):\n    \"\"\"Add a local tool in library.\"\"\"\n    if isinstance(tool, str):\n        if tool in self.special_library.keys():\n            raise ValueError(\n                f\"The special tool name `{tool}` is already in special tool library\"\n            )\n        self.special_library.append(tool)\n    else:\n        name = (getattr(tool, \"name\", None) or getattr(tool, \"__name__\", None))\n        if name in self.library.keys():\n            raise ValueError(f\"The tool name `{name}` is already in tool library\")\n        if not isinstance(tool, Tool):\n            tool = _convert_module_to_nn_tool(tool)\n\n        # Store tool config (may be empty dict for local tools)\n        self.tool_configs[tool.name] = getattr(tool, 'tool_config', {})\n\n        self.library.update({tool.name: tool})\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(tool_callings, model_state=None, vars=None)\n</code></pre> <p>Async version of forward. Executes tool calls with logic for <code>handoff</code>, <code>return_direct</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tool_callings</code> <code>List[Tuple[str, str, Any]]</code> <p>A list of tuples containing the tool id, name and parameters.</p> <p>Example</p> <p>[('123121', 'tool_name1', {'parameter1': 'value1'}), ('322', 'tool_name2', '')]</p> required <code>model_state</code> <code>Optional[List[Dict[str, Any]]]</code> <p>The current state of the Agent for the <code>handoff</code> functionality.</p> <code>None</code> <code>vars</code> <code>Optional[Mapping[str, Any]]</code> <p>Extra kwargs to be used in tools.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ToolResponses</code> <code>ToolResponses</code> <p>Structured object containing all tool call results.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>async def aforward(  # noqa: C901\n    self,\n    tool_callings: List[Tuple[str, str, Any]],\n    model_state: Optional[List[Dict[str, Any]]] = None,\n    vars: Optional[Mapping[str, Any]] = None,\n) -&gt; ToolResponses:\n    \"\"\"Async version of forward. Executes tool calls with logic for `handoff`, `return_direct`.\n\n    Args:\n        tool_callings:\n            A list of tuples containing the tool id, name and parameters.\n            !!! example\n                [('123121', 'tool_name1', {'parameter1': 'value1'}),\n                ('322', 'tool_name2', '')]\n        model_state:\n            The current state of the Agent for the `handoff` functionality.\n        vars:\n            Extra kwargs to be used in tools.\n\n    Returns:\n        ToolResponses:\n            Structured object containing all tool call results.\n    \"\"\"\n    if model_state is None:\n        model_state = {}\n\n    if vars is None:\n        vars = {}\n\n    prepared_calls = []\n    call_metadata = []\n    tool_calls: List[ToolCall] = []\n    return_directly = True if tool_callings else False\n\n    for tool_id, tool_name, tool_params in tool_callings:\n        if tool_name not in self.library:\n            tool_calls.append(\n                ToolCall(\n                    id=tool_id,\n                    name=tool_name,\n                    parameters=tool_params,\n                    error=f\"Error: Tool `{tool_name}` not found.\"\n                )\n            )\n            return_directly = False\n            continue\n\n        # Get tool\n        tool = self.library[tool_name]\n        config = self.tool_configs.get(tool_name, {})\n\n        # Handle inject_vars\n        inject_vars = config.get(\"inject_vars\", False)\n        if inject_vars:\n            if isinstance(inject_vars, list):\n                for key in inject_vars:\n                    if key not in vars:\n                        raise ValueError(\n                            f\"The tool `{tool_name}` requires the injected \"\n                            f\"parameter `{key}`, but it was not found.\"\n                        )\n                    tool_params[key] = vars[key]\n            elif inject_vars is True:\n                tool_params[\"vars\"] = vars\n\n        if config.get(\"background\", False):\n            return_directly = False\n            await F.abackground_task(tool.acall, **(tool_params or {}))\n            tool_calls.append(\n                ToolCall(\n                    id=tool_id,\n                    name=tool_name,\n                    parameters=tool_params,\n                    result=f\"\"\"The `{tool_name}` tool was started in the background.\n                    This tool will not generate a return\"\"\"\n                )\n            )\n            continue\n\n        if config.get(\"call_as_response\", False):  # return function call as response\n            tool_calls.append(\n                ToolCall(id=tool_id, name=tool_name, parameters=tool_params)\n            )\n            return_directly = True\n            continue\n\n        if config.get(\"inject_model_state\", False):  # Add model_state\n            tool_params[\"task_messages\"] = model_state\n\n        if not config.get(\"return_direct\", False):\n            return_directly = False\n\n        tool_params = tool_params or {}\n        # Add tool_call_id for telemetry\n        tool_params[\"tool_call_id\"] = tool_id\n        prepared_calls.append(partial(tool.acall, **tool_params))\n\n        call_metadata.append(\n            dotdict(id=tool_id, name=tool_name, config=config, params=tool_params)\n        )\n\n    if prepared_calls:\n        results = await F.ascatter_gather(prepared_calls)\n        for meta, result in zip(call_metadata, results):\n            if isinstance(meta.params, dict):\n                parameters = meta.params.to_dict()\n                parameters.pop(\"vars\", None)\n                parameters.pop(\"tool_call_id\", None)\n            else:\n                parameters = None\n            tool_calls.append(\n                ToolCall(\n                    id=meta.id,\n                    name=meta.name,\n                    parameters=parameters,\n                    result=result,\n                )\n            )\n\n    return ToolResponses(return_directly=return_directly, tool_calls=tool_calls)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def clear(self):\n    self.library.clear()\n    self.special_library.clear()\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.forward","title":"forward","text":"<pre><code>forward(tool_callings, model_state=None, vars=None)\n</code></pre> <p>Executes tool calls with tool config logic.</p> <p>Parameters:</p> Name Type Description Default <code>tool_callings</code> <code>List[Tuple[str, str, Any]]</code> <p>A list of tuples containing the tool id, name and parameters.</p> <p>Example</p> <p>[('123121', 'tool_name1', {'parameter1': 'value1'}), ('322', 'tool_name2', '')]</p> required <code>model_state</code> <code>Optional[List[Dict[str, Any]]]</code> <p>The current state of the Agent for the <code>handoff</code> functionality.</p> <code>None</code> <code>vars</code> <code>Optional[Mapping[str, Any]]</code> <p>Extra kwargs to be used in tools.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ToolResponses</code> <code>ToolResponses</code> <p>Structured object containing all tool call results.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def forward(  # noqa: C901\n    self,\n    tool_callings: List[Tuple[str, str, Any]],\n    model_state: Optional[List[Dict[str, Any]]] = None,\n    vars: Optional[Mapping[str, Any]] = None,\n) -&gt; ToolResponses:\n    \"\"\"Executes tool calls with tool config logic.\n\n    Args:\n        tool_callings:\n            A list of tuples containing the tool id, name and parameters.\n            !!! example\n                [('123121', 'tool_name1', {'parameter1': 'value1'}),\n                ('322', 'tool_name2', '')]\n        model_state:\n            The current state of the Agent for the `handoff` functionality.\n        vars:\n            Extra kwargs to be used in tools.\n\n    Returns:\n        ToolResponses:\n            Structured object containing all tool call results.\n    \"\"\"\n    # TODO capturar no trace quando o modelo erra algo na tool\n    # TODO capturar no trace o tool config\n    if model_state is None:\n        model_state = {}\n\n    if vars is None:\n        vars = {}\n\n    prepared_calls = []\n    call_metadata = []\n    tool_calls: List[ToolCall] = []\n    return_directly = True if tool_callings else False\n\n    for tool_id, tool_name, tool_params in tool_callings:\n        if tool_name not in self.library:\n            tool_calls.append(\n                ToolCall(\n                    id=tool_id,\n                    name=tool_name,\n                    parameters=tool_params,\n                    error=f\"Error: Tool `{tool_name}` not found.\"\n                )\n            )\n            return_directly = False\n            continue\n\n        # Get tool\n        tool = self.library[tool_name]\n        config = self.tool_configs.get(tool_name, {})\n\n        # Handle inject_vars\n        inject_vars = config.get(\"inject_vars\", False)\n        if inject_vars:\n            if isinstance(inject_vars, list):\n                for key in inject_vars:\n                    if key not in vars:\n                        raise ValueError(\n                            f\"The tool `{tool_name}` requires the injected \"\n                            f\"parameter `{key}`, but it was not found.\"\n                        )\n                    tool_params[key] = vars[key]\n            elif inject_vars is True:\n                tool_params[\"vars\"] = vars\n\n        if config.get(\"background\", False):\n            return_directly = False\n            F.background_task(tool, **(tool_params or {}))\n            tool_calls.append(\n                ToolCall(\n                    id=tool_id,\n                    name=tool_name,\n                    parameters=tool_params,\n                    result=f\"\"\"The `{tool_name}` tool was started in the background.\n                    This tool will not generate a return\"\"\"\n                )\n            )\n            continue\n\n        if config.get(\"call_as_response\", False):  # return function call as response\n            tool_calls.append(\n                ToolCall(id=tool_id, name=tool_name, parameters=tool_params)\n            )\n            return_directly = True\n            continue\n\n        if config.get(\"inject_model_state\", False):  # Add model_state\n            tool_params[\"task_messages\"] = model_state\n\n        if not config.get(\"return_direct\", False):\n            return_directly = False\n\n        tool_params = tool_params or {}\n        # Add tool_call_id for telemetry\n        tool_params[\"tool_call_id\"] = tool_id\n        prepared_calls.append(partial(tool, **tool_params))\n\n        call_metadata.append(\n            dotdict(id=tool_id, name=tool_name, config=config, params=tool_params)\n        )\n\n    if prepared_calls:\n        results = F.scatter_gather(prepared_calls)\n        for meta, result in zip(call_metadata, results):\n            if isinstance(meta.params, dict):\n                parameters = meta.params.to_dict()\n                parameters.pop(\"vars\", None)\n                parameters.pop(\"tool_call_id\", None)\n            else:\n                parameters = None\n            tool_calls.append(\n                ToolCall(\n                    id=meta.id,\n                    name=meta.name,\n                    parameters=parameters,\n                    result=result,\n                )\n            )\n\n    return ToolResponses(return_directly=return_directly, tool_calls=tool_calls)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.get_mcp_tool_names","title":"get_mcp_tool_names","text":"<pre><code>get_mcp_tool_names()\n</code></pre> <p>Get names of all MCP tools (with namespace).</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def get_mcp_tool_names(self) -&gt; List[str]:\n    \"\"\"Get names of all MCP tools (with namespace).\"\"\"\n    tool_names = []\n    for namespace, mcp_data in self.mcp_clients.items():\n        for tool in mcp_data[\"tools\"]:\n            tool_names.append(f\"{namespace}__{tool.name}\")\n    return tool_names\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.get_tool_json_schemas","title":"get_tool_json_schemas","text":"<pre><code>get_tool_json_schemas()\n</code></pre> <p>Returns a list of JSON schemas from local and MCP tools.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def get_tool_json_schemas(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Returns a list of JSON schemas from local and MCP tools.\"\"\"\n    schemas = []\n\n    # Local tools\n    for tool_name in self.library:\n        schemas.append(self.library[tool_name].get_json_schema())\n\n    # MCP tools\n    if self.mcp_clients:\n        for namespace, mcp_data in self.mcp_clients.items():\n            for mcp_tool in mcp_data[\"tools\"]:\n                schema = convert_mcp_schema_to_tool_schema(mcp_tool, namespace)\n                schemas.append(schema)\n\n    return schemas\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.get_tool_names","title":"get_tool_names","text":"<pre><code>get_tool_names()\n</code></pre> <p>Get names of all tools.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def get_tool_names(self) -&gt; List[str]:\n    \"\"\"Get names of all tools.\"\"\"\n    return list(self.library.keys())\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.get_tools","title":"get_tools","text":"<pre><code>get_tools()\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def get_tools(self) -&gt; Iterator[Dict[str, Tool]]:\n    return self.library.items()\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.remove","title":"remove","text":"<pre><code>remove(tool_name)\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def remove(self, tool_name: str):\n    if tool_name in self.library.keys():\n        self.library.pop(tool_name)\n        self.tool_configs.pop(tool_name, None)\n    elif tool_name in self.special_library:\n        self.special_library.remove(tool_name)\n    else:\n        raise ValueError(f\"The tool name `{tool_name}` is not in tool library\")\n</code></pre>"},{"location":"api-reference/nn/modules/transcriber/","title":"Transcriber","text":""},{"location":"api-reference/nn/modules/transcriber/#msgflux.nn.modules.transcriber.Transcriber","title":"Transcriber","text":"<p>               Bases: <code>Module</code></p> <p>Transcriber is a Module type that uses language models to transcribe audios.</p> Source code in <code>src/msgflux/nn/modules/transcriber.py</code> <pre><code>class Transcriber(Module):\n    \"\"\"Transcriber is a Module type that uses language models to transcribe audios.\"\"\"\n\n    def __init__(\n        self,\n        model: Union[SpeechToTextModel, ModelGateway],\n        *,\n        message_fields: Optional[Dict[str, Any]] = None,\n        response_mode: Optional[str] = \"plain_response\",\n        response_template: Optional[str] = None,\n        response_format: Optional[str] = \"text\",\n        prompt: Optional[str] = None,\n        config: Optional[Dict[str, Any]] = None,\n        name: Optional[str] = None,   \n    ):\n        \"\"\"Args:\n        model:\n            Transcriber Model client.\n        message_fields:\n            Dictionary mapping Message field names to their paths in the Message object.\n            Valid keys: \"task_multimodal_inputs\", \"model_preference\"\n            !!! example\n                message_fields={\n                    \"task_multimodal_inputs\": \"audio.user\",\n                    # or dict-based: \"task_multimodal_inputs\": {\"audio\": \"audio.user\"}\n                    \"model_preference\": \"model.preference\"\n                }\n\n            Field descriptions:\n            - task_multimodal_inputs: Field path for audio input (str or dict)\n            - model_preference: Field path for model preference (str, only valid with ModelGateway)\n        response_mode:\n            What the response should be.\n            * `plain_response` (default): Returns the final agent response directly.\n            * other: Write on field in Message object.\n        response_format: How the model should format the output. Options:\n            * text (default)\n            * json\n            * srt\n            * verbose_json\n            * vtt\n        prompt:\n            Useful for instructing the model to follow some transcript\n            generation pattern.\n        config:\n            Dictionary with configuration options. Accepts any keys without validation.\n            Common options: \"language\", \"stream\", \"timestamp_granularities\"\n            !!! example\n                config={\n                    \"language\": \"en\",\n                    \"stream\": False,\n                    \"timestamp_granularities\": \"word\"\n                }\n\n            Configuration options:\n            - language: Spoken language acronym (str)\n            - stream: Transmit response on-the-fly (bool)\n            - timestamp_granularities: Enable timestamp granularities - \"word\", \"segment\", or None\n              (requires response_format=verbose_json)\n        name:\n            Transcriber name in snake case format.              \n        \"\"\"\n        super().__init__()\n        self._set_model(model)\n        self._set_prompt(prompt)\n        self._set_message_fields(message_fields)\n        self._set_response_format(response_format)\n        self._set_response_mode(response_mode)\n        self._set_response_template(response_template)\n        self._set_config(config)\n        if name:\n            self.set_name(name)\n\n    def forward(\n        self, message: Union[bytes, str, Dict[str, str], Message], **kwargs\n    ) -&gt; Union[str, Dict[str, str], Message, ModelStreamResponse]:\n        \"\"\"Execute the transcriber with the given message.\n\n        Args:\n            message: The input message, which can be:\n                - bytes: Direct audio bytes to transcribe\n                - str: Audio file path or URL\n                - dict: Audio input as dictionary\n                - Message: Message object with fields mapped via message_fields\n            **kwargs: Runtime overrides for message_fields. Can include:\n                - task_multimodal_inputs: Override multimodal inputs\n                  (e.g., \"audio.path\" or {\"audio\": \"audio.path\"})\n                - model_preference: Override model preference\n\n        Returns:\n            Transcribed text (str, dict, Message, or ModelStreamResponse depending\n            on configuration).\n        \"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = self._execute_model(**inputs)\n        response = self._process_model_response(model_response, message)\n        return response\n\n    async def aforward(\n        self, message: Union[bytes, str, Dict[str, str], Message], **kwargs\n    ) -&gt; Union[str, Dict[str, str], Message, ModelStreamResponse]:\n        \"\"\"Async version of forward. Execute the transcriber asynchronously.\"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = await self._aexecute_model(**inputs)\n        response = self._process_model_response(model_response, message)\n        return response\n\n    def _execute_model(\n        self, data: Union[str, bytes], model_preference: Optional[str] = None\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        model_execution_params = self._prepare_model_execution(data, model_preference)\n        model_response = self.model(**model_execution_params)\n        return model_response\n\n    async def _aexecute_model(\n        self, data: Union[str, bytes], model_preference: Optional[str] = None\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        model_execution_params = self._prepare_model_execution(data, model_preference)\n        model_response = await self.model.acall(**model_execution_params)\n        return model_response\n\n    def _prepare_model_execution(\n        self, data: Union[str, bytes], model_preference: Optional[str] = None\n    ) -&gt; Dict[str, Any]:\n        model_execution_params = dotdict(\n            data=data,\n            language=self.config.get(\"language\"),\n            response_format=self.response_format,\n            timestamp_granularities=self.config.get(\"timestamp_granularities\"),\n            prompt=self.prompt,\n            stream=self.config.get(\"stream\", False)\n        )\n        if isinstance(self.model, ModelGateway) and model_preference is not None:\n            model_execution_params.model_preference = model_preference\n        return model_execution_params\n\n    def _process_model_response(\n        self,\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        message: Union[str, Message],\n    ) -&gt; Union[str, Dict[str, str], Message, ModelStreamResponse]:\n        if model_response.response_type == \"transcript\":\n            raw_response = self._extract_raw_response(model_response)\n            response = self._prepare_response(raw_response, message)\n            return response\n        else:\n            raise ValueError(\n                f\"Unsupported model response type `{model_response.response_type}`\"\n            )\n\n    def _prepare_task(\n        self, message: Union[bytes, str, Dict[str, str], Message], **kwargs\n    ) -&gt; Dict[str, Union[bytes, str]]:\n        data = self._process_task_multimodal_inputs(message)\n\n        model_preference = kwargs.pop(\"model_preference\", None)\n        if model_preference is None and isinstance(message, Message):\n            model_preference = self.get_model_preference_from_message(message)\n\n        return {\"data\": data, \"model_preference\": model_preference}\n\n    def _process_task_multimodal_inputs(\n        self, message: Union[bytes, str, Dict[str, str], Message]\n    ) -&gt; bytes:\n        if isinstance(message, Message):\n            audio_content = self._extract_message_values(\n                self.task_multimodal_inputs, message\n            )\n        else:\n            audio_content = message\n\n        if isinstance(audio_content, dict):\n            audio = audio_content.get(\"audio\", None)\n            if audio:\n                audio_content = audio\n            else:\n                raise ValueError(\n                    \"`task_multimodal_inputs` path based-on dict requires \"\n                    f\"an `audio` key given {audio_content}\"\n                )\n\n        return audio_content\n\n    def inspect_model_execution_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n        \"\"\"Debug model input parameters.\"\"\"\n        inputs = self._prepare_task(*args, **kwargs)\n        model_execution_params = self._prepare_model_execution(**inputs)\n        return model_execution_params\n\n    def _set_model(self, model: Union[SpeechToTextModel, ModelGateway]):\n        if model.model_type == \"speech_to_text\":\n            self.register_buffer(\"model\", model)\n        else:\n            raise TypeError(\n                f\"`model` need be a `speech_to_text` model, given `{type(model)}`\"\n            )\n\n    def _set_config(self, config: Optional[Dict[str, Any]] = None):\n        if config is None:\n            self.register_buffer(\"config\", {})\n            return\n\n        if not isinstance(config, dict):\n            raise TypeError(\n                f\"`config` must be a dict or None, given `{type(config)}`\"\n            )\n\n        self.register_buffer(\"config\", config.copy())\n\n    def _set_response_format(self, response_format: str):\n        supported_formats = [\"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"]\n        if isinstance(response_format, str):\n            if response_format in supported_formats:\n                self.register_buffer(\"response_format\", response_format)\n            else:\n                raise ValueError(\n                    f\"`response_format` can be `{supported_formats}` \"\n                    f\"given `{response_format}\"\n                )\n        else:\n            raise TypeError(\n                f\"`response_format` need be a str or given `{type(response_format)}\"\n            )\n</code></pre>"},{"location":"api-reference/nn/modules/transcriber/#msgflux.nn.modules.transcriber.Transcriber.__init__","title":"__init__","text":"<pre><code>__init__(\n    model,\n    *,\n    message_fields=None,\n    response_mode=\"plain_response\",\n    response_template=None,\n    response_format=\"text\",\n    prompt=None,\n    config=None,\n    name=None,\n)\n</code></pre> <p>model:     Transcriber Model client. message_fields:     Dictionary mapping Message field names to their paths in the Message object.     Valid keys: \"task_multimodal_inputs\", \"model_preference\"     !!! example         message_fields={             \"task_multimodal_inputs\": \"audio.user\",             # or dict-based: \"task_multimodal_inputs\": {\"audio\": \"audio.user\"}             \"model_preference\": \"model.preference\"         }</p> <pre><code>Field descriptions:\n- task_multimodal_inputs: Field path for audio input (str or dict)\n- model_preference: Field path for model preference (str, only valid with ModelGateway)\n</code></pre> <p>response_mode:     What the response should be.     * <code>plain_response</code> (default): Returns the final agent response directly.     * other: Write on field in Message object. response_format: How the model should format the output. Options:     * text (default)     * json     * srt     * verbose_json     * vtt prompt:     Useful for instructing the model to follow some transcript     generation pattern. config:     Dictionary with configuration options. Accepts any keys without validation.     Common options: \"language\", \"stream\", \"timestamp_granularities\"     !!! example         config={             \"language\": \"en\",             \"stream\": False,             \"timestamp_granularities\": \"word\"         }</p> <pre><code>Configuration options:\n- language: Spoken language acronym (str)\n- stream: Transmit response on-the-fly (bool)\n- timestamp_granularities: Enable timestamp granularities - \"word\", \"segment\", or None\n  (requires response_format=verbose_json)\n</code></pre> <p>name:     Transcriber name in snake case format.</p> Source code in <code>src/msgflux/nn/modules/transcriber.py</code> <pre><code>def __init__(\n    self,\n    model: Union[SpeechToTextModel, ModelGateway],\n    *,\n    message_fields: Optional[Dict[str, Any]] = None,\n    response_mode: Optional[str] = \"plain_response\",\n    response_template: Optional[str] = None,\n    response_format: Optional[str] = \"text\",\n    prompt: Optional[str] = None,\n    config: Optional[Dict[str, Any]] = None,\n    name: Optional[str] = None,   \n):\n    \"\"\"Args:\n    model:\n        Transcriber Model client.\n    message_fields:\n        Dictionary mapping Message field names to their paths in the Message object.\n        Valid keys: \"task_multimodal_inputs\", \"model_preference\"\n        !!! example\n            message_fields={\n                \"task_multimodal_inputs\": \"audio.user\",\n                # or dict-based: \"task_multimodal_inputs\": {\"audio\": \"audio.user\"}\n                \"model_preference\": \"model.preference\"\n            }\n\n        Field descriptions:\n        - task_multimodal_inputs: Field path for audio input (str or dict)\n        - model_preference: Field path for model preference (str, only valid with ModelGateway)\n    response_mode:\n        What the response should be.\n        * `plain_response` (default): Returns the final agent response directly.\n        * other: Write on field in Message object.\n    response_format: How the model should format the output. Options:\n        * text (default)\n        * json\n        * srt\n        * verbose_json\n        * vtt\n    prompt:\n        Useful for instructing the model to follow some transcript\n        generation pattern.\n    config:\n        Dictionary with configuration options. Accepts any keys without validation.\n        Common options: \"language\", \"stream\", \"timestamp_granularities\"\n        !!! example\n            config={\n                \"language\": \"en\",\n                \"stream\": False,\n                \"timestamp_granularities\": \"word\"\n            }\n\n        Configuration options:\n        - language: Spoken language acronym (str)\n        - stream: Transmit response on-the-fly (bool)\n        - timestamp_granularities: Enable timestamp granularities - \"word\", \"segment\", or None\n          (requires response_format=verbose_json)\n    name:\n        Transcriber name in snake case format.              \n    \"\"\"\n    super().__init__()\n    self._set_model(model)\n    self._set_prompt(prompt)\n    self._set_message_fields(message_fields)\n    self._set_response_format(response_format)\n    self._set_response_mode(response_mode)\n    self._set_response_template(response_template)\n    self._set_config(config)\n    if name:\n        self.set_name(name)\n</code></pre>"},{"location":"api-reference/nn/modules/transcriber/#msgflux.nn.modules.transcriber.Transcriber.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(message, **kwargs)\n</code></pre> <p>Async version of forward. Execute the transcriber asynchronously.</p> Source code in <code>src/msgflux/nn/modules/transcriber.py</code> <pre><code>async def aforward(\n    self, message: Union[bytes, str, Dict[str, str], Message], **kwargs\n) -&gt; Union[str, Dict[str, str], Message, ModelStreamResponse]:\n    \"\"\"Async version of forward. Execute the transcriber asynchronously.\"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = await self._aexecute_model(**inputs)\n    response = self._process_model_response(model_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/transcriber/#msgflux.nn.modules.transcriber.Transcriber.forward","title":"forward","text":"<pre><code>forward(message, **kwargs)\n</code></pre> <p>Execute the transcriber with the given message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Union[bytes, str, Dict[str, str], Message]</code> <p>The input message, which can be: - bytes: Direct audio bytes to transcribe - str: Audio file path or URL - dict: Audio input as dictionary - Message: Message object with fields mapped via message_fields</p> required <code>**kwargs</code> <p>Runtime overrides for message_fields. Can include: - task_multimodal_inputs: Override multimodal inputs   (e.g., \"audio.path\" or {\"audio\": \"audio.path\"}) - model_preference: Override model preference</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, Dict[str, str], Message, ModelStreamResponse]</code> <p>Transcribed text (str, dict, Message, or ModelStreamResponse depending</p> <code>Union[str, Dict[str, str], Message, ModelStreamResponse]</code> <p>on configuration).</p> Source code in <code>src/msgflux/nn/modules/transcriber.py</code> <pre><code>def forward(\n    self, message: Union[bytes, str, Dict[str, str], Message], **kwargs\n) -&gt; Union[str, Dict[str, str], Message, ModelStreamResponse]:\n    \"\"\"Execute the transcriber with the given message.\n\n    Args:\n        message: The input message, which can be:\n            - bytes: Direct audio bytes to transcribe\n            - str: Audio file path or URL\n            - dict: Audio input as dictionary\n            - Message: Message object with fields mapped via message_fields\n        **kwargs: Runtime overrides for message_fields. Can include:\n            - task_multimodal_inputs: Override multimodal inputs\n              (e.g., \"audio.path\" or {\"audio\": \"audio.path\"})\n            - model_preference: Override model preference\n\n    Returns:\n        Transcribed text (str, dict, Message, or ModelStreamResponse depending\n        on configuration).\n    \"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = self._execute_model(**inputs)\n    response = self._process_model_response(model_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/transcriber/#msgflux.nn.modules.transcriber.Transcriber.inspect_model_execution_params","title":"inspect_model_execution_params","text":"<pre><code>inspect_model_execution_params(*args, **kwargs)\n</code></pre> <p>Debug model input parameters.</p> Source code in <code>src/msgflux/nn/modules/transcriber.py</code> <pre><code>def inspect_model_execution_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n    \"\"\"Debug model input parameters.\"\"\"\n    inputs = self._prepare_task(*args, **kwargs)\n    model_execution_params = self._prepare_model_execution(**inputs)\n    return model_execution_params\n</code></pre>"},{"location":"blog/","title":"Posts","text":""},{"location":"blog/2023/02/03/english-search-support/","title":"English Support in msgflux","text":"<p>Neste post, vamos explorar as incr\u00edveis possibilidades dos agentes inteligentes.</p>"},{"location":"blog/2023/02/03/english-search-support/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O conte\u00fado completo do post come\u00e7a aqui...</p>"},{"location":"blog/2025/01/03/chinese-search-support/","title":"Chinese Support in msgflux","text":"<p>Neste post, vamos explorar as incr\u00edveis possibilidades dos agentes inteligentes.</p>"},{"location":"blog/2025/01/03/chinese-search-support/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O conte\u00fado completo do post come\u00e7a aqui...</p>"},{"location":"blog/2025/12/03/agent-support/","title":"Amazing: Uma Nova Era","text":"<p>Neste post, vamos as incr\u00edveis possibilidades dos agentes inteligentes.</p>"},{"location":"blog/2025/12/03/agent-support/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O conte\u00fado completo do post come\u00e7a aqui...</p>"},{"location":"data/databases/vector/","title":"faiss","text":"<p>vector</p>"},{"location":"getting-started/agent/","title":"Agents","text":"<pre><code>pip install msgflux[openai]\n</code></pre>"},{"location":"getting-started/multimodal-agent/","title":"MultiModal Agents","text":""},{"location":"guides/sequential/","title":"Sequential","text":"<p>import msgflux.nn as nn</p>"},{"location":"models/chat_completion/","title":"<code>Chat Completion</code>","text":"<p>The <code>chat_completion</code> model is the most common and versatile model for natural language interactions. It processes messages in a conversational format and supports advanced features such as multimodal input and output, structured data generation, and tool (function) calls.</p> <p>We will explain it's features to understand how it works and its limitations. And why we need a higher-level abstraction like <code>nn.Agent</code> to make it easier to create applications.</p>"},{"location":"models/chat_completion/#overview","title":"\u2726\u208a\u207a Overview","text":"<p>All models have the same calling interface, differing only in the initialization of their classes.</p> initcall <p>model_id:     Model ID in provider. max_tokens:     An upper bound for the number of tokens that can be     generated for a completion, including visible output     tokens and reasoning tokens. reasoning_effort:     Constrains effort on reasoning for reasoning models.     Currently supported values are low, medium, and high.     Reducing reasoning effort can result in faster responses     and fewer tokens used on reasoning in a response.     Can be: \"minimal\", \"low\", \"medium\" or \"high\". enable_thinking:     If True, enable the model reasoning. return_reasoning:     If the model returns the <code>reasoning</code> field it will be added     along with the response. reasoning_in_tool_call:     If True, maintains the reasoning for using the tool call. validate_typed_parser_output:     If True, use the generation_schema to validate typed parser output.           temperature:     What sampling temperature to use, between 0 and 2.     Higher values like 0.8 will make the output more random,     while lower values like 0.2 will make it more focused and     deterministic. stop:     Up to 4 sequences where the API will stop generating further     tokens. The returned text will not contain the stop sequence.           top_p:     An alternative to sampling with temperature, called nucleus     sampling, where the model considers the results of the tokens     with top_p probability mass. So 0.1 means only the tokens     comprising the top 10% probability mass are considered. parallel_tool_calls:     If True, enable parallel tool calls.           modalities:     Types of output you would like the model to generate.     Can be: [\"text\"], [\"audio\"] or [\"text\", \"audio\"]. audio:     Audio configurations. Define voice and output format. verbosity:     Constrains the verbosity of the model's response. Lower      values will result in more concise responses, while higher     values will result in more verbose responses. Currently     supported values are low, medium, and high. web_search_options:     This tool searches the web for relevant results to use in a response.     OpenAI and OpenRouter only. verbose:     If True, Prints the model output to the console before it is transformed     into typed structured output. base_url:     URL to model provider. context_length:     The maximum context length supported by the model. reasoning_max_tokens:     Maximum number of tokens for reasoning/thinking. enable_cache:     If True, enable response caching to avoid redundant API calls. cache_size:     Maximum number of cached responses (default: 128).</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, List[Dict[str, Any]]]</code> <p>Conversation history. Can be simple string or list of messages.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>A set of instructions that defines the overarching behavior and role of the model across all interactions.</p> <code>None</code> <code>prefilling</code> <code>Optional[str]</code> <p>Forces an initial message from the model. From that message it will continue its response from there.</p> <code>None</code> <code>stream</code> <code>Optional[bool]</code> <p>Whether generation should be in streaming mode.</p> <code>False</code> <code>generation_schema</code> <code>Optional[Struct]</code> <p>Schema that defines how the output should be structured.</p> <code>None</code> <code>tool_schemas</code> <code>Optional[Dict]</code> <p>JSON schema containing available tools.</p> <code>None</code> <code>tool_choice</code> <code>Optional[Union[str, Dict[str, Any]]]</code> <p>By default the model will determine when and how many tools to use. You can force specific behavior with the tool_choice parameter.     1. auto:         (Default) Call zero, one, or multiple functions.     2. required:         Call one or more functions.     3. Forced Tool:         Call exactly one specific tool e.g: \"get_weather\".</p> <code>None</code> <code>typed_parser</code> <code>Optional[str]</code> <p>Converts the model raw output into a typed-dict. Supported parser: <code>typed_xml</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised if <code>generation_schema</code> and <code>stream=True</code>.</p> <code>ValueError</code> <p>Raised if <code>typed_xml=True</code> and <code>stream=True</code>.</p> <p>{! ../_includes/init_chat_completion_model.md !}</p>"},{"location":"models/chat_completion/#1-stateless","title":"1. Stateless","text":"<p>Chat completion models do not maintain state between calls. All context information (previous messages, system prompt, etc.) must be provided on each new call.</p> <pre><code>prompt = \"What is Deep Learning?\"\nresponse = model(prompt)\n\nassistant = response.consume()\nprint(assistant)\n\nprompt_2 = \"What are the most impactful architectures?\"\n\nmessages = [\n    {\"role\": \"user\", \"content\": prompt},\n    {\"role\": \"assistant\", \"content\": assistant}\n    {\"role\": \"user\", \"content\": prompt_2},     \n]\n\nresponse = model(messages)\nprint(response.consume())\n</code></pre>"},{"location":"models/chat_completion/#2-multimodal","title":"2. Multimodal","text":"<p>Modern models are natively multimodal. This means they can:</p> <ul> <li> <p>Understand and generate text</p> </li> <li> <p>Interpret and generate images</p> </li> <li> <p>Listen to and generate speech</p> </li> </ul> <pre><code>messages = [\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"text\", \"text\": \"What do you see in this image?\"},\n        {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.png\"}},\n    ]}\n]\n</code></pre>"},{"location":"models/chat_completion/#3-structured-generation","title":"3. Structured Generation","text":"<p>The model can be guided to produce structured responses according to a user-defined schema.</p> <p>In msgflux this is called <code>generation_schema</code>. The name shows not only that the model produces a <code>structured output</code>, but also that it follows a schema.</p> <p>For this, we use <code>msgspec.Struct</code> as the structure format:</p> <pre><code>import msgspec\n\nclass Weather(msgspec.Struct):\n    temperature: float\n    condition: str\n\nresponse = model(\n    messages=[...],\n    generation_schema=Weather\n)\n</code></pre> <p>The response is decoded directly as an instance of the Weather class, which simplifies consumption of the response and avoids manual text parsing.</p> <p><code>generation_schema</code> maybe the most important feature in <code>chat_completion</code> models. This feature enables things like <code>ReAct</code>, <code>CoT</code>, new content generation, guided data extraction, etc. In this framework following tutorials we make extensive use of it.</p> <p>We offer a set of schemes that assist in model planning</p> <pre><code>from msgflux.generation.plan import (\n    ChainOfThoughts,\n    ReAct,\n    SelfConsistency,\n    TreeOfThoughts\n)\n</code></pre> <p>In the case of <code>ReAct</code>, where tool calls occur, it's necessary to implement control flow to feed back into the model. This is already present in <code>nn.Agent</code>.</p>"},{"location":"models/chat_completion/#4-tools","title":"4. Tools","text":"<p>When we provide a set of tools (<code>tool_schemas</code>), the model may suggest that one of them be called\u2014but it does not automatically execute them.</p> <p>Instead, it returns a call intent, which is captured and processed by an internal component called the <code>ToolCallAggregator</code>.</p> <p>This class collects and organizes tool calls suggested by the model, especially in streaming mode, where arguments arrive in fragmented form.</p> <p>Main responsibilities:</p> <ul> <li> <p>Reassemble parts of calls during the stream (<code>process</code>)</p> </li> <li> <p>Convert raw data into complete functional calls (<code>get_calls</code>)</p> </li> <li> <p>Insert tool results (<code>insert_results</code>)</p> </li> <li> <p>Generate messages in the correct format to follow the flow with the model (<code>get_messages</code>)</p> </li> </ul>"},{"location":"models/chat_completion/#5-prefilling","title":"5. Prefilling","text":"<p>Prefilling is a technique used to start the model message. A classic usage is: </p> <p><code>let's think step by step</code></p> <p>The model detects that it has started the sequence and then the message it will send next is the continuation of it. This technique is particularly very useful for generating structured outputs.</p>"},{"location":"models/image_text_to_image/","title":"Image Edit","text":""},{"location":"models/image_text_to_image/#msgflux.models.providers.openai.OpenAIImageTextToImage.__init__","title":"__init__","text":"<pre><code>__init__(*, model_id, moderation=None, base_url=None)\n</code></pre> <p>model_id:     Model ID in provider. moderation:     Control the content-moderation level for images generated. base_url:     URL to model provider.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model_id: str,\n    moderation: Optional[Literal[\"auto\", \"low\"]] = None,\n    base_url: Optional[str] = None,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    moderation:\n        Control the content-moderation level for images generated.\n    base_url:\n        URL to model provider.\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    sampling_run_params = {}\n    if moderation:\n        sampling_run_params[\"moderation\"] = moderation\n    self.sampling_run_params = sampling_run_params\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"models/model/","title":"<code>Model</code> \u2014 Unified Interface for AI Models","text":"<p>The <code>Model</code>  class in <code>msgflux</code> serves as a high-level factory and registry for loading AI models across various providers and modalities. It abstracts away boilerplate code and offers a simple, consistent API to instantiate models like chatbots, text embedders, speakers, image and video generators, and more.</p> <pre><code>pip install msgflux[openai]\n</code></pre> <pre><code>import msgflux as mf\n\nmf.set_envs(OPENAI_API_KEY=\"sk-...\")\nmodel = mf.Model.chat_completion(\"openai/gpt-4.1-nano\", temperature=0.7)\n</code></pre>"},{"location":"models/model/#overview","title":"\u2726\u208a\u207a Overview","text":""},{"location":"models/model/#1-unified-api","title":"1. Unified API","text":"<p>No need to memorize individual client APIs or custom wrappers. Just specify the model type, path and parameters:</p> <p>\u1bd3\u2605 \u27a1 \ud81a\udc4e \u26a1\ufe0e \u269b \ud83d\udd6e \u2570\u2508\u27a4 \u2714</p> <pre><code>mf.Model.text_to_speech(\"openai/tts-1\")\nmf.Model.text_embedder(\"openai/text-embedding-ada-003\")\n</code></pre>"},{"location":"models/model/#2-supported-types","title":"2. Supported Types","text":"<p>Supports a wide range of AI capabilities:</p> Type Description <code>chat_completion</code> Understanding and multimodal generation <code>image_embedder</code> Generates a vector representation of an images <code>image_text_to_image</code> Image edit <code>moderation</code> Checks if the content is safe <code>speech_to_text</code> Voice transcription <code>text_classifier</code> Classify text <code>text_embedder</code> Generates a vector representation of a text <code>text_reranker</code> Rerank text options given a query <code>text_to_image</code> Image Generation <code>text_to_speech</code> Generates voice from text <p>You can check all supported model types using: <pre><code>print(mf.Model.supported_model_types)\n</code></pre></p> <p>and also what are the providers availables for each type: <pre><code>print(mf.Model.providers_by_model_type)\n</code></pre></p>"},{"location":"models/model/#3-resilience","title":"3. Resilience","text":"<p>API-based models are protected by a decorator (<code>@model_retry</code>) that applies retry in case of failures.</p> <p>Can manage multiple keys, this is useful in case a key becomes invalid. Separate with commas.</p> <pre><code>mf.set_envs(OPENAI_API_KEY=\"sk-1.., sk-2..\")\n</code></pre>"},{"location":"models/model/#4-responses","title":"4. Responses","text":"<p>Each model returns a model response instance. Making the model response type explicit helps manage that response because you already know what's in that object.</p> <p>The model response which can be one of:</p>"},{"location":"models/model/#41-modelresponse","title":"4.1 ModelResponse","text":"<p>Ideal for non-streaming tasks like embeddings, classification, speech-to-text, etc.</p> <pre><code>from msgflux.models.response import ModelResponse\nresponse = ModelResponse()\nresponse.set_response_type(\"text_embedding\")\nresponse.add([1.2, 3.4]) # Can be any datatype\nprint(response.response_type)\nprint(response.consume())\n</code></pre>"},{"location":"models/model/#42-modelstreamresponse","title":"4.2 ModelStreamResponse","text":"<p>Designed for tasks where data is generated in real time \u2014 such as text and speech generation, tool-calling, etc.</p> <pre><code>from msgflux.models.response import ModelResponse\nstream_response = ModelStreamResponse()\nstream_response.set_response_type(\"text_generation\")\n\nstream_response.add(\"Hello \")\nstream_response.first_chunk_event.set() # Informs that consumption can now begin\nstream_response.add(\"world!\")\nstream_response.add(None)  # Signals the end\n\nasync for chunk in stream_response.consume():\n    print(chunk, flush=True)  # \u2192 \"Hello \", after \"world!\"\n</code></pre>"},{"location":"models/model/#5-model-info","title":"5. Model Info","text":"<p>Some information present when serializing a model is also accessible using</p> <pre><code>print(model.model_type)\n</code></pre> <pre><code>'chat_completion'\n</code></pre> <pre><code>print(model.instance_type())\n</code></pre> <pre><code>{'model_type': 'chat_completion'}\n</code></pre> <pre><code>print(model.get_model_info())\n</code></pre> <pre><code>{'model_id': 'gpt-4.1-nano', 'provider': 'openai'}\n</code></pre>"},{"location":"models/model/#6-serialization","title":"6. Serialization","text":"<p>You can export the internal state of an object from a Model</p> <pre><code>model = mf.Model.chat_completion(\"openai/gpt-4.1-nano\")\nmodel_state = model.serialize()\nprint(model_state)\n</code></pre> <pre><code>{\n    'msgflux_type': 'model',\n    'provider': 'openai',\n    'model_type': 'chat_completion',\n    'state': {\n        'model_id': 'gpt-4.1-nano',\n        'sampling_params': {'organization': None, 'project': None},\n        'sampling_run_params': {\n            'max_tokens': 512,\n            'temperature': None,\n            'top_p': None,\n            'modalities': ['text'],\n            'audio': None\n        }\n    }\n}\n</code></pre> <p>So re-create a model from a serialized object</p> <pre><code>model = mf.Model.from_serialized(**model_state)\n</code></pre> <p>Internally the model is created using <code>__new__</code> and then <code>_initialize</code> method is called which initializes the internal state.</p>"},{"location":"models/model_gateway/","title":"<code>ModelGateway</code> \u2014 Resilient Model Manager","text":"<p>The <code>ModelGateway</code> class is an orchestration layer over multiple models of the same type (e.g., multiple <code>chat_completion</code> models), allowing:</p> <ul> <li>\ud83d\udd01 Automatic fallback between models.</li> <li>\u23f1\ufe0f Time-based model availability constraints.</li> <li>\u2705 Model preference selection.</li> <li>\ud83d\udcc3 Control of execution attempts with exception handling.</li> <li>\ud83d\udd0e Consistent model typing validation.</li> </ul> <p>It's ideal for production-grade model orchestration where reliability and control over model usage are required.</p>"},{"location":"models/model_gateway/#overview","title":"\u2726\u208a\u207a Overview","text":""},{"location":"models/model_gateway/#1-usage","title":"1. Usage","text":"<pre><code>pip install msgflux[openai]\n</code></pre> <p>All you need is:</p> <ul> <li>All models must inherit from <code>BaseModel</code>.</li> <li>All models must be of the same <code>model_type</code>.</li> <li>At least 2 models must be provided.</li> </ul>"},{"location":"models/model_gateway/#11-query","title":"1.1 Query","text":"<pre><code>import msgflux as mf\n\nmf.set_envs(OPENAI_API_KEY=\"sk-...\", TOGETHER_API_KEY=\"&lt;&gt;\")\n\nmodel_openai = mf.Model.chat_completion(\"openai/gpt-4.1-nano\")\nmodel_together = mf.Model.chat_completion(\"together/mistral-7b\")\n\ngateway = mf.ModelGateway([model_openai, model_together], max_model_failures=3)\n\nresponse = gateway(\"Who was Frank Rosenblatt?\")\nprint(response.consume())\n</code></pre>"},{"location":"models/model_gateway/#12-simulated-failure","title":"1.2 Simulated Failure","text":"<pre><code>from msgflux.models.base import BaseModel\nfrom msgflux.models.types import ChatCompletionModel\n\n# Simulate a model that fails\nclass BrokenModel(BaseModel, ChatCompletionModel):\n    provider: \"mock\"\n\n    def __call__(self, **kwargs):\n        raise RuntimeError(\"Simulate failure\")\n\nbroken = BrokenModel()\nfallback = Model.chat_completion(\"openai/gpt-4.1-nano\")\n\ngateway_broken = ModelGateway([broken, fallback], max_model_failures=1)\n\nresponse = gateway_broken(\"Who were Warren McCulloch and Walter Pitts?\")\nprint(response.consume())\n</code></pre>"},{"location":"models/model_gateway/#13-time-constraints","title":"1.3 Time constraints","text":"<pre><code>import random\nfrom msgflux.exceptions import ModelRouterError\nfrom msgflux.models.base import BaseModel\nfrom msgflux.models.gateway import ModelGateway\nfrom msgflux.models.response import ModelResponse\nfrom msgflux.models.types import ChatCompletionModel\n\nclass MockChatCompletion(BaseModel, ChatCompletionModel):\n\n    provider = \"mock\"\n\n    def __init__(\n        self, \n        model_id: str, \n        fail_sometimes: bool = False, \n        success_rate: float = 0.7\n    ):\n        self.model_id = model_id\n        self._fail_sometimes = fail_sometimes\n        self._success_rate = success_rate\n        self._call_count = 0\n\n    def __call__(self, **kwargs: Any):\n        response = ModelResponse()\n        response.set_response_type(\"text_generation\")\n        self._call_count += 1\n        if self._fail_sometimes:\n            if random.random() &gt; self._success_rate:\n                raise ValueError(f\"Simulated failure for {self.model_id}\")\n        messages = kwargs.get(\"messages\", \"Default prompt\")\n        response_text = f\"Response from {self.model_id} to messages: '{messages}' (Call #{self._call_count})\";\n        response.add(response_text)\n        return response\n\nmodel1 = MockBaseModel(model_id=\"model-A\", fail_sometimes=True, success_rate=0.3)\nmodel2 = MockBaseModel(model_id=\"model-B\", fail_sometimes=True, success_rate=0.5)\nmodel3 = MockBaseModel(model_id=\"model-C\") # Always works\nmodel4 = MockBaseModel(model_id=\"model-D\") # Always works\n\nmodels_list = [model1, model2, model3, model4]\n\nconstraints = {\n    \"model-B\": [(\"23:00\", \"07:00\")],\n    \"model-C\": [(\"10:00\", \"11:00\")]\n}\n\ngateway_mock = ModelGateway(\n    models=models_list,\n    max_model_failures=2,\n    time_constraints=constraints\n)\n\ntry:\n    response = gateway_mock(messages=\"Hi\")\n    print(\"Result:\", response.consume())\nexcept ModelRouterError as e:\n    print(\"Error:\", e)\n</code></pre>"},{"location":"models/model_gateway/#2-model-info","title":"2. Model Info","text":"<p>Returns information for all managed models:</p> <pre><code>print(gateway.get_model_info())\n</code></pre> <pre><code>[\n    {'model_id': 'gpt-4.1-nano', 'provider': 'openai'},\n    {'model_id': 'mistral-7b', 'provider': 'together'}\n]\n</code></pre> <p>Returns the type of the models:</p> <pre><code>print(gateway.model_type)\n</code></pre> <pre><code>'chat_completion'\n</code></pre>"},{"location":"models/model_gateway/#3-serialization","title":"3. Serialization","text":"<p>Serializes the state of the gateway and models.</p> <pre><code>print(gateway.serialize())\n</code></pre> <pre><code>{\n    'msgflux_type': 'model_gateway', \n    'state': {\n        'max_model_failures': 3,\n        'models': [\n            {\n                'msgflux_type': 'model',\n                'provider': 'openai',\n                'model_type': 'chat_completion',\n                'state': {\n                    'model_id': 'gpt-4.1-nano',\n                    'sampling_params': {'organization': None, 'project': None},\n                    'sampling_run_params': {\n                        'max_tokens': 512,\n                        'temperature': None,\n                        'top_p': None,\n                        'modalities': ['text'],\n                        'audio': None\n                    }\n                }\n            },\n            {\n                'msgflux_type': 'model',\n                'provider': 'together',\n                'model_type': 'chat_completion',\n                'state': {\n                    'model_id': 'mistral-7b',\n                    'sampling_params': {'organization': None, 'project': None},\n                    'sampling_run_params': {\n                            'max_tokens': 512,\n                            'temperature': None,\n                            'top_p': None,\n                            'modalities': ['text'],\n                            'audio': None\n                    }\n                }\n            },            \n        ]\n    }\n}\n</code></pre>"},{"location":"models/moderation/","title":"Moderation","text":""},{"location":"models/moderation/#msgflux.models.providers.openai.OpenAIModeration","title":"OpenAIModeration","text":"<p>               Bases: <code>_BaseOpenAI</code>, <code>ModerationModel</code></p> <p>OpenAI Moderation.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAIModeration(_BaseOpenAI, ModerationModel):\n    \"\"\"OpenAI Moderation.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        model_id: str,\n        base_url: Optional[str] = None,\n        enable_cache: Optional[bool] = False,\n        cache_size: Optional[int] = 128,\n    ):\n        \"\"\"Args:\n        model_id:\n            Model ID in provider.\n        base_url:\n            URL to model provider.\n        enable_cache:\n            If True, enables response caching to avoid redundant API calls.\n        cache_size:\n            Maximum number of responses to cache (default: 128).\n        \"\"\"\n        super().__init__()\n        self.model_id = model_id\n        self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n        self.enable_cache = enable_cache\n        self.cache_size = cache_size\n        self._initialize()\n        self._get_api_key()\n\n    def _execute_model(self, **kwargs):\n        model_output = self.client.moderations.create(**kwargs)\n        return model_output\n\n    async def _aexecute_model(self, **kwargs):\n        model_output = await self.aclient.moderations.create(**kwargs)\n        return model_output\n\n    def _generate(self, **kwargs):\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        response = ModelResponse()\n        response.set_response_type(\"moderation\")\n        model_output = self._execute_model(**kwargs)\n        moderation = dotdict({\"results\": model_output.results[0].model_dump()})\n        moderation.safe = not moderation.results.flagged\n        response.add(moderation)\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    async def _agenerate(self, **kwargs):\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        response = ModelResponse()\n        response.set_response_type(\"moderation\")\n        model_output = await self._aexecute_model(**kwargs)\n        moderation = dotdict({\"results\": model_output.results[0].model_dump()})\n        moderation.safe = not moderation.results.flagged\n        response.add(moderation)\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    @model_retry\n    def __call__(\n        self,\n        data: Union[str, List[Dict[str, Any]]],\n    ) -&gt; ModelResponse:\n        \"\"\"Args:\n        data:\n            Input (or inputs) to classify. Can be a single string,\n            an array of strings, or an array of multi-modal input\n            objects similar to other models.\n        \"\"\"\n        response = self._generate(input=data, model=self.model_id)\n        return response\n\n    @model_retry\n    async def acall(\n        self,\n        data: Union[str, List[Dict[str, Any]]],\n    ) -&gt; ModelResponse:\n        \"\"\"Async version of __call__. Args:\n        data:\n            Input (or inputs) to classify. Can be a single string,\n            an array of strings, or an array of multi-modal input\n            objects similar to other models.\n        \"\"\"\n        response = await self._agenerate(input=data, model=self.model_id)\n        return response\n</code></pre>"},{"location":"models/moderation/#msgflux.models.providers.openai.OpenAIModeration.cache_size","title":"cache_size  <code>instance-attribute</code>","text":"<pre><code>cache_size = cache_size\n</code></pre>"},{"location":"models/moderation/#msgflux.models.providers.openai.OpenAIModeration.enable_cache","title":"enable_cache  <code>instance-attribute</code>","text":"<pre><code>enable_cache = enable_cache\n</code></pre>"},{"location":"models/moderation/#msgflux.models.providers.openai.OpenAIModeration.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"models/moderation/#msgflux.models.providers.openai.OpenAIModeration.sampling_params","title":"sampling_params  <code>instance-attribute</code>","text":"<pre><code>sampling_params = {'base_url': base_url or _get_base_url()}\n</code></pre>"},{"location":"models/moderation/#msgflux.models.providers.openai.OpenAIModeration.__call__","title":"__call__","text":"<pre><code>__call__(data)\n</code></pre> <p>data:     Input (or inputs) to classify. Can be a single string,     an array of strings, or an array of multi-modal input     objects similar to other models.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    data: Union[str, List[Dict[str, Any]]],\n) -&gt; ModelResponse:\n    \"\"\"Args:\n    data:\n        Input (or inputs) to classify. Can be a single string,\n        an array of strings, or an array of multi-modal input\n        objects similar to other models.\n    \"\"\"\n    response = self._generate(input=data, model=self.model_id)\n    return response\n</code></pre>"},{"location":"models/moderation/#msgflux.models.providers.openai.OpenAIModeration.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    model_id,\n    base_url=None,\n    enable_cache=False,\n    cache_size=128,\n)\n</code></pre> <p>model_id:     Model ID in provider. base_url:     URL to model provider. enable_cache:     If True, enables response caching to avoid redundant API calls. cache_size:     Maximum number of responses to cache (default: 128).</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model_id: str,\n    base_url: Optional[str] = None,\n    enable_cache: Optional[bool] = False,\n    cache_size: Optional[int] = 128,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    base_url:\n        URL to model provider.\n    enable_cache:\n        If True, enables response caching to avoid redundant API calls.\n    cache_size:\n        Maximum number of responses to cache (default: 128).\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    self.enable_cache = enable_cache\n    self.cache_size = cache_size\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"models/moderation/#msgflux.models.providers.openai.OpenAIModeration.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(data)\n</code></pre> <p>Async version of call. Args: data:     Input (or inputs) to classify. Can be a single string,     an array of strings, or an array of multi-modal input     objects similar to other models.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    data: Union[str, List[Dict[str, Any]]],\n) -&gt; ModelResponse:\n    \"\"\"Async version of __call__. Args:\n    data:\n        Input (or inputs) to classify. Can be a single string,\n        an array of strings, or an array of multi-modal input\n        objects similar to other models.\n    \"\"\"\n    response = await self._agenerate(input=data, model=self.model_id)\n    return response\n</code></pre>"},{"location":"models/text_to_image/","title":"Image Generation","text":""},{"location":"models/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage.__init__","title":"__init__","text":"<pre><code>__init__(*, model_id, moderation=None, base_url=None)\n</code></pre> <p>model_id:     Model ID in provider. moderation:     Control the content-moderation level for images generated. base_url:     URL to model provider.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model_id: str,\n    moderation: Optional[Literal[\"auto\", \"low\"]] = None,\n    base_url: Optional[str] = None,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    moderation:\n        Control the content-moderation level for images generated.\n    base_url:\n        URL to model provider.\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    sampling_run_params = {}\n    if moderation:\n        sampling_run_params[\"moderation\"] = moderation\n    self.sampling_run_params = sampling_run_params\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"nn/functional/","title":"Functional","text":""},{"location":"nn/functional/#msgflux.nn.functional.scatter_gather","title":"scatter_gather","text":"<pre><code>scatter_gather(\n    to_send,\n    args_list=None,\n    kwargs_list=None,\n    *,\n    timeout=None,\n)\n</code></pre> <p>Sends different sets of arguments/kwargs to a list of modules and collects the responses.</p> <p>Each callable in <code>to_send</code> receives the positional arguments of the corresponding <code>tuple</code> in <code>args_list</code> and the named arguments of the corresponding <code>dict</code> in <code>kwargs_list</code>. If <code>args_list</code> or <code>kwargs_list</code> are not provided (or are <code>None</code>), the corresponding callables will be called without positional or named arguments, respectively, unless an empty list (<code>[]</code>) or empty tuple (<code>()</code>) is provided for a specific item.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>args_list</code> <code>Optional[List[Tuple[Any, ...]]]</code> <p>Each tuple contains the positional argumentsvfor the corresponding callable in <code>to_send</code>. If <code>None</code>, no positional arguments are passed unless specified individually by an item in <code>kwargs_list</code>.</p> <code>None</code> <code>kwargs_list</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Each dictionary contains the named arguments for the corresponding callable in <code>to_send</code>. If <code>None</code>, no named arguments are passed unless specified individually by an item in <code>args_list</code>.</p> <code>None</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing the responses for each callable. If an error or</p> <code>...</code> <p>timeout occurs for a specific callable, its corresponding response</p> <code>Tuple[Any, ...]</code> <p>in the tuple will be <code>None</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a callable list.</p> <code>ValueError</code> <p>If the lengths of <code>args_list</code> (if provided) or <code>kwargs_list</code> (if provided) do not match the length of <code>to_send</code>.</p> <p>Examples:</p> <p>def add(x, y): return x + y def multiply(x, y=2): return x * y callables = [add, multiply, add]</p>"},{"location":"nn/functional/#msgflux.nn.functional.scatter_gather--example-1-using-only-args_list","title":"Example 1: Using only args_list","text":"<p>args = [ (1, 2), (3,), (10, 20) ] # multiply will use its default y results = F.scatter_gather(callables, args_list=args) print(results) # (3, 6, 30)</p>"},{"location":"nn/functional/#msgflux.nn.functional.scatter_gather--example-2-using-args_list-e-kwargs_list","title":"Example 2: Using args_list e kwargs_list","text":"<p>args = [ (1,), (), (10,) ] kwargs = [ {'y': 2}, {'x': 3, 'y': 3}, {'y': 20} ] results = F.scatter_gather(callables, args_list=args, kwargs_list=kwargs) print(results) # (3, 9, 30)</p>"},{"location":"nn/functional/#msgflux.nn.functional.scatter_gather--example-3-using-only-kwargs_list-useful-if-functions-have","title":"Example 3: Using only kwargs_list (useful if functions have","text":""},{"location":"nn/functional/#msgflux.nn.functional.scatter_gather--defaults-or-dont-need-positional-args","title":"defaults or don't need positional args)","text":"<p>def greet(name=\"World\"): return f\"Hello, {name}\" def farewell(person_name): return f\"Goodbye, {person_name}\" funcs = [greet, greet, farewell] kwargs_for_funcs = [ {}, {'name': \"Earth\"}, {'person_name': \"Commander\"} ] results = F.scatter_gather(funcs, kwargs_list=kwargs_for_funcs) print(results) # (\"Hello, World\", \"Hello, Earth\", \"Goodbye, Commander\")</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@instrument(\"scatter_gather\")\ndef scatter_gather(\n    to_send: List[Callable],\n    args_list: Optional[List[Tuple[Any, ...]]] = None,\n    kwargs_list: Optional[List[Dict[str, Any]]] = None,\n    *,\n    timeout: Optional[float] = None,\n) -&gt; Tuple[Any, ...]:\n    \"\"\"Sends different sets of arguments/kwargs to a list of modules\n    and collects the responses.\n\n    Each callable in `to_send` receives the positional arguments of\n    the corresponding `tuple` in `args_list` and the named arguments\n    of the corresponding `dict` in `kwargs_list`. If `args_list` or\n    `kwargs_list` are not provided (or are `None`), the corresponding\n    callables will be called without positional or named arguments,\n    respectively, unless an empty list (`[]`) or empty tuple (`()`)\n    is provided for a specific item.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        args_list:\n            Each tuple contains the positional argumentsvfor the corresponding callable\n            in `to_send`. If `None`, no positional arguments are passed unless specified\n            individually by an item in `kwargs_list`.\n        kwargs_list:\n            Each dictionary contains the named arguments for the corresponding callable\n            in `to_send`. If `None`, no named arguments are passed unless specified\n            individually by an item in `args_list`.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n\n    Returns:\n        Tuple containing the responses for each callable. If an error or\n        timeout occurs for a specific callable, its corresponding response\n        in the tuple will be `None`.\n\n    Raises:\n        TypeError:\n            If `to_send` is not a callable list.\n        ValueError:\n            If the lengths of `args_list` (if provided) or `kwargs_list`\n            (if provided) do not match the length of `to_send`.\n\n    Examples:\n        def add(x, y): return x + y\n        def multiply(x, y=2): return x * y\n        callables = [add, multiply, add]\n\n        # Example 1: Using only args_list\n        args = [ (1, 2), (3,), (10, 20) ] # multiply will use its default y\n        results = F.scatter_gather(callables, args_list=args)\n        print(results) # (3, 6, 30)\n\n        # Example 2: Using args_list e kwargs_list\n        args = [ (1,), (), (10,) ]\n        kwargs = [ {'y': 2}, {'x': 3, 'y': 3}, {'y': 20} ]\n        results = F.scatter_gather(callables, args_list=args, kwargs_list=kwargs)\n        print(results) # (3, 9, 30)\n\n        # Example 3: Using only kwargs_list (useful if functions have\n        # defaults or don't need positional args)\n        def greet(name=\"World\"): return f\"Hello, {name}\"\n        def farewell(person_name): return f\"Goodbye, {person_name}\"\n        funcs = [greet, greet, farewell]\n        kwargs_for_funcs = [ {}, {'name': \"Earth\"}, {'person_name': \"Commander\"} ]\n        results = F.scatter_gather(funcs, kwargs_list=kwargs_for_funcs)\n        print(results) # (\"Hello, World\", \"Hello, Earth\", \"Goodbye, Commander\")\n    \"\"\"\n    if not isinstance(to_send, list) or not all(callable(f) for f in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    executor = Executor.get_instance()\n    futures = []\n    for i, f in enumerate(to_send):\n        args = args_list[i] if args_list and i &lt; len(args_list) else ()\n        kwargs = kwargs_list[i] if kwargs_list and i &lt; len(kwargs_list) else {}\n        futures.append(executor.submit(f, *args, **kwargs))\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    responses: List[Any] = []\n    for future in futures:\n        try:\n            responses.append(future.result())\n        except Exception as e:\n            logger.error(str(e))\n            responses.append(None)\n    return tuple(responses)\n</code></pre>"},{"location":"nn/functional/#msgflux.nn.functional.msg_scatter_gather","title":"msg_scatter_gather","text":"<pre><code>msg_scatter_gather(to_send, messages, *, timeout=None)\n</code></pre> <p>Scatter a list of messages to a list of modules and gather the responses.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>messages</code> <code>List[dotdict]</code> <p>List of <code>msgflux.dotdict</code> instances to be distributed.</p> required <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[dotdict, ...]</code> <p>Tuple containing the messages updated with the responses.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>messages</code> is not a list of <code>dotdict</code>, <code>to_send</code> is not a list of callables, or <code>prefix</code> is not a string.</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@instrument(\"msg_scatter_gather\")\ndef msg_scatter_gather(\n    to_send: List[Callable],\n    messages: List[dotdict],\n    *,\n    timeout: Optional[float] = None,\n) -&gt; Tuple[dotdict, ...]:\n    \"\"\"Scatter a list of messages to a list of modules and gather the responses.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        messages:\n            List of `msgflux.dotdict` instances to be distributed.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n\n    Returns:\n        Tuple containing the messages updated with the responses.\n\n    Raises:\n        TypeError:\n            If `messages` is not a list of `dotdict`, `to_send` is not a list\n            of callables, or `prefix` is not a string.\n    \"\"\"\n    if not messages or not all(isinstance(msg, dotdict) for msg in messages):\n        raise TypeError(\n            \"`messages` must be a non-empty list of `msgflux.dotdict` instances\"\n        )\n\n    if not to_send or not all(isinstance(f, Callable) for f in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    if len(messages) != len(to_send):\n        raise ValueError(\n            f\"The size of `messages` ({len(messages)}) \"\n            f\"must be equal to that of `to_send`: ({len(to_send)})\"\n        )\n\n    executor = Executor.get_instance()\n    futures = [executor.submit(f, msg) for f, msg in zip(to_send, messages)]\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    for f, future in zip(to_send, futures):\n        f_name = get_callable_name(f)\n        try:\n            future.result()\n        except Exception as e:\n            logger.error(f\"Error in scattered task for `{f_name}`: {e}\")\n    return tuple(messages)\n</code></pre>"},{"location":"nn/functional/#msgflux.nn.functional.bcast_gather","title":"bcast_gather","text":"<pre><code>bcast_gather(to_send, *args, timeout=None, **kwargs)\n</code></pre> <p>Broadcasts arguments to multiple callables and gathers the responses.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>*args</code> <p>Positional arguments.</p> <code>()</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <code>**kwargs</code> <p>Named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Any, ...]</code> <p>Tuple containing the responses.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a list of callables.</p> <p>Examples:</p> <p>def square(x): return x * x def cube(x): return x * x * x def fail(x): raise ValueError(\"Intentional error\")</p>"},{"location":"nn/functional/#msgflux.nn.functional.bcast_gather--example-1","title":"Example 1:","text":"<p>results = F.bcast_gather([square, cube], 3) print(results)  # (9, 27)</p>"},{"location":"nn/functional/#msgflux.nn.functional.bcast_gather--example-2-simulate-error","title":"Example 2: Simulate error","text":"<p>results = F.bcast_gather([square, fail, cube], 2) print(results)  # (4, None, 8)</p>"},{"location":"nn/functional/#msgflux.nn.functional.bcast_gather--example-3-timeout","title":"Example 3: Timeout","text":"<p>results = F.bcast_gather([square, cube], 4, timeout=0.01) print(results) # (16, 64)</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@instrument(\"bcast_gather\")\ndef bcast_gather(\n    to_send: List[Callable], *args, timeout: Optional[float] = None, **kwargs\n) -&gt; Tuple[Any, ...]:\n    \"\"\"Broadcasts arguments to multiple callables and gathers the responses.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        *args:\n            Positional arguments.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n        **kwargs:\n            Named arguments.\n\n    Returns:\n        Tuple containing the responses.\n\n    Raises:\n        TypeError:\n            If `to_send` is not a list of callables.\n\n    Examples:\n        def square(x): return x * x\n        def cube(x): return x * x * x\n        def fail(x): raise ValueError(\"Intentional error\")\n\n        # Example 1:\n        results = F.bcast_gather([square, cube], 3)\n        print(results)  # (9, 27)\n\n        # Example 2: Simulate error\n        results = F.bcast_gather([square, fail, cube], 2)\n        print(results)  # (4, None, 8)\n\n        # Example 3: Timeout\n        results = F.bcast_gather([square, cube], 4, timeout=0.01)\n        print(results) # (16, 64)\n    \"\"\"\n    if not to_send or not all(isinstance(f, Callable) for f in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    executor = Executor.get_instance()\n    futures = [executor.submit(f, *args, **kwargs) for f in to_send]\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    responses: List[Any] = []\n    for future in futures:\n        try:\n            responses.append(future.result())\n        except Exception as e:\n            logger.error(str(e))\n            responses.append(None)\n    return tuple(responses)\n</code></pre>"},{"location":"nn/functional/#msgflux.nn.functional.msg_bcast_gather","title":"msg_bcast_gather","text":"<pre><code>msg_bcast_gather(to_send, message, *, timeout=None)\n</code></pre> <p>Broadcasts a single message to multiple modules and gathers the responses.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>message</code> <code>dotdict</code> <p>Instance of <code>msgflux.dotdict</code> to broadcast.</p> required <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>dotdict</code> <p>The original message with the module responses added.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>message</code> is not an instance of <code>dotdict</code>, <code>to_send</code> is not a list of callables.</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@instrument(\"msg_bcast_gather\")\ndef msg_bcast_gather(\n    to_send: List[Callable],\n    message: dotdict,\n    *,\n    timeout: Optional[float] = None,\n) -&gt; dotdict:\n    \"\"\"Broadcasts a single message to multiple modules and gathers the responses.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        message:\n            Instance of `msgflux.dotdict` to broadcast.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n\n    Returns:\n        The original message with the module responses added.\n\n    Raises:\n        TypeError:\n            If `message` is not an instance of `dotdict`, `to_send` is not a list\n            of callables.\n    \"\"\"\n    if not isinstance(message, dotdict):\n        raise TypeError(\"`message` must be an instance of `msgflux.dotdict`\")\n    if not to_send or not all(isinstance(module, Callable) for module in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    executor = Executor.get_instance()\n    futures = [executor.submit(f, message) for f in to_send]\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    for f, future in zip(to_send, futures):\n        f_name = get_callable_name(f)\n        try:\n            future.result()\n        except Exception as e:\n            logger.error(f\"Error in scattered task for `{f_name}`: {e}\")\n    return message\n</code></pre>"},{"location":"nn/functional/#msgflux.nn.functional.wait_for","title":"wait_for","text":"<pre><code>wait_for(to_send, *args, timeout=None, **kwargs)\n</code></pre> <p>Wait for a callable execution.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>Callable</code> <p>A callable object (e.g. functions or <code>Module</code> instances).</p> required <code>*args</code> <p>Positional arguments.</p> <code>()</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <code>**kwargs</code> <p>Named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Callable responses.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a callable.</p> <p>Examples:</p> <p>async def f1(x):     return x * x</p>"},{"location":"nn/functional/#msgflux.nn.functional.wait_for--example-1","title":"Example 1:","text":"<p>results = F.wait_for(f1, 3) print(results) # 9</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@instrument(\"wait_for\")\ndef wait_for(\n    to_send: Callable, *args, timeout: Optional[float] = None, **kwargs\n) -&gt; Any:\n    \"\"\"Wait for a callable execution.\n\n    Args:\n        to_send:\n            A callable object (e.g. functions or `Module` instances).\n        *args:\n            Positional arguments.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n        **kwargs:\n            Named arguments.\n\n    Returns:\n        Callable responses.\n\n    Raises:\n        TypeError:\n            If `to_send` is not a callable.\n\n    Examples:\n        async def f1(x):\n            return x * x\n\n        # Example 1:\n        results = F.wait_for(f1, 3)\n        print(results) # 9\n    \"\"\"\n    if not callable(to_send):\n        raise TypeError(\"`to_send` must be a callable object\")\n\n    executor = Executor.get_instance()\n    future = executor.submit(to_send, *args, **kwargs)\n    concurrent.futures.wait([future], timeout=timeout)\n    try:\n        return future.result()\n    except Exception as e:\n        logger.error(str(e))\n        return None\n</code></pre>"},{"location":"nn/functional/#msgflux.nn.functional.wait_for_event","title":"wait_for_event","text":"<pre><code>wait_for_event(event)\n</code></pre> <p>Waits synchronously for an asyncio.Event to be set.</p> <p>This function will block until event.set() is called elsewhere.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The asyncio.Event to wait for.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>event</code> is not an instance of asyncio.Event.</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@instrument(\"wait_for_event\")\ndef wait_for_event(event: asyncio.Event) -&gt; None:\n    \"\"\"Waits synchronously for an asyncio.Event to be set.\n\n    This function will block until event.set() is called elsewhere.\n\n    Args:\n        event: The asyncio.Event to wait for.\n\n    Raises:\n        TypeError: If `event` is not an instance of asyncio.Event.\n    \"\"\"\n    if not isinstance(event, asyncio.Event):\n        raise TypeError(\"`event` must be an instance of asyncio.Event\")\n\n    executor = Executor.get_instance()\n    future = executor._submit_to_async_worker(event.wait())\n    try:\n        future.result()\n    except Exception as e:\n        logger.error(str(e))\n</code></pre>"},{"location":"nn/functional/#msgflux.nn.functional.background_task","title":"background_task","text":"<pre><code>background_task(to_send, *args, **kwargs)\n</code></pre> <p>Executes a task in the background asynchronously without blocking, using the AsyncExecutorPool. This function is \"fire-and-forget\".</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>Callable</code> <p>Callable object (function, async function, or module with .acall() method).</p> required <code>*args</code> <p>Positional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Named arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a callable.</p> <p>Examples:</p>"},{"location":"nn/functional/#msgflux.nn.functional.background_task--example-1","title":"Example 1:","text":"<p>import time def print_message(message: str):     time.sleep(1)     print(f\"[Sync] Message: {message}\") F.background_task(print_message, \"Hello from sync function\")</p>"},{"location":"nn/functional/#msgflux.nn.functional.background_task--example-2","title":"Example 2:","text":"<p>import asyncio async def async_print_message(message: str):     await asyncio.sleep(1)     print(f\"[Async] Message: {message}\") F.background_task(async_print_message, \"Hello from async function\")</p>"},{"location":"nn/functional/#msgflux.nn.functional.background_task--example-3-with-error","title":"Example 3 (with error):","text":"<p>def failing_task():     raise ValueError(\"This task failed!\") F.background_task(failing_task)  # Error will be logged</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@instrument(\"background_task\")\ndef background_task(to_send: Callable, *args, **kwargs) -&gt; None:\n    \"\"\"Executes a task in the background asynchronously without blocking,\n    using the AsyncExecutorPool. This function is \"fire-and-forget\".\n\n    Args:\n        to_send:\n            Callable object (function, async function, or module with .acall() method).\n        *args:\n            Positional arguments.\n        **kwargs:\n            Named arguments.\n\n    Raises:\n        TypeError: If `to_send` is not a callable.\n\n    Examples:\n        # Example 1:\n        import time\n        def print_message(message: str):\n            time.sleep(1)\n            print(f\"[Sync] Message: {message}\")\n        F.background_task(print_message, \"Hello from sync function\")\n\n        # Example 2:\n        import asyncio\n        async def async_print_message(message: str):\n            await asyncio.sleep(1)\n            print(f\"[Async] Message: {message}\")\n        F.background_task(async_print_message, \"Hello from async function\")\n\n        # Example 3 (with error):\n        def failing_task():\n            raise ValueError(\"This task failed!\")\n        F.background_task(failing_task)  # Error will be logged\n    \"\"\"\n    if not callable(to_send):\n        raise TypeError(\"`to_send` must be a callable object\")\n\n    def log_future(future: Future) -&gt; None:\n        \"\"\"Callback to log exception of a Future.\"\"\"\n        try:\n            future.result()\n        except Exception as e:\n            logger.error(f\"Background task error: {e!s}\", exc_info=True)\n\n    executor = Executor.get_instance()\n    future = executor.submit(to_send, *args, **kwargs)\n    future.add_done_callback(log_future)\n</code></pre>"},{"location":"nn/agent/intro/","title":"Intro","text":"<p>import msgflux as mf import msgflux.nn as nn</p> <p>msg = mf.Message(     content=\"Describe the image and identify any sounds.\",     images={\"horse\": \"https://example.com/horse.jpg\"},     audios={\"dog\": \"https://example.com/dog.mp3\"} )</p> <p>fake_agent = nn.Agent(\"teste\", fake_model)</p> <p>fake_agent._set_task_inputs(\"content\") fake_agent._set_task_multimodal_inputs({     \"images\": [\"images.horse\"],     #\"audios\": [\"audios.dog\"] })</p> <p>fake_agent._set_task_template(\"The capital of {{country}} is {{city}}\") fake_agent._prepare_task({\"country\": \"France\", \"city\": \"Paris\"})</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/agents/","title":"Agents","text":""},{"location":"blog/category/rag/","title":"RAG","text":""},{"location":"blog/category/multimodal/","title":"MultiModal","text":""},{"location":"blog/category/search/","title":"Search","text":""}]}