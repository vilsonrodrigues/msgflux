{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":""},{"location":"#msgflux","title":"msgFlux","text":"<p>An open-source framework for building multimodal AI applications { .subtitle }</p> <p>          :material-rocket-launch: Get Started               :material-book-open: Documentation      </p> <pre><code>pip install msgflux\n</code></pre>"},{"location":"#core-principles","title":"Core Principles","text":"<ul> <li> <p> Privacy First</p> <p>msgFlux does not collect or transmit user data. All telemetry is fully controlled by the user and remains local, ensuring data sovereignty and compliance.</p> </li> <li> <p> Designed for Simplicity</p> <p>Core building blocks\u2014Model, DataBase, Parser, and Retriever\u2014provide a unified and intuitive interface to interact with diverse AI resources.</p> </li> <li> <p> Powered by Efficiency</p> <p>Leverages high-performance libraries like Msgspec, Uvloop, Jinja, and Ray for fast, scalable, and concurrent applications.</p> </li> <li> <p> Practical</p> <p>Workflow API inspired by <code>torch.nn</code>, enabling seamless composition with native Python. Advanced versioning and reproducibility out of the box.</p> </li> </ul>"},{"location":"#high-level-modules","title":"High-Level Modules","text":"<p>msgFlux introduces a set of high-level modules designed to streamline multimodal inputs and outputs. These modules encapsulate common AI pipeline tasks:</p> <ul> <li> <p> Agent</p> <p>Orchestrates multimodal data, instructions, context, tools, and generation schemas. The cognitive core of complex workflows.</p> </li> <li> <p> Speaker</p> <p>Converts text into natural-sounding speech, enabling voice-based interactions.</p> </li> <li> <p>:material-text-to-speech:{ .lg } Transcriber</p> <p>Transforms spoken language into text, supporting speech-to-text pipelines.</p> </li> <li> <p> Designer</p> <p>Generates visual content from prompts and images, combining textual and visual modalities.</p> </li> <li> <p> Retriever</p> <p>Searches and extracts relevant information based on queries, ideal for grounding models in external knowledge.</p> </li> <li> <p> Predictor</p> <p>Wraps predictive models (e.g., scikit-learn) for smooth integration into larger workflows.</p> </li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"Chat CompletionText EmbeddingsText-to-SpeechNeural Network Module <pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4\")\n\nresponse = model(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(response.consume())\n</code></pre> <pre><code>import msgflux as mf\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\nembeddings = embedder(\n    texts=[\"Hello world\", \"msgFlux is awesome\"]\n)\n\nprint(embeddings.shape)\n</code></pre> <pre><code>import msgflux as mf\n\ntts = mf.Model.text_to_speech(\"openai/tts-1\")\n\naudio = tts(\n    text=\"Hello from msgFlux!\",\n    voice=\"alloy\"\n)\n\naudio.save(\"output.mp3\")\n</code></pre> <pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4\")\n\nagent = mf.nn.Agent(\n    name=\"helpful_assistant\",\n    model=model,\n    instructions=\"You are a helpful assistant\",\n    tools=[search_tool, calculator_tool]\n)\n\nresult = agent(\"What's the weather in Paris?\")\nprint(result)\n</code></pre>"},{"location":"#why-msgflux","title":"Why msgFlux?","text":""},{"location":"#unified-interface","title":"Unified Interface","text":"<p>Work with text, vision, speech, and more through a single, consistent API. No need to learn different SDKs for each provider.</p>"},{"location":"#provider-agnostic","title":"Provider Agnostic","text":"<p>Easily switch between OpenAI, Anthropic, Google, Mistral, and more without changing your code structure.</p>"},{"location":"#production-ready","title":"Production Ready","text":"<p>Built-in support for async operations, retries, error handling, and observability. Deploy with confidence.</p>"},{"location":"#ready-to-build","title":"Ready to Build?","text":"<p>Get Started with msgFlux Explore Examples View on GitHub </p>"},{"location":"dependency-management/","title":"Models","text":""},{"location":"dependency-management/#chat-completion","title":"Chat Completion","text":"Provider Dependency Ollama <code>msgflux[openai]</code> OpenAI <code>msgflux[openai]</code> OpenRouter <code>msgflux[openai]</code> SambaNova <code>msgflux[openai]</code> Together <code>msgflux[openai]</code> vLLM <code>msgflux[openai]</code>"},{"location":"dependency-management/#image-classifier","title":"Image Classifier","text":"Provider Dependency JinaAI <code>msgflux[httpx]</code>"},{"location":"dependency-management/#image-embedder","title":"Image Embedder","text":"Provider Dependency JinaAI <code>msgflux[httpx]</code>"},{"location":"dependency-management/#image-text-to-image","title":"Image Text To Image","text":"Provider Dependency OpenAI <code>msgflux[openai]</code>"},{"location":"dependency-management/#moderation","title":"Moderation","text":"Provider Dependency OpenAI <code>msgflux[openai]</code>"},{"location":"dependency-management/#speech-to-text","title":"Speech To Text","text":"Provider Dependency OpenAI <code>msgflux[openai]</code> vLLM <code>msgflux[openai]</code>"},{"location":"dependency-management/#text-classifier","title":"Text Classifier","text":"Provider Dependency JinaAI <code>msgflux[httpx]</code> vLLM <code>msgflux[openai]</code>"},{"location":"dependency-management/#text-embedder","title":"Text Embedder","text":"Provider Dependency JinaAI <code>msgflux[httpx]</code> Ollama <code>msgflux[openai]</code> OpenAI <code>msgflux[openai]</code> Together <code>msgflux[openai]</code> vLLM <code>msgflux[openai]</code>"},{"location":"dependency-management/#text-reranker","title":"Text Reranker","text":"Provider Dependency JinaAI <code>msgflux[httpx]</code> vLLM <code>msgflux[openai]</code>"},{"location":"dependency-management/#text-to-image","title":"Text To Image","text":"Provider Dependency OpenAI <code>msgflux[openai]</code>"},{"location":"dependency-management/#text-to-speech","title":"Text To Speech","text":"Provider Dependency OpenAI <code>msgflux[openai]</code> Together <code>msgflux[openai]</code>"},{"location":"quickstart/","title":"Quickstart: PIX --- [Voice, Text]","text":"<p>This example demonstrates how to create a simple PIX transaction workflow that can handle both text and audio inputs.</p>"},{"location":"quickstart/#setup","title":"Setup","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\n# Set up API key (use environment variables in production)\nfrom google.colab import userdata\napi_key = userdata.get(\"OPENAI_API_KEY\")\nmf.set_envs(OPENAI_API_KEY=api_key)\n\n# Create models\nchat_model = mf.Model.chat_completion(\"openai/gpt-4o-mini\")\nstt_model = mf.Model.speech_to_text(\"openai/whisper-1\")\n</code></pre>"},{"location":"quickstart/#define-pix-workflow","title":"Define PIX Workflow","text":"<pre><code># Define signature for PIX extraction\nsignature = \"\"\"text -&gt; amount: float, key_type: Literal['cpf', 'cnpj', 'email', 'phone_number', 'name'], key_id: str\"\"\"\n\nclass PIX(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Transcriber agent - converts audio to text\n        self.transcriber = nn.Agent(\n            name=\"transcriber\",\n            model=stt_model,\n            message_fields={\n                \"task_multimodal_inputs\": {\"audio\": \"user_audio\"}\n            },\n            config={\n                \"return_content_only\": True\n            }\n        )\n\n        # Extractor agent - extracts PIX information\n        self.extractor = nn.Agent(\n            name=\"extractor\",\n            model=chat_model,\n            signature=signature,\n            message_fields={\n                \"task_inputs\": \"content\",\n                \"task_multimodal_inputs\": {\"image\": \"user_image\"}\n            },\n            config={\n                \"return_extraction_only\": True\n            }\n        )\n\n        # Define workflow: if audio exists, transcribe first, then extract\n        self.components = nn.ModuleDict({\n            \"transcriber\": self.transcriber,\n            \"extractor\": self.extractor\n        })\n\n        self.register_buffer(\"flux\", \"{user_audio is not None? transcriber} -&gt; extractor\")\n\n    def forward(self, msg):\n        return mf.inline(self.flux, self.components, msg)\n\n# Create PIX instance\npix = PIX()\n</code></pre>"},{"location":"quickstart/#usage-examples","title":"Usage Examples","text":""},{"location":"quickstart/#text-input","title":"Text Input","text":"<pre><code># Create message with text content\n# en: \"Send 22.40 to 123.456.789-00\"\n# cpf: Brazilian personal ID (CPF)\nmsg = mf.Message(content=\"Envie 22,40 para 123.456.789-00\")\n\n# Process through PIX workflow\nresult = pix(msg)\n\n# View extracted information\nprint(result.response)\n# {\n#     'amount': 22.40,\n#     'key_type': 'cpf',\n#     'key_id': '123.456.789-00'\n# }\n</code></pre>"},{"location":"quickstart/#audio-input","title":"Audio Input","text":"<pre><code># Create message with audio file\nmsg = mf.Message()\nmsg.set(\"user_audio\", \"audio-pix-direct-pt-br.ogg\")\n\n# Process: transcribe audio -&gt; extract PIX info\nresult = pix(msg)\n\n# Transcription is stored in content\nprint(result.content)  # Transcribed text\n\n# Extraction result\nprint(result.response)\n# {\n#     'amount': 23.50,\n#     'key_type': 'phone_number',\n#     'key_id': '84999242111'\n# }\n</code></pre>"},{"location":"quickstart/#text-image-input","title":"Text + Image Input","text":"<pre><code># Create message with text and image\nmsg = mf.Message(\n    content=\"Pague o valor para o destinat\u00e1rio na imagem\",\n    user_image=\"pix_qrcode.png\"\n)\n\n# Extractor can use both text instruction and image content\nresult = pix(msg)\n\nprint(result.response)\n</code></pre>"},{"location":"quickstart/#how-it-works","title":"How It Works","text":"<ol> <li>Conditional Flow: The workflow checks if <code>user_audio</code> exists</li> <li>If audio exists: <code>transcriber</code> \u2192 <code>extractor</code></li> <li> <p>If no audio: <code>extractor</code> only</p> </li> <li> <p>Transcriber Agent:</p> </li> <li>Takes audio from <code>msg.user_audio</code></li> <li>Uses speech-to-text model</li> <li> <p>Outputs transcribed text to <code>msg.content</code></p> </li> <li> <p>Extractor Agent:</p> </li> <li>Takes text from <code>msg.content</code></li> <li>Optionally takes image from <code>msg.user_image</code></li> <li>Uses signature to extract structured PIX data</li> <li>Returns: amount, key_type, and key_id</li> </ol>"},{"location":"quickstart/#inspecting-the-module","title":"Inspecting the Module","text":"<pre><code># View module structure\nprint(pix)\n\n# View state dictionary\nstate = pix.state_dict()\nprint(state)\n\n# Save module configuration\nmf.save(state, \"pix_workflow.json\")\n\n# Load later\nloaded_state = mf.load(\"pix_workflow.json\")\npix_loaded = PIX()\npix_loaded.load_state_dict(loaded_state)\n</code></pre>"},{"location":"_includes/init_chat_completion_model/","title":"Init chat completion model","text":"<p>Init Model (<code>Check Dependencies</code>)</p> OpenAIOpenRouterSambaNovavLLMMore \u25bc <p>Configure your access key <code>OPENAI_API_KEY</code> <pre><code>mf.set_envs(OPENAI_API_KEY=\"&lt;&gt;\")\nmodel = mf.Model.chat_completion(\"openai/gpt-4.1-nano\", temperature=0.7\")\n</code></pre></p> <p>Configure your access key <code>OPENROUTER_API_KEY</code> <pre><code>mf.set_envs(OPENROUTER_API_KEY=\"&lt;&gt;\")\nmodel = mf.Model.chat_completion(\"openrouter/deepseek/deepseek-r1-distill-qwen-7b\")\n</code></pre></p> <p>Configure your access key <code>SAMBANOVA_API_KEY</code> <pre><code>mf.set_envs(SAMBANOVA_API_KEY=\"&lt;&gt;\")\nmodel = mf.Model.chat_completion(\"Llama-4-Maverick-17B-128E-Instruct\")\n</code></pre></p> <p>Install vLLM (optional) <pre><code>pip install uv # for fast package install\nuv pip install vllm --torch-backend=auto\n</code></pre> Start vLLM server <pre><code>vllm serve Qwen/Qwen2.5-1.5B-Instruct \n</code></pre> Configure your URL <code>VLLM_BASE_URL</code> <pre><code>base_url = \"http://localhost:8000/v1\"\n# mf.set_envs(VLLM_BASE_URL=base_url) # or pass as a `base_url` param\nmodel = mf.Model.chat_completion(\"Qwen/Qwen2.5-1.5B-Instruct\", base_url=base_url)\n</code></pre></p> <p>Configure your access key <code>OPENROUTER_API_KEY</code> <pre><code>mf.set_envs(OPENROUTER_API_KEY=\"&lt;&gt;\")\nmodel = mf.Model.chat_completion(\"openrouter/deepseek/deepseek-r1-distill-qwen-7b\")\n</code></pre></p> OI <p>Configure your access key <code>OPENROUTER_API_KEY</code> <pre><code>mf.set_envs(OPENROUTER_API_KEY=\"&lt;&gt;\")\nmodel = mf.Model.chat_completion(\"openrouter/deepseek/deepseek-r1-distill-qwen-7b\")\n</code></pre></p> SOM <p>Configure your access key <code>OPENROUTER_API_KEY</code> <pre><code>mf.set_envs(OPENROUTER_API_KEY=\"&lt;&gt;\")\nmodel = mf.Model.chat_completion(\"openrouter/deepseek/deepseek-r1-distill-qwen-7b\")\n</code></pre></p>"},{"location":"api-reference/databases/types/kv/","title":"KV","text":""},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.cachetools.CacheToolsKVDB","title":"CacheToolsKVDB","text":"<p>               Bases: <code>BaseKV</code>, <code>BaseDB</code>, <code>KVDB</code></p> <p>CacheTools Key-Value DB.</p> Source code in <code>src/msgflux/data/dbs/providers/cachetools.py</code> <pre><code>@register_db\nclass CacheToolsKVDB(BaseKV, BaseDB, KVDB):\n    \"\"\"CacheTools Key-Value DB.\"\"\"\n\n    provider = \"cachetools\"\n\n    def __init__(\n        self,\n        *,\n        ttl: Optional[int] = 3600,\n        maxsize: Optional[int] = 10000,\n        hash_key: Optional[bool] = True,\n    ):\n        \"\"\"Args:\n        ttl:\n            The time-to-live (TTL) for each cache entry in seconds.\n        maxsize:\n            The maximum number of items the cache can store.\n        hash_key:\n            Whether to hash the keys before storing them in the cache.\n        \"\"\"\n        if TTLCache is None:\n            raise ImportError(\n                \"`cachetools` client is not available. Install with \"\n                \"`pip install cachetools`\"\n            )\n        self.hash_key = hash_key\n        self.maxsize = maxsize\n        self.ttl = ttl\n        self._initialize()\n\n    def _initialize(self):\n        self.client = TTLCache(maxsize=self.maxsize, ttl=self.ttl)\n\n    def add(self, documents: Union[List[Dict[str, Any]], Dict[str, Any]]):\n        if not isinstance(documents, list):\n            documents = [documents]\n        for document in documents:\n            for key, value in document.items():\n                encoded_key = convert_str_to_hash(key) if self.hash_key else key\n                encoded_value = msgspec.msgpack.encode(value)\n                self.client[encoded_key] = encoded_value\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.cachetools.CacheToolsKVDB.hash_key","title":"hash_key  <code>instance-attribute</code>","text":"<pre><code>hash_key = hash_key\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.cachetools.CacheToolsKVDB.maxsize","title":"maxsize  <code>instance-attribute</code>","text":"<pre><code>maxsize = maxsize\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.cachetools.CacheToolsKVDB.provider","title":"provider  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>provider = 'cachetools'\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.cachetools.CacheToolsKVDB.ttl","title":"ttl  <code>instance-attribute</code>","text":"<pre><code>ttl = ttl\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.cachetools.CacheToolsKVDB.__init__","title":"__init__","text":"<pre><code>__init__(*, ttl=3600, maxsize=10000, hash_key=True)\n</code></pre> <p>ttl:     The time-to-live (TTL) for each cache entry in seconds. maxsize:     The maximum number of items the cache can store. hash_key:     Whether to hash the keys before storing them in the cache.</p> Source code in <code>src/msgflux/data/dbs/providers/cachetools.py</code> <pre><code>def __init__(\n    self,\n    *,\n    ttl: Optional[int] = 3600,\n    maxsize: Optional[int] = 10000,\n    hash_key: Optional[bool] = True,\n):\n    \"\"\"Args:\n    ttl:\n        The time-to-live (TTL) for each cache entry in seconds.\n    maxsize:\n        The maximum number of items the cache can store.\n    hash_key:\n        Whether to hash the keys before storing them in the cache.\n    \"\"\"\n    if TTLCache is None:\n        raise ImportError(\n            \"`cachetools` client is not available. Install with \"\n            \"`pip install cachetools`\"\n        )\n    self.hash_key = hash_key\n    self.maxsize = maxsize\n    self.ttl = ttl\n    self._initialize()\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.cachetools.CacheToolsKVDB.add","title":"add","text":"<pre><code>add(documents)\n</code></pre> Source code in <code>src/msgflux/data/dbs/providers/cachetools.py</code> <pre><code>def add(self, documents: Union[List[Dict[str, Any]], Dict[str, Any]]):\n    if not isinstance(documents, list):\n        documents = [documents]\n    for document in documents:\n        for key, value in document.items():\n            encoded_key = convert_str_to_hash(key) if self.hash_key else key\n            encoded_value = msgspec.msgpack.encode(value)\n            self.client[encoded_key] = encoded_value\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.diskcache.DiskCacheKVDB","title":"DiskCacheKVDB","text":"<p>               Bases: <code>BaseKV</code>, <code>BaseDB</code>, <code>KVDB</code></p> <p>DiskCache Key-Value DB.</p> Source code in <code>src/msgflux/data/dbs/providers/diskcache.py</code> <pre><code>@register_db\nclass DiskCacheKVDB(BaseKV, BaseDB, KVDB):\n    \"\"\"DiskCache Key-Value DB.\"\"\"\n\n    provider = \"diskcache\"\n\n    def __init__(self, *, ttl: Optional[int] = 3600, hash_key: Optional[bool] = True):\n        \"\"\"Args:\n        ttl:\n            The time-to-live (TTL) for each cache entry in seconds.\n        hash_key:\n            Whether to hash the keys before storing them in the cache.\n        \"\"\"\n        if Cache is None:\n            raise ImportError(\n                \"`diskcache` client is not available. Install with \"\n                \"`pip install diskcache`\"\n            )\n        self.hash_key = hash_key\n        self.ttl = ttl\n        self._initialize()\n\n    def _initialize(self):\n        self.client = Cache(timeout=1)\n\n    def add(self, documents: Union[List[Dict[str, Any]], Dict[str, Any]]):\n        if not isinstance(documents, list):\n            documents = [documents]\n        for document in documents:\n            for k, v in document.items():\n                encoded_k = convert_str_to_hash(k) if self.hash_key else k\n                encoded_v = msgspec.msgpack.encode(v)\n                self.client.set(encoded_k, encoded_v, expire=self.ttl)\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.diskcache.DiskCacheKVDB.hash_key","title":"hash_key  <code>instance-attribute</code>","text":"<pre><code>hash_key = hash_key\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.diskcache.DiskCacheKVDB.provider","title":"provider  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>provider = 'diskcache'\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.diskcache.DiskCacheKVDB.ttl","title":"ttl  <code>instance-attribute</code>","text":"<pre><code>ttl = ttl\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.diskcache.DiskCacheKVDB.__init__","title":"__init__","text":"<pre><code>__init__(*, ttl=3600, hash_key=True)\n</code></pre> <p>ttl:     The time-to-live (TTL) for each cache entry in seconds. hash_key:     Whether to hash the keys before storing them in the cache.</p> Source code in <code>src/msgflux/data/dbs/providers/diskcache.py</code> <pre><code>def __init__(self, *, ttl: Optional[int] = 3600, hash_key: Optional[bool] = True):\n    \"\"\"Args:\n    ttl:\n        The time-to-live (TTL) for each cache entry in seconds.\n    hash_key:\n        Whether to hash the keys before storing them in the cache.\n    \"\"\"\n    if Cache is None:\n        raise ImportError(\n            \"`diskcache` client is not available. Install with \"\n            \"`pip install diskcache`\"\n        )\n    self.hash_key = hash_key\n    self.ttl = ttl\n    self._initialize()\n</code></pre>"},{"location":"api-reference/databases/types/kv/#src.msgflux.data.dbs.providers.diskcache.DiskCacheKVDB.add","title":"add","text":"<pre><code>add(documents)\n</code></pre> Source code in <code>src/msgflux/data/dbs/providers/diskcache.py</code> <pre><code>def add(self, documents: Union[List[Dict[str, Any]], Dict[str, Any]]):\n    if not isinstance(documents, list):\n        documents = [documents]\n    for document in documents:\n        for k, v in document.items():\n            encoded_k = convert_str_to_hash(k) if self.hash_key else k\n            encoded_v = msgspec.msgpack.encode(v)\n            self.client.set(encoded_k, encoded_v, expire=self.ttl)\n</code></pre>"},{"location":"api-reference/models/model/","title":"Model","text":""},{"location":"api-reference/models/model/#msgflux.models.model.Model","title":"Model","text":"Source code in <code>src/msgflux/models/model.py</code> <pre><code>class Model:\n    @classmethod\n    def providers(cls):\n        return {k: list(v.keys()) for k, v in model_registry.items()}\n\n    @classmethod\n    def model_types(cls):\n        return list(model_registry.keys())\n\n    @classmethod\n    def _model_path_parser(cls, model_id: str) -&gt; tuple[str, str]:\n        provider, model_id = model_id.split(\"/\", 1)\n        return provider, model_id\n\n    @classmethod\n    def _get_model_class(cls, model_type: str, provider: str) -&gt; Type[BaseModel]:\n        if model_type not in model_registry:\n            raise ValueError(f\"Model type `{model_type}` is not supported\")\n        if provider not in model_registry[model_type]:\n            raise ValueError(\n                f\"Provider `{provider}` not registered for type `{model_type}`\"\n            )\n        model_cls = model_registry[model_type][provider]\n        return model_cls\n\n    @classmethod\n    def _create_model(\n        cls, model_type: str, model_path: str, **kwargs\n    ) -&gt; Type[BaseModel]:\n        provider, model_id = cls._model_path_parser(model_path)\n        model_cls = cls._get_model_class(model_type, provider)\n        return model_cls(model_id=model_id, **kwargs)\n\n    @classmethod\n    def from_serialized(\n        cls, provider: str, model_type: str, state: Mapping[str, Any]\n    ) -&gt; Type[BaseModel]:\n        \"\"\"Creates a model instance from serialized parameters.\n\n        Args:\n            provider:\n                The model provider (e.g., \"openai\", \"google\").\n            model_type:\n                The type of model (e.g., \"chat_completion\", \"text_embedder\").\n            state:\n                Dictionary containing the serialized model parameters.\n\n        Returns:\n            An instance of the appropriate model class with restored state\n        \"\"\"\n        model_cls = cls._get_model_class(model_type, provider)\n        # Create instance without calling __init__\n        instance = object.__new__(model_cls)\n        # Restore the instance state\n        instance.from_serialized(state)\n        return instance\n\n    @classmethod\n    def chat_completion(cls, model_path: str, **kwargs) -&gt; ChatCompletionModel:\n        return cls._create_model(\"chat_completion\", model_path, **kwargs)\n\n    @classmethod\n    def image_classifier(cls, model_path: str, **kwargs) -&gt; ImageClassifierModel:\n        return cls._create_model(\"image_classifier\", model_path, **kwargs)\n\n    @classmethod\n    def image_embedder(cls, model_path: str, **kwargs) -&gt; ImageEmbedderModel:\n        return cls._create_model(\"image_embedder\", model_path, **kwargs)\n\n    @classmethod\n    def image_text_to_image(cls, model_path: str, **kwargs) -&gt; ImageTextToImageModel:\n        return cls._create_model(\"image_text_to_image\", model_path, **kwargs)\n\n    @classmethod\n    def moderation(cls, model_path: str, **kwargs) -&gt; ModerationModel:\n        return cls._create_model(\"moderation\", model_path, **kwargs)\n\n    @classmethod\n    def speech_to_text(cls, model_path: str, **kwargs) -&gt; SpeechToTextModel:\n        return cls._create_model(\"speech_to_text\", model_path, **kwargs)\n\n    @classmethod\n    def text_classifier(cls, model_path: str, **kwargs) -&gt; TextClassifierModel:\n        return cls._create_model(\"text_classifier\", model_path, **kwargs)\n\n    @classmethod\n    def text_embedder(cls, model_path: str, **kwargs) -&gt; TextEmbedderModel:\n        return cls._create_model(\"text_embedder\", model_path, **kwargs)\n\n    @classmethod\n    def text_reranker(cls, model_path: str, **kwargs) -&gt; TextRerankerModel:\n        return cls._create_model(\"text_reranker\", model_path, **kwargs)\n\n    @classmethod\n    def text_to_image(cls, model_path: str, **kwargs) -&gt; TextToImageModel:\n        return cls._create_model(\"text_to_image\", model_path, **kwargs)\n\n    @classmethod\n    def text_to_speech(cls, model_path: str, **kwargs) -&gt; TextToSpeechModel:\n        return cls._create_model(\"text_to_speech\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.chat_completion","title":"chat_completion  <code>classmethod</code>","text":"<pre><code>chat_completion(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef chat_completion(cls, model_path: str, **kwargs) -&gt; ChatCompletionModel:\n    return cls._create_model(\"chat_completion\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.from_serialized","title":"from_serialized  <code>classmethod</code>","text":"<pre><code>from_serialized(provider, model_type, state)\n</code></pre> <p>Creates a model instance from serialized parameters.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>The model provider (e.g., \"openai\", \"google\").</p> required <code>model_type</code> <code>str</code> <p>The type of model (e.g., \"chat_completion\", \"text_embedder\").</p> required <code>state</code> <code>Mapping[str, Any]</code> <p>Dictionary containing the serialized model parameters.</p> required <p>Returns:</p> Type Description <code>Type[BaseModel]</code> <p>An instance of the appropriate model class with restored state</p> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef from_serialized(\n    cls, provider: str, model_type: str, state: Mapping[str, Any]\n) -&gt; Type[BaseModel]:\n    \"\"\"Creates a model instance from serialized parameters.\n\n    Args:\n        provider:\n            The model provider (e.g., \"openai\", \"google\").\n        model_type:\n            The type of model (e.g., \"chat_completion\", \"text_embedder\").\n        state:\n            Dictionary containing the serialized model parameters.\n\n    Returns:\n        An instance of the appropriate model class with restored state\n    \"\"\"\n    model_cls = cls._get_model_class(model_type, provider)\n    # Create instance without calling __init__\n    instance = object.__new__(model_cls)\n    # Restore the instance state\n    instance.from_serialized(state)\n    return instance\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.image_classifier","title":"image_classifier  <code>classmethod</code>","text":"<pre><code>image_classifier(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef image_classifier(cls, model_path: str, **kwargs) -&gt; ImageClassifierModel:\n    return cls._create_model(\"image_classifier\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.image_embedder","title":"image_embedder  <code>classmethod</code>","text":"<pre><code>image_embedder(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef image_embedder(cls, model_path: str, **kwargs) -&gt; ImageEmbedderModel:\n    return cls._create_model(\"image_embedder\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.image_text_to_image","title":"image_text_to_image  <code>classmethod</code>","text":"<pre><code>image_text_to_image(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef image_text_to_image(cls, model_path: str, **kwargs) -&gt; ImageTextToImageModel:\n    return cls._create_model(\"image_text_to_image\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.model_types","title":"model_types  <code>classmethod</code>","text":"<pre><code>model_types()\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef model_types(cls):\n    return list(model_registry.keys())\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.moderation","title":"moderation  <code>classmethod</code>","text":"<pre><code>moderation(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef moderation(cls, model_path: str, **kwargs) -&gt; ModerationModel:\n    return cls._create_model(\"moderation\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.providers","title":"providers  <code>classmethod</code>","text":"<pre><code>providers()\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef providers(cls):\n    return {k: list(v.keys()) for k, v in model_registry.items()}\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.speech_to_text","title":"speech_to_text  <code>classmethod</code>","text":"<pre><code>speech_to_text(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef speech_to_text(cls, model_path: str, **kwargs) -&gt; SpeechToTextModel:\n    return cls._create_model(\"speech_to_text\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.text_classifier","title":"text_classifier  <code>classmethod</code>","text":"<pre><code>text_classifier(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef text_classifier(cls, model_path: str, **kwargs) -&gt; TextClassifierModel:\n    return cls._create_model(\"text_classifier\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.text_embedder","title":"text_embedder  <code>classmethod</code>","text":"<pre><code>text_embedder(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef text_embedder(cls, model_path: str, **kwargs) -&gt; TextEmbedderModel:\n    return cls._create_model(\"text_embedder\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.text_reranker","title":"text_reranker  <code>classmethod</code>","text":"<pre><code>text_reranker(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef text_reranker(cls, model_path: str, **kwargs) -&gt; TextRerankerModel:\n    return cls._create_model(\"text_reranker\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.text_to_image","title":"text_to_image  <code>classmethod</code>","text":"<pre><code>text_to_image(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef text_to_image(cls, model_path: str, **kwargs) -&gt; TextToImageModel:\n    return cls._create_model(\"text_to_image\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model/#msgflux.models.model.Model.text_to_speech","title":"text_to_speech  <code>classmethod</code>","text":"<pre><code>text_to_speech(model_path, **kwargs)\n</code></pre> Source code in <code>src/msgflux/models/model.py</code> <pre><code>@classmethod\ndef text_to_speech(cls, model_path: str, **kwargs) -&gt; TextToSpeechModel:\n    return cls._create_model(\"text_to_speech\", model_path, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model_gateway/","title":"Model gateway","text":""},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway","title":"ModelGateway","text":"<p>Routes calls to a list of supported AI models, with fallback, retries, initial model selection, and timing constraints (configured via HH:MM strings).</p> Source code in <code>src/msgflux/models/gateway.py</code> <pre><code>class ModelGateway:\n    \"\"\"Routes calls to a list of supported AI models, with fallback, retries,\n    initial model selection, and timing constraints (configured via HH:MM strings).\n    \"\"\"\n\n    msgflux_type = \"model_gateway\"\n    model_types = None\n\n    def __init__(\n        self,\n        models: List[BaseModel],\n        time_constraints: Optional[Dict[str, List[Tuple[str, str]]]] = None,\n    ):\n        \"\"\"Args:\n            models:\n                A list of BaseModel instances (at least 2).\n            time_constraints:\n                An optional dictionary mapping model_id to a list of string\n                tuples (start_time, end_time). The listed models will NOT be\n                used if the current time is within any of the specified ranges.\n                Strings must be in the format \"HH:MM\" (e.g. \"22:00\", \"06:00\").\n\n                !!! example:\n\n                    {'model-A': [('22:00', '06:00')]}\n                    prohibits 'model-A' between 22:00 and 06:00.\n\n        Raises:\n            ModelRouterError:\n                Raised when all models fail or are restricted.\n            ValueError:\n                Raised for misconfiguration in time formats or duplicate model IDs.\n            TypeError:\n                Raised for invalid argument types.\n        \"\"\"\n        self._model_id_to_index: Dict[str, int] = {}\n        self.raw_time_constraints = time_constraints\n        self._set_models(models)\n\n        try:\n            self.parsed_time_constraints = (\n                self._parse_time_constraints(time_constraints)\n                if time_constraints\n                else {}\n            )\n        except ValueError as e:\n            logger.error(f\"Error to parse time_constraints: {e}\")\n            raise ValueError(f\"Invalid format in time_constraints: {e}\") from e\n\n        # Validates if the model_ids in time_constraints exist\n        # (uses the keys from the parsed dict)\n        for model_id in self.parsed_time_constraints:\n            if model_id not in self._model_id_to_index:\n                logger.warning(\n                    f\"The model_id `{model_id}` in time constraints \"\n                    \"not found in the provided models\"\n                )\n\n        self.current_model_index = 0\n        logger.debug(\n            f\"ModelGateway initialized with {len(self.models)} models. Type: \"\n            f\"`{self.model_type}`\"\n        )\n        if self.parsed_time_constraints:\n            logger.debug(\n                \"Time constraints applied to models: \"\n                f\"{list(self.parsed_time_constraints.keys())}\"\n            )\n\n    def _parse_time_constraints(\n        self, constraints: Optional[Dict[str, List[Tuple[str, str]]]] = None\n    ) -&gt; Dict[str, List[Tuple[time, time]]]:\n        \"\"\"Validates and converts \"HH:MM\" time strings into datetime.time objects.\n\n        Raises:\n            ValueError: If a time string is in an invalid format.\n            TypeError: If the constraint data structure is incorrect.\n        \"\"\"\n        if constraints is None:\n            return {}\n\n        parsed_constraints: Dict[str, List[Tuple[time, time]]] = {}\n        time_format = \"%H:%M\"\n\n        for model_id, intervals in constraints.items():\n            if not isinstance(intervals, list):\n                raise TypeError(\n                    f\"Constraints for `{model_id}` must be a list of tuples \"\n                    f\"(start, end). Given: `{type(intervals)}`\"\n                )\n            parsed_intervals = []\n            for i, interval in enumerate(intervals):\n                if (\n                    not isinstance(interval, (tuple, list)) or len(interval) != 2\n                ):  # Tuples or lists\n                    raise TypeError(\n                        f\"Interval #{i + 1} for `{model_id}` must be a tuple/list \"\n                        \"of two strings (start_time_str, end_time_str). Given: \"\n                        f\"`{interval}`\"\n                    )\n\n                start, end = interval\n                if not isinstance(start, str) or not isinstance(end, str):\n                    raise TypeError(\n                        f\"Start and end times in range #{i + 1} for `{model_id}` \"\n                        f\"must be strings. Given: `({type(start)}, {type(end)})`\"\n                    )\n\n                try:\n                    start_dt = datetime.strptime(start, time_format).replace(\n                        tzinfo=timezone.utc\n                    )\n                    end_dt = datetime.strptime(end, time_format).replace(\n                        tzinfo=timezone.utc\n                    )\n                    start_t = start_dt.time()\n                    end_t = end_dt.time()\n                    parsed_intervals.append((start_t, end_t))\n                except ValueError as e:\n                    raise ValueError(\n                        f\"Invalid time format in range #{i + 1} for `{model_id}`. \"\n                        f\"Use 'HH:MM'. Error parsing `{start}` or `{end}`: {e}\"\n                    ) from e\n\n            parsed_constraints[model_id] = parsed_intervals\n        return parsed_constraints\n\n    def _is_time_restricted(self, model_id: str) -&gt; bool:\n        \"\"\"Checks if the model is constrained at the current time\n        using the parsed constraints.\n        \"\"\"\n        # Access constraints already converted to `time`\n        if model_id not in self.parsed_time_constraints:\n            return False\n\n        now = datetime.now(tz=timezone.utc).time()\n\n        for start_time, end_time in self.parsed_time_constraints[model_id]:\n            if start_time &lt;= end_time:\n                if start_time &lt;= now &lt;= end_time:\n                    logger.debug(\n                        f\"Model `{model_id}` restricted. Current time `{now}` \"\n                        f\"is between `{start_time}` and `{end_time}`\"\n                    )\n                    return True\n            elif now &gt;= start_time or now &lt;= end_time:\n                logger.debug(\n                    f\"Restricted model `{model_id}`. Current time `{now}` is \"\n                    f\"in the range crosses midnight: `{start_time} - {end_time}`\"\n                )\n                return True\n        return False\n\n    def _set_models(self, models: List[BaseModel]):\n        if not models or not isinstance(models, list):\n            raise TypeError(\n                \"`models` must be a non-empty list of `BaseModel` instances\"\n            )\n\n        if not all(isinstance(model, BaseModel) for model in models):\n            raise TypeError(\"`models` requires inheriting from `BaseModel`\")\n\n        if len(models) &lt; 2:\n            logger.warning(\n                f\"`models` has only {len(models)} models. \"\n                \"Fallback will not be effective\"\n            )\n\n        model_types = set()\n        model_ids = set()\n        for i, model in enumerate(models):\n            if not hasattr(model, \"model_type\") or not model.model_type:\n                raise AttributeError(\n                    f\"Model in {i} position does not have a valid \"\n                    \"`model_type` attribute\"\n                )\n            if not hasattr(model, \"model_id\") or not model.model_id:\n                raise AttributeError(\n                    f\"Model in {i} position  does not have a valid `model_id` attribute\"\n                )\n            if not hasattr(model, \"provider\"):\n                raise AttributeError(\n                    f\"Model `{model.model_id}` does not have a valid \"\n                    \"`provider` attribute\"\n                )\n\n            model_types.add(model.model_type)\n            if model.model_id in model_ids:\n                raise ValueError(\n                    f\"Duplicate model ID found: `{model.model_id}`. IDs must be unique\"\n                )\n            model_ids.add(model.model_id)\n            self._model_id_to_index[model.model_id] = i\n\n        if len(model_types) &gt; 1:\n            raise TypeError(\n                \"All models in `models` must be of the same `model_type`. \"\n                f\"Given: `{model_types}`\"\n            )\n\n        self.models = models\n        self.model_type = next(iter(model_types))\n\n        # Determine if gateway supports batch processing\n        # Only True if ALL models support batch\n        self.batch_support = (\n            all(getattr(model, \"batch_support\", False) for model in models)\n            if models\n            else False\n        )\n\n    def _execute_model(\n        self, model_preference: Optional[str] = None, **kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Attempts to execute the call on the configured models, respecting\n        time constraints and failure limits.\n        \"\"\"\n        if not self.models:\n            raise ModelRouterError([], [], message=\"No model configured on gateway\")\n\n        available_models = [\n            model\n            for model in self.models\n            if not self._is_time_restricted(model.model_id)\n        ]\n\n        if not available_models:\n            raise ModelRouterError(\n                [], [], message=\"No model available due to time constraints\"\n            )\n\n        if model_preference:\n            preferred_model = next(\n                (m for m in available_models if m.model_id == model_preference), None\n            )\n            if preferred_model:\n                available_models = [preferred_model] + [\n                    m for m in available_models if m != preferred_model\n                ]\n\n        failures = []\n\n        for model in available_models:\n            try:\n                response = model(**kwargs)\n                return response\n            except Exception as e:\n                logger.debug(\n                    f\"\"\"Model `{model.model_id}` ({model.provider})\n                    failed to execute: {e}\"\"\",\n                    exc_info=False,\n                )\n                failures.append((model.model_id, model.provider, e))\n\n        error_message = f\"All {len(available_models)} available models failed\"\n        logger.error(error_message)\n        raise ModelRouterError(\n            [failure[2] for failure in failures], failures, message=error_message\n        )\n\n    async def _aexecute_model(\n        self, model_preference: Optional[str] = None, **kwargs: Any\n    ) -&gt; Any:\n        \"\"\"Async version of _execute_model. Attempts to execute the call on the\n        configured models, respecting time constraints and failure limits.\n        \"\"\"\n        if not self.models:\n            raise ModelRouterError([], [], message=\"No model configured on gateway\")\n\n        available_models = [\n            model\n            for model in self.models\n            if not self._is_time_restricted(model.model_id)\n        ]\n\n        if not available_models:\n            raise ModelRouterError(\n                [], [], message=\"No model available due to time constraints\"\n            )\n\n        if model_preference:\n            preferred_model = next(\n                (m for m in available_models if m.model_id == model_preference), None\n            )\n            if preferred_model:\n                available_models = [preferred_model] + [\n                    m for m in available_models if m != preferred_model\n                ]\n\n        failures = []\n\n        for model in available_models:\n            try:\n                response = await model.acall(**kwargs)\n                return response\n            except Exception as e:\n                logger.debug(\n                    f\"\"\"Model `{model.model_id}` ({model.provider})\n                    failed to execute: {e}\"\"\",\n                    exc_info=False,\n                )\n                failures.append((model.model_id, model.provider, e))\n\n        error_message = f\"All {len(available_models)} available models failed\"\n        logger.error(error_message)\n        raise ModelRouterError(\n            [failure[2] for failure in failures], failures, message=error_message\n        )\n\n    def __call__(\n        self, *, model_preference: Optional[str] = None, **kwargs\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Executes the call on the gateway.\n\n        Args:\n            model_preference:\n                The ID of the model that should be tried first.\n                If None, starts from the last model used or the first one.\n            kwargs:\n                Arguments to pass to the __call__ method of the selected model.\n\n        Returns:\n            The response of the first model that executes successfully.\n\n        Raises:\n            ModelRouterError:\n                If all models fail consecutively up to the `max_retries`\n                limit, or if no models are available/functional.\n        \"\"\"\n        return self._execute_model(model_preference=model_preference, **kwargs)\n\n    async def acall(\n        self, *, model_preference: Optional[str] = None, **kwargs\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Async version of __call__. Executes the call on the gateway.\n\n        Args:\n            model_preference:\n                The ID of the model that should be tried first.\n                If None, starts from the last model used or the first one.\n            kwargs:\n                Arguments to pass to the acall method of the selected model.\n\n        Returns:\n            The response of the first model that executes successfully.\n\n        Raises:\n            ModelRouterError:\n                If all models fail consecutively up to the `max_retries`\n                limit, or if no models are available/functional.\n        \"\"\"\n        return await self._aexecute_model(model_preference=model_preference, **kwargs)\n\n    def serialize(self) -&gt; Dict[str, Any]:\n        \"\"\"Serializes the gateway state including time constraints as strings.\"\"\"\n        serialized_models = [model.serialize() for model in self.models]\n        state = {\n            \"time_constraints\": self.raw_time_constraints,\n            \"models\": serialized_models,\n        }\n        data = {\"msgflux_type\": self.msgflux_type, \"state\": state}\n        return data\n\n    @classmethod\n    def from_serialized(cls, data: Dict[str, Any]) -&gt; \"ModelGateway\":\n        \"\"\"Creates a ModelGateway instance from serialized data.\n\n        Args:\n            data: The dictionary of serialized models.\n        \"\"\"\n        if data.get(\"msgflux_type\") != cls.msgflux_type:\n            raise ValueError(\n                f\"Incorrect msgflux type. Expected `{cls.msgflux_type}`, \"\n                f\"given `{data.get('msgflux_type')}`\"\n            )\n\n        state = data.get(\"state\", {})\n        serialized_models = state.get(\"models\", [])\n        if not serialized_models:\n            raise ValueError(\"Serialized data does not contain templates\")\n\n        models = [Model.from_serialized(**m_data) for m_data in serialized_models]\n        time_constraints = state.get(\"time_constraints\")\n\n        return cls(models=models, time_constraints=time_constraints)\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.current_model_index","title":"current_model_index  <code>instance-attribute</code>","text":"<pre><code>current_model_index = 0\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.model_types","title":"model_types  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_types = None\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.msgflux_type","title":"msgflux_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>msgflux_type = 'model_gateway'\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.parsed_time_constraints","title":"parsed_time_constraints  <code>instance-attribute</code>","text":"<pre><code>parsed_time_constraints = (\n    _parse_time_constraints(time_constraints)\n    if time_constraints\n    else {}\n)\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.raw_time_constraints","title":"raw_time_constraints  <code>instance-attribute</code>","text":"<pre><code>raw_time_constraints = time_constraints\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.__call__","title":"__call__","text":"<pre><code>__call__(*, model_preference=None, **kwargs)\n</code></pre> <p>Executes the call on the gateway.</p> <p>Parameters:</p> Name Type Description Default <code>model_preference</code> <code>Optional[str]</code> <p>The ID of the model that should be tried first. If None, starts from the last model used or the first one.</p> <code>None</code> <code>kwargs</code> <p>Arguments to pass to the call method of the selected model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[ModelResponse, ModelStreamResponse]</code> <p>The response of the first model that executes successfully.</p> <p>Raises:</p> Type Description <code>ModelRouterError</code> <p>If all models fail consecutively up to the <code>max_retries</code> limit, or if no models are available/functional.</p> Source code in <code>src/msgflux/models/gateway.py</code> <pre><code>def __call__(\n    self, *, model_preference: Optional[str] = None, **kwargs\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Executes the call on the gateway.\n\n    Args:\n        model_preference:\n            The ID of the model that should be tried first.\n            If None, starts from the last model used or the first one.\n        kwargs:\n            Arguments to pass to the __call__ method of the selected model.\n\n    Returns:\n        The response of the first model that executes successfully.\n\n    Raises:\n        ModelRouterError:\n            If all models fail consecutively up to the `max_retries`\n            limit, or if no models are available/functional.\n    \"\"\"\n    return self._execute_model(model_preference=model_preference, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.__init__","title":"__init__","text":"<pre><code>__init__(models, time_constraints=None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>List[BaseModel]</code> <p>A list of BaseModel instances (at least 2).</p> required <code>time_constraints</code> <code>Optional[Dict[str, List[Tuple[str, str]]]]</code> <p>An optional dictionary mapping model_id to a list of string tuples (start_time, end_time). The listed models will NOT be used if the current time is within any of the specified ranges. Strings must be in the format \"HH:MM\" (e.g. \"22:00\", \"06:00\").</p> <p>!!! example:</p> <pre><code>{'model-A': [('22:00', '06:00')]}\nprohibits 'model-A' between 22:00 and 06:00.\n</code></pre> <code>None</code> <p>Raises:</p> Type Description <code>ModelRouterError</code> <p>Raised when all models fail or are restricted.</p> <code>ValueError</code> <p>Raised for misconfiguration in time formats or duplicate model IDs.</p> <code>TypeError</code> <p>Raised for invalid argument types.</p> Source code in <code>src/msgflux/models/gateway.py</code> <pre><code>def __init__(\n    self,\n    models: List[BaseModel],\n    time_constraints: Optional[Dict[str, List[Tuple[str, str]]]] = None,\n):\n    \"\"\"Args:\n        models:\n            A list of BaseModel instances (at least 2).\n        time_constraints:\n            An optional dictionary mapping model_id to a list of string\n            tuples (start_time, end_time). The listed models will NOT be\n            used if the current time is within any of the specified ranges.\n            Strings must be in the format \"HH:MM\" (e.g. \"22:00\", \"06:00\").\n\n            !!! example:\n\n                {'model-A': [('22:00', '06:00')]}\n                prohibits 'model-A' between 22:00 and 06:00.\n\n    Raises:\n        ModelRouterError:\n            Raised when all models fail or are restricted.\n        ValueError:\n            Raised for misconfiguration in time formats or duplicate model IDs.\n        TypeError:\n            Raised for invalid argument types.\n    \"\"\"\n    self._model_id_to_index: Dict[str, int] = {}\n    self.raw_time_constraints = time_constraints\n    self._set_models(models)\n\n    try:\n        self.parsed_time_constraints = (\n            self._parse_time_constraints(time_constraints)\n            if time_constraints\n            else {}\n        )\n    except ValueError as e:\n        logger.error(f\"Error to parse time_constraints: {e}\")\n        raise ValueError(f\"Invalid format in time_constraints: {e}\") from e\n\n    # Validates if the model_ids in time_constraints exist\n    # (uses the keys from the parsed dict)\n    for model_id in self.parsed_time_constraints:\n        if model_id not in self._model_id_to_index:\n            logger.warning(\n                f\"The model_id `{model_id}` in time constraints \"\n                \"not found in the provided models\"\n            )\n\n    self.current_model_index = 0\n    logger.debug(\n        f\"ModelGateway initialized with {len(self.models)} models. Type: \"\n        f\"`{self.model_type}`\"\n    )\n    if self.parsed_time_constraints:\n        logger.debug(\n            \"Time constraints applied to models: \"\n            f\"{list(self.parsed_time_constraints.keys())}\"\n        )\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(*, model_preference=None, **kwargs)\n</code></pre> <p>Async version of call. Executes the call on the gateway.</p> <p>Parameters:</p> Name Type Description Default <code>model_preference</code> <code>Optional[str]</code> <p>The ID of the model that should be tried first. If None, starts from the last model used or the first one.</p> <code>None</code> <code>kwargs</code> <p>Arguments to pass to the acall method of the selected model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[ModelResponse, ModelStreamResponse]</code> <p>The response of the first model that executes successfully.</p> <p>Raises:</p> Type Description <code>ModelRouterError</code> <p>If all models fail consecutively up to the <code>max_retries</code> limit, or if no models are available/functional.</p> Source code in <code>src/msgflux/models/gateway.py</code> <pre><code>async def acall(\n    self, *, model_preference: Optional[str] = None, **kwargs\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Async version of __call__. Executes the call on the gateway.\n\n    Args:\n        model_preference:\n            The ID of the model that should be tried first.\n            If None, starts from the last model used or the first one.\n        kwargs:\n            Arguments to pass to the acall method of the selected model.\n\n    Returns:\n        The response of the first model that executes successfully.\n\n    Raises:\n        ModelRouterError:\n            If all models fail consecutively up to the `max_retries`\n            limit, or if no models are available/functional.\n    \"\"\"\n    return await self._aexecute_model(model_preference=model_preference, **kwargs)\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.from_serialized","title":"from_serialized  <code>classmethod</code>","text":"<pre><code>from_serialized(data)\n</code></pre> <p>Creates a ModelGateway instance from serialized data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>The dictionary of serialized models.</p> required Source code in <code>src/msgflux/models/gateway.py</code> <pre><code>@classmethod\ndef from_serialized(cls, data: Dict[str, Any]) -&gt; \"ModelGateway\":\n    \"\"\"Creates a ModelGateway instance from serialized data.\n\n    Args:\n        data: The dictionary of serialized models.\n    \"\"\"\n    if data.get(\"msgflux_type\") != cls.msgflux_type:\n        raise ValueError(\n            f\"Incorrect msgflux type. Expected `{cls.msgflux_type}`, \"\n            f\"given `{data.get('msgflux_type')}`\"\n        )\n\n    state = data.get(\"state\", {})\n    serialized_models = state.get(\"models\", [])\n    if not serialized_models:\n        raise ValueError(\"Serialized data does not contain templates\")\n\n    models = [Model.from_serialized(**m_data) for m_data in serialized_models]\n    time_constraints = state.get(\"time_constraints\")\n\n    return cls(models=models, time_constraints=time_constraints)\n</code></pre>"},{"location":"api-reference/models/model_gateway/#msgflux.models.gateway.ModelGateway.serialize","title":"serialize","text":"<pre><code>serialize()\n</code></pre> <p>Serializes the gateway state including time constraints as strings.</p> Source code in <code>src/msgflux/models/gateway.py</code> <pre><code>def serialize(self) -&gt; Dict[str, Any]:\n    \"\"\"Serializes the gateway state including time constraints as strings.\"\"\"\n    serialized_models = [model.serialize() for model in self.models]\n    state = {\n        \"time_constraints\": self.raw_time_constraints,\n        \"models\": serialized_models,\n    }\n    data = {\"msgflux_type\": self.msgflux_type, \"state\": state}\n    return data\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/","title":"ChatCompletion","text":""},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion","title":"OpenAIChatCompletion","text":"<p>               Bases: <code>_BaseOpenAI</code>, <code>ChatCompletionModel</code></p> <p>OpenAI Chat Completion.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAIChatCompletion(_BaseOpenAI, ChatCompletionModel):\n    \"\"\"OpenAI Chat Completion.\"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        *,\n        max_tokens: Optional[int] = None,\n        reasoning_effort: Optional[str] = None,\n        enable_thinking: Optional[bool] = None,\n        return_reasoning: Optional[bool] = False,\n        reasoning_in_tool_call: Optional[bool] = True,\n        validate_typed_parser_output: Optional[bool] = False,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        stop: Optional[Union[str, List[str]]] = None,\n        parallel_tool_calls: Optional[bool] = True,\n        modalities: Optional[List[str]] = None,\n        audio: Optional[Dict[str, str]] = None,\n        verbosity: Optional[str] = None,\n        web_search_options: Optional[Dict[str, Any]] = None,\n        verbose: Optional[bool] = False,\n        base_url: Optional[str] = None,\n        context_length: Optional[int] = None,\n        reasoning_max_tokens: Optional[int] = None,\n        enable_cache: Optional[bool] = False,\n        cache_size: Optional[int] = 128,\n    ):\n        \"\"\"Args:\n        model_id:\n            Model ID in provider.\n        max_tokens:\n            An upper bound for the number of tokens that can be\n            generated for a completion, including visible output\n            tokens and reasoning tokens.\n        reasoning_effort:\n            Constrains effort on reasoning for reasoning models.\n            Currently supported values are low, medium, and high.\n            Reducing reasoning effort can result in faster responses\n            and fewer tokens used on reasoning in a response.\n            Can be: \"minimal\", \"low\", \"medium\" or \"high\".\n        enable_thinking:\n            If True, enable the model reasoning.\n        return_reasoning:\n            If the model returns the `reasoning` field it will be added\n            along with the response.\n        reasoning_in_tool_call:\n            If True, maintains the reasoning for using the tool call.\n        validate_typed_parser_output:\n            If True, use the generation_schema to validate typed parser output.\n        temperature:\n            What sampling temperature to use, between 0 and 2.\n            Higher values like 0.8 will make the output more random,\n            while lower values like 0.2 will make it more focused and\n            deterministic.\n        stop:\n            Up to 4 sequences where the API will stop generating further\n            tokens. The returned text will not contain the stop sequence.\n        top_p:\n            An alternative to sampling with temperature, called nucleus\n            sampling, where the model considers the results of the tokens\n            with top_p probability mass. So 0.1 means only the tokens\n            comprising the top 10% probability mass are considered.\n        parallel_tool_calls:\n            If True, enable parallel tool calls.\n        modalities:\n            Types of output you would like the model to generate.\n            Can be: [\"text\"], [\"audio\"] or [\"text\", \"audio\"].\n        audio:\n            Audio configurations. Define voice and output format.\n        verbosity:\n            Constrains the verbosity of the model's response. Lower\n            values will result in more concise responses, while higher\n            values will result in more verbose responses. Currently\n            supported values are low, medium, and high.\n        web_search_options:\n            This tool searches the web for relevant results to use in a response.\n            OpenAI and OpenRouter only.\n        verbose:\n            If True, Prints the model output to the console before it is transformed\n            into typed structured output.\n        base_url:\n            URL to model provider.\n        context_length:\n            The maximum context length supported by the model.\n        reasoning_max_tokens:\n            Maximum number of tokens for reasoning/thinking.\n        enable_cache:\n            If True, enable response caching to avoid redundant API calls.\n        cache_size:\n            Maximum number of cached responses (default: 128).\n        \"\"\"\n        super().__init__()\n        self.model_id = model_id\n        self.context_length = context_length\n        self.reasoning_max_tokens = reasoning_max_tokens\n        self.enable_cache = enable_cache\n        self.cache_size = cache_size\n        self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n        sampling_run_params = {\"max_tokens\": max_tokens}\n        if temperature:\n            sampling_run_params[\"temperature\"] = temperature\n        if top_p:\n            sampling_run_params[\"top_p\"] = top_p\n        if stop:\n            sampling_run_params[\"stop\"] = stop\n        if verbosity:\n            sampling_run_params[\"verbosity\"] = verbosity\n        if modalities:\n            sampling_run_params[\"modalities\"] = modalities\n        if web_search_options:\n            sampling_run_params[\"web_search_options\"] = web_search_options\n        if audio:\n            sampling_run_params[\"audio\"] = audio\n        if reasoning_effort:\n            sampling_run_params[\"reasoning_effort\"] = reasoning_effort\n        self.sampling_run_params = sampling_run_params\n        self.enable_thinking = enable_thinking\n        self.parallel_tool_calls = parallel_tool_calls\n        self.reasoning_in_tool_call = reasoning_in_tool_call\n        self.validate_typed_parser_output = validate_typed_parser_output\n        self.return_reasoning = return_reasoning\n        self.verbose = verbose\n        self._initialize()\n        self._get_api_key()\n\n    def _adapt_params(self, params: Dict[str, Any]) -&gt; Dict[str, Any]:\n        if self.provider in \"openai\":\n            params[\"max_completion_tokens\"] = params.pop(\"max_tokens\")\n        return params\n\n    def _execute_model(self, **kwargs):\n        prefilling = kwargs.pop(\"prefilling\")\n        if prefilling:\n            kwargs.get(\"messages\").append({\"role\": \"assistant\", \"content\": prefilling})\n        params = {**kwargs, **self.sampling_run_params}\n        adapted_params = self._adapt_params(params)\n        model_output = self.client.chat.completions.create(**adapted_params)\n\n        return model_output\n\n    async def _aexecute_model(self, **kwargs):\n        prefilling = kwargs.pop(\"prefilling\")\n        if prefilling:\n            kwargs.get(\"messages\").append({\"role\": \"assistant\", \"content\": prefilling})\n        params = {**kwargs, **self.sampling_run_params}\n        adapted_params = self._adapt_params(params)\n        model_output = await self.aclient.chat.completions.create(**adapted_params)\n\n        return model_output\n\n    def _process_model_output(  # noqa: C901\n        self, model_output, typed_parser=None, generation_schema=None\n    ):\n        \"\"\"Shared logic to process model output for both sync and async.\"\"\"\n        response = ModelResponse()\n        metadata = dotdict()\n\n        metadata.update({\"usage\": model_output.usage.to_dict()})\n\n        choice = model_output.choices[0]\n\n        reasoning = (\n            getattr(choice.message, \"reasoning_content\", None)\n            or getattr(choice.message, \"reasoning\", None)\n            or getattr(choice.message, \"thinking\", None)\n        )\n\n        reasoning_tool_call = None\n        if self.reasoning_in_tool_call is True:\n            reasoning_tool_call = reasoning\n\n        prefix_response_type = \"\"\n        reasoning_content = None\n        if self.return_reasoning is True:\n            reasoning_content = reasoning\n            if reasoning_content is not None:\n                prefix_response_type = \"reasoning_\"\n\n        if choice.message.annotations:  # Extra responses (e.g web search references)\n            annotations_content = [\n                item.model_dump() for item in choice.message.annotations\n            ]\n            metadata.annotations = annotations_content\n\n        if choice.message.tool_calls:\n            aggregator = ToolCallAggregator(reasoning_tool_call)\n            response.set_response_type(\"tool_call\")\n            for call_index, tool_call in enumerate(choice.message.tool_calls):\n                tool_id = tool_call.id\n                name = tool_call.function.name\n                arguments = tool_call.function.arguments\n                aggregator.process(call_index, tool_id, name, arguments)\n            response_content = aggregator\n        elif choice.message.content:\n            if (typed_parser or generation_schema) and self.verbose:\n                repr_str = f\"[{self.model_id}][raw_response] {choice.message.content}\"\n                cprint(repr_str, lc=\"r\", ls=\"b\")\n            if typed_parser is not None:\n                response.set_response_type(f\"{prefix_response_type}structured\")\n                parser = typed_parser_registry[typed_parser]\n                response_content = dotdict(parser.decode(choice.message.content))\n                # Type validation\n                if generation_schema and self.validate_typed_parser_output:\n                    encoded_response_content = msgspec.json.encode(response_content)\n                    msgspec.json.decode(\n                        encoded_response_content, type=generation_schema\n                    )\n            elif generation_schema is not None:\n                response.set_response_type(f\"{prefix_response_type}structured\")\n                struct = msgspec.json.decode(\n                    choice.message.content, type=generation_schema\n                )\n                response_content = dotdict(struct_to_dict(struct))\n            else:\n                response.set_response_type(f\"{prefix_response_type}text_generation\")\n                if reasoning_content is not None:\n                    response_content = dotdict({\"answer\": choice.message.content})\n                else:\n                    response_content = choice.message.content\n        elif choice.message.audio:\n            response_content = dotdict(\n                {\n                    \"id\": choice.message.audio.id,\n                    \"audio\": base64.b64decode(choice.message.audio.data),\n                }\n            )\n            if choice.message.audio.transcript:\n                response.set_response_type(\"audio_text_generation\")\n                response_content.text = choice.message.audio.transcript\n            else:\n                response.set_response_type(\"audio_generation\")\n\n        if reasoning_content is not None:\n            response_content.think = reasoning_content\n\n        response.add(response_content)\n        response.set_metadata(metadata)\n        return response\n\n    def _generate(self, **kwargs: Mapping[str, Any]) -&gt; ModelResponse:\n        typed_parser = kwargs.get(\"typed_parser\")\n        generation_schema = kwargs.get(\"generation_schema\")\n\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        # Pop after cache check to avoid modifying kwargs during cache key generation\n        typed_parser = kwargs.pop(\"typed_parser\")\n        generation_schema = kwargs.pop(\"generation_schema\")\n\n        if generation_schema is not None and typed_parser is None:\n            response_format = response_format_from_msgspec_struct(generation_schema)\n            kwargs[\"response_format\"] = response_format\n\n        model_output = self._execute_model(**kwargs)\n        response = self._process_model_output(\n            model_output, typed_parser, generation_schema\n        )\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            # Re-add popped values for cache key\n            cache_kwargs = {\n                **kwargs,\n                \"typed_parser\": typed_parser,\n                \"generation_schema\": generation_schema,\n            }\n            cache_key = generate_cache_key(**cache_kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    async def _agenerate(self, **kwargs: Mapping[str, Any]) -&gt; ModelResponse:\n        typed_parser = kwargs.get(\"typed_parser\")\n        generation_schema = kwargs.get(\"generation_schema\")\n\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        # Pop after cache check to avoid modifying kwargs during cache key generation\n        typed_parser = kwargs.pop(\"typed_parser\")\n        generation_schema = kwargs.pop(\"generation_schema\")\n\n        if generation_schema is not None and typed_parser is None:\n            response_format = response_format_from_msgspec_struct(generation_schema)\n            kwargs[\"response_format\"] = response_format\n\n        model_output = await self._aexecute_model(**kwargs)\n        response = self._process_model_output(\n            model_output, typed_parser, generation_schema\n        )\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            # Re-add popped values for cache key\n            cache_kwargs = {\n                **kwargs,\n                \"typed_parser\": typed_parser,\n                \"generation_schema\": generation_schema,\n            }\n            cache_key = generate_cache_key(**cache_kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    async def _stream_generate(  # noqa: C901\n        self, **kwargs: Mapping[str, Any]\n    ) -&gt; ModelStreamResponse:\n        aggregator = ToolCallAggregator()\n        metadata = dotdict()\n\n        stream_response = kwargs.pop(\"stream_response\")\n        model_output = self._execute_model(**kwargs)\n\n        reasoning_tool_call = \"\"\n\n        for chunk in model_output:\n            if chunk.choices:\n                delta = chunk.choices[0].delta\n\n                reasoning_chunk = (\n                    getattr(delta, \"reasoning_content\", None)\n                    or getattr(delta, \"reasoning\", None)\n                    or getattr(delta, \"thinking\", None)\n                )\n\n                if self.reasoning_in_tool_call and reasoning_chunk:\n                    reasoning_tool_call += reasoning_chunk\n\n                if self.return_reasoning and reasoning_chunk:\n                    if stream_response.response_type is None:\n                        stream_response.set_response_type(\"reasoning_text_generation\")\n                        stream_response.first_chunk_event.set()\n                    stream_response.add(reasoning_chunk)\n                    continue\n\n                if getattr(delta, \"content\", None):\n                    if stream_response.response_type is None:\n                        stream_response.set_response_type(\"text_generation\")\n                        stream_response.first_chunk_event.set()\n                    stream_response.add(delta.content)\n                    continue\n\n                if getattr(delta, \"tool_calls\", None):\n                    if stream_response.response_type is None:\n                        stream_response.set_response_type(\"tool_call\")\n                    tool_call = delta.tool_calls[0]\n                    call_index = tool_call.index\n                    tool_id = tool_call.id\n                    name = tool_call.function.name\n                    arguments = tool_call.function.arguments\n                    aggregator.process(call_index, tool_id, name, arguments)\n                    continue\n\n                if hasattr(delta, \"annotations\") and delta.annotations is not None:\n                    metadata.annotations = [\n                        item.model_dump() for item in delta.annotations\n                    ]\n                    continue\n\n            elif chunk.usage:\n                metadata.update(chunk.usage.to_dict())\n\n        if aggregator.tool_calls:\n            if reasoning_tool_call:\n                aggregator.reasoning = reasoning_tool_call\n            stream_response.data = aggregator  # For tool calls save as 'data'\n            stream_response.first_chunk_event.set()\n\n        stream_response.set_metadata(metadata)\n        stream_response.add(None)\n\n    async def _astream_generate(  # noqa: C901\n        self, **kwargs: Mapping[str, Any]\n    ) -&gt; ModelStreamResponse:\n        aggregator = ToolCallAggregator()\n        metadata = dotdict()\n\n        stream_response = kwargs.pop(\"stream_response\")\n        model_output = await self._aexecute_model(**kwargs)\n\n        reasoning_tool_call = \"\"\n\n        async for chunk in model_output:\n            if chunk.choices:\n                delta = chunk.choices[0].delta\n\n                reasoning_chunk = (\n                    getattr(delta, \"reasoning_content\", None)\n                    or getattr(delta, \"reasoning\", None)\n                    or getattr(delta, \"thinking\", None)\n                )\n\n                if self.reasoning_in_tool_call and reasoning_chunk:\n                    reasoning_tool_call += reasoning_chunk\n\n                if self.return_reasoning and reasoning_chunk:\n                    if stream_response.response_type is None:\n                        stream_response.set_response_type(\"reasoning_text_generation\")\n                        stream_response.first_chunk_event.set()\n                    stream_response.add(reasoning_chunk)\n                    continue\n\n                if getattr(delta, \"content\", None):\n                    if stream_response.response_type is None:\n                        stream_response.set_response_type(\"text_generation\")\n                        stream_response.first_chunk_event.set()\n                    stream_response.add(delta.content)\n                    continue\n\n                if getattr(delta, \"tool_calls\", None):\n                    if stream_response.response_type is None:\n                        stream_response.set_response_type(\"tool_call\")\n                    tool_call = delta.tool_calls[0]\n                    call_index = tool_call.index\n                    tool_id = tool_call.id\n                    name = tool_call.function.name\n                    arguments = tool_call.function.arguments\n                    aggregator.process(call_index, tool_id, name, arguments)\n                    continue\n\n                if hasattr(delta, \"annotations\") and delta.annotations is not None:\n                    metadata.annotations = [\n                        item.model_dump() for item in delta.annotations\n                    ]\n                    continue\n\n            elif chunk.usage:\n                metadata.update(chunk.usage.to_dict())\n\n        if aggregator.tool_calls:\n            if reasoning_tool_call:\n                aggregator.reasoning = reasoning_tool_call\n            stream_response.data = aggregator  # For tool calls save as 'data'\n            stream_response.first_chunk_event.set()\n\n        stream_response.set_metadata(metadata)\n        stream_response.add(None)\n\n    @model_retry\n    def __call__(\n        self,\n        messages: Union[str, List[Dict[str, Any]]],\n        *,\n        system_prompt: Optional[str] = None,\n        prefilling: Optional[str] = None,\n        stream: Optional[bool] = False,\n        generation_schema: Optional[msgspec.Struct] = None,\n        tool_schemas: Optional[Dict] = None,\n        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n        typed_parser: Optional[str] = None,\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Args:\n            messages:\n                Conversation history. Can be simple string or list of messages.\n            system_prompt:\n                A set of instructions that defines the overarching behavior\n                and role of the model across all interactions.\n            prefilling:\n                Forces an initial message from the model. From that message\n                it will continue its response from there.\n            stream:\n                Whether generation should be in streaming mode.\n            generation_schema:\n                Schema that defines how the output should be structured.\n            tool_schemas:\n                JSON schema containing available tools.\n            tool_choice:\n                By default the model will determine when and how many tools to use.\n                You can force specific behavior with the tool_choice parameter.\n                    1. auto:\n                        (Default) Call zero, one, or multiple functions.\n                    2. required:\n                        Call one or more functions.\n                    3. Forced Tool:\n                        Call exactly one specific tool e.g: \"get_weather\".\n            typed_parser:\n                Converts the model raw output into a typed-dict. Supported parser:\n                `typed_xml`.\n\n        Raises:\n            ValueError:\n                Raised if `generation_schema` and `stream=True`.\n            ValueError:\n                Raised if `typed_xml=True` and `stream=True`.\n        \"\"\"\n        if isinstance(messages, str):\n            messages = [ChatBlock.user(messages)]\n        if isinstance(system_prompt, str):\n            messages.insert(0, ChatBlock.system(system_prompt))\n\n        if isinstance(tool_choice, str):\n            if tool_choice not in [\"auto\", \"required\", \"none\"]:\n                tool_choice = {\n                    \"type\": \"function\",\n                    \"function\": {\"name\": tool_choice},\n                }\n\n        generation_params = {\n            \"messages\": messages,\n            \"prefilling\": prefilling,\n            \"tool_choice\": tool_choice,\n            \"tools\": tool_schemas,\n            \"model\": self.model_id,\n        }\n\n        if tool_schemas:\n            generation_params[\"parallel_tool_calls\"] = self.parallel_tool_calls\n\n        if stream is True:\n            if typed_parser is not None:\n                raise ValueError(\"`typed_parser` is not `stream=True` compatible\")\n\n            stream_response = ModelStreamResponse()\n            F.background_task(\n                self._stream_generate,\n                **generation_params,\n                stream=stream,\n                stream_response=stream_response,\n                stream_options={\"include_usage\": True},\n            )\n            F.wait_for_event(stream_response.first_chunk_event)\n            return stream_response\n        else:\n            if typed_parser and typed_parser not in typed_parser_registry:\n                available = \", \".join(typed_parser_registry.keys())\n                raise TypedParserNotFoundError(\n                    f\"Typed parser `{typed_parser}` not found. \"\n                    f\"Available parsers: {available}\"\n                )\n            response = self._generate(\n                **generation_params,\n                typed_parser=typed_parser,\n                generation_schema=generation_schema,\n            )\n            return response\n\n    @model_retry\n    async def acall(\n        self,\n        messages: Union[str, List[Dict[str, Any]]],\n        *,\n        system_prompt: Optional[str] = None,\n        prefilling: Optional[str] = None,\n        stream: Optional[bool] = False,\n        generation_schema: Optional[msgspec.Struct] = None,\n        tool_schemas: Optional[Dict] = None,\n        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n        typed_parser: Optional[str] = None,\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Async version of __call__. Args:\n            messages:\n                Conversation history. Can be simple string or list of messages.\n            system_prompt:\n                A set of instructions that defines the overarching behavior\n                and role of the model across all interactions.\n            prefilling:\n                Forces an initial message from the model. From that message\n                it will continue its response from there.\n            stream:\n                Whether generation should be in streaming mode.\n            generation_schema:\n                Schema that defines how the output should be structured.\n            tool_schemas:\n                JSON schema containing available tools.\n            tool_choice:\n                By default the model will determine when and how many tools to use.\n                You can force specific behavior with the tool_choice parameter.\n                    1. auto:\n                        (Default) Call zero, one, or multiple functions.\n                    2. required:\n                        Call one or more functions.\n                    3. Forced Tool:\n                        Call exactly one specific tool e.g: \"get_weather\".\n            typed_parser:\n                Converts the model raw output into a typed-dict. Supported parser:\n                `typed_xml`.\n\n        Raises:\n            ValueError:\n                Raised if `generation_schema` and `stream=True`.\n            ValueError:\n                Raised if `typed_xml=True` and `stream=True`.\n        \"\"\"\n        if isinstance(messages, str):\n            messages = [ChatBlock.user(messages)]\n        if isinstance(system_prompt, str):\n            messages.insert(0, ChatBlock.system(system_prompt))\n\n        if isinstance(tool_choice, str):\n            if tool_choice not in [\"auto\", \"required\", \"none\"]:\n                tool_choice = {\n                    \"type\": \"function\",\n                    \"function\": {\"name\": tool_choice},\n                }\n\n        generation_params = {\n            \"messages\": messages,\n            \"prefilling\": prefilling,\n            \"tool_choice\": tool_choice,\n            \"tools\": tool_schemas,\n            \"model\": self.model_id,\n        }\n\n        if tool_schemas:\n            generation_params[\"parallel_tool_calls\"] = self.parallel_tool_calls\n\n        if stream is True:\n            if typed_parser is not None:\n                raise ValueError(\"`typed_parser` is not `stream=True` compatible\")\n\n            stream_response = ModelStreamResponse()\n            await F.abackground_task(\n                self._astream_generate,\n                **generation_params,\n                stream=stream,\n                stream_response=stream_response,\n                stream_options={\"include_usage\": True},\n            )\n            await F.await_for_event(stream_response.first_chunk_event)\n            return stream_response\n        else:\n            if typed_parser and typed_parser not in typed_parser_registry:\n                available = \", \".join(typed_parser_registry.keys())\n                raise TypedParserNotFoundError(\n                    f\"Typed parser `{typed_parser}` not found. \"\n                    f\"Available parsers: {available}\"\n                )\n            response = await self._agenerate(\n                **generation_params,\n                typed_parser=typed_parser,\n                generation_schema=generation_schema,\n            )\n            return response\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.cache_size","title":"cache_size  <code>instance-attribute</code>","text":"<pre><code>cache_size = cache_size\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.context_length","title":"context_length  <code>instance-attribute</code>","text":"<pre><code>context_length = context_length\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.enable_cache","title":"enable_cache  <code>instance-attribute</code>","text":"<pre><code>enable_cache = enable_cache\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.enable_thinking","title":"enable_thinking  <code>instance-attribute</code>","text":"<pre><code>enable_thinking = enable_thinking\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.parallel_tool_calls","title":"parallel_tool_calls  <code>instance-attribute</code>","text":"<pre><code>parallel_tool_calls = parallel_tool_calls\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.reasoning_in_tool_call","title":"reasoning_in_tool_call  <code>instance-attribute</code>","text":"<pre><code>reasoning_in_tool_call = reasoning_in_tool_call\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.reasoning_max_tokens","title":"reasoning_max_tokens  <code>instance-attribute</code>","text":"<pre><code>reasoning_max_tokens = reasoning_max_tokens\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.return_reasoning","title":"return_reasoning  <code>instance-attribute</code>","text":"<pre><code>return_reasoning = return_reasoning\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.sampling_params","title":"sampling_params  <code>instance-attribute</code>","text":"<pre><code>sampling_params = {'base_url': base_url or _get_base_url()}\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.sampling_run_params","title":"sampling_run_params  <code>instance-attribute</code>","text":"<pre><code>sampling_run_params = sampling_run_params\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.validate_typed_parser_output","title":"validate_typed_parser_output  <code>instance-attribute</code>","text":"<pre><code>validate_typed_parser_output = validate_typed_parser_output\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.verbose","title":"verbose  <code>instance-attribute</code>","text":"<pre><code>verbose = verbose\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.__call__","title":"__call__","text":"<pre><code>__call__(\n    messages,\n    *,\n    system_prompt=None,\n    prefilling=None,\n    stream=False,\n    generation_schema=None,\n    tool_schemas=None,\n    tool_choice=None,\n    typed_parser=None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[str, List[Dict[str, Any]]]</code> <p>Conversation history. Can be simple string or list of messages.</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>A set of instructions that defines the overarching behavior and role of the model across all interactions.</p> <code>None</code> <code>prefilling</code> <code>Optional[str]</code> <p>Forces an initial message from the model. From that message it will continue its response from there.</p> <code>None</code> <code>stream</code> <code>Optional[bool]</code> <p>Whether generation should be in streaming mode.</p> <code>False</code> <code>generation_schema</code> <code>Optional[Struct]</code> <p>Schema that defines how the output should be structured.</p> <code>None</code> <code>tool_schemas</code> <code>Optional[Dict]</code> <p>JSON schema containing available tools.</p> <code>None</code> <code>tool_choice</code> <code>Optional[Union[str, Dict[str, Any]]]</code> <p>By default the model will determine when and how many tools to use. You can force specific behavior with the tool_choice parameter.     1. auto:         (Default) Call zero, one, or multiple functions.     2. required:         Call one or more functions.     3. Forced Tool:         Call exactly one specific tool e.g: \"get_weather\".</p> <code>None</code> <code>typed_parser</code> <code>Optional[str]</code> <p>Converts the model raw output into a typed-dict. Supported parser: <code>typed_xml</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised if <code>generation_schema</code> and <code>stream=True</code>.</p> <code>ValueError</code> <p>Raised if <code>typed_xml=True</code> and <code>stream=True</code>.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    messages: Union[str, List[Dict[str, Any]]],\n    *,\n    system_prompt: Optional[str] = None,\n    prefilling: Optional[str] = None,\n    stream: Optional[bool] = False,\n    generation_schema: Optional[msgspec.Struct] = None,\n    tool_schemas: Optional[Dict] = None,\n    tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n    typed_parser: Optional[str] = None,\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Args:\n        messages:\n            Conversation history. Can be simple string or list of messages.\n        system_prompt:\n            A set of instructions that defines the overarching behavior\n            and role of the model across all interactions.\n        prefilling:\n            Forces an initial message from the model. From that message\n            it will continue its response from there.\n        stream:\n            Whether generation should be in streaming mode.\n        generation_schema:\n            Schema that defines how the output should be structured.\n        tool_schemas:\n            JSON schema containing available tools.\n        tool_choice:\n            By default the model will determine when and how many tools to use.\n            You can force specific behavior with the tool_choice parameter.\n                1. auto:\n                    (Default) Call zero, one, or multiple functions.\n                2. required:\n                    Call one or more functions.\n                3. Forced Tool:\n                    Call exactly one specific tool e.g: \"get_weather\".\n        typed_parser:\n            Converts the model raw output into a typed-dict. Supported parser:\n            `typed_xml`.\n\n    Raises:\n        ValueError:\n            Raised if `generation_schema` and `stream=True`.\n        ValueError:\n            Raised if `typed_xml=True` and `stream=True`.\n    \"\"\"\n    if isinstance(messages, str):\n        messages = [ChatBlock.user(messages)]\n    if isinstance(system_prompt, str):\n        messages.insert(0, ChatBlock.system(system_prompt))\n\n    if isinstance(tool_choice, str):\n        if tool_choice not in [\"auto\", \"required\", \"none\"]:\n            tool_choice = {\n                \"type\": \"function\",\n                \"function\": {\"name\": tool_choice},\n            }\n\n    generation_params = {\n        \"messages\": messages,\n        \"prefilling\": prefilling,\n        \"tool_choice\": tool_choice,\n        \"tools\": tool_schemas,\n        \"model\": self.model_id,\n    }\n\n    if tool_schemas:\n        generation_params[\"parallel_tool_calls\"] = self.parallel_tool_calls\n\n    if stream is True:\n        if typed_parser is not None:\n            raise ValueError(\"`typed_parser` is not `stream=True` compatible\")\n\n        stream_response = ModelStreamResponse()\n        F.background_task(\n            self._stream_generate,\n            **generation_params,\n            stream=stream,\n            stream_response=stream_response,\n            stream_options={\"include_usage\": True},\n        )\n        F.wait_for_event(stream_response.first_chunk_event)\n        return stream_response\n    else:\n        if typed_parser and typed_parser not in typed_parser_registry:\n            available = \", \".join(typed_parser_registry.keys())\n            raise TypedParserNotFoundError(\n                f\"Typed parser `{typed_parser}` not found. \"\n                f\"Available parsers: {available}\"\n            )\n        response = self._generate(\n            **generation_params,\n            typed_parser=typed_parser,\n            generation_schema=generation_schema,\n        )\n        return response\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.__init__","title":"__init__","text":"<pre><code>__init__(\n    model_id,\n    *,\n    max_tokens=None,\n    reasoning_effort=None,\n    enable_thinking=None,\n    return_reasoning=False,\n    reasoning_in_tool_call=True,\n    validate_typed_parser_output=False,\n    temperature=None,\n    top_p=None,\n    stop=None,\n    parallel_tool_calls=True,\n    modalities=None,\n    audio=None,\n    verbosity=None,\n    web_search_options=None,\n    verbose=False,\n    base_url=None,\n    context_length=None,\n    reasoning_max_tokens=None,\n    enable_cache=False,\n    cache_size=128,\n)\n</code></pre> <p>model_id:     Model ID in provider. max_tokens:     An upper bound for the number of tokens that can be     generated for a completion, including visible output     tokens and reasoning tokens. reasoning_effort:     Constrains effort on reasoning for reasoning models.     Currently supported values are low, medium, and high.     Reducing reasoning effort can result in faster responses     and fewer tokens used on reasoning in a response.     Can be: \"minimal\", \"low\", \"medium\" or \"high\". enable_thinking:     If True, enable the model reasoning. return_reasoning:     If the model returns the <code>reasoning</code> field it will be added     along with the response. reasoning_in_tool_call:     If True, maintains the reasoning for using the tool call. validate_typed_parser_output:     If True, use the generation_schema to validate typed parser output. temperature:     What sampling temperature to use, between 0 and 2.     Higher values like 0.8 will make the output more random,     while lower values like 0.2 will make it more focused and     deterministic. stop:     Up to 4 sequences where the API will stop generating further     tokens. The returned text will not contain the stop sequence. top_p:     An alternative to sampling with temperature, called nucleus     sampling, where the model considers the results of the tokens     with top_p probability mass. So 0.1 means only the tokens     comprising the top 10% probability mass are considered. parallel_tool_calls:     If True, enable parallel tool calls. modalities:     Types of output you would like the model to generate.     Can be: [\"text\"], [\"audio\"] or [\"text\", \"audio\"]. audio:     Audio configurations. Define voice and output format. verbosity:     Constrains the verbosity of the model's response. Lower     values will result in more concise responses, while higher     values will result in more verbose responses. Currently     supported values are low, medium, and high. web_search_options:     This tool searches the web for relevant results to use in a response.     OpenAI and OpenRouter only. verbose:     If True, Prints the model output to the console before it is transformed     into typed structured output. base_url:     URL to model provider. context_length:     The maximum context length supported by the model. reasoning_max_tokens:     Maximum number of tokens for reasoning/thinking. enable_cache:     If True, enable response caching to avoid redundant API calls. cache_size:     Maximum number of cached responses (default: 128).</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    model_id: str,\n    *,\n    max_tokens: Optional[int] = None,\n    reasoning_effort: Optional[str] = None,\n    enable_thinking: Optional[bool] = None,\n    return_reasoning: Optional[bool] = False,\n    reasoning_in_tool_call: Optional[bool] = True,\n    validate_typed_parser_output: Optional[bool] = False,\n    temperature: Optional[float] = None,\n    top_p: Optional[float] = None,\n    stop: Optional[Union[str, List[str]]] = None,\n    parallel_tool_calls: Optional[bool] = True,\n    modalities: Optional[List[str]] = None,\n    audio: Optional[Dict[str, str]] = None,\n    verbosity: Optional[str] = None,\n    web_search_options: Optional[Dict[str, Any]] = None,\n    verbose: Optional[bool] = False,\n    base_url: Optional[str] = None,\n    context_length: Optional[int] = None,\n    reasoning_max_tokens: Optional[int] = None,\n    enable_cache: Optional[bool] = False,\n    cache_size: Optional[int] = 128,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    max_tokens:\n        An upper bound for the number of tokens that can be\n        generated for a completion, including visible output\n        tokens and reasoning tokens.\n    reasoning_effort:\n        Constrains effort on reasoning for reasoning models.\n        Currently supported values are low, medium, and high.\n        Reducing reasoning effort can result in faster responses\n        and fewer tokens used on reasoning in a response.\n        Can be: \"minimal\", \"low\", \"medium\" or \"high\".\n    enable_thinking:\n        If True, enable the model reasoning.\n    return_reasoning:\n        If the model returns the `reasoning` field it will be added\n        along with the response.\n    reasoning_in_tool_call:\n        If True, maintains the reasoning for using the tool call.\n    validate_typed_parser_output:\n        If True, use the generation_schema to validate typed parser output.\n    temperature:\n        What sampling temperature to use, between 0 and 2.\n        Higher values like 0.8 will make the output more random,\n        while lower values like 0.2 will make it more focused and\n        deterministic.\n    stop:\n        Up to 4 sequences where the API will stop generating further\n        tokens. The returned text will not contain the stop sequence.\n    top_p:\n        An alternative to sampling with temperature, called nucleus\n        sampling, where the model considers the results of the tokens\n        with top_p probability mass. So 0.1 means only the tokens\n        comprising the top 10% probability mass are considered.\n    parallel_tool_calls:\n        If True, enable parallel tool calls.\n    modalities:\n        Types of output you would like the model to generate.\n        Can be: [\"text\"], [\"audio\"] or [\"text\", \"audio\"].\n    audio:\n        Audio configurations. Define voice and output format.\n    verbosity:\n        Constrains the verbosity of the model's response. Lower\n        values will result in more concise responses, while higher\n        values will result in more verbose responses. Currently\n        supported values are low, medium, and high.\n    web_search_options:\n        This tool searches the web for relevant results to use in a response.\n        OpenAI and OpenRouter only.\n    verbose:\n        If True, Prints the model output to the console before it is transformed\n        into typed structured output.\n    base_url:\n        URL to model provider.\n    context_length:\n        The maximum context length supported by the model.\n    reasoning_max_tokens:\n        Maximum number of tokens for reasoning/thinking.\n    enable_cache:\n        If True, enable response caching to avoid redundant API calls.\n    cache_size:\n        Maximum number of cached responses (default: 128).\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.context_length = context_length\n    self.reasoning_max_tokens = reasoning_max_tokens\n    self.enable_cache = enable_cache\n    self.cache_size = cache_size\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    sampling_run_params = {\"max_tokens\": max_tokens}\n    if temperature:\n        sampling_run_params[\"temperature\"] = temperature\n    if top_p:\n        sampling_run_params[\"top_p\"] = top_p\n    if stop:\n        sampling_run_params[\"stop\"] = stop\n    if verbosity:\n        sampling_run_params[\"verbosity\"] = verbosity\n    if modalities:\n        sampling_run_params[\"modalities\"] = modalities\n    if web_search_options:\n        sampling_run_params[\"web_search_options\"] = web_search_options\n    if audio:\n        sampling_run_params[\"audio\"] = audio\n    if reasoning_effort:\n        sampling_run_params[\"reasoning_effort\"] = reasoning_effort\n    self.sampling_run_params = sampling_run_params\n    self.enable_thinking = enable_thinking\n    self.parallel_tool_calls = parallel_tool_calls\n    self.reasoning_in_tool_call = reasoning_in_tool_call\n    self.validate_typed_parser_output = validate_typed_parser_output\n    self.return_reasoning = return_reasoning\n    self.verbose = verbose\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openai.OpenAIChatCompletion.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(\n    messages,\n    *,\n    system_prompt=None,\n    prefilling=None,\n    stream=False,\n    generation_schema=None,\n    tool_schemas=None,\n    tool_choice=None,\n    typed_parser=None,\n)\n</code></pre> <p>Async version of call. Args:     messages:         Conversation history. Can be simple string or list of messages.     system_prompt:         A set of instructions that defines the overarching behavior         and role of the model across all interactions.     prefilling:         Forces an initial message from the model. From that message         it will continue its response from there.     stream:         Whether generation should be in streaming mode.     generation_schema:         Schema that defines how the output should be structured.     tool_schemas:         JSON schema containing available tools.     tool_choice:         By default the model will determine when and how many tools to use.         You can force specific behavior with the tool_choice parameter.             1. auto:                 (Default) Call zero, one, or multiple functions.             2. required:                 Call one or more functions.             3. Forced Tool:                 Call exactly one specific tool e.g: \"get_weather\".     typed_parser:         Converts the model raw output into a typed-dict. Supported parser:         <code>typed_xml</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised if <code>generation_schema</code> and <code>stream=True</code>.</p> <code>ValueError</code> <p>Raised if <code>typed_xml=True</code> and <code>stream=True</code>.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    messages: Union[str, List[Dict[str, Any]]],\n    *,\n    system_prompt: Optional[str] = None,\n    prefilling: Optional[str] = None,\n    stream: Optional[bool] = False,\n    generation_schema: Optional[msgspec.Struct] = None,\n    tool_schemas: Optional[Dict] = None,\n    tool_choice: Optional[Union[str, Dict[str, Any]]] = None,\n    typed_parser: Optional[str] = None,\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Async version of __call__. Args:\n        messages:\n            Conversation history. Can be simple string or list of messages.\n        system_prompt:\n            A set of instructions that defines the overarching behavior\n            and role of the model across all interactions.\n        prefilling:\n            Forces an initial message from the model. From that message\n            it will continue its response from there.\n        stream:\n            Whether generation should be in streaming mode.\n        generation_schema:\n            Schema that defines how the output should be structured.\n        tool_schemas:\n            JSON schema containing available tools.\n        tool_choice:\n            By default the model will determine when and how many tools to use.\n            You can force specific behavior with the tool_choice parameter.\n                1. auto:\n                    (Default) Call zero, one, or multiple functions.\n                2. required:\n                    Call one or more functions.\n                3. Forced Tool:\n                    Call exactly one specific tool e.g: \"get_weather\".\n        typed_parser:\n            Converts the model raw output into a typed-dict. Supported parser:\n            `typed_xml`.\n\n    Raises:\n        ValueError:\n            Raised if `generation_schema` and `stream=True`.\n        ValueError:\n            Raised if `typed_xml=True` and `stream=True`.\n    \"\"\"\n    if isinstance(messages, str):\n        messages = [ChatBlock.user(messages)]\n    if isinstance(system_prompt, str):\n        messages.insert(0, ChatBlock.system(system_prompt))\n\n    if isinstance(tool_choice, str):\n        if tool_choice not in [\"auto\", \"required\", \"none\"]:\n            tool_choice = {\n                \"type\": \"function\",\n                \"function\": {\"name\": tool_choice},\n            }\n\n    generation_params = {\n        \"messages\": messages,\n        \"prefilling\": prefilling,\n        \"tool_choice\": tool_choice,\n        \"tools\": tool_schemas,\n        \"model\": self.model_id,\n    }\n\n    if tool_schemas:\n        generation_params[\"parallel_tool_calls\"] = self.parallel_tool_calls\n\n    if stream is True:\n        if typed_parser is not None:\n            raise ValueError(\"`typed_parser` is not `stream=True` compatible\")\n\n        stream_response = ModelStreamResponse()\n        await F.abackground_task(\n            self._astream_generate,\n            **generation_params,\n            stream=stream,\n            stream_response=stream_response,\n            stream_options={\"include_usage\": True},\n        )\n        await F.await_for_event(stream_response.first_chunk_event)\n        return stream_response\n    else:\n        if typed_parser and typed_parser not in typed_parser_registry:\n            available = \", \".join(typed_parser_registry.keys())\n            raise TypedParserNotFoundError(\n                f\"Typed parser `{typed_parser}` not found. \"\n                f\"Available parsers: {available}\"\n            )\n        response = await self._agenerate(\n            **generation_params,\n            typed_parser=typed_parser,\n            generation_schema=generation_schema,\n        )\n        return response\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.vllm.VLLMChatCompletion","title":"VLLMChatCompletion","text":"<p>               Bases: <code>_BaseVLLM</code>, <code>OpenAIChatCompletion</code></p> <p>vLLM Chat Completion.</p> Source code in <code>src/msgflux/models/providers/vllm.py</code> <pre><code>@register_model\nclass VLLMChatCompletion(_BaseVLLM, OpenAIChatCompletion):\n    \"\"\"vLLM Chat Completion.\"\"\"\n\n    def _adapt_params(self, params: Dict[str, Any]) -&gt; Dict[str, Any]:\n        response_format = params.pop(\"response_format\", None)\n        extra_body = params.get(\"extra_body\", {})\n\n        if response_format is not None:\n            extra_body[\"guided_json\"] = response_format\n\n        if self.enable_thinking is not None:\n            extra_body[\"chat_template_kwargs\"] = {\n                \"enable_thinking\": self.enable_thinking\n            }\n\n        params[\"extra_body\"] = extra_body\n        return params\n</code></pre>"},{"location":"api-reference/models/types/chat_completion/#msgflux.models.providers.openrouter.OpenRouterChatCompletion","title":"OpenRouterChatCompletion","text":"<p>               Bases: <code>_BaseOpenRouter</code>, <code>OpenAIChatCompletion</code></p> <p>OpenRouter Chat Completion.</p> Source code in <code>src/msgflux/models/providers/openrouter.py</code> <pre><code>@register_model\nclass OpenRouterChatCompletion(_BaseOpenRouter, OpenAIChatCompletion):\n    \"\"\"OpenRouter Chat Completion.\"\"\"\n\n    def _adapt_params(self, params: Dict[str, Any]) -&gt; Dict[str, Any]:\n        extra_body = params.get(\"extra_body\", {})\n        plugins = []\n\n        if params[\"tool_choice\"] is None:\n            if params[\"tools\"] is not None:\n                params[\"tool_choice\"] = \"auto\"\n            else:\n                params[\"tool_choice\"] = \"none\"\n\n        reasoning_effort = params.pop(\"reasoning_effort\", None)\n        if reasoning_effort is not None:\n            extra_body[\"reasoning\"] = {\"effort\": reasoning_effort}\n\n        # For non-OpenAI models enable web-search plugin\n        web_search_options = params.get(\"web_search_options\", None)\n        if web_search_options is not None and \"openai\" not in params[\"model\"]:\n            params.pop(\"web_search_options\")\n            web_pluging = {\"id\": \"web\"}\n            web_pluging.update(web_search_options)\n            plugins.append(web_pluging)\n\n        if plugins:\n            extra_body[\"plugins\"] = plugins\n\n        params[\"extra_body\"] = extra_body\n        params[\"extra_headers\"] = {\n            \"HTTP-Referer\": \"msgflux.com\",\n            \"X-Title\": \"msgflux\",\n        }\n        return params\n</code></pre>"},{"location":"api-reference/models/types/image_text_to_image/","title":"Image text to image","text":""},{"location":"api-reference/models/types/image_text_to_image/#msgflux.models.providers.openai.OpenAIImageTextToImage","title":"OpenAIImageTextToImage","text":"<p>               Bases: <code>OpenAITextToImage</code>, <code>ImageTextToImageModel</code></p> <p>OpenAI Image Edit.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAIImageTextToImage(OpenAITextToImage, ImageTextToImageModel):\n    \"\"\"OpenAI Image Edit.\"\"\"\n\n    def _execute_model(self, **kwargs):\n        model_output = self.client.images.edit(**kwargs, **self.sampling_run_params)\n        return model_output\n\n    async def _aexecute_model(self, **kwargs):\n        model_output = await self.aclient.images.edit(\n            **kwargs, **self.sampling_run_params\n        )\n        return model_output\n\n    def _prepare_inputs(self, image, mask):\n        inputs = {}\n        if isinstance(image, str):\n            image = [image]\n        inputs[\"image\"] = [encode_data_to_bytes(item) for item in image]\n        if mask:\n            inputs[\"mask\"] = encode_data_to_bytes(mask)\n        return inputs\n\n    @model_retry\n    def __call__(\n        self,\n        prompt: str,\n        image: Union[str, List[str]],\n        *,\n        mask: Optional[str] = None,\n        response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n        n: Optional[int] = 1,\n    ) -&gt; ModelResponse:\n        \"\"\"Args:\n        prompt:\n            A text description of the desired image(s).\n        image:\n            The image(s) to edit. Can be a path, an url or base64 string.\n        mask:\n            An additional image whose fully transparent areas\n            (e.g. where alpha is zero) indicate where image\n            should be edited. If there are multiple images provided,\n            the mask will be applied on the first image.\n        response_format:\n            Format in which images are returned.\n        n:\n            The number of images to generate.\n        \"\"\"\n        generation_params = dotdict(prompt=prompt, n=n, model=self.model_id)\n\n        if response_format is not None:\n            if response_format == \"base64\":\n                response_format = \"b64_json\"\n            generation_params.response_format = response_format\n\n        inputs = self._prepare_inputs(image, mask)\n        response = self._generate(**generation_params, **inputs)\n        return response\n\n    @model_retry\n    async def acall(\n        self,\n        prompt: str,\n        image: Union[str, List[str]],\n        *,\n        mask: Optional[str] = None,\n        response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n        n: Optional[int] = 1,\n    ) -&gt; ModelResponse:\n        \"\"\"Async version of __call__. Args:\n        prompt:\n            A text description of the desired image(s).\n        image:\n            The image(s) to edit. Can be a path, an url or base64 string.\n        mask:\n            An additional image whose fully transparent areas\n            (e.g. where alpha is zero) indicate where image\n            should be edited. If there are multiple images provided,\n            the mask will be applied on the first image.\n        response_format:\n            Format in which images are returned.\n        n:\n            The number of images to generate.\n        \"\"\"\n        generation_params = dotdict(prompt=prompt, n=n, model=self.model_id)\n\n        if response_format is not None:\n            if response_format == \"base64\":\n                response_format = \"b64_json\"\n            generation_params.response_format = response_format\n\n        inputs = self._prepare_inputs(image, mask)\n        response = await self._agenerate(**generation_params, **inputs)\n        return response\n</code></pre>"},{"location":"api-reference/models/types/image_text_to_image/#msgflux.models.providers.openai.OpenAIImageTextToImage.__call__","title":"__call__","text":"<pre><code>__call__(\n    prompt, image, *, mask=None, response_format=None, n=1\n)\n</code></pre> <p>prompt:     A text description of the desired image(s). image:     The image(s) to edit. Can be a path, an url or base64 string. mask:     An additional image whose fully transparent areas     (e.g. where alpha is zero) indicate where image     should be edited. If there are multiple images provided,     the mask will be applied on the first image. response_format:     Format in which images are returned. n:     The number of images to generate.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    prompt: str,\n    image: Union[str, List[str]],\n    *,\n    mask: Optional[str] = None,\n    response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n    n: Optional[int] = 1,\n) -&gt; ModelResponse:\n    \"\"\"Args:\n    prompt:\n        A text description of the desired image(s).\n    image:\n        The image(s) to edit. Can be a path, an url or base64 string.\n    mask:\n        An additional image whose fully transparent areas\n        (e.g. where alpha is zero) indicate where image\n        should be edited. If there are multiple images provided,\n        the mask will be applied on the first image.\n    response_format:\n        Format in which images are returned.\n    n:\n        The number of images to generate.\n    \"\"\"\n    generation_params = dotdict(prompt=prompt, n=n, model=self.model_id)\n\n    if response_format is not None:\n        if response_format == \"base64\":\n            response_format = \"b64_json\"\n        generation_params.response_format = response_format\n\n    inputs = self._prepare_inputs(image, mask)\n    response = self._generate(**generation_params, **inputs)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/image_text_to_image/#msgflux.models.providers.openai.OpenAIImageTextToImage.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(\n    prompt, image, *, mask=None, response_format=None, n=1\n)\n</code></pre> <p>Async version of call. Args: prompt:     A text description of the desired image(s). image:     The image(s) to edit. Can be a path, an url or base64 string. mask:     An additional image whose fully transparent areas     (e.g. where alpha is zero) indicate where image     should be edited. If there are multiple images provided,     the mask will be applied on the first image. response_format:     Format in which images are returned. n:     The number of images to generate.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    prompt: str,\n    image: Union[str, List[str]],\n    *,\n    mask: Optional[str] = None,\n    response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n    n: Optional[int] = 1,\n) -&gt; ModelResponse:\n    \"\"\"Async version of __call__. Args:\n    prompt:\n        A text description of the desired image(s).\n    image:\n        The image(s) to edit. Can be a path, an url or base64 string.\n    mask:\n        An additional image whose fully transparent areas\n        (e.g. where alpha is zero) indicate where image\n        should be edited. If there are multiple images provided,\n        the mask will be applied on the first image.\n    response_format:\n        Format in which images are returned.\n    n:\n        The number of images to generate.\n    \"\"\"\n    generation_params = dotdict(prompt=prompt, n=n, model=self.model_id)\n\n    if response_format is not None:\n        if response_format == \"base64\":\n            response_format = \"b64_json\"\n        generation_params.response_format = response_format\n\n    inputs = self._prepare_inputs(image, mask)\n    response = await self._agenerate(**generation_params, **inputs)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/moderation/","title":"Moderation","text":""},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration","title":"OpenAIModeration","text":"<p>               Bases: <code>_BaseOpenAI</code>, <code>ModerationModel</code></p> <p>OpenAI Moderation.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAIModeration(_BaseOpenAI, ModerationModel):\n    \"\"\"OpenAI Moderation.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        model_id: str,\n        base_url: Optional[str] = None,\n        enable_cache: Optional[bool] = False,\n        cache_size: Optional[int] = 128,\n    ):\n        \"\"\"Args:\n        model_id:\n            Model ID in provider.\n        base_url:\n            URL to model provider.\n        enable_cache:\n            If True, enables response caching to avoid redundant API calls.\n        cache_size:\n            Maximum number of responses to cache (default: 128).\n        \"\"\"\n        super().__init__()\n        self.model_id = model_id\n        self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n        self.enable_cache = enable_cache\n        self.cache_size = cache_size\n        self._initialize()\n        self._get_api_key()\n\n    def _execute_model(self, **kwargs):\n        model_output = self.client.moderations.create(**kwargs)\n        return model_output\n\n    async def _aexecute_model(self, **kwargs):\n        model_output = await self.aclient.moderations.create(**kwargs)\n        return model_output\n\n    def _generate(self, **kwargs):\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        response = ModelResponse()\n        response.set_response_type(\"moderation\")\n        model_output = self._execute_model(**kwargs)\n        moderation = dotdict({\"results\": model_output.results[0].model_dump()})\n        moderation.safe = not moderation.results.flagged\n        response.add(moderation)\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    async def _agenerate(self, **kwargs):\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        response = ModelResponse()\n        response.set_response_type(\"moderation\")\n        model_output = await self._aexecute_model(**kwargs)\n        moderation = dotdict({\"results\": model_output.results[0].model_dump()})\n        moderation.safe = not moderation.results.flagged\n        response.add(moderation)\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    @model_retry\n    def __call__(\n        self,\n        data: Union[str, List[Dict[str, Any]]],\n    ) -&gt; ModelResponse:\n        \"\"\"Args:\n        data:\n            Input (or inputs) to classify. Can be a single string,\n            an array of strings, or an array of multi-modal input\n            objects similar to other models.\n        \"\"\"\n        response = self._generate(input=data, model=self.model_id)\n        return response\n\n    @model_retry\n    async def acall(\n        self,\n        data: Union[str, List[Dict[str, Any]]],\n    ) -&gt; ModelResponse:\n        \"\"\"Async version of __call__. Args:\n        data:\n            Input (or inputs) to classify. Can be a single string,\n            an array of strings, or an array of multi-modal input\n            objects similar to other models.\n        \"\"\"\n        response = await self._agenerate(input=data, model=self.model_id)\n        return response\n</code></pre>"},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration.cache_size","title":"cache_size  <code>instance-attribute</code>","text":"<pre><code>cache_size = cache_size\n</code></pre>"},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration.enable_cache","title":"enable_cache  <code>instance-attribute</code>","text":"<pre><code>enable_cache = enable_cache\n</code></pre>"},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration.sampling_params","title":"sampling_params  <code>instance-attribute</code>","text":"<pre><code>sampling_params = {'base_url': base_url or _get_base_url()}\n</code></pre>"},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration.__call__","title":"__call__","text":"<pre><code>__call__(data)\n</code></pre> <p>data:     Input (or inputs) to classify. Can be a single string,     an array of strings, or an array of multi-modal input     objects similar to other models.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    data: Union[str, List[Dict[str, Any]]],\n) -&gt; ModelResponse:\n    \"\"\"Args:\n    data:\n        Input (or inputs) to classify. Can be a single string,\n        an array of strings, or an array of multi-modal input\n        objects similar to other models.\n    \"\"\"\n    response = self._generate(input=data, model=self.model_id)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    model_id,\n    base_url=None,\n    enable_cache=False,\n    cache_size=128,\n)\n</code></pre> <p>model_id:     Model ID in provider. base_url:     URL to model provider. enable_cache:     If True, enables response caching to avoid redundant API calls. cache_size:     Maximum number of responses to cache (default: 128).</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model_id: str,\n    base_url: Optional[str] = None,\n    enable_cache: Optional[bool] = False,\n    cache_size: Optional[int] = 128,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    base_url:\n        URL to model provider.\n    enable_cache:\n        If True, enables response caching to avoid redundant API calls.\n    cache_size:\n        Maximum number of responses to cache (default: 128).\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    self.enable_cache = enable_cache\n    self.cache_size = cache_size\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"api-reference/models/types/moderation/#msgflux.models.providers.openai.OpenAIModeration.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(data)\n</code></pre> <p>Async version of call. Args: data:     Input (or inputs) to classify. Can be a single string,     an array of strings, or an array of multi-modal input     objects similar to other models.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    data: Union[str, List[Dict[str, Any]]],\n) -&gt; ModelResponse:\n    \"\"\"Async version of __call__. Args:\n    data:\n        Input (or inputs) to classify. Can be a single string,\n        an array of strings, or an array of multi-modal input\n        objects similar to other models.\n    \"\"\"\n    response = await self._agenerate(input=data, model=self.model_id)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/speech_to_text/","title":"Speech to text","text":""},{"location":"api-reference/models/types/speech_to_text/#msgflux.models.providers.openai.OpenAISpeechToText","title":"OpenAISpeechToText","text":"<p>               Bases: <code>_BaseOpenAI</code>, <code>SpeechToTextModel</code></p> <p>OpenAI Speech to Text.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAISpeechToText(_BaseOpenAI, SpeechToTextModel):\n    \"\"\"OpenAI Speech to Text.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        model_id: str,\n        temperature: Optional[float] = 0.0,\n        base_url: Optional[str] = None,\n    ):\n        \"\"\"Args:\n        model_id:\n            Model ID in provider.\n        temperature:\n            The sampling temperature, between 0 and 1.\n        base_url:\n            URL to model provider.\n        \"\"\"\n        super().__init__()\n        self.model_id = model_id\n        self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n        self.sampling_run_params = {\"temperature\": temperature}\n        self._initialize()\n        self._get_api_key()\n\n    def _execute_model(self, **kwargs):\n        model_output = self.client.audio.transcriptions.create(\n            **kwargs, **self.sampling_run_params\n        )\n        return model_output\n\n    async def _aexecute_model(self, **kwargs):\n        model_output = await self.aclient.audio.transcriptions.create(\n            **kwargs, **self.sampling_run_params\n        )\n        return model_output\n\n    def _generate(self, **kwargs):\n        response = ModelResponse()\n\n        model_output = self._execute_model(**kwargs)\n\n        response.set_response_type(\"transcript\")\n\n        transcript = {}\n\n        if isinstance(model_output, str):\n            transcript[\"text\"] = model_output\n        else:\n            if model_output.text:\n                transcript[\"text\"] = model_output.text\n            if model_output.words:\n                words = [\n                    {\"word\": w.word, \"start\": w.start, \"end\": w.end}\n                    for w in model_output.words\n                ]\n                transcript[\"words\"] = words\n            if model_output.segment:\n                segments = [\n                    {\"id\": seg.id, \"start\": seg.start, \"end\": seg.end, \"text\": seg.text}\n                    for seg in model_output.segments\n                ]\n                transcript[\"segments\"] = segments\n\n        response.add(transcript)\n\n        return response\n\n    async def _agenerate(self, **kwargs):\n        response = ModelResponse()\n\n        model_output = await self._aexecute_model(**kwargs)\n\n        response.set_response_type(\"transcript\")\n\n        transcript = {}\n\n        if isinstance(model_output, str):\n            transcript[\"text\"] = model_output\n        else:\n            if model_output.text:\n                transcript[\"text\"] = model_output.text\n            if model_output.words:\n                words = [\n                    {\"word\": w.word, \"start\": w.start, \"end\": w.end}\n                    for w in model_output.words\n                ]\n                transcript[\"words\"] = words\n            if model_output.segment:\n                segments = [\n                    {\"id\": seg.id, \"start\": seg.start, \"end\": seg.end, \"text\": seg.text}\n                    for seg in model_output.segments\n                ]\n                transcript[\"segments\"] = segments\n\n        response.add(transcript)\n\n        return response\n\n    def _stream_generate(self, **kwargs):\n        stream_response = kwargs.pop(\"stream_response\")\n        stream_response.set_response_type(\"transcript\")\n\n        model_output = self._execute_model(**kwargs)\n\n        for event in model_output:\n            chunk = event.transcript.text.delta\n            if chunk:\n                stream_response.add(chunk)\n                if not stream_response.first_chunk_event.is_set():\n                    stream_response.first_chunk_event.set()\n            elif event.transcript.text.done:\n                stream_response.add(None)\n\n        return stream_response\n\n    async def _astream_generate(self, **kwargs):\n        stream_response = kwargs.pop(\"stream_response\")\n        stream_response.set_response_type(\"transcript\")\n\n        model_output = await self._aexecute_model(**kwargs)\n\n        async for event in model_output:\n            chunk = event.transcript.text.delta\n            if chunk:\n                stream_response.add(chunk)\n                if not stream_response.first_chunk_event.is_set():\n                    stream_response.first_chunk_event.set()\n            elif event.transcript.text.done:\n                stream_response.add(None)\n\n        return stream_response\n\n    @model_retry\n    def __call__(\n        self,\n        data: str,\n        *,\n        stream: Optional[bool] = False,\n        response_format: Optional[\n            Literal[\"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"]\n        ] = \"text\",\n        timestamp_granularities: Optional[List[str]] = None,\n        prompt: Optional[str] = None,\n        language: Optional[str] = None,\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Args:\n        data:\n            Url, path, base64 to audio.\n        stream:\n            Whether generation should be in streaming mode.\n        response_format:\n            The format of the output, in one of these options:\n            json, text, srt, verbose_json, or vtt.\n        timestamp_granularities:\n            The timestamp granularities to populate for this\n            transcription. `response_format` must be set `verbose_json`\n            to use timestamp granularities. Either or both of these\n            options are supported: word, or segment. Note: There is no\n            additional latency for segment timestamps, but generating\n            word timestamps incurs additional latency.\n        prompt:\n            An optional text to guide the model's style or continue a\n            previous audio segment. The prompt should match the audio language.\n        language:\n            The language of the input audio. Supplying the input language in\n            ISO-639-1 (e.g. en) format will improve accuracy and latency.\n        \"\"\"\n        file = encode_data_to_bytes(data)\n        params = {\n            \"file\": file,\n            \"language\": language,\n            \"response_format\": response_format,\n            \"timestamp_granularities\": timestamp_granularities,\n            \"prompt\": prompt,\n            \"model\": self.model_id,\n        }\n        if stream:\n            stream_response = ModelStreamResponse()\n            params[\"stream_response\"] = stream_response\n            params[\"stream\"] = stream\n            F.background_task(self._stream_generate, **params)\n            F.wait_for_event(stream_response.first_chunk_event)\n            return stream_response\n        else:\n            response = self._generate(**params)\n            return response\n\n    @model_retry\n    async def acall(\n        self,\n        data: str,\n        *,\n        stream: Optional[bool] = False,\n        response_format: Optional[\n            Literal[\"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"]\n        ] = \"text\",\n        timestamp_granularities: Optional[List[str]] = None,\n        prompt: Optional[str] = None,\n        language: Optional[str] = None,\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Async version of __call__. Args:\n        data:\n            Url, path, base64 to audio.\n        stream:\n            Whether generation should be in streaming mode.\n        response_format:\n            The format of the output, in one of these options:\n            json, text, srt, verbose_json, or vtt.\n        timestamp_granularities:\n            The timestamp granularities to populate for this\n            transcription. `response_format` must be set `verbose_json`\n            to use timestamp granularities. Either or both of these\n            options are supported: word, or segment. Note: There is no\n            additional latency for segment timestamps, but generating\n            word timestamps incurs additional latency.\n        prompt:\n            An optional text to guide the model's style or continue a\n            previous audio segment. The prompt should match the audio language.\n        language:\n            The language of the input audio. Supplying the input language in\n            ISO-639-1 (e.g. en) format will improve accuracy and latency.\n        \"\"\"\n        file = encode_data_to_bytes(data)\n        params = {\n            \"file\": file,\n            \"language\": language,\n            \"response_format\": response_format,\n            \"timestamp_granularities\": timestamp_granularities,\n            \"prompt\": prompt,\n            \"model\": self.model_id,\n        }\n        if stream:\n            stream_response = ModelStreamResponse()\n            params[\"stream_response\"] = stream_response\n            params[\"stream\"] = stream\n            await F.abackground_task(self._astream_generate, **params)\n            await F.await_for_event(stream_response.first_chunk_event)\n            return stream_response\n        else:\n            response = await self._agenerate(**params)\n            return response\n</code></pre>"},{"location":"api-reference/models/types/speech_to_text/#msgflux.models.providers.openai.OpenAISpeechToText.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"api-reference/models/types/speech_to_text/#msgflux.models.providers.openai.OpenAISpeechToText.sampling_params","title":"sampling_params  <code>instance-attribute</code>","text":"<pre><code>sampling_params = {'base_url': base_url or _get_base_url()}\n</code></pre>"},{"location":"api-reference/models/types/speech_to_text/#msgflux.models.providers.openai.OpenAISpeechToText.sampling_run_params","title":"sampling_run_params  <code>instance-attribute</code>","text":"<pre><code>sampling_run_params = {'temperature': temperature}\n</code></pre>"},{"location":"api-reference/models/types/speech_to_text/#msgflux.models.providers.openai.OpenAISpeechToText.__call__","title":"__call__","text":"<pre><code>__call__(\n    data,\n    *,\n    stream=False,\n    response_format=\"text\",\n    timestamp_granularities=None,\n    prompt=None,\n    language=None,\n)\n</code></pre> <p>data:     Url, path, base64 to audio. stream:     Whether generation should be in streaming mode. response_format:     The format of the output, in one of these options:     json, text, srt, verbose_json, or vtt. timestamp_granularities:     The timestamp granularities to populate for this     transcription. <code>response_format</code> must be set <code>verbose_json</code>     to use timestamp granularities. Either or both of these     options are supported: word, or segment. Note: There is no     additional latency for segment timestamps, but generating     word timestamps incurs additional latency. prompt:     An optional text to guide the model's style or continue a     previous audio segment. The prompt should match the audio language. language:     The language of the input audio. Supplying the input language in     ISO-639-1 (e.g. en) format will improve accuracy and latency.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    data: str,\n    *,\n    stream: Optional[bool] = False,\n    response_format: Optional[\n        Literal[\"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"]\n    ] = \"text\",\n    timestamp_granularities: Optional[List[str]] = None,\n    prompt: Optional[str] = None,\n    language: Optional[str] = None,\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Args:\n    data:\n        Url, path, base64 to audio.\n    stream:\n        Whether generation should be in streaming mode.\n    response_format:\n        The format of the output, in one of these options:\n        json, text, srt, verbose_json, or vtt.\n    timestamp_granularities:\n        The timestamp granularities to populate for this\n        transcription. `response_format` must be set `verbose_json`\n        to use timestamp granularities. Either or both of these\n        options are supported: word, or segment. Note: There is no\n        additional latency for segment timestamps, but generating\n        word timestamps incurs additional latency.\n    prompt:\n        An optional text to guide the model's style or continue a\n        previous audio segment. The prompt should match the audio language.\n    language:\n        The language of the input audio. Supplying the input language in\n        ISO-639-1 (e.g. en) format will improve accuracy and latency.\n    \"\"\"\n    file = encode_data_to_bytes(data)\n    params = {\n        \"file\": file,\n        \"language\": language,\n        \"response_format\": response_format,\n        \"timestamp_granularities\": timestamp_granularities,\n        \"prompt\": prompt,\n        \"model\": self.model_id,\n    }\n    if stream:\n        stream_response = ModelStreamResponse()\n        params[\"stream_response\"] = stream_response\n        params[\"stream\"] = stream\n        F.background_task(self._stream_generate, **params)\n        F.wait_for_event(stream_response.first_chunk_event)\n        return stream_response\n    else:\n        response = self._generate(**params)\n        return response\n</code></pre>"},{"location":"api-reference/models/types/speech_to_text/#msgflux.models.providers.openai.OpenAISpeechToText.__init__","title":"__init__","text":"<pre><code>__init__(*, model_id, temperature=0.0, base_url=None)\n</code></pre> <p>model_id:     Model ID in provider. temperature:     The sampling temperature, between 0 and 1. base_url:     URL to model provider.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model_id: str,\n    temperature: Optional[float] = 0.0,\n    base_url: Optional[str] = None,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    temperature:\n        The sampling temperature, between 0 and 1.\n    base_url:\n        URL to model provider.\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    self.sampling_run_params = {\"temperature\": temperature}\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"api-reference/models/types/speech_to_text/#msgflux.models.providers.openai.OpenAISpeechToText.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(\n    data,\n    *,\n    stream=False,\n    response_format=\"text\",\n    timestamp_granularities=None,\n    prompt=None,\n    language=None,\n)\n</code></pre> <p>Async version of call. Args: data:     Url, path, base64 to audio. stream:     Whether generation should be in streaming mode. response_format:     The format of the output, in one of these options:     json, text, srt, verbose_json, or vtt. timestamp_granularities:     The timestamp granularities to populate for this     transcription. <code>response_format</code> must be set <code>verbose_json</code>     to use timestamp granularities. Either or both of these     options are supported: word, or segment. Note: There is no     additional latency for segment timestamps, but generating     word timestamps incurs additional latency. prompt:     An optional text to guide the model's style or continue a     previous audio segment. The prompt should match the audio language. language:     The language of the input audio. Supplying the input language in     ISO-639-1 (e.g. en) format will improve accuracy and latency.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    data: str,\n    *,\n    stream: Optional[bool] = False,\n    response_format: Optional[\n        Literal[\"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"]\n    ] = \"text\",\n    timestamp_granularities: Optional[List[str]] = None,\n    prompt: Optional[str] = None,\n    language: Optional[str] = None,\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Async version of __call__. Args:\n    data:\n        Url, path, base64 to audio.\n    stream:\n        Whether generation should be in streaming mode.\n    response_format:\n        The format of the output, in one of these options:\n        json, text, srt, verbose_json, or vtt.\n    timestamp_granularities:\n        The timestamp granularities to populate for this\n        transcription. `response_format` must be set `verbose_json`\n        to use timestamp granularities. Either or both of these\n        options are supported: word, or segment. Note: There is no\n        additional latency for segment timestamps, but generating\n        word timestamps incurs additional latency.\n    prompt:\n        An optional text to guide the model's style or continue a\n        previous audio segment. The prompt should match the audio language.\n    language:\n        The language of the input audio. Supplying the input language in\n        ISO-639-1 (e.g. en) format will improve accuracy and latency.\n    \"\"\"\n    file = encode_data_to_bytes(data)\n    params = {\n        \"file\": file,\n        \"language\": language,\n        \"response_format\": response_format,\n        \"timestamp_granularities\": timestamp_granularities,\n        \"prompt\": prompt,\n        \"model\": self.model_id,\n    }\n    if stream:\n        stream_response = ModelStreamResponse()\n        params[\"stream_response\"] = stream_response\n        params[\"stream\"] = stream\n        await F.abackground_task(self._astream_generate, **params)\n        await F.await_for_event(stream_response.first_chunk_event)\n        return stream_response\n    else:\n        response = await self._agenerate(**params)\n        return response\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/","title":"Text embedder","text":""},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder","title":"OpenAITextEmbedder","text":"<p>               Bases: <code>_BaseOpenAI</code>, <code>TextEmbedderModel</code></p> <p>OpenAI Text Embedder.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAITextEmbedder(_BaseOpenAI, TextEmbedderModel):\n    \"\"\"OpenAI Text Embedder.\"\"\"\n\n    batch_support: bool = True\n\n    def __init__(\n        self,\n        *,\n        model_id: str,\n        dimensions: Optional[int] = None,\n        base_url: Optional[str] = None,\n        enable_cache: Optional[bool] = False,\n        cache_size: Optional[int] = 128,\n    ):\n        \"\"\"Args:\n        model_id:\n            Model ID in provider.\n        dimensions:\n            The number of dimensions the resulting output embeddings should have.\n        base_url:\n            URL to model provider.\n        enable_cache:\n            If True, enables response caching to avoid redundant API calls.\n        cache_size:\n            Maximum number of responses to cache (default: 128).\n        \"\"\"\n        super().__init__()\n        self.model_id = model_id\n        self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n        self.sampling_run_params = {\"dimensions\": dimensions}\n        self.enable_cache = enable_cache\n        self.cache_size = cache_size\n        self._initialize()\n        self._get_api_key()\n\n    def _execute_model(self, **kwargs):\n        model_output = self.client.embeddings.create(\n            **kwargs,\n            **self.sampling_run_params,\n        )\n        return model_output\n\n    async def _aexecute_model(self, **kwargs):\n        model_output = await self.aclient.embeddings.create(\n            **kwargs,\n            **self.sampling_run_params,\n        )\n        return model_output\n\n    def _generate(self, **kwargs):\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        response = ModelResponse()\n        response.set_response_type(\"text_embedding\")\n        model_output = self._execute_model(**kwargs)\n        embeddings = [item.embedding for item in model_output.data]\n        metadata = dotdict({\"usage\": model_output.usage.to_dict()})\n        response.add(embeddings)\n        response.set_metadata(metadata)\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    async def _agenerate(self, **kwargs):\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        response = ModelResponse()\n        response.set_response_type(\"text_embedding\")\n        model_output = await self._aexecute_model(**kwargs)\n        embeddings = [item.embedding for item in model_output.data]\n        metadata = dotdict({\"usage\": model_output.usage.to_dict()})\n        response.add(embeddings)\n        response.set_metadata(metadata)\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    @model_retry\n    def __call__(\n        self,\n        data: Union[str, List[str]],\n    ):\n        \"\"\"Args:\n        data:\n            Input text to embed.\n        \"\"\"\n        response = self._generate(input=data, model=self.model_id)\n        return response\n\n    @model_retry\n    async def acall(\n        self,\n        data: Union[str, List[str]],\n    ):\n        \"\"\"Async version of __call__. Args:\n        data:\n            Input text to embed.\n        \"\"\"\n        response = await self._agenerate(input=data, model=self.model_id)\n        return response\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.batch_support","title":"batch_support  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>batch_support = True\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.cache_size","title":"cache_size  <code>instance-attribute</code>","text":"<pre><code>cache_size = cache_size\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.enable_cache","title":"enable_cache  <code>instance-attribute</code>","text":"<pre><code>enable_cache = enable_cache\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.sampling_params","title":"sampling_params  <code>instance-attribute</code>","text":"<pre><code>sampling_params = {'base_url': base_url or _get_base_url()}\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.sampling_run_params","title":"sampling_run_params  <code>instance-attribute</code>","text":"<pre><code>sampling_run_params = {'dimensions': dimensions}\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.__call__","title":"__call__","text":"<pre><code>__call__(data)\n</code></pre> <p>data:     Input text to embed.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    data: Union[str, List[str]],\n):\n    \"\"\"Args:\n    data:\n        Input text to embed.\n    \"\"\"\n    response = self._generate(input=data, model=self.model_id)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    model_id,\n    dimensions=None,\n    base_url=None,\n    enable_cache=False,\n    cache_size=128,\n)\n</code></pre> <p>model_id:     Model ID in provider. dimensions:     The number of dimensions the resulting output embeddings should have. base_url:     URL to model provider. enable_cache:     If True, enables response caching to avoid redundant API calls. cache_size:     Maximum number of responses to cache (default: 128).</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model_id: str,\n    dimensions: Optional[int] = None,\n    base_url: Optional[str] = None,\n    enable_cache: Optional[bool] = False,\n    cache_size: Optional[int] = 128,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    dimensions:\n        The number of dimensions the resulting output embeddings should have.\n    base_url:\n        URL to model provider.\n    enable_cache:\n        If True, enables response caching to avoid redundant API calls.\n    cache_size:\n        Maximum number of responses to cache (default: 128).\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    self.sampling_run_params = {\"dimensions\": dimensions}\n    self.enable_cache = enable_cache\n    self.cache_size = cache_size\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"api-reference/models/types/text_embedder/#msgflux.models.providers.openai.OpenAITextEmbedder.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(data)\n</code></pre> <p>Async version of call. Args: data:     Input text to embed.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    data: Union[str, List[str]],\n):\n    \"\"\"Async version of __call__. Args:\n    data:\n        Input text to embed.\n    \"\"\"\n    response = await self._agenerate(input=data, model=self.model_id)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/text_to_image/","title":"Text to image","text":""},{"location":"api-reference/models/types/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage","title":"OpenAITextToImage","text":"<p>               Bases: <code>_BaseOpenAI</code>, <code>TextToImageModel</code></p> <p>OpenAI Image Generation.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAITextToImage(_BaseOpenAI, TextToImageModel):\n    \"\"\"OpenAI Image Generation.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        model_id: str,\n        moderation: Optional[Literal[\"auto\", \"low\"]] = None,\n        base_url: Optional[str] = None,\n    ):\n        \"\"\"Args:\n        model_id:\n            Model ID in provider.\n        moderation:\n            Control the content-moderation level for images generated.\n        base_url:\n            URL to model provider.\n        \"\"\"\n        super().__init__()\n        self.model_id = model_id\n        self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n        sampling_run_params = {}\n        if moderation:\n            sampling_run_params[\"moderation\"] = moderation\n        self.sampling_run_params = sampling_run_params\n        self._initialize()\n        self._get_api_key()\n\n    def _execute_model(self, **kwargs):\n        model_output = self.client.images.generate(**kwargs, **self.sampling_run_params)\n        return model_output\n\n    async def _aexecute_model(self, **kwargs):\n        model_output = await self.aclient.images.generate(\n            **kwargs, **self.sampling_run_params\n        )\n        return model_output\n\n    def _get_metadata(self, model_output):\n        metadata = dotdict(\n            usage=model_output.usage.to_dict(),\n            details={\n                \"size\": model_output.size,\n                \"quality\": model_output.quality,\n                \"output_format\": model_output.output_format,\n                \"background\": model_output.background,\n            },\n        )\n        return metadata\n\n    def _generate(self, **kwargs):\n        response = ModelResponse()\n        response.set_response_type(\"image_generation\")\n\n        model_output = self._execute_model(**kwargs)\n\n        metadata = self._get_metadata(model_output)\n\n        images = []\n        for item in model_output.data:\n            if item.url:\n                images.append(item.url)\n            if item.b64_json:\n                images.append(item.b64_json)\n\n        if len(images) == 1:\n            images = images[0]\n\n        response.add(images)\n        response.set_metadata(metadata)\n\n        return response\n\n    async def _agenerate(self, **kwargs):\n        response = ModelResponse()\n        response.set_response_type(\"image_generation\")\n\n        model_output = await self._aexecute_model(**kwargs)\n\n        metadata = self._get_metadata(model_output)\n\n        images = []\n        for item in model_output.data:\n            if item.url:\n                images.append(item.url)\n            if item.b64_json:\n                images.append(item.b64_json)\n\n        if len(images) == 1:\n            images = images[0]\n\n        response.add(images)\n        response.set_metadata(metadata)\n\n        return response\n\n    @model_retry\n    def __call__(\n        self,\n        prompt: str,\n        *,\n        response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n        n: Optional[int] = 1,\n        size: Optional[str] = \"auto\",\n        quality: Optional[str] = \"auto\",\n        background: Optional[Literal[\"transparent\", \"opaque\", \"auto\"]] = None,\n    ) -&gt; ModelResponse:\n        \"\"\"Args:\n        prompt:\n            A text description of the desired image(s).\n        response_format:\n            Format in which images are returned.\n        n:\n            The number of images to generate.\n        size:\n            The size of the generated images.\n        quality:\n            The quality of the image that will be generated.\n        background:\n            Allows to set transparency for the background of the generated image(s).\n        \"\"\"\n        generation_params = dotdict(\n            prompt=prompt,\n            n=n,\n            size=size,\n            quality=quality,\n            background=background,\n            model=self.model_id,\n        )\n\n        if response_format is not None:\n            if response_format == \"base64\":\n                response_format = \"b64_json\"\n            generation_params.response_format = response_format\n\n        response = self._generate(**generation_params)\n        return response\n\n    @model_retry\n    async def acall(\n        self,\n        prompt: str,\n        *,\n        response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n        n: Optional[int] = 1,\n        size: Optional[str] = \"auto\",\n        quality: Optional[str] = \"auto\",\n        background: Optional[Literal[\"transparent\", \"opaque\", \"auto\"]] = None,\n    ) -&gt; ModelResponse:\n        \"\"\"Async version of __call__. Args:\n        prompt:\n            A text description of the desired image(s).\n        response_format:\n            Format in which images are returned.\n        n:\n            The number of images to generate.\n        size:\n            The size of the generated images.\n        quality:\n            The quality of the image that will be generated.\n        background:\n            Allows to set transparency for the background of the generated image(s).\n        \"\"\"\n        generation_params = dotdict(\n            prompt=prompt,\n            n=n,\n            size=size,\n            quality=quality,\n            background=background,\n            model=self.model_id,\n        )\n\n        if response_format is not None:\n            if response_format == \"base64\":\n                response_format = \"b64_json\"\n            generation_params.response_format = response_format\n\n        response = await self._agenerate(**generation_params)\n        return response\n</code></pre>"},{"location":"api-reference/models/types/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"api-reference/models/types/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage.sampling_params","title":"sampling_params  <code>instance-attribute</code>","text":"<pre><code>sampling_params = {'base_url': base_url or _get_base_url()}\n</code></pre>"},{"location":"api-reference/models/types/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage.sampling_run_params","title":"sampling_run_params  <code>instance-attribute</code>","text":"<pre><code>sampling_run_params = sampling_run_params\n</code></pre>"},{"location":"api-reference/models/types/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage.__call__","title":"__call__","text":"<pre><code>__call__(\n    prompt,\n    *,\n    response_format=None,\n    n=1,\n    size=\"auto\",\n    quality=\"auto\",\n    background=None,\n)\n</code></pre> <p>prompt:     A text description of the desired image(s). response_format:     Format in which images are returned. n:     The number of images to generate. size:     The size of the generated images. quality:     The quality of the image that will be generated. background:     Allows to set transparency for the background of the generated image(s).</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    prompt: str,\n    *,\n    response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n    n: Optional[int] = 1,\n    size: Optional[str] = \"auto\",\n    quality: Optional[str] = \"auto\",\n    background: Optional[Literal[\"transparent\", \"opaque\", \"auto\"]] = None,\n) -&gt; ModelResponse:\n    \"\"\"Args:\n    prompt:\n        A text description of the desired image(s).\n    response_format:\n        Format in which images are returned.\n    n:\n        The number of images to generate.\n    size:\n        The size of the generated images.\n    quality:\n        The quality of the image that will be generated.\n    background:\n        Allows to set transparency for the background of the generated image(s).\n    \"\"\"\n    generation_params = dotdict(\n        prompt=prompt,\n        n=n,\n        size=size,\n        quality=quality,\n        background=background,\n        model=self.model_id,\n    )\n\n    if response_format is not None:\n        if response_format == \"base64\":\n            response_format = \"b64_json\"\n        generation_params.response_format = response_format\n\n    response = self._generate(**generation_params)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage.__init__","title":"__init__","text":"<pre><code>__init__(*, model_id, moderation=None, base_url=None)\n</code></pre> <p>model_id:     Model ID in provider. moderation:     Control the content-moderation level for images generated. base_url:     URL to model provider.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model_id: str,\n    moderation: Optional[Literal[\"auto\", \"low\"]] = None,\n    base_url: Optional[str] = None,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    moderation:\n        Control the content-moderation level for images generated.\n    base_url:\n        URL to model provider.\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    sampling_run_params = {}\n    if moderation:\n        sampling_run_params[\"moderation\"] = moderation\n    self.sampling_run_params = sampling_run_params\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"api-reference/models/types/text_to_image/#msgflux.models.providers.openai.OpenAITextToImage.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(\n    prompt,\n    *,\n    response_format=None,\n    n=1,\n    size=\"auto\",\n    quality=\"auto\",\n    background=None,\n)\n</code></pre> <p>Async version of call. Args: prompt:     A text description of the desired image(s). response_format:     Format in which images are returned. n:     The number of images to generate. size:     The size of the generated images. quality:     The quality of the image that will be generated. background:     Allows to set transparency for the background of the generated image(s).</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    prompt: str,\n    *,\n    response_format: Optional[Literal[\"url\", \"base64\"]] = None,\n    n: Optional[int] = 1,\n    size: Optional[str] = \"auto\",\n    quality: Optional[str] = \"auto\",\n    background: Optional[Literal[\"transparent\", \"opaque\", \"auto\"]] = None,\n) -&gt; ModelResponse:\n    \"\"\"Async version of __call__. Args:\n    prompt:\n        A text description of the desired image(s).\n    response_format:\n        Format in which images are returned.\n    n:\n        The number of images to generate.\n    size:\n        The size of the generated images.\n    quality:\n        The quality of the image that will be generated.\n    background:\n        Allows to set transparency for the background of the generated image(s).\n    \"\"\"\n    generation_params = dotdict(\n        prompt=prompt,\n        n=n,\n        size=size,\n        quality=quality,\n        background=background,\n        model=self.model_id,\n    )\n\n    if response_format is not None:\n        if response_format == \"base64\":\n            response_format = \"b64_json\"\n        generation_params.response_format = response_format\n\n    response = await self._agenerate(**generation_params)\n    return response\n</code></pre>"},{"location":"api-reference/models/types/text_to_speech/","title":"Text to speech","text":""},{"location":"api-reference/models/types/text_to_speech/#msgflux.models.providers.openai.OpenAITextToSpeech","title":"OpenAITextToSpeech","text":"<p>               Bases: <code>_BaseOpenAI</code>, <code>TextToSpeechModel</code></p> <p>OpenAI Text to Speech.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAITextToSpeech(_BaseOpenAI, TextToSpeechModel):\n    \"\"\"OpenAI Text to Speech.\"\"\"\n\n    def __init__(\n        self,\n        model_id: str,\n        voice: Optional[str] = \"alloy\",\n        speed: Optional[float] = 1.0,\n        base_url: Optional[str] = None,\n    ):\n        \"\"\"Args:\n        model_id:\n            Model ID in provider.\n        voice:\n            The voice to use when generating the audio.\n        speed:\n            the speed of the generated audio. Select a value\n            from 0.25 to 4.0. 1.0 is the default.\n        base_url:\n            URL to model provider.\n        \"\"\"\n        super().__init__()\n        self.model_id = model_id\n        self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n        self.sampling_run_params = {\n            \"voice\": voice,\n            \"speed\": speed,\n        }\n        self._initialize()\n        self._get_api_key()\n\n    @contextmanager\n    def _execute_model(self, **kwargs):\n        with self.client.audio.speech.with_streaming_response.create(\n            model=self.model_id, **kwargs, **self.sampling_run_params\n        ) as model_output:\n            yield model_output\n\n    @asynccontextmanager\n    async def _aexecute_model(self, **kwargs):\n        async with self.aclient.audio.speech.with_streaming_response.create(\n            model=self.model_id, **kwargs, **self.sampling_run_params\n        ) as model_output:\n            yield model_output\n\n    def _generate(self, **kwargs):\n        response = ModelResponse()\n\n        with self._execute_model(**kwargs) as model_output:\n            with tempfile.NamedTemporaryFile(\n                suffix=f\".{kwargs.get('response_format')}\", delete=False\n            ) as temp_file:\n                temp_file_path = temp_file.name\n                model_output.stream_to_file(temp_file_path)\n\n            response.set_response_type(\"audio_generation\")\n            response.add(temp_file_path)\n\n        return response\n\n    async def _agenerate(self, **kwargs):\n        response = ModelResponse()\n\n        async with self._aexecute_model(**kwargs) as model_output:\n            with tempfile.NamedTemporaryFile(\n                suffix=f\".{kwargs.get('response_format')}\", delete=False\n            ) as temp_file:\n                temp_file_path = temp_file.name\n                await model_output.astream_to_file(temp_file_path)\n\n            response.set_response_type(\"audio_generation\")\n            response.add(temp_file_path)\n\n        return response\n\n    def _stream_generate(self, **kwargs):\n        stream_response = kwargs.pop(\"stream_response\")\n        stream_response.set_response_type(\"audio_generation\")\n\n        with self._execute_model(**kwargs) as model_output:\n            for chunk in model_output.iter_bytes(chunk_size=1024):\n                stream_response.add(chunk)\n                if not stream_response.first_chunk_event.is_set():\n                    stream_response.first_chunk_event.set()\n\n        stream_response.add(None)\n\n    async def _astream_generate(self, **kwargs):\n        stream_response = kwargs.pop(\"stream_response\")\n        stream_response.set_response_type(\"audio_generation\")\n\n        async with self._aexecute_model(**kwargs) as model_output:\n            async for chunk in model_output.aiter_bytes(chunk_size=1024):\n                stream_response.add(chunk)\n                if not stream_response.first_chunk_event.is_set():\n                    stream_response.first_chunk_event.set()\n\n        stream_response.add(None)\n\n    @model_retry\n    def __call__(\n        self,\n        data: str,\n        *,\n        stream: Optional[bool] = False,\n        prompt: Optional[str] = None,\n        response_format: Optional[\n            Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"]\n        ] = \"opus\",\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Args:\n        data:\n            The text to generate audio for.\n        stream:\n            Whether generation should be in streaming mode.\n        prompt:\n            Control the voice of your generated audio with additional instructions.\n        response_format:\n            The format to audio in.\n        \"\"\"\n        params = dotdict({\"input\": data, \"response_format\": response_format})\n        if prompt:\n            params.instructions = prompt\n        if stream:\n            stream_response = ModelStreamResponse()\n            params.stream_response = stream_response\n            F.background_task(self._stream_generate, **params)\n            F.wait_for_event(stream_response.first_chunk_event)\n            return stream_response\n        else:\n            response = self._generate(**params)\n            return response\n\n    @model_retry\n    async def acall(\n        self,\n        data: str,\n        *,\n        stream: Optional[bool] = False,\n        prompt: Optional[str] = None,\n        response_format: Optional[\n            Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"]\n        ] = \"opus\",\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        \"\"\"Async version of __call__. Args:\n        data:\n            The text to generate audio for.\n        stream:\n            Whether generation should be in streaming mode.\n        prompt:\n            Control the voice of your generated audio with additional instructions.\n        response_format:\n            The format to audio in.\n        \"\"\"\n        params = dotdict({\"input\": data, \"response_format\": response_format})\n        if prompt:\n            params.instructions = prompt\n        if stream:\n            stream_response = ModelStreamResponse()\n            params.stream_response = stream_response\n            await F.abackground_task(self._astream_generate, **params)\n            await F.await_for_event(stream_response.first_chunk_event)\n            return stream_response\n        else:\n            response = await self._agenerate(**params)\n            return response\n</code></pre>"},{"location":"api-reference/models/types/text_to_speech/#msgflux.models.providers.openai.OpenAITextToSpeech.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"api-reference/models/types/text_to_speech/#msgflux.models.providers.openai.OpenAITextToSpeech.sampling_params","title":"sampling_params  <code>instance-attribute</code>","text":"<pre><code>sampling_params = {'base_url': base_url or _get_base_url()}\n</code></pre>"},{"location":"api-reference/models/types/text_to_speech/#msgflux.models.providers.openai.OpenAITextToSpeech.sampling_run_params","title":"sampling_run_params  <code>instance-attribute</code>","text":"<pre><code>sampling_run_params = {'voice': voice, 'speed': speed}\n</code></pre>"},{"location":"api-reference/models/types/text_to_speech/#msgflux.models.providers.openai.OpenAITextToSpeech.__call__","title":"__call__","text":"<pre><code>__call__(\n    data,\n    *,\n    stream=False,\n    prompt=None,\n    response_format=\"opus\",\n)\n</code></pre> <p>data:     The text to generate audio for. stream:     Whether generation should be in streaming mode. prompt:     Control the voice of your generated audio with additional instructions. response_format:     The format to audio in.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    data: str,\n    *,\n    stream: Optional[bool] = False,\n    prompt: Optional[str] = None,\n    response_format: Optional[\n        Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"]\n    ] = \"opus\",\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Args:\n    data:\n        The text to generate audio for.\n    stream:\n        Whether generation should be in streaming mode.\n    prompt:\n        Control the voice of your generated audio with additional instructions.\n    response_format:\n        The format to audio in.\n    \"\"\"\n    params = dotdict({\"input\": data, \"response_format\": response_format})\n    if prompt:\n        params.instructions = prompt\n    if stream:\n        stream_response = ModelStreamResponse()\n        params.stream_response = stream_response\n        F.background_task(self._stream_generate, **params)\n        F.wait_for_event(stream_response.first_chunk_event)\n        return stream_response\n    else:\n        response = self._generate(**params)\n        return response\n</code></pre>"},{"location":"api-reference/models/types/text_to_speech/#msgflux.models.providers.openai.OpenAITextToSpeech.__init__","title":"__init__","text":"<pre><code>__init__(model_id, voice='alloy', speed=1.0, base_url=None)\n</code></pre> <p>model_id:     Model ID in provider. voice:     The voice to use when generating the audio. speed:     the speed of the generated audio. Select a value     from 0.25 to 4.0. 1.0 is the default. base_url:     URL to model provider.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    model_id: str,\n    voice: Optional[str] = \"alloy\",\n    speed: Optional[float] = 1.0,\n    base_url: Optional[str] = None,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    voice:\n        The voice to use when generating the audio.\n    speed:\n        the speed of the generated audio. Select a value\n        from 0.25 to 4.0. 1.0 is the default.\n    base_url:\n        URL to model provider.\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    self.sampling_run_params = {\n        \"voice\": voice,\n        \"speed\": speed,\n    }\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"api-reference/models/types/text_to_speech/#msgflux.models.providers.openai.OpenAITextToSpeech.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(\n    data,\n    *,\n    stream=False,\n    prompt=None,\n    response_format=\"opus\",\n)\n</code></pre> <p>Async version of call. Args: data:     The text to generate audio for. stream:     Whether generation should be in streaming mode. prompt:     Control the voice of your generated audio with additional instructions. response_format:     The format to audio in.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    data: str,\n    *,\n    stream: Optional[bool] = False,\n    prompt: Optional[str] = None,\n    response_format: Optional[\n        Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"]\n    ] = \"opus\",\n) -&gt; Union[ModelResponse, ModelStreamResponse]:\n    \"\"\"Async version of __call__. Args:\n    data:\n        The text to generate audio for.\n    stream:\n        Whether generation should be in streaming mode.\n    prompt:\n        Control the voice of your generated audio with additional instructions.\n    response_format:\n        The format to audio in.\n    \"\"\"\n    params = dotdict({\"input\": data, \"response_format\": response_format})\n    if prompt:\n        params.instructions = prompt\n    if stream:\n        stream_response = ModelStreamResponse()\n        params.stream_response = stream_response\n        await F.abackground_task(self._astream_generate, **params)\n        await F.await_for_event(stream_response.first_chunk_event)\n        return stream_response\n    else:\n        response = await self._agenerate(**params)\n        return response\n</code></pre>"},{"location":"api-reference/nn/functional/","title":"Functional","text":""},{"location":"api-reference/nn/functional/#msgflux.nn.functional.scatter_gather","title":"scatter_gather","text":"<pre><code>scatter_gather(\n    to_send,\n    args_list=None,\n    kwargs_list=None,\n    *,\n    timeout=None,\n)\n</code></pre> <p>Sends different sets of arguments/kwargs to a list of modules and collects the responses.</p> <p>Each callable in <code>to_send</code> receives the positional arguments of the corresponding <code>tuple</code> in <code>args_list</code> and the named arguments of the corresponding <code>dict</code> in <code>kwargs_list</code>. If <code>args_list</code> or <code>kwargs_list</code> are not provided (or are <code>None</code>), the corresponding callables will be called without positional or named arguments, respectively, unless an empty list (<code>[]</code>) or empty tuple (<code>()</code>) is provided for a specific item.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>args_list</code> <code>Optional[List[Tuple[Any, ...]]]</code> <p>Each tuple contains the positional argumentsvfor the corresponding callable in <code>to_send</code>. If <code>None</code>, no positional arguments are passed unless specified individually by an item in <code>kwargs_list</code>.</p> <code>None</code> <code>kwargs_list</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Each dictionary contains the named arguments for the corresponding callable in <code>to_send</code>. If <code>None</code>, no named arguments are passed unless specified individually by an item in <code>args_list</code>.</p> <code>None</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing the responses for each callable. If an error or</p> <code>...</code> <p>timeout occurs for a specific callable, its corresponding response</p> <code>Tuple[Any, ...]</code> <p>in the tuple will be <code>None</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a callable list.</p> <code>ValueError</code> <p>If the lengths of <code>args_list</code> (if provided) or <code>kwargs_list</code> (if provided) do not match the length of <code>to_send</code>.</p> <p>Examples:</p> <p>def add(x, y): return x + y def multiply(x, y=2): return x * y callables = [add, multiply, add]</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.scatter_gather--example-1-using-only-args_list","title":"Example 1: Using only args_list","text":"<p>args = [ (1, 2), (3,), (10, 20) ] # multiply will use its default y results = F.scatter_gather(callables, args_list=args) print(results) # (3, 6, 30)</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.scatter_gather--example-2-using-args_list-e-kwargs_list","title":"Example 2: Using args_list e kwargs_list","text":"<p>args = [ (1,), (), (10,) ] kwargs = [ {'y': 2}, {'x': 3, 'y': 3}, {'y': 20} ] results = F.scatter_gather(callables, args_list=args, kwargs_list=kwargs) print(results) # (3, 9, 30)</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.scatter_gather--example-3-using-only-kwargs_list-useful-if-functions-have","title":"Example 3: Using only kwargs_list (useful if functions have","text":""},{"location":"api-reference/nn/functional/#msgflux.nn.functional.scatter_gather--defaults-or-dont-need-positional-args","title":"defaults or don't need positional args)","text":"<p>def greet(name=\"World\"): return f\"Hello, {name}\" def farewell(person_name): return f\"Goodbye, {person_name}\" funcs = [greet, greet, farewell] kwargs_for_funcs = [ {}, {'name': \"Earth\"}, {'person_name': \"Commander\"} ] results = F.scatter_gather(funcs, kwargs_list=kwargs_for_funcs) print(results) # (\"Hello, World\", \"Hello, Earth\", \"Goodbye, Commander\")</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef scatter_gather(\n    to_send: List[Callable],\n    args_list: Optional[List[Tuple[Any, ...]]] = None,\n    kwargs_list: Optional[List[Dict[str, Any]]] = None,\n    *,\n    timeout: Optional[float] = None,\n) -&gt; Tuple[Any, ...]:\n    \"\"\"Sends different sets of arguments/kwargs to a list of modules\n    and collects the responses.\n\n    Each callable in `to_send` receives the positional arguments of\n    the corresponding `tuple` in `args_list` and the named arguments\n    of the corresponding `dict` in `kwargs_list`. If `args_list` or\n    `kwargs_list` are not provided (or are `None`), the corresponding\n    callables will be called without positional or named arguments,\n    respectively, unless an empty list (`[]`) or empty tuple (`()`)\n    is provided for a specific item.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        args_list:\n            Each tuple contains the positional argumentsvfor the corresponding callable\n            in `to_send`. If `None`, no positional arguments are passed unless specified\n            individually by an item in `kwargs_list`.\n        kwargs_list:\n            Each dictionary contains the named arguments for the corresponding callable\n            in `to_send`. If `None`, no named arguments are passed unless specified\n            individually by an item in `args_list`.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n\n    Returns:\n        Tuple containing the responses for each callable. If an error or\n        timeout occurs for a specific callable, its corresponding response\n        in the tuple will be `None`.\n\n    Raises:\n        TypeError:\n            If `to_send` is not a callable list.\n        ValueError:\n            If the lengths of `args_list` (if provided) or `kwargs_list`\n            (if provided) do not match the length of `to_send`.\n\n    Examples:\n        def add(x, y): return x + y\n        def multiply(x, y=2): return x * y\n        callables = [add, multiply, add]\n\n        # Example 1: Using only args_list\n        args = [ (1, 2), (3,), (10, 20) ] # multiply will use its default y\n        results = F.scatter_gather(callables, args_list=args)\n        print(results) # (3, 6, 30)\n\n        # Example 2: Using args_list e kwargs_list\n        args = [ (1,), (), (10,) ]\n        kwargs = [ {'y': 2}, {'x': 3, 'y': 3}, {'y': 20} ]\n        results = F.scatter_gather(callables, args_list=args, kwargs_list=kwargs)\n        print(results) # (3, 9, 30)\n\n        # Example 3: Using only kwargs_list (useful if functions have\n        # defaults or don't need positional args)\n        def greet(name=\"World\"): return f\"Hello, {name}\"\n        def farewell(person_name): return f\"Goodbye, {person_name}\"\n        funcs = [greet, greet, farewell]\n        kwargs_for_funcs = [ {}, {'name': \"Earth\"}, {'person_name': \"Commander\"} ]\n        results = F.scatter_gather(funcs, kwargs_list=kwargs_for_funcs)\n        print(results) # (\"Hello, World\", \"Hello, Earth\", \"Goodbye, Commander\")\n    \"\"\"\n    if not isinstance(to_send, list) or not all(callable(f) for f in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    executor = Executor.get_instance()\n    futures = []\n    for i, f in enumerate(to_send):\n        args = args_list[i] if args_list and i &lt; len(args_list) else ()\n        kwargs = kwargs_list[i] if kwargs_list and i &lt; len(kwargs_list) else {}\n        futures.append(executor.submit(f, *args, **kwargs))\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    responses: List[Any] = []\n    for future in futures:\n        try:\n            responses.append(future.result())\n        except Exception as e:\n            logger.error(str(e))\n            responses.append(None)\n    return tuple(responses)\n</code></pre>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.msg_scatter_gather","title":"msg_scatter_gather","text":"<pre><code>msg_scatter_gather(to_send, messages, *, timeout=None)\n</code></pre> <p>Scatter a list of messages to a list of modules and gather the responses.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>messages</code> <code>List[dotdict]</code> <p>List of <code>msgflux.dotdict</code> instances to be distributed.</p> required <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[dotdict, ...]</code> <p>Tuple containing the messages updated with the responses.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>messages</code> is not a list of <code>dotdict</code>, <code>to_send</code> is not a list of callables, or <code>prefix</code> is not a string.</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef msg_scatter_gather(\n    to_send: List[Callable],\n    messages: List[dotdict],\n    *,\n    timeout: Optional[float] = None,\n) -&gt; Tuple[dotdict, ...]:\n    \"\"\"Scatter a list of messages to a list of modules and gather the responses.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        messages:\n            List of `msgflux.dotdict` instances to be distributed.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n\n    Returns:\n        Tuple containing the messages updated with the responses.\n\n    Raises:\n        TypeError:\n            If `messages` is not a list of `dotdict`, `to_send` is not a list\n            of callables, or `prefix` is not a string.\n    \"\"\"\n    if not messages or not all(isinstance(msg, dotdict) for msg in messages):\n        raise TypeError(\n            \"`messages` must be a non-empty list of `msgflux.dotdict` instances\"\n        )\n\n    if not to_send or not all(isinstance(f, Callable) for f in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    if len(messages) != len(to_send):\n        raise ValueError(\n            f\"The size of `messages` ({len(messages)}) \"\n            f\"must be equal to that of `to_send`: ({len(to_send)})\"\n        )\n\n    executor = Executor.get_instance()\n    futures = [executor.submit(f, msg) for f, msg in zip(to_send, messages)]\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    for f, future in zip(to_send, futures):\n        f_name = get_callable_name(f)\n        try:\n            future.result()\n        except Exception as e:\n            logger.error(f\"Error in scattered task for `{f_name}`: {e}\")\n    return tuple(messages)\n</code></pre>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.bcast_gather","title":"bcast_gather","text":"<pre><code>bcast_gather(to_send, *args, timeout=None, **kwargs)\n</code></pre> <p>Broadcasts arguments to multiple callables and gathers the responses.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>*args</code> <p>Positional arguments.</p> <code>()</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <code>**kwargs</code> <p>Named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Any, ...]</code> <p>Tuple containing the responses.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a list of callables.</p> <p>Examples:</p> <p>def square(x): return x * x def cube(x): return x * x * x def fail(x): raise ValueError(\"Intentional error\")</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.bcast_gather--example-1","title":"Example 1:","text":"<p>results = F.bcast_gather([square, cube], 3) print(results)  # (9, 27)</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.bcast_gather--example-2-simulate-error","title":"Example 2: Simulate error","text":"<p>results = F.bcast_gather([square, fail, cube], 2) print(results)  # (4, None, 8)</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.bcast_gather--example-3-timeout","title":"Example 3: Timeout","text":"<p>results = F.bcast_gather([square, cube], 4, timeout=0.01) print(results) # (16, 64)</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef bcast_gather(\n    to_send: List[Callable], *args, timeout: Optional[float] = None, **kwargs\n) -&gt; Tuple[Any, ...]:\n    \"\"\"Broadcasts arguments to multiple callables and gathers the responses.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        *args:\n            Positional arguments.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n        **kwargs:\n            Named arguments.\n\n    Returns:\n        Tuple containing the responses.\n\n    Raises:\n        TypeError:\n            If `to_send` is not a list of callables.\n\n    Examples:\n        def square(x): return x * x\n        def cube(x): return x * x * x\n        def fail(x): raise ValueError(\"Intentional error\")\n\n        # Example 1:\n        results = F.bcast_gather([square, cube], 3)\n        print(results)  # (9, 27)\n\n        # Example 2: Simulate error\n        results = F.bcast_gather([square, fail, cube], 2)\n        print(results)  # (4, None, 8)\n\n        # Example 3: Timeout\n        results = F.bcast_gather([square, cube], 4, timeout=0.01)\n        print(results) # (16, 64)\n    \"\"\"\n    if not to_send or not all(isinstance(f, Callable) for f in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    executor = Executor.get_instance()\n    futures = [executor.submit(f, *args, **kwargs) for f in to_send]\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    responses: List[Any] = []\n    for future in futures:\n        try:\n            responses.append(future.result())\n        except Exception as e:\n            logger.error(str(e))\n            responses.append(None)\n    return tuple(responses)\n</code></pre>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.msg_bcast_gather","title":"msg_bcast_gather","text":"<pre><code>msg_bcast_gather(to_send, message, *, timeout=None)\n</code></pre> <p>Broadcasts a single message to multiple modules and gathers the responses.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>message</code> <code>dotdict</code> <p>Instance of <code>msgflux.dotdict</code> to broadcast.</p> required <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>dotdict</code> <p>The original message with the module responses added.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>message</code> is not an instance of <code>dotdict</code>, <code>to_send</code> is not a list of callables.</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef msg_bcast_gather(\n    to_send: List[Callable],\n    message: dotdict,\n    *,\n    timeout: Optional[float] = None,\n) -&gt; dotdict:\n    \"\"\"Broadcasts a single message to multiple modules and gathers the responses.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        message:\n            Instance of `msgflux.dotdict` to broadcast.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n\n    Returns:\n        The original message with the module responses added.\n\n    Raises:\n        TypeError:\n            If `message` is not an instance of `dotdict`, `to_send` is not a list\n            of callables.\n    \"\"\"\n    if not isinstance(message, dotdict):\n        raise TypeError(\"`message` must be an instance of `msgflux.dotdict`\")\n    if not to_send or not all(isinstance(module, Callable) for module in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    executor = Executor.get_instance()\n    futures = [executor.submit(f, message) for f in to_send]\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    for f, future in zip(to_send, futures):\n        f_name = get_callable_name(f)\n        try:\n            future.result()\n        except Exception as e:\n            logger.error(f\"Error in scattered task for `{f_name}`: {e}\")\n    return message\n</code></pre>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.wait_for","title":"wait_for","text":"<pre><code>wait_for(to_send, *args, timeout=None, **kwargs)\n</code></pre> <p>Wait for a callable execution.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>Callable</code> <p>A callable object (e.g. functions or <code>Module</code> instances).</p> required <code>*args</code> <p>Positional arguments.</p> <code>()</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <code>**kwargs</code> <p>Named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Callable responses.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a callable.</p> <p>Examples:</p> <p>async def f1(x):     return x * x</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.wait_for--example-1","title":"Example 1:","text":"<p>results = F.wait_for(f1, 3) print(results) # 9</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef wait_for(\n    to_send: Callable, *args, timeout: Optional[float] = None, **kwargs\n) -&gt; Any:\n    \"\"\"Wait for a callable execution.\n\n    Args:\n        to_send:\n            A callable object (e.g. functions or `Module` instances).\n        *args:\n            Positional arguments.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n        **kwargs:\n            Named arguments.\n\n    Returns:\n        Callable responses.\n\n    Raises:\n        TypeError:\n            If `to_send` is not a callable.\n\n    Examples:\n        async def f1(x):\n            return x * x\n\n        # Example 1:\n        results = F.wait_for(f1, 3)\n        print(results) # 9\n    \"\"\"\n    if not callable(to_send):\n        raise TypeError(\"`to_send` must be a callable object\")\n\n    executor = Executor.get_instance()\n    future = executor.submit(to_send, *args, **kwargs)\n    concurrent.futures.wait([future], timeout=timeout)\n    try:\n        return future.result()\n    except Exception as e:\n        logger.error(str(e))\n        return None\n</code></pre>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.wait_for_event","title":"wait_for_event","text":"<pre><code>wait_for_event(event)\n</code></pre> <p>Waits synchronously for an asyncio.Event to be set.</p> <p>This function will block until event.set() is called elsewhere.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The asyncio.Event to wait for.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>event</code> is not an instance of asyncio.Event.</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef wait_for_event(event: asyncio.Event) -&gt; None:\n    \"\"\"Waits synchronously for an asyncio.Event to be set.\n\n    This function will block until event.set() is called elsewhere.\n\n    Args:\n        event: The asyncio.Event to wait for.\n\n    Raises:\n        TypeError: If `event` is not an instance of asyncio.Event.\n    \"\"\"\n    if not isinstance(event, asyncio.Event):\n        raise TypeError(\"`event` must be an instance of asyncio.Event\")\n\n    executor = Executor.get_instance()\n    future = executor._submit_to_async_worker(event.wait())\n    try:\n        future.result()\n    except Exception as e:\n        logger.error(str(e))\n</code></pre>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.background_task","title":"background_task","text":"<pre><code>background_task(to_send, *args, **kwargs)\n</code></pre> <p>Executes a task in the background asynchronously without blocking, using the AsyncExecutorPool. This function is \"fire-and-forget\".</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>Callable</code> <p>Callable object (function, async function, or module with .acall() method).</p> required <code>*args</code> <p>Positional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Named arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a callable.</p> <p>Examples:</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.background_task--example-1","title":"Example 1:","text":"<p>import time def print_message(message: str):     time.sleep(1)     print(f\"[Sync] Message: {message}\") F.background_task(print_message, \"Hello from sync function\")</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.background_task--example-2","title":"Example 2:","text":"<p>import asyncio async def async_print_message(message: str):     await asyncio.sleep(1)     print(f\"[Async] Message: {message}\") F.background_task(async_print_message, \"Hello from async function\")</p>"},{"location":"api-reference/nn/functional/#msgflux.nn.functional.background_task--example-3-with-error","title":"Example 3 (with error):","text":"<p>def failing_task():     raise ValueError(\"This task failed!\") F.background_task(failing_task)  # Error will be logged</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef background_task(to_send: Callable, *args, **kwargs) -&gt; None:\n    \"\"\"Executes a task in the background asynchronously without blocking,\n    using the AsyncExecutorPool. This function is \"fire-and-forget\".\n\n    Args:\n        to_send:\n            Callable object (function, async function, or module with .acall() method).\n        *args:\n            Positional arguments.\n        **kwargs:\n            Named arguments.\n\n    Raises:\n        TypeError: If `to_send` is not a callable.\n\n    Examples:\n        # Example 1:\n        import time\n        def print_message(message: str):\n            time.sleep(1)\n            print(f\"[Sync] Message: {message}\")\n        F.background_task(print_message, \"Hello from sync function\")\n\n        # Example 2:\n        import asyncio\n        async def async_print_message(message: str):\n            await asyncio.sleep(1)\n            print(f\"[Async] Message: {message}\")\n        F.background_task(async_print_message, \"Hello from async function\")\n\n        # Example 3 (with error):\n        def failing_task():\n            raise ValueError(\"This task failed!\")\n        F.background_task(failing_task)  # Error will be logged\n    \"\"\"\n    if not callable(to_send):\n        raise TypeError(\"`to_send` must be a callable object\")\n\n    def log_future(future: Future) -&gt; None:\n        \"\"\"Callback to log exception of a Future.\"\"\"\n        try:\n            future.result()\n        except Exception as e:\n            logger.error(f\"Background task error: {e!s}\", exc_info=True)\n\n    executor = Executor.get_instance()\n    future = executor.submit(to_send, *args, **kwargs)\n    future.add_done_callback(log_future)\n</code></pre>"},{"location":"api-reference/nn/modules/agent/","title":"Agent","text":""},{"location":"api-reference/nn/modules/agent/#msgflux.nn.modules.agent.Agent","title":"Agent","text":"<p>               Bases: <code>Module</code></p> <p>Agent is a Module type that uses language models to solve tasks.</p> <p>An Agent can perform actions in an environment using tools calls. For an Agent, a tool is any callable object.</p> <p>An Agent can handle multimodal inputs and outputs.</p> Source code in <code>src/msgflux/nn/modules/agent.py</code> <pre><code>class Agent(Module, metaclass=AutoParams):\n    \"\"\"Agent is a Module type that uses language models to solve tasks.\n\n    An Agent can perform actions in an environment using tools calls.\n    For an Agent, a tool is any callable object.\n\n    An Agent can handle multimodal inputs and outputs.\n    \"\"\"\n\n    # Configure AutoParams to use docstring as 'description' parameter\n    _autoparams_use_docstring_for = \"description\"\n    # Configure AutoParams to use class name as 'name' parameter\n    _autoparams_use_classname_for = \"name\"\n\n    _supported_outputs: List[str] = [\n        \"reasoning_structured\",\n        \"reasoning_text_generation\",\n        \"structured\",\n        \"text_generation\",\n        \"audio_generation\",\n        \"audio_text_generation\",\n        \"tool_responses\",\n    ]\n\n    def __init__(\n        self,\n        name: str,\n        model: Union[ChatCompletionModel, ModelGateway, LM],\n        *,\n        system_message: Optional[str] = None,\n        instructions: Optional[str] = None,\n        expected_output: Optional[str] = None,\n        examples: Optional[Union[str, List[Union[Example, Mapping[str, Any]]]]] = None,\n        system_extra_message: Optional[str] = None,\n        guardrails: Optional[Dict[str, Callable]] = None,\n        message_fields: Optional[Dict[str, Any]] = None,\n        config: Optional[Dict[str, Any]] = None,\n        templates: Optional[Dict[str, str]] = None,\n        context_cache: Optional[str] = None,\n        prefilling: Optional[str] = None,\n        generation_schema: Optional[msgspec.Struct] = None,\n        typed_parser: Optional[str] = None,\n        response_mode: Optional[str] = \"plain_response\",\n        tools: Optional[List[Callable]] = None,\n        mcp_servers: Optional[List[Mapping[str, Any]]] = None,\n        fixed_messages: Optional[List[Mapping[str, Any]]] = None,\n        signature: Optional[Union[str, Signature]] = None,\n        description: Optional[str] = None,\n        annotations: Optional[Mapping[str, type]] = None,\n    ):\n        \"\"\"Args:\n        name:\n            Agent name in snake case format.\n        model:\n            Chat Completation Model client.\n        system_message:\n            The Agent behaviour.\n        instructions:\n            What the Agent should do.\n        expected_output:\n            What the response should be like.\n\n        Examples:\n            Examples of inputs, reasoning and outputs.\n        system_extra_message:\n            An extra message in system prompt.\n        guardrails:\n            Dictionary mapping guardrail types to callables.\n            Valid keys: \"input\", \"output\"\n            !!! example\n                guardrails={\"input\": input_checker, \"output\": output_checker}\n        message_fields:\n            Dictionary mapping Message field names to their paths in the Message object.\n            Valid keys: \"task_inputs\", \"task_multimodal_inputs\", \"model_state\",\n            \"context_inputs\", \"model_preference\", \"vars\"\n            !!! example\n                message_fields={\n                    \"task_inputs\": \"input.user\",\n                    \"task_multimodal_inputs\": {\"audio\": \"audio.user\"},\n                    \"model_state\": \"messages.history\",\n                    \"context_inputs\": \"context.data\",\n                    \"model_preference\": \"model.preference\",\n                    \"vars\": \"vars.data\"\n                }\n\n            Field descriptions:\n            - task_inputs: Field path for task input (str, dict, or tuple)\n            - task_multimodal_inputs: Map datatype (image, video, audio, file)\n              to field paths\n            - model_state: Field path for list of chats in ChatML format\n            - context_inputs: Field path for context (str or list of str)\n            - model_preference: Field path for model preference (str, only valid\n              with ModelGateway)\n            - vars: Field path for inputs to templates and tools (str)\n        config:\n            Dictionary with configuration options.\n            Valid keys: \"verbose\", \"return_model_state\", \"tool_choice\",\n            \"stream\", \"image_block_kwargs\", \"video_block_kwargs\", \"include_date\"\n            !!! example\n                config={\n                    \"verbose\": True,\n                    \"return_model_state\": False,\n                    \"tool_choice\": \"auto\",\n                    \"stream\": False,\n                    \"image_block_kwargs\": {\"detail\": \"high\"},\n                    \"video_block_kwargs\": {\"format\": \"mp4\"},\n                    \"include_date\": False\n                }\n\n            Configuration options:\n            - verbose: Print model output and tool calls to console (bool)\n            - return_model_state: Return dict with model_state and response (bool)\n            - tool_choice: Control tool selection (\"auto\", \"required\", or function name)\n            - stream: Transmit response on-the-fly (bool)\n            - image_block_kwargs: Dict of kwargs to pass to ChatBlock.image\n              (e.g., {\"detail\": \"high\"})\n            - video_block_kwargs: Dict of kwargs to pass to ChatBlock.video\n              (e.g., {\"format\": \"mp4\"})\n            - include_date: Include current date with weekday in system prompt\n              (bool). Format: \"Weekday, Month DD, YYYY\" (e.g., \"Monday, December 09, 2025\")\n        templates:\n            Dictionary mapping template types to Jinja template strings.\n            Valid keys: \"task\", \"response\", \"context\", \"system_prompt\"\n            !!! example\n                templates={\n                    \"task\": \"Who was {{person}}?\",\n                    \"response\": \"{{final_answer}}\",\n                    \"context\": \"Context: {{context}}\",\n                    \"system_prompt\": \"Custom system prompt: {% if system_message %}{{ system_message }}{% endif %}\"\n                }\n\n            Template descriptions:\n            - task: Formats the task/prompt sent to the model\n            - response: Formats the model's response\n            - context: Formats context_inputs (does NOT apply to context_cache)\n            - system_prompt: Overrides the default system prompt template. If not provided,\n              uses SYSTEM_PROMPT_TEMPLATE. Available variables: system_message, instructions,\n              expected_output, examples, system_extra_message, current_date (if include_date=True)\n        context_cache:\n            A fixed context.\n        prefilling:\n            Forces an initial message from the model. From that message it\n            will continue its response from there.\n        generation_schema:\n            Schema that defines how the output should be structured.\n        typed_parser:\n            Converts the model raw output into a typed-dict. Supported parser:\n            `typed_xml`.\n        response_mode:\n            What the response should be.\n            * `plain_response` (default): Returns the final agent response directly.\n            * other: Write on field in Message object.\n        tools:\n            A list of callable objects.\n        mcp_servers:\n            List of MCP (Model Context Protocol) server configurations.\n            Each config should contain:\n            - name: Namespace for tools from this server\n            - transport: \"stdio\" or \"http\"\n            - For stdio: command, args, cwd, env\n            - For http: base_url, headers\n            - Optional: include_tools, exclude_tools, tool_config\n            !!! example\n                mcp_servers=[{\n                    \"name\": \"fs\",\n                    \"transport\": \"stdio\",\n                    \"command\": \"npx\",\n                    \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\"],\n                    \"include_tools\": [\"read_file\", \"write_file\"],\n                    \"tool_config\": {\"read_file\": {\"inject_vars\": [\"context\"]}}\n                }]\n        fixed_messages:\n            A fixed list of chats in ChatML format.\n        signature:\n            A DSPy-based signature. A signature creates a task_template,\n            a generation_schema, instructions and examples (both if passed).\n            Can be combined with standard generation_schemas like `ReAct` and\n            `ChainOfThought`. Can also be combined with `typed_parser`.\n        description:\n            The Agent description. It's useful when using an agent-as-a-tool.\n        annotations\n            Define the input and output annotations to use the agent-as-a-function.\n        \"\"\"\n        if annotations is None:\n            annotations = {\"message\": str, \"return\": str}\n\n        # Validate that signature and custom annotations are not both provided\n        if signature is not None and annotations != {\"message\": str, \"return\": str}:\n            raise ValueError(\n                \"Cannot specify both 'signature' and custom 'annotations'. \"\n                \"When using a signature, annotations are generated automatically \"\n                \"from the signature inputs. Remove the 'annotations' parameter.\"\n            )\n\n        super().__init__()\n        self.set_name(name)\n        self.set_description(description)\n\n        # Only set annotations if signature is not provided\n        # (signature will set annotations automatically in _set_signature)\n        if signature is None:\n            self.set_annotations(annotations)\n        else:\n            # Set default temporarily, will be overridden by _set_signature\n            self.set_annotations({\"message\": str, \"return\": str})\n\n        self._set_config(config)\n\n        stream = config.get(\"stream\", False) if config else False\n\n        if stream is True:\n            if generation_schema is not None:\n                raise ValueError(\"`generation_schema` is not `stream=True` compatible\")\n\n            if guardrails is not None and \"output\" in guardrails:\n                raise ValueError(\n                    \"`guardrails['output']` is not `stream=True` compatible\"\n                )\n\n            if templates is not None and templates.get(\"response\") is not None:\n                raise ValueError(\n                    \"`templates['response']` is not `stream=True` compatible\"\n                )\n\n            if typed_parser is not None:\n                raise ValueError(\"`typed_parser` is not `stream=True` compatible\")\n\n        self._set_context_cache(context_cache)\n        self._set_fixed_messages(fixed_messages)\n        self._set_guardrails(guardrails)\n        self._set_message_fields(message_fields)\n        self._set_model(model)\n        self._set_prefilling(prefilling)\n        self._set_system_extra_message(system_extra_message)\n        self._set_response_mode(response_mode)\n        self._set_templates(templates)\n        self._set_tools(tools, mcp_servers)\n\n        if signature is not None:\n            signature_params = dotdict(\n                signature=signature,\n                examples=examples,\n                instructions=instructions,\n                system_message=system_message,\n                typed_parser=typed_parser,\n            )\n            if generation_schema is not None:\n                signature_params.generation_schema = generation_schema\n            self._set_signature(**signature_params)\n        else:\n            self._set_typed_parser(typed_parser)\n            self._set_examples(examples)\n            self._set_generation_schema(generation_schema)\n            self._set_expected_output(expected_output)\n            self._set_instructions(instructions)\n            self._set_system_message(system_message)\n\n    def forward(\n        self, message: Optional[Union[str, Mapping[str, Any], Message]] = None, **kwargs\n    ) -&gt; Union[str, Mapping[str, None], ModelStreamResponse, Message]:\n        \"\"\"Execute the agent with the given message.\n\n        Args:\n            message: The input message, which can be:\n                - str: Direct task input (used as task_inputs)\n                - Message: Message object with fields mapped via message_fields.\n                  Requires message_fields configuration, e.g.:\n                  message_fields={\"task_inputs\": \"input.user\"}\n                - dict: Task inputs as a dictionary\n                - None: When using named task arguments (see below)\n            **kwargs: Can include:\n                - Reserved kwargs (runtime overrides for message_fields):\n                    - task_multimodal_inputs: Override multimodal inputs\n                    - model_state: Override chat messages (model state)\n                    - context_inputs: Override context\n                    - model_preference: Override model preference\n                    - vars: Override template/tool variables\n                - Named task arguments: When message=None and a task template is\n                  configured, any other kwargs are treated as task inputs.\n                  Example: agent(name=\"Vilson\", age=27)\n                  This is useful when using agents as tools with typed annotations.\n\n        Returns:\n            Agent response (str, Message, or ModelStreamResponse depending on\n            configuration)\n\n        Raises:\n            ValueError: If both message and named task arguments are provided,\n                or if named arguments are used without a task template.\n\n        Examples:\n            &gt;&gt;&gt; # String input\n            &gt;&gt;&gt; agent(\"What is the weather?\")\n\n            &gt;&gt;&gt; # Dict input\n            &gt;&gt;&gt; agent({\"city\": \"Natal\"})\n\n            &gt;&gt;&gt; # Message input (requires message_fields configuration)\n            &gt;&gt;&gt; agent_with_message = Agent(\n            ...     model=model,\n            ...     message_fields={\"task_inputs\": \"user.query\"}\n            ... )\n            &gt;&gt;&gt; msg = Message()\n            &gt;&gt;&gt; msg.set(\"user.query\", \"Hello\")\n            &gt;&gt;&gt; agent_with_message(msg)\n\n            &gt;&gt;&gt; # Named arguments (requires task template)\n            &gt;&gt;&gt; agent = Agent(\n            ...     model=model,\n            ...     templates={\"task\": \"Greet {{name}} who is {{age}} years old\"},\n            ... )\n            &gt;&gt;&gt; agent(name=\"Vilson\", age=27)\n        \"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = self._execute_model(prefilling=self.prefilling, **inputs)\n        response = self._process_model_response(message, model_response, **inputs)\n        return response\n\n    async def aforward(\n        self, message: Optional[Union[str, Mapping[str, Any], Message]] = None, **kwargs\n    ) -&gt; Union[str, Mapping[str, None], ModelStreamResponse, Message]:\n        \"\"\"Async version of forward.\"\"\"\n        inputs = await self._aprepare_task(message, **kwargs)\n        model_response = await self._aexecute_model(\n            prefilling=self.prefilling, **inputs\n        )\n        response = await self._aprocess_model_response(\n            message, model_response, **inputs\n        )\n        return response\n\n    def _execute_model(\n        self,\n        model_state: List[Mapping[str, Any]],\n        vars: Mapping[str, Any],\n        prefilling: Optional[str] = None,\n        model_preference: Optional[str] = None,\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        model_execution_params = self._prepare_model_execution(\n            model_state=model_state,\n            prefilling=prefilling,\n            model_preference=model_preference,\n            vars=vars,\n        )\n        if self.guardrails.get(\"input\"):\n            self._execute_input_guardrail(model_execution_params)\n        if self.config.get(\"verbose\", False):\n            cprint(f\"[{self.name}][call_model]\", bc=\"br1\", ls=\"b\")\n        model_response = self.lm(**model_execution_params)\n        return model_response\n\n    async def _aexecute_model(\n        self,\n        model_state: List[Mapping[str, Any]],\n        vars: Mapping[str, Any],\n        prefilling: Optional[str] = None,\n        model_preference: Optional[str] = None,\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        model_execution_params = self._prepare_model_execution(\n            model_state=model_state,\n            prefilling=prefilling,\n            model_preference=model_preference,\n            vars=vars,\n        )\n        if self.guardrails.get(\"input\"):\n            await self._aexecute_input_guardrail(model_execution_params)\n        if self.config.get(\"verbose\", False):\n            cprint(f\"[{self.name}][call_model]\", bc=\"br1\", ls=\"b\")\n        model_response = await self.lm.acall(**model_execution_params)\n        return model_response\n\n    def _prepare_model_execution(\n        self,\n        model_state: List[Mapping[str, Any]],\n        vars: Mapping[str, Any],\n        prefilling: Optional[str] = None,\n        model_preference: Optional[str] = None,\n    ) -&gt; Mapping[str, Any]:\n        # model_state, prefilling, model_preference, vars\n        agent_state = []\n\n        if self.fixed_messages:\n            agent_state.extend(self.fixed_messages)\n\n        agent_state.extend(model_state)\n\n        system_prompt = self._get_system_prompt(vars)\n\n        tool_schemas = self.tool_library.get_tool_json_schemas()\n        if not tool_schemas:\n            tool_schemas = None\n\n        tool_choice = self.config.get(\"tool_choice\")\n\n        if is_subclass_of(self.generation_schema, ToolFlowControl) and tool_schemas:\n            tools_template = self.generation_schema.tools_template\n            inputs = {\"tool_schemas\": tool_schemas, \"tool_choice\": tool_choice}\n            flow_control_tools = self._format_template(inputs, tools_template)\n            if system_prompt:\n                system_prompt = flow_control_tools + \"\\n\\n\" + system_prompt\n            else:\n                system_prompt = flow_control_tools\n            tool_schemas = None  # Disable tool_schemas to controlflow preference\n            tool_choice = None  # Disable tool_choice to controlflow preference\n\n        model_execution_params = dotdict(\n            messages=agent_state,\n            system_prompt=system_prompt or None,\n            prefilling=prefilling,\n            stream=self.config.get(\"stream\", False),\n            tool_schemas=tool_schemas,\n            tool_choice=tool_choice,\n            generation_schema=self.generation_schema,\n            typed_parser=self.typed_parser,\n        )\n\n        if model_preference:\n            model_execution_params.model_preference = model_preference\n\n        return model_execution_params\n\n    def _prepare_input_guardrail_execution(\n        self, model_execution_params: Mapping[str, Any]\n    ) -&gt; Mapping[str, Any]:\n        messages = model_execution_params.get(\"messages\")\n        last_message = messages[-1]\n        if isinstance(last_message.get(\"content\"), list):\n            if last_message.get(\"content\")[0][\"type\"] == \"image_url\":\n                data = [last_message]\n            else:  # audio, file\n                data = last_message.get(\"content\")[-1]  # text input\n        else:\n            data = last_message.get(\"content\")\n        guardrail_params = {\"data\": data}\n        return guardrail_params\n\n    def _process_model_response(\n        self,\n        message: Union[str, Mapping[str, Any], Message],\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        model_state: List[Mapping[str, Any]],\n        vars: Mapping[str, Any],\n        model_preference: Optional[str] = None,\n    ) -&gt; Union[str, Mapping[str, Any], Message, ModelStreamResponse]:\n        if \"tool_call\" in model_response.response_type:\n            model_response, model_state = self._process_tool_call_response(\n                model_response, model_state, vars, model_preference\n            )\n        elif is_subclass_of(self.generation_schema, ToolFlowControl):\n            model_response, model_state = self._process_tool_flow_control_response(\n                model_response, model_state, vars, model_preference\n            )\n\n        if isinstance(model_response, (ModelResponse, ModelStreamResponse)):\n            raw_response = self._extract_raw_response(model_response)\n            response_type = model_response.response_type\n        else:  # returns tool result as response or tool call as response\n            raw_response = model_response\n            response_type = \"tool_responses\"\n\n        if response_type in self._supported_outputs:\n            response = self._prepare_response(\n                raw_response, response_type, model_state, message, vars\n            )\n            return response\n        else:\n            raise ValueError(f\"Unsupported `response_type={response_type}`\")\n\n    async def _aprocess_model_response(\n        self,\n        message: Union[str, Mapping[str, Any], Message],\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        model_state: List[Mapping[str, Any]],\n        vars: Mapping[str, Any],\n        model_preference: Optional[str] = None,\n    ) -&gt; Union[str, Mapping[str, Any], Message, ModelStreamResponse]:\n        if \"tool_call\" in model_response.response_type:\n            model_response, model_state = await self._aprocess_tool_call_response(\n                model_response, model_state, vars, model_preference\n            )\n        elif is_subclass_of(self.generation_schema, ToolFlowControl):\n            (\n                model_response,\n                model_state,\n            ) = await self._aprocess_tool_flow_control_response(\n                model_response, model_state, vars, model_preference\n            )\n\n        if isinstance(model_response, (ModelResponse, ModelStreamResponse)):\n            raw_response = self._extract_raw_response(model_response)\n            response_type = model_response.response_type\n        else:  # returns tool result as response or tool call as response\n            raw_response = model_response\n            response_type = \"tool_responses\"\n\n        if response_type in self._supported_outputs:\n            response = await self._aprepare_response(\n                raw_response, response_type, model_state, message, vars\n            )\n            return response\n        else:\n            raise ValueError(f\"Unsupported `response_type={response_type}`\")\n\n    def _process_tool_flow_control_response(\n        self,\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        model_state: Mapping[str, Any],\n        vars: Mapping[str, Any],\n        model_preference: Optional[str] = None,\n    ) -&gt; Tuple[Union[str, Mapping[str, Any], ModelStreamResponse], Mapping[str, Any]]:\n        \"\"\"Handle the fields returned by `ReAct`. If the fields are different,\n        you must rewrite this function.\n        \"\"\"\n        while True:\n            raw_response = self._extract_raw_response(model_response)\n\n            if getattr(raw_response, \"final_answer\", None):\n                return model_response, model_state\n\n            if getattr(raw_response, \"current_step\", None):\n                step = raw_response.current_step\n                actions = step.actions\n                reasoning = step.thought\n\n                if self.config.get(\"verbose\", False):\n                    repr_str = f\"[{self.name}][tool_calls_reasoning] {reasoning}\"\n                    cprint(repr_str, bc=\"br2\", ls=\"b\")\n\n                for act in actions:\n                    act.id = str(uuid4())  # Add tool_id\n\n                tool_callings = [(act.id, act.name, act.arguments) for act in actions]\n                tool_results = self._process_tool_call(tool_callings, model_state, vars)\n\n                if tool_results.return_directly:\n                    tool_calls = tool_results.to_dict().pop(\"return_directly\")\n                    tool_calls[\"reasoning\"] = reasoning\n                    tool_responses = dotdict(tool_responses=tool_calls)\n                    # TODO converter tool calls em tool call msgs\n                    return tool_responses, model_state\n\n                for act in actions:  # Add results\n                    result = tool_results.get_by_id(act.id).result\n                    error = tool_results.get_by_id(act.id).error\n                    act.result = result or error\n\n                # Compact steps history\n                if model_state and model_state[-1].get(\"role\") == \"assistant\":\n                    last_react_msg = model_state[-1].get(\"content\")\n                    react_state = msgspec.json.decode(last_react_msg)\n                    react_state.append(raw_response)\n                    model_state[-1] = ChatBlock.assist(react_state)\n                else:\n                    react_state = [raw_response]\n                    model_state.append(ChatBlock.assist(react_state))\n\n            model_response = self._execute_model(\n                model_state=model_state, model_preference=model_preference, vars=vars\n            )\n\n    async def _aprocess_tool_flow_control_response(\n        self,\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        model_state: Mapping[str, Any],\n        vars: Mapping[str, Any],\n        model_preference: Optional[str] = None,\n    ) -&gt; Tuple[Union[str, Mapping[str, Any], ModelStreamResponse], Mapping[str, Any]]:\n        \"\"\"Async version of _process_tool_flow_control_response.\n        Handle the fields returned by `ReAct`. If the fields are different,\n        you must rewrite this function.\n        \"\"\"\n        while True:\n            raw_response = self._extract_raw_response(model_response)\n\n            if getattr(raw_response, \"final_answer\", None):\n                return model_response, model_state\n\n            if getattr(raw_response, \"current_step\", None):\n                step = raw_response.current_step\n                actions = step.actions\n                reasoning = step.thought\n\n                if self.config.get(\"verbose\", False):\n                    repr_str = f\"[{self.name}][tool_calls_reasoning] {reasoning}\"\n                    cprint(repr_str, bc=\"br2\", ls=\"b\")\n\n                for act in actions:\n                    act.id = str(uuid4())  # Add tool_id\n\n                tool_callings = [(act.id, act.name, act.arguments) for act in actions]\n                tool_results = await self._aprocess_tool_call(\n                    tool_callings, model_state, vars\n                )\n\n                if tool_results.return_directly:\n                    tool_calls = tool_results.to_dict().pop(\"return_directly\")\n                    tool_calls[\"reasoning\"] = reasoning\n                    tool_responses = dotdict(tool_responses=tool_calls)\n                    # TODO converter tool calls em tool call msgs\n                    return tool_responses, model_state\n\n                for act in actions:  # Add results\n                    result = tool_results.get_by_id(act.id).result\n                    error = tool_results.get_by_id(act.id).error\n                    act.result = result or error\n\n                # Compact steps history\n                if model_state and model_state[-1].get(\"role\") == \"assistant\":\n                    last_react_msg = model_state[-1].get(\"content\")\n                    react_state = msgspec.json.decode(last_react_msg)\n                    react_state.append(raw_response)\n                    model_state[-1] = ChatBlock.assist(react_state)\n                else:\n                    react_state = [raw_response]\n                    model_state.append(ChatBlock.assist(react_state))\n\n            model_response = await self._aexecute_model(\n                model_state=model_state, model_preference=model_preference, vars=vars\n            )\n\n    def _process_tool_call_response(\n        self,\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        model_state: Mapping[str, Any],\n        vars: Mapping[str, Any],\n        model_preference: Optional[str] = None,\n    ) -&gt; Tuple[Union[str, Mapping[str, Any], ModelStreamResponse], Mapping[str, Any]]:\n        \"\"\"ToolCall example:\n        [{'role': 'assistant', 'tool_responses': [{'id': 'call_1YL',\n        'type': 'function', 'function': {'arguments': '{\"order_id\":\"order_12345\"}',\n        'name': 'get_delivery_date'}}]}, {'role': 'tool', 'tool_call_id': 'call_HA',\n        'content': '2024-10-15'}].\n        \"\"\"\n        while True:\n            if model_response.response_type == \"tool_call\":\n                raw_response = model_response.data\n                reasoning = raw_response.reasoning\n\n                if self.config.get(\"verbose\", False):\n                    if reasoning:\n                        repr_str = f\"[{self.name}][tool_calls_reasoning] {reasoning}\"\n                        cprint(repr_str, bc=\"br2\", ls=\"b\")\n\n                tool_callings = raw_response.get_calls()\n                tool_results = self._process_tool_call(tool_callings, model_state, vars)\n\n                if tool_results.return_directly:\n                    tool_calls = tool_results.to_dict()\n                    tool_calls.pop(\"return_directly\")\n                    tool_calls[\"reasoning\"] = reasoning\n                    tool_responses = dotdict(tool_responses=tool_calls)\n                    return tool_responses, model_state\n\n                id_results = {\n                    call.id: call.result or call.error\n                    for call in tool_results.tool_calls\n                }\n                raw_response.insert_results(id_results)\n                tool_responses_message = raw_response.get_messages()\n                model_state.extend(tool_responses_message)\n            else:\n                return model_response, model_state\n\n            model_response = self._execute_model(\n                model_state=model_state, model_preference=model_preference, vars=vars\n            )\n\n    async def _aprocess_tool_call_response(\n        self,\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        model_state: Mapping[str, Any],\n        vars: Mapping[str, Any],\n        model_preference: Optional[str] = None,\n    ) -&gt; Tuple[Union[str, Mapping[str, Any], ModelStreamResponse], Mapping[str, Any]]:\n        \"\"\"Async version of _process_tool_call_response.\n        ToolCall example: [{'role': 'assistant', 'tool_responses': [{'id': 'call_1YL',\n        'type': 'function', 'function': {'arguments': '{\"order_id\":\"order_12345\"}',\n        'name': 'get_delivery_date'}}]}, {'role': 'tool', 'tool_call_id': 'call_HA',\n        'content': '2024-10-15'}].\n        \"\"\"\n        while True:\n            if model_response.response_type == \"tool_call\":\n                raw_response = model_response.data\n                reasoning = raw_response.reasoning\n\n                if self.config.get(\"verbose\", False):\n                    if reasoning:\n                        repr_str = f\"[{self.name}][tool_calls_reasoning] {reasoning}\"\n                        cprint(repr_str, bc=\"br2\", ls=\"b\")\n\n                tool_callings = raw_response.get_calls()\n                tool_results = await self._aprocess_tool_call(\n                    tool_callings, model_state, vars\n                )\n\n                if tool_results.return_directly:\n                    tool_calls = tool_results.to_dict()\n                    tool_calls.pop(\"return_directly\")\n                    tool_calls[\"reasoning\"] = reasoning\n                    tool_responses = dotdict(tool_responses=tool_calls)\n                    return tool_responses, model_state\n\n                id_results = {\n                    call.id: call.result or call.error\n                    for call in tool_results.tool_calls\n                }\n                raw_response.insert_results(id_results)\n                tool_responses_message = raw_response.get_messages()\n                model_state.extend(tool_responses_message)\n            else:\n                return model_response, model_state\n\n            model_response = await self._aexecute_model(\n                model_state=model_state, model_preference=model_preference, vars=vars\n            )\n\n    def _process_tool_call(\n        self,\n        tool_callings: Mapping[str, Any],\n        model_state: List[Mapping[str, Any]],\n        vars: Mapping[str, Any],\n    ) -&gt; ToolResponses:\n        if self.config.get(\"verbose\", False):\n            for call in tool_callings:\n                repr_str = f\"[{self.name}][tool_call] {call[1]}: {call[2]}\"\n                cprint(repr_str, bc=\"br2\", ls=\"b\")\n        tool_results = self.tool_library(\n            tool_callings=tool_callings,\n            model_state=model_state,\n            vars=vars,\n        )\n        if self.config.get(\"verbose\", False):\n            repr_str = f\"[{self.name}][tool_responses]\"\n            if tool_results.return_directly:\n                repr_str += \" return directly\"\n            cprint(repr_str, bc=\"br1\", ls=\"b\")\n            for call in tool_results.tool_calls:\n                result = call.result or call.error or \"\"\n                repr_str = f\"[{self.name}][tool_response] {call.name}: {result}\"\n                cprint(repr_str, ls=\"b\")\n        return tool_results\n\n    async def _aprocess_tool_call(\n        self,\n        tool_callings: Mapping[str, Any],\n        model_state: List[Mapping[str, Any]],\n        vars: Mapping[str, Any],\n    ) -&gt; ToolResponses:\n        \"\"\"Async version of _process_tool_call.\"\"\"\n        if self.config.get(\"verbose\", False):\n            for call in tool_callings:\n                repr_str = f\"[{self.name}][tool_call] {call[1]}: {call[2]}\"\n                cprint(repr_str, bc=\"br2\", ls=\"b\")\n        tool_results = await self.tool_library.acall(\n            tool_callings=tool_callings,\n            model_state=model_state,\n            vars=vars,\n        )\n        if self.config.get(\"verbose\", False):\n            repr_str = f\"[{self.name}][tool_responses]\"\n            if tool_results.return_directly:\n                repr_str += \" return directly\"\n            cprint(repr_str, bc=\"br1\", ls=\"b\")\n            for call in tool_results.tool_calls:\n                result = call.result or call.error or \"\"\n                repr_str = f\"[{self.name}][tool_response] {call.name}: {result}\"\n                cprint(repr_str, ls=\"b\")\n        return tool_results\n\n    def _prepare_response(\n        self,\n        raw_response: Union[str, Mapping[str, Any], ModelStreamResponse],\n        response_type: str,\n        model_state: List[Mapping[str, Any]],\n        message: Union[str, Mapping[str, Any], Message],\n        vars: Mapping[str, Any],\n    ) -&gt; Union[str, Mapping[str, Any], ModelStreamResponse]:\n        formatted_response = None\n        if not isinstance(raw_response, ModelStreamResponse):\n            if response_type == \"text_generation\" or \"structured\" in response_type:\n                if self.config.get(\"verbose\", False):\n                    cprint(f\"[{self.name}][response] {raw_response}\", bc=\"y\", ls=\"b\")\n                if self.guardrails.get(\"output\"):\n                    self._execute_output_guardrail(raw_response)\n                if self.templates.get(\"response\"):\n                    if isinstance(raw_response, str):\n                        pre_response = self._format_response_template(vars)\n                        formatted_response = self._format_template(\n                            raw_response, pre_response\n                        )\n                    elif isinstance(raw_response, dict):\n                        raw_response.update(vars)\n                        formatted_response = self._format_response_template(\n                            raw_response\n                        )\n\n        response = formatted_response or raw_response\n        if self.config.get(\"return_model_state\", False):\n            if response_type == \"tool_responses\":\n                response.model_state = model_state\n            else:\n                response = dotdict(agent_response=response, model_state=model_state)\n        return self._define_response_mode(response, message)\n\n    async def _aprepare_response(\n        self,\n        raw_response: Union[str, Mapping[str, Any], ModelStreamResponse],\n        response_type: str,\n        model_state: List[Mapping[str, Any]],\n        message: Union[str, Mapping[str, Any], Message],\n        vars: Mapping[str, Any],\n    ) -&gt; Union[str, Mapping[str, Any], ModelStreamResponse]:\n        \"\"\"Async version of _prepare_response with async output guardrail support.\"\"\"\n        formatted_response = None\n        if not isinstance(raw_response, ModelStreamResponse):\n            if response_type == \"text_generation\" or \"structured\" in response_type:\n                if self.config.get(\"verbose\", False):\n                    cprint(f\"[{self.name}][response] {raw_response}\", bc=\"y\", ls=\"b\")\n                if self.guardrails.get(\"output\"):\n                    await self._aexecute_output_guardrail(raw_response)\n                if self.templates.get(\"response\"):\n                    if isinstance(raw_response, str):\n                        pre_response = self._format_response_template(vars)\n                        formatted_response = self._format_template(\n                            raw_response, pre_response\n                        )\n                    elif isinstance(raw_response, dict):\n                        raw_response.update(vars)\n                        formatted_response = self._format_response_template(\n                            raw_response\n                        )\n\n        response = formatted_response or raw_response\n        if self.config.get(\"return_model_state\", False):\n            if response_type == \"tool_responses\":\n                response.model_state = model_state\n            else:\n                response = dotdict(model_response=response, model_state=model_state)\n        return self._define_response_mode(response, message)\n\n    def _prepare_output_guardrail_execution(\n        self, model_response: Union[str, Mapping[str, Any]]\n    ) -&gt; Mapping[str, Any]:\n        if isinstance(model_response, str):\n            data = model_response\n        else:\n            data = str(model_response)\n        guardrail_params = {\"data\": data}\n        return guardrail_params\n\n    def _prepare_task(\n        self, message: Optional[Union[str, Message, Mapping[str, Any]]] = None, **kwargs\n    ) -&gt; Mapping[str, Any]:\n        \"\"\"Prepare model input in ChatML format and execution params.\"\"\"\n        # Extract reserved kwargs\n        vars = kwargs.pop(\"vars\", {})\n        model_state = kwargs.pop(\"model_state\", [])\n        model_preference = kwargs.pop(\"model_preference\", None)\n\n        # Get remaining kwargs (potential task inputs)\n        remaining_kwargs = {\n            k: v for k, v in kwargs.items() if k not in _RESERVED_KWARGS\n        }\n\n        # Handle named task arguments\n        if message is None and remaining_kwargs:\n            if not self.templates.get(\"task\"):\n                raise ValueError(\n                    f\"Named task arguments require a 'task' template to be configured. \"\n                    f\"Received kwargs: {list(remaining_kwargs.keys())}. \"\n                    f\"Either configure a task template or pass arguments as: \"\n                    f\"agent({{'key': 'value'}}) or agent(Message(...))\"\n                )\n            # Convert named kwargs to dict for template rendering\n            message = remaining_kwargs\n            # Clear kwargs to avoid passing them down\n            for key in remaining_kwargs:\n                kwargs.pop(key)\n        elif message is not None and remaining_kwargs:\n            raise ValueError(\n                f\"Cannot pass both 'message' argument and named task arguments. \"\n                f\"Received message={type(message).__name__} and \"\n                f\"kwargs={list(remaining_kwargs.keys())}. \"\n                f\"Use either agent(message) or agent(key1=value1, key2=value2)\"\n            )\n\n        # Extract vars from Message if not provided\n        if not vars and isinstance(message, Message) and self.vars is not None:\n            vars = message.get(self.vars, {})\n\n        # Extract model_state from Message if not provided\n        if (\n            model_state == []\n            and isinstance(message, Message)\n            and self.model_state is not None\n        ):\n            model_state = self._get_content_from_message(self.model_state, message)\n\n        content = self._process_task_inputs(message, vars=vars, **kwargs)\n\n        if content is None and model_state == []:\n            raise ValueError(\n                \"No task input provided. Expected one of:\\n\"\n                \"  - agent('your text')\\n\"\n                \"  - agent({'key': 'value'})\\n\"\n                \"  - agent(message=Message(...))\\n\"\n                \"  - agent(param1=..., param2=...) with task template configured\"\n            )\n\n        if content is not None:\n            chat_content = [ChatBlock.user(content)]\n            if model_state == []:\n                model_state = chat_content\n            else:\n                model_state.extend(chat_content)\n\n        if model_preference is None and isinstance(message, Message):\n            model_preference = self.get_model_preference_from_message(message)\n\n        return {\n            \"model_state\": model_state,\n            \"model_preference\": model_preference,\n            \"vars\": vars,\n        }\n\n    async def _aprepare_task(\n        self, message: Optional[Union[str, Message, Mapping[str, Any]]] = None, **kwargs\n    ) -&gt; Mapping[str, Any]:\n        \"\"\"Async version of _prepare_task.\n        Prepare model input in ChatML format and execution params.\n        \"\"\"\n        # Extract reserved kwargs\n        vars = kwargs.pop(\"vars\", {})\n        model_state = kwargs.pop(\"model_state\", [])\n        model_preference = kwargs.pop(\"model_preference\", None)\n\n        # Get remaining kwargs (potential task inputs)\n        remaining_kwargs = {\n            k: v for k, v in kwargs.items() if k not in _RESERVED_KWARGS\n        }\n\n        # Handle named task arguments\n        if message is None and remaining_kwargs:\n            if not self.templates.get(\"task\"):\n                raise ValueError(\n                    f\"Named task arguments require a 'task' template to be configured. \"\n                    f\"Received kwargs: {list(remaining_kwargs.keys())}. \"\n                    f\"Either configure a task template or pass arguments as: \"\n                    f\"agent({{'key': 'value'}}) or agent(Message(...))\"\n                )\n            # Convert named kwargs to dict for template rendering\n            message = remaining_kwargs\n            # Clear kwargs to avoid passing them down\n            for key in remaining_kwargs:\n                kwargs.pop(key)\n        elif message is not None and remaining_kwargs:\n            raise ValueError(\n                f\"Cannot pass both 'message' argument and named task arguments. \"\n                f\"Received message={type(message).__name__} and \"\n                f\"kwargs={list(remaining_kwargs.keys())}. \"\n                f\"Use either agent(message) or agent(key1=value1, key2=value2)\"\n            )\n\n        # Extract vars from Message if not provided\n        if not vars and isinstance(message, Message) and self.vars is not None:\n            vars = message.get(self.vars, {})\n\n        # Extract model_state from Message if not provided\n        if (\n            model_state == []\n            and isinstance(message, Message)\n            and self.model_state is not None\n        ):\n            model_state = self._get_content_from_message(self.model_state, message)\n\n        content = await self._aprocess_task_inputs(message, vars=vars, **kwargs)\n\n        if content is None and model_state == []:\n            raise ValueError(\n                \"No task input provided. Expected one of:\\n\"\n                \"  - agent('your text')\\n\"\n                \"  - agent({'key': 'value'})\\n\"\n                \"  - agent(message=Message(...))\\n\"\n                \"  - agent(param1=..., param2=...) with task template configured\"\n            )\n\n        if content is not None:\n            chat_content = [ChatBlock.user(content)]\n            if model_state == []:\n                model_state = chat_content\n            else:\n                model_state.extend(chat_content)\n        # model_state is already set when content is None\n\n        if model_preference is None and isinstance(message, Message):\n            model_preference = self.get_model_preference_from_message(message)\n\n        return {\n            \"model_state\": model_state,\n            \"model_preference\": model_preference,\n            \"vars\": vars,\n        }\n\n    def _process_task_inputs(\n        self,\n        message: Union[str, Message, Mapping[str, Any]],\n        vars: Mapping[str, Any],\n        **kwargs,\n    ) -&gt; Optional[Union[str, Mapping[str, Any]]]:\n        content = \"\"\n\n        context_content = self._context_manager(message, vars=vars, **kwargs)\n        if context_content:\n            content += context_content\n\n        if isinstance(message, Message):\n            task_inputs = self._extract_message_values(self.task_inputs, message)\n        else:\n            task_inputs = message\n\n        if task_inputs is None and self.templates.get(\"task\") is None:\n            return None\n\n        if self.templates.get(\"task\"):\n            if task_inputs:\n                if isinstance(task_inputs, str):\n                    pre_task = self._format_task_template(vars)\n                    task_content = self._format_template(task_inputs, pre_task)\n                elif isinstance(task_inputs, dict):\n                    task_inputs.update(vars)\n                    task_content = self._format_task_template(task_inputs)\n            # It's possible to use `task_template` as the default task message\n            # if no `task_inputs` is selected. This can be useful for multimodal\n            # models that require a text message to be sent along with the data\n            elif vars:\n                task_content = self._format_task_template(vars)\n            else:\n                task_content = self.templates.get(\"task\")\n        else:\n            task_content = task_inputs\n            if isinstance(task_content, Mapping):  # dict -&gt; str\n                task_content = \"\\n\".join(f\"{k}: {v}\" for k, v in task_content.items())\n\n        task_content = apply_xml_tags(\"task\", task_content)\n        content += task_content\n        content = content.strip()  # Remove whitespace\n\n        multimodal_content = self._process_task_multimodal_inputs(message, **kwargs)\n        if multimodal_content:\n            multimodal_content.append(ChatBlock.text(content))\n            return multimodal_content\n        return content\n\n    async def _aprocess_task_inputs(\n        self,\n        message: Union[str, Message, Mapping[str, Any]],\n        vars: Mapping[str, Any],\n        **kwargs,\n    ) -&gt; Optional[Union[str, Mapping[str, Any]]]:\n        \"\"\"Async version of _process_task_inputs.\"\"\"\n        content = \"\"\n\n        context_content = self._context_manager(message, vars=vars, **kwargs)\n        if context_content:\n            content += context_content\n\n        if isinstance(message, Message):\n            task_inputs = self._extract_message_values(self.task_inputs, message)\n        else:\n            task_inputs = message\n\n        if task_inputs is None and self.templates.get(\"task\") is None:\n            return None\n\n        if self.templates.get(\"task\"):\n            if task_inputs:\n                if isinstance(task_inputs, str):\n                    pre_task = self._format_task_template(vars)\n                    task_content = self._format_template(task_inputs, pre_task)\n                elif isinstance(task_inputs, dict):\n                    task_inputs.update(vars)\n                    task_content = self._format_task_template(task_inputs)\n            # It's possible to use `task_template` as the default task message\n            # if no `task_inputs` is selected. This can be useful for multimodal\n            # models that require a text message to be sent along with the data\n            elif vars:\n                task_content = self._format_task_template(vars)\n            else:\n                task_content = self.templates.get(\"task\")\n        else:\n            task_content = task_inputs\n            if isinstance(task_content, Mapping):  # dict -&gt; str\n                task_content = \"\\n\".join(f\"{k}: {v}\" for k, v in task_content.items())\n\n        task_content = apply_xml_tags(\"task\", task_content)\n        content += task_content\n        content = content.strip()  # Remove whitespace\n\n        multimodal_content = await self._aprocess_task_multimodal_inputs(\n            message, **kwargs\n        )\n        if multimodal_content:\n            multimodal_content.append(ChatBlock.text(content))\n            return multimodal_content\n        return content\n\n    def _context_manager(\n        self,\n        message: Union[str, Message, Mapping[str, Any]],\n        vars: Mapping[str, Any],\n        **kwargs,\n    ) -&gt; Optional[str]:\n        \"\"\"Mount context.\"\"\"\n        context_content = \"\"\n\n        if self.context_cache:  # Fixed Context Cache\n            context_content += self.context_cache\n\n        context_inputs = None\n        runtime_context_inputs = kwargs.pop(\"context_inputs\", None)\n        if runtime_context_inputs is not None:\n            context_inputs = runtime_context_inputs\n        elif isinstance(message, Message):\n            context_inputs = self._extract_message_values(self.context_inputs, message)\n\n        if context_inputs is not None:\n            if self.templates.get(\"context\"):\n                if isinstance(context_inputs, Mapping):\n                    context_inputs.update(vars)\n                    msg_context = self._format_template(\n                        context_inputs, self.templates.get(\"context\")\n                    )\n                else:\n                    pre_msg_context = self._format_template(\n                        vars, self.templates.get(\"context\")\n                    )\n                    msg_context = self._format_template(context_inputs, pre_msg_context)\n            elif isinstance(context_inputs, str):\n                msg_context = context_inputs\n            elif isinstance(context_inputs, list):\n                msg_context = \" \".join(str(v) for v in context_inputs if v is not None)\n            elif isinstance(context_inputs, dict):\n                msg_context = \"\\n\".join(\n                    f\"{k}: {v if not isinstance(v, list) else ', '.join(v)}\"\n                    for k, v in context_inputs.items()\n                )\n            context_content += msg_context\n\n        if context_content:\n            if vars:\n                context_content = self._format_template(vars, context_content)\n            return apply_xml_tags(\"context\", context_content) + \"\\n\\n\"\n        return None\n\n    def _process_task_multimodal_inputs(\n        self, message: Union[str, Message, Mapping[str, Any]], **kwargs\n    ) -&gt; Optional[List[Mapping[str, Any]]]:\n        \"\"\"Processes multimodal inputs (image, audio, video, file) via kwargs or\n        message.\n        Returns a list of multimodal content in ChatML format.\n        \"\"\"\n        multimodal_paths = None\n        task_multimodal_inputs = kwargs.get(\"task_multimodal_inputs\", None)\n        if task_multimodal_inputs is not None:\n            multimodal_paths = task_multimodal_inputs\n        elif isinstance(message, Message) and self.task_multimodal_inputs is not None:\n            multimodal_paths = self._extract_message_values(\n                self.task_multimodal_inputs, message\n            )\n\n        if multimodal_paths is None:\n            return None\n\n        content = []\n\n        formatters = {\n            \"image\": self._format_image_input,\n            \"audio\": self._format_audio_input,\n            \"video\": self._format_video_input,\n            \"file\": self._format_file_input,\n        }\n\n        for media_type, formatter in formatters.items():\n            media_sources = multimodal_paths.get(media_type, [])\n            if not isinstance(media_sources, list):\n                media_sources = [media_sources]\n            for media_source in media_sources:\n                if media_source is not None:\n                    formatted_input = formatter(media_source)\n                    if formatted_input:\n                        content.append(formatted_input)\n\n        return content\n\n    async def _aprocess_task_multimodal_inputs(\n        self, message: Union[str, Message, Mapping[str, Any]], **kwargs\n    ) -&gt; Optional[List[Mapping[str, Any]]]:\n        \"\"\"Async version of _process_task_multimodal_inputs.\n        Processes multimodal inputs (image, audio, video, file) via kwargs or message.\n        Returns a list of multimodal content in ChatML format.\n        \"\"\"\n        multimodal_paths = None\n        task_multimodal_inputs = kwargs.get(\"task_multimodal_inputs\", None)\n        if task_multimodal_inputs is not None:\n            multimodal_paths = task_multimodal_inputs\n        elif isinstance(message, Message) and self.task_multimodal_inputs is not None:\n            multimodal_paths = self._extract_message_values(\n                self.task_multimodal_inputs, message\n            )\n\n        if multimodal_paths is None:\n            return None\n\n        content = []\n\n        formatters = {\n            \"image\": self._aformat_image_input,\n            \"audio\": self._aformat_audio_input,\n            \"video\": self._aformat_video_input,\n            \"file\": self._aformat_file_input,\n        }\n\n        for media_type, formatter in formatters.items():\n            media_sources = multimodal_paths.get(media_type, [])\n            if not isinstance(media_sources, list):\n                media_sources = [media_sources]\n            for media_source in media_sources:\n                if media_source is not None:\n                    formatted_input = await formatter(media_source)\n                    if formatted_input:\n                        content.append(formatted_input)\n\n        return content\n\n    def _format_image_input(self, image_source: str) -&gt; Optional[Mapping[str, Any]]:\n        \"\"\"Formats the image input for the model.\"\"\"\n        img = Image(image_source, **self.config.get(\"image_block_kwargs\", {}))\n        return img()\n\n    def _format_video_input(self, video_source: str) -&gt; Optional[Mapping[str, Any]]:\n        \"\"\"Formats the video input for the model.\"\"\"\n        # URLs: don't force encode (keep URL), local files: encode\n        is_url = video_source.startswith(\"http\")\n        vid = Video(\n            video_source,\n            force_encode=not is_url,\n            **self.config.get(\"video_block_kwargs\", {}),\n        )\n        return vid()\n\n    def _format_audio_input(self, audio_source: str) -&gt; Optional[Mapping[str, Any]]:\n        \"\"\"Formats the audio input for the model.\"\"\"\n        aud = Audio(audio_source)\n        return aud()\n\n    def _format_file_input(self, file_source: str) -&gt; Optional[Mapping[str, Any]]:\n        \"\"\"Formats the file input for the model.\"\"\"\n        f = File(file_source)\n        return f()\n\n    async def _aformat_image_input(\n        self, image_source: str\n    ) -&gt; Optional[Mapping[str, Any]]:\n        \"\"\"Async version of _format_image_input.\"\"\"\n        img = Image(image_source, **self.config.get(\"image_block_kwargs\", {}))\n        return await img.acall()\n\n    async def _aformat_video_input(\n        self, video_source: str\n    ) -&gt; Optional[Mapping[str, Any]]:\n        \"\"\"Async version of _format_video_input.\"\"\"\n        # URLs: don't force encode (keep URL), local files: encode\n        is_url = video_source.startswith(\"http\")\n        vid = Video(\n            video_source,\n            force_encode=not is_url,\n            **self.config.get(\"video_block_kwargs\", {}),\n        )\n        return await vid.acall()\n\n    async def _aformat_audio_input(\n        self, audio_source: str\n    ) -&gt; Optional[Mapping[str, Any]]:\n        \"\"\"Async version of _format_audio_input.\"\"\"\n        aud = Audio(audio_source)\n        return await aud.acall()\n\n    async def _aformat_file_input(\n        self, file_source: str\n    ) -&gt; Optional[Mapping[str, Any]]:\n        \"\"\"Async version of _format_file_input.\"\"\"\n        f = File(file_source)\n        return await f.acall()\n\n    def inspect_model_execution_params(\n        self, message: Optional[Union[str, Mapping[str, Any], Message]] = None, **kwargs\n    ) -&gt; Mapping[str, Any]:\n        \"\"\"Debug model input parameters.\n\n        Accepts the same arguments as forward() to inspect what would be sent to\n        the model.\n        \"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_execution_params = self._prepare_model_execution(\n            prefilling=self.prefilling, **inputs\n        )\n        return model_execution_params\n\n    def _set_context_inputs(\n        self, context_inputs: Optional[Union[str, List[str]]] = None\n    ):\n        if isinstance(context_inputs, (str, list)) or context_inputs is None:\n            if isinstance(context_inputs, str) and context_inputs == \"\":\n                raise ValueError(\n                    \"`context_inputs` requires a string not empty\"\n                    f\"given `{context_inputs}`\"\n                )\n            if isinstance(context_inputs, list) and not context_inputs:\n                raise ValueError(\n                    \"`context_inputs` requires a list not empty\"\n                    f\"given `{context_inputs}`\"\n                )\n            self.register_buffer(\"context_inputs\", context_inputs)\n        else:\n            raise TypeError(\n                \"`context_inputs` requires a string, list or None\"\n                f\"given `{type(context_inputs)}`\"\n            )\n\n    def _set_context_cache(self, context_cache: Optional[str] = None):\n        if isinstance(context_cache, str) or context_cache is None:\n            self.register_buffer(\"context_cache\", context_cache)\n        else:\n            raise TypeError(\n                \"`context_cache` requires a string or None\"\n                f\"given `{type(context_cache)}`\"\n            )\n\n    def _set_prefilling(self, prefilling: Optional[str] = None):\n        if isinstance(prefilling, str) or prefilling is None:\n            self.register_buffer(\"prefilling\", prefilling)\n        else:\n            raise TypeError(\n                f\"`prefilling` requires a string or Nonegiven `{type(prefilling)}`\"\n            )\n\n    def _set_tools(\n        self,\n        tools: Optional[List[Callable]] = None,\n        mcp_servers: Optional[List[Mapping[str, Any]]] = None,\n    ):\n        self.tool_library = ToolLibrary(\n            self.get_module_name(), tools or [], mcp_servers=mcp_servers\n        )\n\n    def _set_fixed_messages(\n        self, fixed_messages: Optional[List[Mapping[str, Any]]] = None\n    ):\n        if (\n            isinstance(fixed_messages, list)\n            and all(dict(obj) for obj in fixed_messages)\n        ) or fixed_messages is None:\n            self.register_buffer(\"fixed_messages\", fixed_messages)\n        else:\n            raise TypeError(\n                \"`fixed_messages` need be a list of dict or None\"\n                f\"given `{type(fixed_messages)}`\"\n            )\n\n    def _set_generation_schema(\n        self, generation_schema: Optional[msgspec.Struct] = None\n    ):\n        if generation_schema is None or is_subclass_of(\n            generation_schema, msgspec.Struct\n        ):\n            self.register_buffer(\"generation_schema\", generation_schema)\n        else:\n            raise TypeError(\n                \"`generation_schema` need be a `msgspec.Struct` or None \"\n                f\"given `{type(generation_schema)}`\"\n            )\n\n    def _set_model(self, model: Union[ChatCompletionModel, ModelGateway, LM]):\n        if isinstance(model, LM):  # If already LM, use directly\n            self.lm = model\n        else:  # LM will validate model type\n            self.lm = LM(model)\n\n    @property\n    def model(self):\n        \"\"\"Access underlying model for convenience.\n\n        Returns:\n            The wrapped model instance\n        \"\"\"\n        return self.lm.model\n\n    @model.setter\n    def model(self, value: Union[ChatCompletionModel, ModelGateway, LM]):\n        \"\"\"Update the agent's model.\n\n        Args:\n            value: New model (can be Model or LM)\n        \"\"\"\n        self._set_model(value)\n\n    def _set_system_message(self, system_message: Optional[str] = None):\n        if isinstance(system_message, str) or system_message is None:\n            if (\n                hasattr(self.generation_schema, \"system_message\")\n                and self.generation_schema.system_message is not None\n            ):\n                if system_message is None:\n                    system_message = self.generation_schema.system_message\n                else:\n                    system_message = (\n                        self.generation_schema.system_message + system_message\n                    )\n            self.system_message = Parameter(system_message, PromptSpec.SYSTEM_MESSAGE)\n        else:\n            raise TypeError(\n                \"`system_message` requires a string or None \"\n                f\"given `{type(system_message)}`\"\n            )\n\n    def _set_instructions(self, instructions: Optional[str] = None):\n        if isinstance(instructions, str) or instructions is None:\n            typed_parser_cls = typed_parser_registry.get(self.typed_parser, None)\n            if typed_parser_cls is not None:\n                instructions = self._format_template(\n                    {\"instructions\": instructions}, typed_parser_cls.template\n                )\n            self.instructions = Parameter(instructions, PromptSpec.INSTRUCTIONS)\n        else:\n            raise TypeError(\n                f\"`instructions` requires a string or None given `{type(instructions)}`\"\n            )\n\n    def _set_expected_output(self, expected_output: Optional[str] = None):\n        if isinstance(expected_output, str) or expected_output is None:  # TODO\n            expected_output_temp = \"\"\n            if expected_output:\n                expected_output_temp += expected_output\n            typed_parser_cls = typed_parser_registry.get(self.typed_parser, None)\n            if typed_parser_cls is not None:  # Schema as expected output\n                response_format = response_format_from_msgspec_struct(\n                    self.generation_schema\n                )\n                schema = typed_parser_cls.schema_from_response_format(response_format)\n                content = {\"expected_outputs\": schema}\n                rendered = self._format_template(content, EXPECTED_OUTPUTS_TEMPLATE)\n                expected_output_temp += rendered\n            self.expected_output = Parameter(\n                expected_output_temp or None, PromptSpec.EXPECTED_OUTPUT\n            )\n        else:\n            raise TypeError(\n                \"`expected_output` requires a string or None \"\n                f\"given `{type(expected_output)}`\"\n            )\n\n    def _set_examples(\n        self,\n        examples: Optional[Union[str, List[Union[Example, Mapping[str, Any]]]]] = None,\n    ):\n        if isinstance(examples, (str, list)) or examples is None:\n            if isinstance(examples, list):\n                typed_parser_cls = typed_parser_registry.get(self.typed_parser, None)\n                collection = ExampleCollection(examples)\n                if typed_parser_cls is not None:\n                    serialize_func = typed_parser_cls.encode\n                else:\n                    serialize_func = msgspec_dumps\n                examples = collection.get_formatted(serialize_func, serialize_func)\n            self.examples = Parameter(examples, PromptSpec.EXAMPLES)\n        else:\n            raise TypeError(\n                f\"`examples` requires a List[Example] or None given `{type(examples)}`\"\n            )\n\n    def _set_model_state(self, model_state: Optional[str] = None):\n        if isinstance(model_state, str) or model_state is None:\n            self.register_buffer(\"model_state\", model_state)\n        else:\n            raise TypeError(\n                \"`model_state` requires a string or None \"\n                f\"given `{type(model_state)}`\"\n            )\n\n    def _set_config(self, config: Optional[Dict[str, Any]] = None):\n        \"\"\"Set agent configuration.\n\n        Args:\n            config:\n                Dictionary with configuration options.\n                Valid keys: \"verbose\", \"return_model_state\", \"tool_choice\",\n                \"stream\", \"image_block_kwargs\", \"video_block_kwargs\", \"include_date\"\n\n        Raises:\n            TypeError:\n                If config is not a dict or None.\n            ValueError:\n                If invalid keys are provided.\n        \"\"\"\n        # Define valid keys for Agent\n        valid_keys = {\n            \"verbose\",\n            \"return_model_state\",\n            \"tool_choice\",\n            \"stream\",\n            \"image_block_kwargs\",\n            \"video_block_kwargs\",\n            \"include_date\",\n            \"execution\",  # Added for execution settings\n        }\n\n        if config is None:\n            self.register_buffer(\"config\", {})\n            return\n\n        if not isinstance(config, dict):\n            raise TypeError(f\"`config` must be a dict or None, given `{type(config)}`\")\n\n        invalid_keys = set(config.keys()) - valid_keys\n        if invalid_keys:\n            raise ValueError(\n                f\"Invalid config keys: {invalid_keys}. Valid keys are: {valid_keys}\"\n            )\n\n        if \"image_block_kwargs\" in config:\n            if not isinstance(config[\"image_block_kwargs\"], dict):\n                raise TypeError(\n                    f\"`image_block_kwargs` must be a dict, \"\n                    f\"given `{type(config['image_block_kwargs'])}`\"\n                )\n\n        if \"video_block_kwargs\" in config:\n            if not isinstance(config[\"video_block_kwargs\"], dict):\n                raise TypeError(\n                    f\"`video_block_kwargs` must be a dict, \"\n                    f\"given `{type(config['video_block_kwargs'])}`\"\n                )\n\n        self.register_buffer(\"config\", config.copy())\n\n    def _set_system_extra_message(self, system_extra_message: Optional[str] = None):\n        if isinstance(system_extra_message, str) or system_extra_message is None:\n            self.register_buffer(\"system_extra_message\", system_extra_message)\n        else:\n            raise TypeError(\n                \"`system_extra_message` requires a string or None \"\n                f\"given `{type(system_extra_message)}`\"\n            )\n\n    def _set_vars(self, vars: Optional[str] = None):\n        if isinstance(vars, str) or vars is None:\n            self.register_buffer(\"vars\", vars)\n        else:\n            raise TypeError(f\"`vars` requires a string or None given `{type(vars)}`\")\n\n    def _set_message_fields(self, message_fields: Optional[Dict[str, Any]] = None):\n        \"\"\"Set message field mappings for Agent.\n\n        Args:\n            message_fields: Dictionary mapping field names to their values.\n                Valid keys: \"task_inputs\", \"task_multimodal_inputs\", \"model_state\",\n                \"context_inputs\", \"model_preference\", \"vars\"\n\n        Raises:\n            TypeError: If message_fields is not a dict or None\n            ValueError: If invalid keys are provided\n        \"\"\"\n        # Define valid keys for Agent class\n        valid_keys = {\n            \"task_inputs\",\n            \"task_multimodal_inputs\",\n            \"model_state\",\n            \"context_inputs\",\n            \"model_preference\",\n            \"vars\",\n        }\n\n        if message_fields is None:\n            # Set all fields to None\n            self._set_task_inputs(None)\n            self._set_task_multimodal_inputs(None)\n            self._set_model_preference(None)\n            self._set_context_inputs(None)\n            self._set_model_state(None)\n            self._set_vars(None)\n            return\n\n        if not isinstance(message_fields, dict):\n            raise TypeError(\n                f\"`message_fields` must be a dict or None, given \"\n                f\"`{type(message_fields)}`\"\n            )\n\n        # Validate keys\n        invalid_keys = set(message_fields.keys()) - valid_keys\n        if invalid_keys:\n            raise ValueError(\n                f\"Invalid message_fields keys: {invalid_keys}. \"\n                f\"Valid keys are: {valid_keys}\"\n            )\n\n        # Set each field using its setter, defaulting to None if not provided\n        self._set_task_inputs(message_fields.get(\"task_inputs\"))\n        self._set_task_multimodal_inputs(message_fields.get(\"task_multimodal_inputs\"))\n        self._set_model_preference(message_fields.get(\"model_preference\"))\n        self._set_context_inputs(message_fields.get(\"context_inputs\"))\n        self._set_model_state(message_fields.get(\"model_state\"))\n        self._set_vars(message_fields.get(\"vars\"))\n\n    def _set_typed_parser(self, typed_parser: Optional[str] = None):\n        if isinstance(typed_parser, str) or typed_parser is None:\n            if (\n                isinstance(typed_parser, str)\n                and typed_parser not in typed_parser_registry\n            ):\n                raise ValueError(\n                    f\"`typed_parser` supports only `{typed_parser_registry.keys()}`\"\n                    f\" given `{typed_parser}`\"\n                )\n            self.register_buffer(\"typed_parser\", typed_parser)\n        else:\n            raise TypeError(\n                f\"`typed_parser` requires a str given `{type(typed_parser)}`\"\n            )\n\n    def _set_signature(\n        self,\n        *,\n        signature: Optional[Union[str, Signature]] = None,\n        examples: Optional[List[Example]] = None,\n        generation_schema: Optional[msgspec.Struct] = None,\n        instructions: Optional[str] = None,\n        system_message: Optional[str] = None,\n        typed_parser: Optional[str] = None,\n    ):\n        if signature is not None:\n            typed_parser_cls = typed_parser_registry.get(typed_parser, None)\n\n            examples = examples or []\n            output_descriptions = None\n            signature_instructions = None\n\n            if isinstance(signature, str):\n                input_str_signature, output_str_signature = signature.split(\"-&gt;\")\n                inputs_info = StructFactory._parse_annotations(input_str_signature)\n                outputs_info = StructFactory._parse_annotations(output_str_signature)\n            elif issubclass(signature, Signature):\n                output_str_signature = signature.get_str_signature().split(\"-&gt;\")[-1]\n                inputs_info = signature.get_inputs_info()\n                outputs_info = signature.get_outputs_info()\n                output_descriptions = signature.get_output_descriptions()\n                signature_instructions = signature.get_instructions()\n                signature_examples = SignatureFactory.get_examples_from_signature(\n                    signature\n                )\n                if signature_examples:\n                    examples.extend(signature_examples)\n            else:\n                raise TypeError(\n                    \"`signature` requires a string, `Signature` or None \"\n                    f\"given `{type(signature)}`\"\n                )\n\n            # typed_parser\n            self._set_typed_parser(typed_parser)\n\n            # task template - add to templates dict, overriding if present\n            task_template = SignatureFactory.get_task_template_from_signature(\n                inputs_info\n            )\n            self.templates[\"task\"] = task_template\n\n            # instructions\n            self._set_instructions(instructions or signature_instructions)\n\n            # generation schema\n            signature_output_struct = StructFactory.from_signature(\n                output_str_signature, \"Outputs\", output_descriptions\n            )\n            fused_output_struct = None\n            if generation_schema is not None:\n                signature_as_type = cast(Type[msgspec.Struct], signature_output_struct)\n                if is_optional_field(generation_schema, \"final_answer\"):\n                    signature_as_type = Optional[signature_output_struct]  # type: ignore\n\n                # Merge parent annotations with new final_answer annotation\n                merged_annotations = {\n                    **generation_schema.__annotations__,\n                    \"final_answer\": signature_as_type,\n                }\n\n                Output = type(\n                    \"Output\",\n                    (generation_schema,),\n                    {\"__annotations__\": merged_annotations},\n                )\n                fused_output_struct = Output\n            self._set_generation_schema(fused_output_struct or signature_output_struct)\n\n            # system message\n            self._set_system_message(system_message)\n\n            # expected output\n            expected_output = SignatureFactory.get_expected_output_from_signature(\n                inputs_info, outputs_info, typed_parser_cls\n            )\n            self._set_expected_output(expected_output)\n\n            # examples\n            self._set_examples(examples)\n\n            # Generate and set annotations from signature inputs\n            generated_annotations = generate_annotations_from_signature(\n                inputs_info, signature\n            )\n            self.set_annotations(generated_annotations)\n\n    def _get_system_prompt(self, vars: Optional[Mapping[str, Any]] = None) -&gt; str:\n        \"\"\"Render the system prompt using the Jinja template.\n        Returns an empty string if no segments are provided.\n        \"\"\"\n        template_inputs = dotdict(\n            system_message=self.system_message.data,\n            instructions=self.instructions.data,\n            expected_output=self.expected_output.data,\n            examples=self.examples.data,\n            system_extra_message=self.system_extra_message,\n        )\n\n        if self.config.get(\"include_date\", False):\n            now = datetime.now(tz=timezone.utc)\n            # Format: \"Monday, December 09, 2025\"\n            template_inputs.current_date = now.strftime(\"%A, %B %d, %Y\")\n\n        system_prompt = self._format_template(\n            template_inputs, self.system_prompt_template\n        )\n\n        if vars:  # Runtime inputs to system template\n            system_prompt = self._format_template(vars, system_prompt)\n        return system_prompt\n\n    @property\n    def system_prompt_template(self) -&gt; str:\n        \"\"\"Get the system prompt template.\n\n        Returns the custom template if provided in templates dict,\n        otherwise returns the default SYSTEM_PROMPT_TEMPLATE.\n        \"\"\"\n        return self.templates.get(\"system_prompt\", SYSTEM_PROMPT_TEMPLATE)\n</code></pre>"},{"location":"api-reference/nn/modules/agent/#msgflux.nn.modules.agent.Agent.model","title":"model  <code>property</code> <code>writable</code>","text":"<pre><code>model\n</code></pre> <p>Access underlying model for convenience.</p> <p>Returns:</p> Type Description <p>The wrapped model instance</p>"},{"location":"api-reference/nn/modules/agent/#msgflux.nn.modules.agent.Agent.system_prompt_template","title":"system_prompt_template  <code>property</code>","text":"<pre><code>system_prompt_template\n</code></pre> <p>Get the system prompt template.</p> <p>Returns the custom template if provided in templates dict, otherwise returns the default SYSTEM_PROMPT_TEMPLATE.</p>"},{"location":"api-reference/nn/modules/agent/#msgflux.nn.modules.agent.Agent.__init__","title":"__init__","text":"<pre><code>__init__(\n    name,\n    model,\n    *,\n    system_message=None,\n    instructions=None,\n    expected_output=None,\n    examples=None,\n    system_extra_message=None,\n    guardrails=None,\n    message_fields=None,\n    config=None,\n    templates=None,\n    context_cache=None,\n    prefilling=None,\n    generation_schema=None,\n    typed_parser=None,\n    response_mode=\"plain_response\",\n    tools=None,\n    mcp_servers=None,\n    fixed_messages=None,\n    signature=None,\n    description=None,\n    annotations=None,\n)\n</code></pre> <p>name:     Agent name in snake case format. model:     Chat Completation Model client. system_message:     The Agent behaviour. instructions:     What the Agent should do. expected_output:     What the response should be like.</p> <p>Examples:</p> <p>Examples of inputs, reasoning and outputs.</p> <p>system_extra_message:     An extra message in system prompt. guardrails:     Dictionary mapping guardrail types to callables.     Valid keys: \"input\", \"output\"     !!! example         guardrails={\"input\": input_checker, \"output\": output_checker} message_fields:     Dictionary mapping Message field names to their paths in the Message object.     Valid keys: \"task_inputs\", \"task_multimodal_inputs\", \"model_state\",     \"context_inputs\", \"model_preference\", \"vars\"     !!! example         message_fields={             \"task_inputs\": \"input.user\",             \"task_multimodal_inputs\": {\"audio\": \"audio.user\"},             \"model_state\": \"messages.history\",             \"context_inputs\": \"context.data\",             \"model_preference\": \"model.preference\",             \"vars\": \"vars.data\"         }</p> <pre><code>Field descriptions:\n- task_inputs: Field path for task input (str, dict, or tuple)\n- task_multimodal_inputs: Map datatype (image, video, audio, file)\n  to field paths\n- model_state: Field path for list of chats in ChatML format\n- context_inputs: Field path for context (str or list of str)\n- model_preference: Field path for model preference (str, only valid\n  with ModelGateway)\n- vars: Field path for inputs to templates and tools (str)\n</code></pre> <p>config:     Dictionary with configuration options.     Valid keys: \"verbose\", \"return_model_state\", \"tool_choice\",     \"stream\", \"image_block_kwargs\", \"video_block_kwargs\", \"include_date\"     !!! example         config={             \"verbose\": True,             \"return_model_state\": False,             \"tool_choice\": \"auto\",             \"stream\": False,             \"image_block_kwargs\": {\"detail\": \"high\"},             \"video_block_kwargs\": {\"format\": \"mp4\"},             \"include_date\": False         }</p> <pre><code>Configuration options:\n- verbose: Print model output and tool calls to console (bool)\n- return_model_state: Return dict with model_state and response (bool)\n- tool_choice: Control tool selection (\"auto\", \"required\", or function name)\n- stream: Transmit response on-the-fly (bool)\n- image_block_kwargs: Dict of kwargs to pass to ChatBlock.image\n  (e.g., {\"detail\": \"high\"})\n- video_block_kwargs: Dict of kwargs to pass to ChatBlock.video\n  (e.g., {\"format\": \"mp4\"})\n- include_date: Include current date with weekday in system prompt\n  (bool). Format: \"Weekday, Month DD, YYYY\" (e.g., \"Monday, December 09, 2025\")\n</code></pre> <p>templates:     Dictionary mapping template types to Jinja template strings.     Valid keys: \"task\", \"response\", \"context\", \"system_prompt\"     !!! example         templates={             \"task\": \"Who was {{person}}?\",             \"response\": \"{{final_answer}}\",             \"context\": \"Context: {{context}}\",             \"system_prompt\": \"Custom system prompt: {% if system_message %}{{ system_message }}{% endif %}\"         }</p> <pre><code>Template descriptions:\n- task: Formats the task/prompt sent to the model\n- response: Formats the model's response\n- context: Formats context_inputs (does NOT apply to context_cache)\n- system_prompt: Overrides the default system prompt template. If not provided,\n  uses SYSTEM_PROMPT_TEMPLATE. Available variables: system_message, instructions,\n  expected_output, examples, system_extra_message, current_date (if include_date=True)\n</code></pre> <p>context_cache:     A fixed context. prefilling:     Forces an initial message from the model. From that message it     will continue its response from there. generation_schema:     Schema that defines how the output should be structured. typed_parser:     Converts the model raw output into a typed-dict. Supported parser:     <code>typed_xml</code>. response_mode:     What the response should be.     * <code>plain_response</code> (default): Returns the final agent response directly.     * other: Write on field in Message object. tools:     A list of callable objects. mcp_servers:     List of MCP (Model Context Protocol) server configurations.     Each config should contain:     - name: Namespace for tools from this server     - transport: \"stdio\" or \"http\"     - For stdio: command, args, cwd, env     - For http: base_url, headers     - Optional: include_tools, exclude_tools, tool_config     !!! example         mcp_servers=[{             \"name\": \"fs\",             \"transport\": \"stdio\",             \"command\": \"npx\",             \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\"],             \"include_tools\": [\"read_file\", \"write_file\"],             \"tool_config\": {\"read_file\": {\"inject_vars\": [\"context\"]}}         }] fixed_messages:     A fixed list of chats in ChatML format. signature:     A DSPy-based signature. A signature creates a task_template,     a generation_schema, instructions and examples (both if passed).     Can be combined with standard generation_schemas like <code>ReAct</code> and     <code>ChainOfThought</code>. Can also be combined with <code>typed_parser</code>. description:     The Agent description. It's useful when using an agent-as-a-tool. annotations     Define the input and output annotations to use the agent-as-a-function.</p> Source code in <code>src/msgflux/nn/modules/agent.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    model: Union[ChatCompletionModel, ModelGateway, LM],\n    *,\n    system_message: Optional[str] = None,\n    instructions: Optional[str] = None,\n    expected_output: Optional[str] = None,\n    examples: Optional[Union[str, List[Union[Example, Mapping[str, Any]]]]] = None,\n    system_extra_message: Optional[str] = None,\n    guardrails: Optional[Dict[str, Callable]] = None,\n    message_fields: Optional[Dict[str, Any]] = None,\n    config: Optional[Dict[str, Any]] = None,\n    templates: Optional[Dict[str, str]] = None,\n    context_cache: Optional[str] = None,\n    prefilling: Optional[str] = None,\n    generation_schema: Optional[msgspec.Struct] = None,\n    typed_parser: Optional[str] = None,\n    response_mode: Optional[str] = \"plain_response\",\n    tools: Optional[List[Callable]] = None,\n    mcp_servers: Optional[List[Mapping[str, Any]]] = None,\n    fixed_messages: Optional[List[Mapping[str, Any]]] = None,\n    signature: Optional[Union[str, Signature]] = None,\n    description: Optional[str] = None,\n    annotations: Optional[Mapping[str, type]] = None,\n):\n    \"\"\"Args:\n    name:\n        Agent name in snake case format.\n    model:\n        Chat Completation Model client.\n    system_message:\n        The Agent behaviour.\n    instructions:\n        What the Agent should do.\n    expected_output:\n        What the response should be like.\n\n    Examples:\n        Examples of inputs, reasoning and outputs.\n    system_extra_message:\n        An extra message in system prompt.\n    guardrails:\n        Dictionary mapping guardrail types to callables.\n        Valid keys: \"input\", \"output\"\n        !!! example\n            guardrails={\"input\": input_checker, \"output\": output_checker}\n    message_fields:\n        Dictionary mapping Message field names to their paths in the Message object.\n        Valid keys: \"task_inputs\", \"task_multimodal_inputs\", \"model_state\",\n        \"context_inputs\", \"model_preference\", \"vars\"\n        !!! example\n            message_fields={\n                \"task_inputs\": \"input.user\",\n                \"task_multimodal_inputs\": {\"audio\": \"audio.user\"},\n                \"model_state\": \"messages.history\",\n                \"context_inputs\": \"context.data\",\n                \"model_preference\": \"model.preference\",\n                \"vars\": \"vars.data\"\n            }\n\n        Field descriptions:\n        - task_inputs: Field path for task input (str, dict, or tuple)\n        - task_multimodal_inputs: Map datatype (image, video, audio, file)\n          to field paths\n        - model_state: Field path for list of chats in ChatML format\n        - context_inputs: Field path for context (str or list of str)\n        - model_preference: Field path for model preference (str, only valid\n          with ModelGateway)\n        - vars: Field path for inputs to templates and tools (str)\n    config:\n        Dictionary with configuration options.\n        Valid keys: \"verbose\", \"return_model_state\", \"tool_choice\",\n        \"stream\", \"image_block_kwargs\", \"video_block_kwargs\", \"include_date\"\n        !!! example\n            config={\n                \"verbose\": True,\n                \"return_model_state\": False,\n                \"tool_choice\": \"auto\",\n                \"stream\": False,\n                \"image_block_kwargs\": {\"detail\": \"high\"},\n                \"video_block_kwargs\": {\"format\": \"mp4\"},\n                \"include_date\": False\n            }\n\n        Configuration options:\n        - verbose: Print model output and tool calls to console (bool)\n        - return_model_state: Return dict with model_state and response (bool)\n        - tool_choice: Control tool selection (\"auto\", \"required\", or function name)\n        - stream: Transmit response on-the-fly (bool)\n        - image_block_kwargs: Dict of kwargs to pass to ChatBlock.image\n          (e.g., {\"detail\": \"high\"})\n        - video_block_kwargs: Dict of kwargs to pass to ChatBlock.video\n          (e.g., {\"format\": \"mp4\"})\n        - include_date: Include current date with weekday in system prompt\n          (bool). Format: \"Weekday, Month DD, YYYY\" (e.g., \"Monday, December 09, 2025\")\n    templates:\n        Dictionary mapping template types to Jinja template strings.\n        Valid keys: \"task\", \"response\", \"context\", \"system_prompt\"\n        !!! example\n            templates={\n                \"task\": \"Who was {{person}}?\",\n                \"response\": \"{{final_answer}}\",\n                \"context\": \"Context: {{context}}\",\n                \"system_prompt\": \"Custom system prompt: {% if system_message %}{{ system_message }}{% endif %}\"\n            }\n\n        Template descriptions:\n        - task: Formats the task/prompt sent to the model\n        - response: Formats the model's response\n        - context: Formats context_inputs (does NOT apply to context_cache)\n        - system_prompt: Overrides the default system prompt template. If not provided,\n          uses SYSTEM_PROMPT_TEMPLATE. Available variables: system_message, instructions,\n          expected_output, examples, system_extra_message, current_date (if include_date=True)\n    context_cache:\n        A fixed context.\n    prefilling:\n        Forces an initial message from the model. From that message it\n        will continue its response from there.\n    generation_schema:\n        Schema that defines how the output should be structured.\n    typed_parser:\n        Converts the model raw output into a typed-dict. Supported parser:\n        `typed_xml`.\n    response_mode:\n        What the response should be.\n        * `plain_response` (default): Returns the final agent response directly.\n        * other: Write on field in Message object.\n    tools:\n        A list of callable objects.\n    mcp_servers:\n        List of MCP (Model Context Protocol) server configurations.\n        Each config should contain:\n        - name: Namespace for tools from this server\n        - transport: \"stdio\" or \"http\"\n        - For stdio: command, args, cwd, env\n        - For http: base_url, headers\n        - Optional: include_tools, exclude_tools, tool_config\n        !!! example\n            mcp_servers=[{\n                \"name\": \"fs\",\n                \"transport\": \"stdio\",\n                \"command\": \"npx\",\n                \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\"],\n                \"include_tools\": [\"read_file\", \"write_file\"],\n                \"tool_config\": {\"read_file\": {\"inject_vars\": [\"context\"]}}\n            }]\n    fixed_messages:\n        A fixed list of chats in ChatML format.\n    signature:\n        A DSPy-based signature. A signature creates a task_template,\n        a generation_schema, instructions and examples (both if passed).\n        Can be combined with standard generation_schemas like `ReAct` and\n        `ChainOfThought`. Can also be combined with `typed_parser`.\n    description:\n        The Agent description. It's useful when using an agent-as-a-tool.\n    annotations\n        Define the input and output annotations to use the agent-as-a-function.\n    \"\"\"\n    if annotations is None:\n        annotations = {\"message\": str, \"return\": str}\n\n    # Validate that signature and custom annotations are not both provided\n    if signature is not None and annotations != {\"message\": str, \"return\": str}:\n        raise ValueError(\n            \"Cannot specify both 'signature' and custom 'annotations'. \"\n            \"When using a signature, annotations are generated automatically \"\n            \"from the signature inputs. Remove the 'annotations' parameter.\"\n        )\n\n    super().__init__()\n    self.set_name(name)\n    self.set_description(description)\n\n    # Only set annotations if signature is not provided\n    # (signature will set annotations automatically in _set_signature)\n    if signature is None:\n        self.set_annotations(annotations)\n    else:\n        # Set default temporarily, will be overridden by _set_signature\n        self.set_annotations({\"message\": str, \"return\": str})\n\n    self._set_config(config)\n\n    stream = config.get(\"stream\", False) if config else False\n\n    if stream is True:\n        if generation_schema is not None:\n            raise ValueError(\"`generation_schema` is not `stream=True` compatible\")\n\n        if guardrails is not None and \"output\" in guardrails:\n            raise ValueError(\n                \"`guardrails['output']` is not `stream=True` compatible\"\n            )\n\n        if templates is not None and templates.get(\"response\") is not None:\n            raise ValueError(\n                \"`templates['response']` is not `stream=True` compatible\"\n            )\n\n        if typed_parser is not None:\n            raise ValueError(\"`typed_parser` is not `stream=True` compatible\")\n\n    self._set_context_cache(context_cache)\n    self._set_fixed_messages(fixed_messages)\n    self._set_guardrails(guardrails)\n    self._set_message_fields(message_fields)\n    self._set_model(model)\n    self._set_prefilling(prefilling)\n    self._set_system_extra_message(system_extra_message)\n    self._set_response_mode(response_mode)\n    self._set_templates(templates)\n    self._set_tools(tools, mcp_servers)\n\n    if signature is not None:\n        signature_params = dotdict(\n            signature=signature,\n            examples=examples,\n            instructions=instructions,\n            system_message=system_message,\n            typed_parser=typed_parser,\n        )\n        if generation_schema is not None:\n            signature_params.generation_schema = generation_schema\n        self._set_signature(**signature_params)\n    else:\n        self._set_typed_parser(typed_parser)\n        self._set_examples(examples)\n        self._set_generation_schema(generation_schema)\n        self._set_expected_output(expected_output)\n        self._set_instructions(instructions)\n        self._set_system_message(system_message)\n</code></pre>"},{"location":"api-reference/nn/modules/agent/#msgflux.nn.modules.agent.Agent.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(message=None, **kwargs)\n</code></pre> <p>Async version of forward.</p> Source code in <code>src/msgflux/nn/modules/agent.py</code> <pre><code>async def aforward(\n    self, message: Optional[Union[str, Mapping[str, Any], Message]] = None, **kwargs\n) -&gt; Union[str, Mapping[str, None], ModelStreamResponse, Message]:\n    \"\"\"Async version of forward.\"\"\"\n    inputs = await self._aprepare_task(message, **kwargs)\n    model_response = await self._aexecute_model(\n        prefilling=self.prefilling, **inputs\n    )\n    response = await self._aprocess_model_response(\n        message, model_response, **inputs\n    )\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/agent/#msgflux.nn.modules.agent.Agent.forward","title":"forward","text":"<pre><code>forward(message=None, **kwargs)\n</code></pre> <p>Execute the agent with the given message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Optional[Union[str, Mapping[str, Any], Message]]</code> <p>The input message, which can be: - str: Direct task input (used as task_inputs) - Message: Message object with fields mapped via message_fields.   Requires message_fields configuration, e.g.:   message_fields={\"task_inputs\": \"input.user\"} - dict: Task inputs as a dictionary - None: When using named task arguments (see below)</p> <code>None</code> <code>**kwargs</code> <p>Can include: - Reserved kwargs (runtime overrides for message_fields):     - task_multimodal_inputs: Override multimodal inputs     - model_state: Override chat messages (model state)     - context_inputs: Override context     - model_preference: Override model preference     - vars: Override template/tool variables - Named task arguments: When message=None and a task template is   configured, any other kwargs are treated as task inputs.   Example: agent(name=\"Vilson\", age=27)   This is useful when using agents as tools with typed annotations.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, Mapping[str, None], ModelStreamResponse, Message]</code> <p>Agent response (str, Message, or ModelStreamResponse depending on</p> <code>Union[str, Mapping[str, None], ModelStreamResponse, Message]</code> <p>configuration)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both message and named task arguments are provided, or if named arguments are used without a task template.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # String input\n&gt;&gt;&gt; agent(\"What is the weather?\")\n</code></pre> <pre><code>&gt;&gt;&gt; # Dict input\n&gt;&gt;&gt; agent({\"city\": \"Natal\"})\n</code></pre> <pre><code>&gt;&gt;&gt; # Message input (requires message_fields configuration)\n&gt;&gt;&gt; agent_with_message = Agent(\n...     model=model,\n...     message_fields={\"task_inputs\": \"user.query\"}\n... )\n&gt;&gt;&gt; msg = Message()\n&gt;&gt;&gt; msg.set(\"user.query\", \"Hello\")\n&gt;&gt;&gt; agent_with_message(msg)\n</code></pre> <pre><code>&gt;&gt;&gt; # Named arguments (requires task template)\n&gt;&gt;&gt; agent = Agent(\n...     model=model,\n...     templates={\"task\": \"Greet {{name}} who is {{age}} years old\"},\n... )\n&gt;&gt;&gt; agent(name=\"Vilson\", age=27)\n</code></pre> Source code in <code>src/msgflux/nn/modules/agent.py</code> <pre><code>def forward(\n    self, message: Optional[Union[str, Mapping[str, Any], Message]] = None, **kwargs\n) -&gt; Union[str, Mapping[str, None], ModelStreamResponse, Message]:\n    \"\"\"Execute the agent with the given message.\n\n    Args:\n        message: The input message, which can be:\n            - str: Direct task input (used as task_inputs)\n            - Message: Message object with fields mapped via message_fields.\n              Requires message_fields configuration, e.g.:\n              message_fields={\"task_inputs\": \"input.user\"}\n            - dict: Task inputs as a dictionary\n            - None: When using named task arguments (see below)\n        **kwargs: Can include:\n            - Reserved kwargs (runtime overrides for message_fields):\n                - task_multimodal_inputs: Override multimodal inputs\n                - model_state: Override chat messages (model state)\n                - context_inputs: Override context\n                - model_preference: Override model preference\n                - vars: Override template/tool variables\n            - Named task arguments: When message=None and a task template is\n              configured, any other kwargs are treated as task inputs.\n              Example: agent(name=\"Vilson\", age=27)\n              This is useful when using agents as tools with typed annotations.\n\n    Returns:\n        Agent response (str, Message, or ModelStreamResponse depending on\n        configuration)\n\n    Raises:\n        ValueError: If both message and named task arguments are provided,\n            or if named arguments are used without a task template.\n\n    Examples:\n        &gt;&gt;&gt; # String input\n        &gt;&gt;&gt; agent(\"What is the weather?\")\n\n        &gt;&gt;&gt; # Dict input\n        &gt;&gt;&gt; agent({\"city\": \"Natal\"})\n\n        &gt;&gt;&gt; # Message input (requires message_fields configuration)\n        &gt;&gt;&gt; agent_with_message = Agent(\n        ...     model=model,\n        ...     message_fields={\"task_inputs\": \"user.query\"}\n        ... )\n        &gt;&gt;&gt; msg = Message()\n        &gt;&gt;&gt; msg.set(\"user.query\", \"Hello\")\n        &gt;&gt;&gt; agent_with_message(msg)\n\n        &gt;&gt;&gt; # Named arguments (requires task template)\n        &gt;&gt;&gt; agent = Agent(\n        ...     model=model,\n        ...     templates={\"task\": \"Greet {{name}} who is {{age}} years old\"},\n        ... )\n        &gt;&gt;&gt; agent(name=\"Vilson\", age=27)\n    \"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = self._execute_model(prefilling=self.prefilling, **inputs)\n    response = self._process_model_response(message, model_response, **inputs)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/agent/#msgflux.nn.modules.agent.Agent.inspect_model_execution_params","title":"inspect_model_execution_params","text":"<pre><code>inspect_model_execution_params(message=None, **kwargs)\n</code></pre> <p>Debug model input parameters.</p> <p>Accepts the same arguments as forward() to inspect what would be sent to the model.</p> Source code in <code>src/msgflux/nn/modules/agent.py</code> <pre><code>def inspect_model_execution_params(\n    self, message: Optional[Union[str, Mapping[str, Any], Message]] = None, **kwargs\n) -&gt; Mapping[str, Any]:\n    \"\"\"Debug model input parameters.\n\n    Accepts the same arguments as forward() to inspect what would be sent to\n    the model.\n    \"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_execution_params = self._prepare_model_execution(\n        prefilling=self.prefilling, **inputs\n    )\n    return model_execution_params\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/","title":"Module dict","text":""},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict","title":"ModuleDict","text":"<p>               Bases: <code>Module</code>, <code>Mapping</code></p> <p>Holds submodules in a dictionary.</p> <p><code>msgflux.nn.ModuleDict</code> can be indexed like a regular Python dictionary, but modules it contains are properly registered, and will be visible by all <code>msgflux.nn.Module</code> methods.</p> <p><code>msgflux.nn.ModuleDict</code> is an ordered dictionary that respects:</p> <pre><code>* the order of insertion, and\n\n* in `msgflux.nn.ModuleDict.update` the order of the merged\n`OrderedDict`, `dict` (started from Python 3.6) or another\n`msgflux.nn.ModuleDict` (the argument to `msgflux.nn.ModuleDict.update`).\n</code></pre> <p>Note that <code>msgflux.nn.ModuleDict.update</code> with other unordered mapping types (e.g., Python's plain <code>dict</code> before Python version 3.6) does not preserve the order of the merged mapping.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>class ModuleDict(Module, container_abcs.Mapping):\n    \"\"\"Holds submodules in a dictionary.\n\n    `msgflux.nn.ModuleDict` can be indexed like a regular Python dictionary,\n    but modules it contains are properly registered, and will be visible by all\n    `msgflux.nn.Module` methods.\n\n    `msgflux.nn.ModuleDict` is an **ordered** dictionary that respects:\n\n        * the order of insertion, and\n\n        * in `msgflux.nn.ModuleDict.update` the order of the merged\n        `OrderedDict`, `dict` (started from Python 3.6) or another\n        `msgflux.nn.ModuleDict` (the argument to `msgflux.nn.ModuleDict.update`).\n\n    Note that `msgflux.nn.ModuleDict.update` with other unordered mapping\n    types (e.g., Python's plain `dict` before Python version 3.6) does not\n    preserve the order of the merged mapping.\n    \"\"\"\n\n    _modules: Dict[str, Module]  # type: ignore[assignment]\n\n    def __init__(self, modules: Optional[Mapping[str, Module]] = None) -&gt; None:\n        \"\"\"Args:\n            modules (iterable, optional): a mapping (dictionary) of (string: module)\n                or an iterable of key-value pairs of type (string, module).\n\n        !!! example\n            ```python\n            import random\n            import msgflux.nn as nn\n\n            class ExpertSales(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n                def forward(self, msg: str):\n                    return msg + self.response\n\n            class ExpertSupport(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.register_buffer(\"response\", \"Hi, call 190\")\n\n                def forward(self, msg: str):\n                    return msg + self.response\n\n            def draw_choice(choices: list[str]) -&gt; str:\n                return random.choice(choices)\n\n            class Router(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.choices = nn.ModuleDict({\n                        \"sales\": ExpertSales(),\n                        \"support\": ExpertSupport()\n                    })\n\n                def forward(self, msg: str) -&gt; str:\n                    choice = draw_choice(list(self.choices.keys()))\n                    msg = self.choices[choice](msg)\n                    return msg\n\n            router = Router()\n            router(\"I need help with my tv.\")\n            ```\n        \"\"\"\n        super().__init__()\n        if modules is not None:\n            self.update(modules)\n\n    def __getitem__(self, key: str) -&gt; Module:\n        return self._modules[key]\n\n    def __setitem__(self, key: str, module: Module) -&gt; None:\n        self.add_module(key, module)\n\n    def __delitem__(self, key: str) -&gt; None:\n        del self._modules[key]\n\n    def __len__(self) -&gt; int:\n        return len(self._modules)\n\n    def __iter__(self) -&gt; Iterator[str]:\n        return iter(self._modules)\n\n    def __contains__(self, key: str) -&gt; bool:\n        return key in self._modules\n\n    def clear(self) -&gt; None:\n        \"\"\"Remove all items from the ModuleDict.\"\"\"\n        self._modules.clear()\n\n    def pop(self, key: str) -&gt; Module:\n        \"\"\"Remove key from the ModuleDict and return its module.\n\n        Args:\n            key:\n                key to pop from the ModuleDict.\n        \"\"\"\n        v = self[key]\n        del self[key]\n        return v\n\n    def keys(self) -&gt; Iterable[str]:\n        \"\"\"Return an iterable of the ModuleDict keys.\"\"\"\n        return self._modules.keys()\n\n    def items(self) -&gt; Iterable[Tuple[str, Module]]:\n        \"\"\"Return an iterable of the ModuleDict key/value pairs.\"\"\"\n        return self._modules.items()\n\n    def values(self) -&gt; Iterable[Module]:\n        \"\"\"Return an iterable of the ModuleDict values.\"\"\"\n        return self._modules.values()\n\n    def update(self, modules: Mapping[str, Module]) -&gt; None:\n        \"\"\"Update the class **msgflux.nn.ModuleDict** with\n        key-value pairs from a mapping, overwriting existing keys.\n\n        !!! note\n\n            If `modules` is an `OrderedDict`, a `msgflux.nn.ModuleDict`, or\n            an iterable of key-value pairs, the order of new elements in\n            it is preserved.\n\n        Args:\n            modules:\n                A mapping (dictionary) from string to `msgflux.nn.Module`,\n                or an iterable of key-value pairs of type (string, `msgflux.nn.Module`).\n        \"\"\"\n        if not isinstance(modules, container_abcs.Iterable):\n            raise TypeError(\n                \"ModuleDict.update should be called with an \"\n                \"iterable of key/value pairs, but got \" + type(modules).__name__\n            )\n\n        if isinstance(modules, (OrderedDict, ModuleDict, container_abcs.Mapping)):\n            for key, module in modules.items():\n                self[key] = module\n        else:\n            # modules here can be a list with two items\n            for j, m in enumerate(modules):\n                if not isinstance(m, container_abcs.Iterable):\n                    raise TypeError(\n                        \"ModuleDict update sequence element \"\n                        \"#\" + str(j) + \" should be Iterable; is\" + type(m).__name__\n                    )\n                if not len(m) == 2:\n                    raise ValueError(\n                        \"ModuleDict update sequence element \"\n                        \"#\" + str(j) + \" has length \" + str(len(m)) + \"; 2 is required\"\n                    )\n                # modules can be Mapping (what it's typed at),\n                # or a list: [(name1, module1), (name2, module2)]\n                # that's too cumbersome to type correctly with overloads,\n                # so we add an ignore here\n                self[m[0]] = m[1]  # type: ignore[assignment]islice\n\n    def get(self, key: str, default: Optional[Module] = None) -&gt; Optional[Module]:\n        \"\"\"Return the module for the given key if it exists,\n        else return the default value.\n\n        Args:\n            key:\n                The key to look up in the ModuleDict.\n            default:\n                The value to return if the key is not found. Defaults to None.\n\n        Returns:\n            The module associated with the key, or the default\n            value if the key is not found.\n        \"\"\"\n        return self._modules.get(key, default)\n\n    def set(self, key: str, module: Module) -&gt; None:\n        \"\"\"Set a single key-value pair in the ModuleDict.\n\n        Args:\n            key:\n                The key to set in the ModuleDict.\n            module:\n                The module to associate with the key.\n\n        !!! note\n            This method registers the module using `add_module` to ensure\n            proper registration and preserves the order of insertion,\n            consistent with `ModuleDict` behavior.\n        \"\"\"\n        self.add_module(key, module)\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.__contains__","title":"__contains__","text":"<pre><code>__contains__(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __contains__(self, key: str) -&gt; bool:\n    return key in self._modules\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __delitem__(self, key: str) -&gt; None:\n    del self._modules[key]\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Module:\n    return self._modules[key]\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.__init__","title":"__init__","text":"<pre><code>__init__(modules=None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>iterable</code> <p>a mapping (dictionary) of (string: module) or an iterable of key-value pairs of type (string, module).</p> <code>None</code> <p>Example</p> <pre><code>import random\nimport msgflux.nn as nn\n\nclass ExpertSales(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass ExpertSupport(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, call 190\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\ndef draw_choice(choices: list[str]) -&gt; str:\n    return random.choice(choices)\n\nclass Router(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.choices = nn.ModuleDict({\n            \"sales\": ExpertSales(),\n            \"support\": ExpertSupport()\n        })\n\n    def forward(self, msg: str) -&gt; str:\n        choice = draw_choice(list(self.choices.keys()))\n        msg = self.choices[choice](msg)\n        return msg\n\nrouter = Router()\nrouter(\"I need help with my tv.\")\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __init__(self, modules: Optional[Mapping[str, Module]] = None) -&gt; None:\n    \"\"\"Args:\n        modules (iterable, optional): a mapping (dictionary) of (string: module)\n            or an iterable of key-value pairs of type (string, module).\n\n    !!! example\n        ```python\n        import random\n        import msgflux.nn as nn\n\n        class ExpertSales(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        class ExpertSupport(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, call 190\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        def draw_choice(choices: list[str]) -&gt; str:\n            return random.choice(choices)\n\n        class Router(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.choices = nn.ModuleDict({\n                    \"sales\": ExpertSales(),\n                    \"support\": ExpertSupport()\n                })\n\n            def forward(self, msg: str) -&gt; str:\n                choice = draw_choice(list(self.choices.keys()))\n                msg = self.choices[choice](msg)\n                return msg\n\n        router = Router()\n        router(\"I need help with my tv.\")\n        ```\n    \"\"\"\n    super().__init__()\n    if modules is not None:\n        self.update(modules)\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iter__(self) -&gt; Iterator[str]:\n    return iter(self._modules)\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._modules)\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key, module)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __setitem__(self, key: str, module: Module) -&gt; None:\n    self.add_module(key, module)\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Remove all items from the ModuleDict.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Remove all items from the ModuleDict.\"\"\"\n    self._modules.clear()\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.get","title":"get","text":"<pre><code>get(key, default=None)\n</code></pre> <p>Return the module for the given key if it exists, else return the default value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to look up in the ModuleDict.</p> required <code>default</code> <code>Optional[Module]</code> <p>The value to return if the key is not found. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Module]</code> <p>The module associated with the key, or the default</p> <code>Optional[Module]</code> <p>value if the key is not found.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def get(self, key: str, default: Optional[Module] = None) -&gt; Optional[Module]:\n    \"\"\"Return the module for the given key if it exists,\n    else return the default value.\n\n    Args:\n        key:\n            The key to look up in the ModuleDict.\n        default:\n            The value to return if the key is not found. Defaults to None.\n\n    Returns:\n        The module associated with the key, or the default\n        value if the key is not found.\n    \"\"\"\n    return self._modules.get(key, default)\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.items","title":"items","text":"<pre><code>items()\n</code></pre> <p>Return an iterable of the ModuleDict key/value pairs.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def items(self) -&gt; Iterable[Tuple[str, Module]]:\n    \"\"\"Return an iterable of the ModuleDict key/value pairs.\"\"\"\n    return self._modules.items()\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.keys","title":"keys","text":"<pre><code>keys()\n</code></pre> <p>Return an iterable of the ModuleDict keys.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def keys(self) -&gt; Iterable[str]:\n    \"\"\"Return an iterable of the ModuleDict keys.\"\"\"\n    return self._modules.keys()\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.pop","title":"pop","text":"<pre><code>pop(key)\n</code></pre> <p>Remove key from the ModuleDict and return its module.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key to pop from the ModuleDict.</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def pop(self, key: str) -&gt; Module:\n    \"\"\"Remove key from the ModuleDict and return its module.\n\n    Args:\n        key:\n            key to pop from the ModuleDict.\n    \"\"\"\n    v = self[key]\n    del self[key]\n    return v\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.set","title":"set","text":"<pre><code>set(key, module)\n</code></pre> <p>Set a single key-value pair in the ModuleDict.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set in the ModuleDict.</p> required <code>module</code> <code>Module</code> <p>The module to associate with the key.</p> required <p>Note</p> <p>This method registers the module using <code>add_module</code> to ensure proper registration and preserves the order of insertion, consistent with <code>ModuleDict</code> behavior.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def set(self, key: str, module: Module) -&gt; None:\n    \"\"\"Set a single key-value pair in the ModuleDict.\n\n    Args:\n        key:\n            The key to set in the ModuleDict.\n        module:\n            The module to associate with the key.\n\n    !!! note\n        This method registers the module using `add_module` to ensure\n        proper registration and preserves the order of insertion,\n        consistent with `ModuleDict` behavior.\n    \"\"\"\n    self.add_module(key, module)\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.update","title":"update","text":"<pre><code>update(modules)\n</code></pre> <p>Update the class msgflux.nn.ModuleDict with key-value pairs from a mapping, overwriting existing keys.</p> <p>Note</p> <p>If <code>modules</code> is an <code>OrderedDict</code>, a <code>msgflux.nn.ModuleDict</code>, or an iterable of key-value pairs, the order of new elements in it is preserved.</p> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>Mapping[str, Module]</code> <p>A mapping (dictionary) from string to <code>msgflux.nn.Module</code>, or an iterable of key-value pairs of type (string, <code>msgflux.nn.Module</code>).</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def update(self, modules: Mapping[str, Module]) -&gt; None:\n    \"\"\"Update the class **msgflux.nn.ModuleDict** with\n    key-value pairs from a mapping, overwriting existing keys.\n\n    !!! note\n\n        If `modules` is an `OrderedDict`, a `msgflux.nn.ModuleDict`, or\n        an iterable of key-value pairs, the order of new elements in\n        it is preserved.\n\n    Args:\n        modules:\n            A mapping (dictionary) from string to `msgflux.nn.Module`,\n            or an iterable of key-value pairs of type (string, `msgflux.nn.Module`).\n    \"\"\"\n    if not isinstance(modules, container_abcs.Iterable):\n        raise TypeError(\n            \"ModuleDict.update should be called with an \"\n            \"iterable of key/value pairs, but got \" + type(modules).__name__\n        )\n\n    if isinstance(modules, (OrderedDict, ModuleDict, container_abcs.Mapping)):\n        for key, module in modules.items():\n            self[key] = module\n    else:\n        # modules here can be a list with two items\n        for j, m in enumerate(modules):\n            if not isinstance(m, container_abcs.Iterable):\n                raise TypeError(\n                    \"ModuleDict update sequence element \"\n                    \"#\" + str(j) + \" should be Iterable; is\" + type(m).__name__\n                )\n            if not len(m) == 2:\n                raise ValueError(\n                    \"ModuleDict update sequence element \"\n                    \"#\" + str(j) + \" has length \" + str(len(m)) + \"; 2 is required\"\n                )\n            # modules can be Mapping (what it's typed at),\n            # or a list: [(name1, module1), (name2, module2)]\n            # that's too cumbersome to type correctly with overloads,\n            # so we add an ignore here\n            self[m[0]] = m[1]  # type: ignore[assignment]islice\n</code></pre>"},{"location":"api-reference/nn/modules/module_dict/#msgflux.nn.modules.container.ModuleDict.values","title":"values","text":"<pre><code>values()\n</code></pre> <p>Return an iterable of the ModuleDict values.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def values(self) -&gt; Iterable[Module]:\n    \"\"\"Return an iterable of the ModuleDict values.\"\"\"\n    return self._modules.values()\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/","title":"Module list","text":""},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList","title":"ModuleList","text":"<p>               Bases: <code>Module</code></p> <p>Holds submodules in a list.</p> <p><code>msgflux.nn.ModuleList</code> can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all <code>msgflux.nn.Module</code> methods.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>class ModuleList(Module):\n    \"\"\"Holds submodules in a list.\n\n    `msgflux.nn.ModuleList` can be indexed like a regular Python list, but\n    modules it contains are properly registered, and will be visible by all\n    `msgflux.nn.Module` methods.\n    \"\"\"\n\n    _modules: Dict[str, Module]\n\n    def __init__(self, modules: Optional[Iterable[Module]] = None) -&gt; None:\n        \"\"\"Args:\n            modules (iterable, optional):\n                An iterable of modules to add.\n\n        !!! example\n\n            ```python\n\n            class ExpertSales(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n                def forward(self, msg: str):\n                    return msg + self.response\n\n            class ExpertSupport(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.register_buffer(\"response\", \"Hi, call 190\")\n\n                def forward(self, msg: str):\n                    return msg + self.response\n\n            class Expert(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.experts = nn.ModuleList([ExpertSales(), ExpertSupport()])\n\n                def forward(self, msg: str) -&gt; str:\n                    # ModuleList can act as an iterable, or be indexed using ints\n                    for i, l in enumerate(self.experts):\n                        msg = self.experts[i](msg)\n                    return msg\n\n            expert = Expert()\n            expert(\"I need help with my tv.\")\n            ```\n        \"\"\"\n        super().__init__()\n        if modules is not None:\n            self += modules\n\n    def _get_abs_string_index(self, idx):\n        \"\"\"Get the absolute index for the list of modules.\"\"\"\n        idx = operator.index(idx)\n        if not (-len(self) &lt;= idx &lt; len(self)):\n            raise IndexError(f\"index {idx} is out of range\")\n        if idx &lt; 0:\n            idx += len(self)\n        return str(idx)\n\n    def __getitem__(self, idx: Union[int, slice]) -&gt; Union[Module, \"ModuleList\"]:\n        if isinstance(idx, slice):\n            return self.__class__(list(self._modules.values())[idx])\n        else:\n            return self._modules[self._get_abs_string_index(idx)]\n\n    def __setitem__(self, idx: int, module: Module) -&gt; None:\n        idx = self._get_abs_string_index(idx)\n        return setattr(self, str(idx), module)\n\n    def __delitem__(self, idx: Union[int, slice]) -&gt; None:\n        if isinstance(idx, slice):\n            for k in range(len(self._modules))[idx]:\n                delattr(self, str(k))\n        else:\n            delattr(self, self._get_abs_string_index(idx))\n        # To preserve numbering, self._modules is being\n        # reconstructed with modules after deletion\n        str_indices = [str(i) for i in range(len(self._modules))]\n        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n\n    def __len__(self) -&gt; int:\n        return len(self._modules)\n\n    def __iter__(self) -&gt; Iterator[Module]:\n        return iter(self._modules.values())\n\n    def __iadd__(self, modules: Iterable[Module]) -&gt; Self:\n        return self.extend(modules)\n\n    def __add__(self, other: Iterable[Module]) -&gt; \"ModuleList\":\n        combined = ModuleList()\n        for i, module in enumerate(chain(self, other)):\n            combined.add_module(str(i), module)\n        return combined\n\n    def __repr__(self):\n        \"\"\"Return a custom repr for ModuleList that compresses\n        repeated module representations.\n        \"\"\"\n        list_of_reprs = [repr(item) for item in self]\n        if len(list_of_reprs) == 0:\n            return self._get_name() + \"()\"\n\n        start_end_indices = [[0, 0]]\n        repeated_blocks = [list_of_reprs[0]]\n        for i, r in enumerate(list_of_reprs[1:], 1):\n            if r == repeated_blocks[-1]:\n                start_end_indices[-1][1] += 1\n                continue\n\n            start_end_indices.append([i, i])\n            repeated_blocks.append(r)\n\n        lines = []\n        main_str = self._get_name() + \"(\"\n        for (start_id, end_id), b in zip(start_end_indices, repeated_blocks):\n            local_repr = f\"({start_id}): {b}\"  # default repr\n\n            if start_id != end_id:\n                n = end_id - start_id + 1\n                local_repr = f\"({start_id}-{end_id}): {n} x {b}\"\n\n            local_repr = _addindent(local_repr, 2)\n            lines.append(local_repr)\n\n        main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n        main_str += \")\"\n        return main_str\n\n    def __dir__(self):\n        keys = super().__dir__()\n        keys = [key for key in keys if not key.isdigit()]\n        return keys\n\n    def insert(self, index: int, module: Module) -&gt; None:\n        \"\"\"Insert a given module before a given index in the list.\n\n        Args:\n            index (int): index to insert.\n            module (nn.Module): module to insert\n        \"\"\"\n        for i in range(len(self._modules), index, -1):\n            self._modules[str(i)] = self._modules[str(i - 1)]\n        self._modules[str(index)] = module\n\n    def append(self, module: Module) -&gt; \"ModuleList\":\n        \"\"\"Append a given module to the end of the list.\n\n        Args:\n            module (nn.Module): module to append\n        \"\"\"\n        self.add_module(str(len(self)), module)\n        return self\n\n    def pop(self, key: Union[int, slice]) -&gt; Module:\n        v = self[key]\n        del self[key]\n        return v\n\n    def extend(self, modules: Iterable[Module]) -&gt; Self:\n        \"\"\"Append modules from a Python iterable to the end of the list.\n\n        Args:\n            modules (iterable): iterable of modules to append\n        \"\"\"\n        if not isinstance(modules, container_abcs.Iterable):\n            raise TypeError(\n                \"ModuleList.extend should be called with an \"\n                \"iterable, but got \" + type(modules).__name__\n            )\n        offset = len(self)\n        for i, module in enumerate(modules):\n            self.add_module(str(offset + i), module)\n        return self\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __add__(self, other: Iterable[Module]) -&gt; \"ModuleList\":\n    combined = ModuleList()\n    for i, module in enumerate(chain(self, other)):\n        combined.add_module(str(i), module)\n    return combined\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(idx)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __delitem__(self, idx: Union[int, slice]) -&gt; None:\n    if isinstance(idx, slice):\n        for k in range(len(self._modules))[idx]:\n            delattr(self, str(k))\n    else:\n        delattr(self, self._get_abs_string_index(idx))\n    # To preserve numbering, self._modules is being\n    # reconstructed with modules after deletion\n    str_indices = [str(i) for i in range(len(self._modules))]\n    self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__dir__","title":"__dir__","text":"<pre><code>__dir__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __dir__(self):\n    keys = super().__dir__()\n    keys = [key for key in keys if not key.isdigit()]\n    return keys\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __getitem__(self, idx: Union[int, slice]) -&gt; Union[Module, \"ModuleList\"]:\n    if isinstance(idx, slice):\n        return self.__class__(list(self._modules.values())[idx])\n    else:\n        return self._modules[self._get_abs_string_index(idx)]\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__iadd__","title":"__iadd__","text":"<pre><code>__iadd__(modules)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iadd__(self, modules: Iterable[Module]) -&gt; Self:\n    return self.extend(modules)\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__init__","title":"__init__","text":"<pre><code>__init__(modules=None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>iterable</code> <p>An iterable of modules to add.</p> <code>None</code> <p>Example</p> <pre><code>class ExpertSales(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass ExpertSupport(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, call 190\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass Expert(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.experts = nn.ModuleList([ExpertSales(), ExpertSupport()])\n\n    def forward(self, msg: str) -&gt; str:\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.experts):\n            msg = self.experts[i](msg)\n        return msg\n\nexpert = Expert()\nexpert(\"I need help with my tv.\")\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __init__(self, modules: Optional[Iterable[Module]] = None) -&gt; None:\n    \"\"\"Args:\n        modules (iterable, optional):\n            An iterable of modules to add.\n\n    !!! example\n\n        ```python\n\n        class ExpertSales(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        class ExpertSupport(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, call 190\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        class Expert(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.experts = nn.ModuleList([ExpertSales(), ExpertSupport()])\n\n            def forward(self, msg: str) -&gt; str:\n                # ModuleList can act as an iterable, or be indexed using ints\n                for i, l in enumerate(self.experts):\n                    msg = self.experts[i](msg)\n                return msg\n\n        expert = Expert()\n        expert(\"I need help with my tv.\")\n        ```\n    \"\"\"\n    super().__init__()\n    if modules is not None:\n        self += modules\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Module]:\n    return iter(self._modules.values())\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._modules)\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Return a custom repr for ModuleList that compresses repeated module representations.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a custom repr for ModuleList that compresses\n    repeated module representations.\n    \"\"\"\n    list_of_reprs = [repr(item) for item in self]\n    if len(list_of_reprs) == 0:\n        return self._get_name() + \"()\"\n\n    start_end_indices = [[0, 0]]\n    repeated_blocks = [list_of_reprs[0]]\n    for i, r in enumerate(list_of_reprs[1:], 1):\n        if r == repeated_blocks[-1]:\n            start_end_indices[-1][1] += 1\n            continue\n\n        start_end_indices.append([i, i])\n        repeated_blocks.append(r)\n\n    lines = []\n    main_str = self._get_name() + \"(\"\n    for (start_id, end_id), b in zip(start_end_indices, repeated_blocks):\n        local_repr = f\"({start_id}): {b}\"  # default repr\n\n        if start_id != end_id:\n            n = end_id - start_id + 1\n            local_repr = f\"({start_id}-{end_id}): {n} x {b}\"\n\n        local_repr = _addindent(local_repr, 2)\n        lines.append(local_repr)\n\n    main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n    main_str += \")\"\n    return main_str\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(idx, module)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __setitem__(self, idx: int, module: Module) -&gt; None:\n    idx = self._get_abs_string_index(idx)\n    return setattr(self, str(idx), module)\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.append","title":"append","text":"<pre><code>append(module)\n</code></pre> <p>Append a given module to the end of the list.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module to append</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def append(self, module: Module) -&gt; \"ModuleList\":\n    \"\"\"Append a given module to the end of the list.\n\n    Args:\n        module (nn.Module): module to append\n    \"\"\"\n    self.add_module(str(len(self)), module)\n    return self\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.extend","title":"extend","text":"<pre><code>extend(modules)\n</code></pre> <p>Append modules from a Python iterable to the end of the list.</p> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>iterable</code> <p>iterable of modules to append</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def extend(self, modules: Iterable[Module]) -&gt; Self:\n    \"\"\"Append modules from a Python iterable to the end of the list.\n\n    Args:\n        modules (iterable): iterable of modules to append\n    \"\"\"\n    if not isinstance(modules, container_abcs.Iterable):\n        raise TypeError(\n            \"ModuleList.extend should be called with an \"\n            \"iterable, but got \" + type(modules).__name__\n        )\n    offset = len(self)\n    for i, module in enumerate(modules):\n        self.add_module(str(offset + i), module)\n    return self\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.insert","title":"insert","text":"<pre><code>insert(index, module)\n</code></pre> <p>Insert a given module before a given index in the list.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>index to insert.</p> required <code>module</code> <code>Module</code> <p>module to insert</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def insert(self, index: int, module: Module) -&gt; None:\n    \"\"\"Insert a given module before a given index in the list.\n\n    Args:\n        index (int): index to insert.\n        module (nn.Module): module to insert\n    \"\"\"\n    for i in range(len(self._modules), index, -1):\n        self._modules[str(i)] = self._modules[str(i - 1)]\n    self._modules[str(index)] = module\n</code></pre>"},{"location":"api-reference/nn/modules/module_list/#msgflux.nn.modules.container.ModuleList.pop","title":"pop","text":"<pre><code>pop(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def pop(self, key: Union[int, slice]) -&gt; Module:\n    v = self[key]\n    del self[key]\n    return v\n</code></pre>"},{"location":"api-reference/nn/modules/predictor/","title":"Predictor","text":""},{"location":"api-reference/nn/modules/predictor/#msgflux.nn.modules.predictor.Predictor","title":"Predictor","text":"<p>               Bases: <code>Module</code></p> <p>Predictor is a generic Module type that uses Classifier, Regressors, Detectors and Segmenters to generate insights above data.</p> Source code in <code>src/msgflux/nn/modules/predictor.py</code> <pre><code>class Predictor(Module, metaclass=AutoParams):\n    \"\"\"Predictor is a generic Module type that uses Classifier, Regressors,\n    Detectors and Segmenters to generate insights above data.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: Union[BaseModel, ModelGateway],\n        *,\n        message_fields: Optional[Dict[str, Any]] = None,\n        response_mode: Optional[str] = \"plain_response\",\n        response_template: Optional[str] = None,\n        config: Optional[Dict[str, Any]] = None,\n        name: Optional[str] = None,\n    ):\n        \"\"\"Initialize the Predictor module.\n\n        Args:\n        model:\n            Predictor Model client.\n        message_fields:\n            Dictionary mapping Message field names to their paths in the Message object.\n            Valid keys: \"task_inputs\", \"model_preference\"\n            !!! example\n                message_fields={\n                    \"task_inputs\": \"data.input\",\n                    \"model_preference\": \"model.preference\"\n                }\n\n            Field descriptions:\n            - task_inputs: Field path for task input (str)\n            - model_preference: Field path for model preference (str, only valid\n              with ModelGateway)\n        response_mode:\n            What the response should be.\n            * `plain_response` (default): Returns the final agent response directly.\n            * other: Write on field in Message object.\n        response_template:\n            A Jinja template to format response.\n        config:\n            Dictionary with configuration options. Accepts any keys without validation.\n            All parameters will be passed directly to model execution.\n            !!! example\n                config={\"temperature\": 0.7, \"top_k\": 50}\n        name:\n            Predictor name in snake case format.\n        \"\"\"\n        super().__init__()\n        self._set_model(model)\n        self._set_message_fields(message_fields)\n        self._set_response_mode(response_mode)\n        self._set_response_template(response_template)\n        self._set_config(config)\n        if name:\n            self.set_name(name)\n\n    def forward(self, message: Union[Any, Message], **kwargs) -&gt; Any:\n        \"\"\"Execute the predictor with the given message.\n\n        Args:\n            message: The input message, which can be:\n                - Any: Direct data input for prediction (text, image, audio, etc.)\n                - Message: Message object with fields mapped via message_fields\n            **kwargs: Runtime overrides for message_fields. Can include:\n                - task_inputs: Override field path or direct value\n                - model_preference: Override model preference\n\n        Returns:\n            Prediction results (type depends on model and response_mode)\n        \"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = self._execute_model(**inputs)\n        response = self._process_model_response(model_response, message)\n        return response\n\n    async def aforward(self, message: Union[Any, Message], **kwargs) -&gt; Any:\n        \"\"\"Async version of forward. Execute the predictor asynchronously.\"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = await self._aexecute_model(**inputs)\n        response = self._process_model_response(model_response, message)\n        return response\n\n    def _execute_model(\n        self, data: Any, model_preference: Optional[str] = None\n    ) -&gt; ModelResponse:\n        model_execution_params = self._prepare_model_execution(data, model_preference)\n        model_response = self.model(**model_execution_params)\n        return model_response\n\n    async def _aexecute_model(\n        self, data: Any, model_preference: Optional[str] = None\n    ) -&gt; ModelResponse:\n        model_execution_params = self._prepare_model_execution(data, model_preference)\n        model_response = await self.model.acall(**model_execution_params)\n        return model_response\n\n    def _prepare_model_execution(\n        self, data: Any, model_preference: Optional[str] = None\n    ) -&gt; Dict[str, Any]:\n        model_execution_params = dotdict(self.config) if self.config else dotdict()\n        model_execution_params.data = data\n        if model_preference:\n            model_execution_params.model_preference = model_preference\n        return model_execution_params\n\n    def _process_model_response(\n        self, model_response: ModelResponse, message: Union[Any, Message]\n    ) -&gt; Any:\n        if model_response.response_type == \"audio_generation\":\n            raw_response = self._extract_raw_response(model_response)\n            response = self._prepare_response(raw_response, message)\n            return response\n        else:\n            raise ValueError(\n                f\"Unsupported model response type `{model_response.response_type}`\"\n            )\n\n    def _prepare_task(self, message: Union[Any, Message], **kwargs) -&gt; Dict[str, Any]:\n        inputs = dotdict()\n\n        if isinstance(message, Message):\n            data = self._extract_message_values(self.task_inputs, message)\n        else:\n            data = message\n\n        inputs.data = data\n\n        model_preference = kwargs.pop(\"model_preference\", None)\n        if model_preference is None and isinstance(message, Message):\n            model_preference = self.get_model_preference_from_message(message)\n\n        if model_preference:\n            inputs.model_preference = model_preference\n\n        return inputs\n\n    def inspect_model_execution_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n        \"\"\"Debug model input parameters.\"\"\"\n        inputs = self._prepare_task(*args, **kwargs)\n        model_execution_params = self._prepare_model_execution(**inputs)\n        return model_execution_params\n\n    def _set_model(self, model: Union[BaseModel, ModelGateway]):\n        if isinstance(model, (BaseModel, ModelGateway)):\n            self.register_buffer(\"model\", model)\n        else:\n            raise TypeError(\n                f\"`model` need be a `BaseModel` model, given `{type(model)}`\"\n            )\n\n    def _set_config(self, config: Optional[Dict[str, Any]] = None):\n        if config is None:\n            self.register_buffer(\"config\", {})\n            return\n\n        if not isinstance(config, dict):\n            raise TypeError(f\"`config` must be a dict or None, given `{type(config)}`\")\n\n        self.register_buffer(\"config\", config.copy())\n\n    def _set_message_fields(self, message_fields: Optional[Dict[str, Any]] = None):\n        \"\"\"Set message field mappings.\n\n        Args:\n            message_fields: Dictionary mapping field names to their values.\n                Valid keys: \"task_inputs\", \"model_preference\"\n\n        Raises:\n            TypeError: If message_fields is not a dict or None\n            ValueError: If invalid keys are provided\n        \"\"\"\n        valid_keys = {\"task_inputs\", \"model_preference\"}\n\n        if message_fields is None:\n            self._set_task_inputs(None)\n            self._set_model_preference(None)\n            return\n\n        if not isinstance(message_fields, dict):\n            raise TypeError(f\"`message_fields` must be a dict or None, given `{type(message_fields)}`\")\n\n        # Check for invalid keys\n        invalid_keys = set(message_fields.keys()) - valid_keys\n        if invalid_keys:\n            raise ValueError(\n                f\"Invalid keys in message_fields: {invalid_keys}. \"\n                f\"Valid keys are: {valid_keys}\"\n            )\n\n        # Set individual fields\n        self._set_task_inputs(message_fields.get(\"task_inputs\"))\n        self._set_model_preference(message_fields.get(\"model_preference\"))\n\n    def _set_task_inputs(self, task_inputs: Optional[str] = None):\n        \"\"\"Set task_inputs field mapping.\"\"\"\n        if isinstance(task_inputs, str) or task_inputs is None:\n            self.register_buffer(\"task_inputs\", task_inputs)\n        else:\n            raise TypeError(\n                f\"`task_inputs` requires a string or None, given `{type(task_inputs)}`\"\n            )\n\n    def _set_model_preference(self, model_preference: Optional[str] = None):\n        \"\"\"Set model_preference field mapping.\"\"\"\n        if isinstance(model_preference, str) or model_preference is None:\n            self.register_buffer(\"model_preference\", model_preference)\n        else:\n            raise TypeError(\n                f\"`model_preference` requires a string or None, given `{type(model_preference)}`\"\n            )\n\n    def _set_response_mode(self, response_mode: Optional[str] = None):\n        \"\"\"Set response mode.\"\"\"\n        if isinstance(response_mode, str) or response_mode is None:\n            self.register_buffer(\"response_mode\", response_mode or \"plain_response\")\n        else:\n            raise TypeError(\n                f\"`response_mode` requires a string or None, given `{type(response_mode)}`\"\n            )\n\n    def _set_response_template(self, response_template: Optional[str] = None):\n        \"\"\"Set response template.\"\"\"\n        if isinstance(response_template, str) or response_template is None:\n            self.register_buffer(\"response_template\", response_template)\n        else:\n            raise TypeError(\n                f\"`response_template` requires a string or None, given `{type(response_template)}`\"\n            )\n</code></pre>"},{"location":"api-reference/nn/modules/predictor/#msgflux.nn.modules.predictor.Predictor.__init__","title":"__init__","text":"<pre><code>__init__(\n    model,\n    *,\n    message_fields=None,\n    response_mode=\"plain_response\",\n    response_template=None,\n    config=None,\n    name=None,\n)\n</code></pre> <p>Initialize the Predictor module.</p> <p>model:     Predictor Model client. message_fields:     Dictionary mapping Message field names to their paths in the Message object.     Valid keys: \"task_inputs\", \"model_preference\"     !!! example         message_fields={             \"task_inputs\": \"data.input\",             \"model_preference\": \"model.preference\"         }</p> <pre><code>Field descriptions:\n- task_inputs: Field path for task input (str)\n- model_preference: Field path for model preference (str, only valid\n  with ModelGateway)\n</code></pre> <p>response_mode:     What the response should be.     * <code>plain_response</code> (default): Returns the final agent response directly.     * other: Write on field in Message object. response_template:     A Jinja template to format response. config:     Dictionary with configuration options. Accepts any keys without validation.     All parameters will be passed directly to model execution.     !!! example         config={\"temperature\": 0.7, \"top_k\": 50} name:     Predictor name in snake case format.</p> Source code in <code>src/msgflux/nn/modules/predictor.py</code> <pre><code>def __init__(\n    self,\n    model: Union[BaseModel, ModelGateway],\n    *,\n    message_fields: Optional[Dict[str, Any]] = None,\n    response_mode: Optional[str] = \"plain_response\",\n    response_template: Optional[str] = None,\n    config: Optional[Dict[str, Any]] = None,\n    name: Optional[str] = None,\n):\n    \"\"\"Initialize the Predictor module.\n\n    Args:\n    model:\n        Predictor Model client.\n    message_fields:\n        Dictionary mapping Message field names to their paths in the Message object.\n        Valid keys: \"task_inputs\", \"model_preference\"\n        !!! example\n            message_fields={\n                \"task_inputs\": \"data.input\",\n                \"model_preference\": \"model.preference\"\n            }\n\n        Field descriptions:\n        - task_inputs: Field path for task input (str)\n        - model_preference: Field path for model preference (str, only valid\n          with ModelGateway)\n    response_mode:\n        What the response should be.\n        * `plain_response` (default): Returns the final agent response directly.\n        * other: Write on field in Message object.\n    response_template:\n        A Jinja template to format response.\n    config:\n        Dictionary with configuration options. Accepts any keys without validation.\n        All parameters will be passed directly to model execution.\n        !!! example\n            config={\"temperature\": 0.7, \"top_k\": 50}\n    name:\n        Predictor name in snake case format.\n    \"\"\"\n    super().__init__()\n    self._set_model(model)\n    self._set_message_fields(message_fields)\n    self._set_response_mode(response_mode)\n    self._set_response_template(response_template)\n    self._set_config(config)\n    if name:\n        self.set_name(name)\n</code></pre>"},{"location":"api-reference/nn/modules/predictor/#msgflux.nn.modules.predictor.Predictor.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(message, **kwargs)\n</code></pre> <p>Async version of forward. Execute the predictor asynchronously.</p> Source code in <code>src/msgflux/nn/modules/predictor.py</code> <pre><code>async def aforward(self, message: Union[Any, Message], **kwargs) -&gt; Any:\n    \"\"\"Async version of forward. Execute the predictor asynchronously.\"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = await self._aexecute_model(**inputs)\n    response = self._process_model_response(model_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/predictor/#msgflux.nn.modules.predictor.Predictor.forward","title":"forward","text":"<pre><code>forward(message, **kwargs)\n</code></pre> <p>Execute the predictor with the given message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Union[Any, Message]</code> <p>The input message, which can be: - Any: Direct data input for prediction (text, image, audio, etc.) - Message: Message object with fields mapped via message_fields</p> required <code>**kwargs</code> <p>Runtime overrides for message_fields. Can include: - task_inputs: Override field path or direct value - model_preference: Override model preference</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Prediction results (type depends on model and response_mode)</p> Source code in <code>src/msgflux/nn/modules/predictor.py</code> <pre><code>def forward(self, message: Union[Any, Message], **kwargs) -&gt; Any:\n    \"\"\"Execute the predictor with the given message.\n\n    Args:\n        message: The input message, which can be:\n            - Any: Direct data input for prediction (text, image, audio, etc.)\n            - Message: Message object with fields mapped via message_fields\n        **kwargs: Runtime overrides for message_fields. Can include:\n            - task_inputs: Override field path or direct value\n            - model_preference: Override model preference\n\n    Returns:\n        Prediction results (type depends on model and response_mode)\n    \"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = self._execute_model(**inputs)\n    response = self._process_model_response(model_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/predictor/#msgflux.nn.modules.predictor.Predictor.inspect_model_execution_params","title":"inspect_model_execution_params","text":"<pre><code>inspect_model_execution_params(*args, **kwargs)\n</code></pre> <p>Debug model input parameters.</p> Source code in <code>src/msgflux/nn/modules/predictor.py</code> <pre><code>def inspect_model_execution_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n    \"\"\"Debug model input parameters.\"\"\"\n    inputs = self._prepare_task(*args, **kwargs)\n    model_execution_params = self._prepare_model_execution(**inputs)\n    return model_execution_params\n</code></pre>"},{"location":"api-reference/nn/modules/retriever/","title":"Retriever","text":""},{"location":"api-reference/nn/modules/retriever/#msgflux.nn.modules.retriever.Retriever","title":"Retriever","text":"<p>               Bases: <code>Module</code></p> <p>Retriever is a Module type that uses information retrivers.</p> Source code in <code>src/msgflux/nn/modules/retriever.py</code> <pre><code>class Retriever(Module, metaclass=AutoParams):\n    \"\"\"Retriever is a Module type that uses information retrivers.\"\"\"\n\n    def __init__(\n        self,\n        retriever: RETRIVERS,\n        *,\n        model: Optional[Union[EMBEDDER_MODELS, Embedder]] = None,\n        message_fields: Optional[Dict[str, Any]] = None,\n        response_mode: Optional[str] = \"plain_response\",\n        templates: Optional[Dict[str, str]] = None,\n        config: Optional[Dict[str, Any]] = None,\n        name: Optional[str] = None,\n    ):\n        \"\"\"Initialize the Retriever module.\n\n        Args:\n        retriever:\n            Retriever client.\n        model:\n            Embedding model for converting queries to embeddings. Can be either:\n            - Embedder: Custom Embedder instance (for advanced usage with hooks)\n            - EmbedderModel/ModelGateway: Will be auto-wrapped in Embedder\n            Optional - only needed for semantic retrieval.\n        message_fields:\n            Dictionary mapping Message field names to their paths in the Message object.\n            Valid keys: \"task_inputs\"\n            !!! example\n                message_fields={\"task_inputs\": \"query.user\"}\n\n            Field description:\n            - task_inputs: Field path for query input (str or dict)\n        response_mode:\n            What the response should be.\n            * `plain_response` (default): Returns the final agent response directly.\n            * other: Write on field in Message object.\n        templates:\n            Dictionary mapping template types to Jinja template strings.\n            Valid keys: \"response\"\n            !!! example\n                templates={\"response\": \"Results: {{ content }}\"}\n        config:\n            Dictionary with configuration options. Accepts any keys without validation.\n            Common options: \"top_k\", \"threshold\", \"return_score\", \"dict_key\"\n            !!! example\n                config={\n                    \"top_k\": 4,\n                    \"threshold\": 0.0,\n                    \"return_score\": False,\n                    \"dict_key\": \"name\"\n                }\n\n            Configuration options:\n            - top_k: Maximum return of similar points (int)\n            - threshold: Retriever threshold (float)\n            - return_score: If True, return similarity score (bool)\n            - dict_key: Help to extract a value from task_inputs if dict (str)\n        name:\n            Retriever name in snake case format.\n        \"\"\"\n        super().__init__()\n        self._set_retriever(retriever)\n        self._set_model(model)\n        self._set_message_fields(message_fields)\n        self._set_response_mode(response_mode)\n        self._set_templates(templates)\n        self._set_config(config)\n        if name:\n            self.set_name(name)\n\n    def forward(\n        self, message: Union[str, List[str], List[Dict[str, Any]], Message], **kwargs\n    ) -&gt; Union[str, Dict[str, str], Message]:\n        \"\"\"Execute the retriever with the given message.\n\n        Args:\n            message: The input message, which can be:\n                - str: Direct query string for retrieval\n                - List[str]: List of query strings\n                - List[Dict[str, Any]]: List of query dictionaries\n                - Message: Message object with fields mapped via message_fields\n            **kwargs: Runtime overrides for message_fields. Can include:\n                - task_inputs: Override field path or direct value\n\n        Returns:\n            Retrieved results (str, dict, or Message depending on response_mode)\n        \"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        retriever_response = self._execute_retriever(**inputs)\n        response = self._prepare_response(retriever_response, message)\n        return response\n\n    async def aforward(\n        self, message: Union[str, List[str], List[Dict[str, Any]], Message], **kwargs\n    ) -&gt; Union[str, Dict[str, str], Message]:\n        \"\"\"Async version of forward. Execute the retriever asynchronously.\"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        retriever_response = await self._aexecute_retriever(**inputs)\n        response = self._prepare_response(retriever_response, message)\n        return response\n\n    def _execute_retriever(\n        self, queries: List[str], model_preference: Optional[str] = None\n    ) -&gt; List[Dict[str, Any]]:\n        queries_embed = None\n        if self.embedder:\n            queries_embed = self.embedder(queries, model_preference=model_preference)\n            # Ensure list format\n            if not isinstance(queries_embed[0], list):\n                queries_embed = [queries_embed]\n\n        retriever_execution_params = self._prepare_retriever_execution(\n            queries_embed or queries\n        )\n        retriever_response = self.retriever(**retriever_execution_params)\n\n        results = []\n\n        for query, query_results in zip(queries, retriever_response):\n            formatted_result = {\n                \"results\": [\n                    {\"data\": item.get(\"data\", None), \"score\": item.get(\"score\", None)}\n                    for item in query_results\n                ],\n            }\n            if isinstance(query, str):\n                formatted_result[\"query\"] = query\n            results.append(formatted_result)\n\n        return results\n\n    async def _aexecute_retriever(\n        self, queries: List[str], model_preference: Optional[str] = None\n    ) -&gt; List[Dict[str, Any]]:\n        queries_embed = None\n        if self.embedder:\n            queries_embed = await self.embedder.aforward(\n                queries, model_preference=model_preference\n            )\n            # Ensure list format\n            if not isinstance(queries_embed[0], list):\n                queries_embed = [queries_embed]\n\n        retriever_execution_params = self._prepare_retriever_execution(\n            queries_embed or queries\n        )\n        retriever_response = self.retriever(**retriever_execution_params)\n\n        results = []\n\n        for query, query_results in zip(queries, retriever_response):\n            formatted_result = {\n                \"results\": [\n                    {\"data\": item.get(\"data\", None), \"score\": item.get(\"score\", None)}\n                    for item in query_results\n                ],\n            }\n            if isinstance(query, str):\n                formatted_result[\"query\"] = query\n            results.append(formatted_result)\n\n        return results\n\n    def _prepare_retriever_execution(\n        self, queries: List[Union[str, List[float]]]\n    ) -&gt; Dict[str, Any]:\n        retriever_execution_params = dotdict(\n            queries=queries,\n            top_k=self.config.get(\"top_k\", 4),\n            return_score=self.config.get(\"return_score\", False),\n        )\n        threshold = self.config.get(\"threshold\")\n        if threshold:\n            retriever_execution_params.threshold = threshold\n        return retriever_execution_params\n\n    def _prepare_task(\n        self, message: Union[str, List[str], List[Dict[str, Any]], Message], **kwargs\n    ) -&gt; List[str]:\n        if isinstance(message, Message):\n            queries = self._extract_message_values(self.task_inputs, message)\n        else:\n            queries = message\n\n        if isinstance(queries, str):\n            queries = [queries]\n        elif isinstance(queries, list):\n            if isinstance(queries[0], dict):\n                queries = self._process_list_of_dict_inputs(queries)\n\n        model_preference = kwargs.pop(\"model_preference\", None)\n        if model_preference is None and isinstance(message, Message):\n            model_preference = self.get_model_preference_from_message(message)\n\n        return {\"queries\": queries, \"model_preference\": model_preference}\n\n    def _process_list_of_dict_inputs(self, queries: List[Dict[str, Any]]) -&gt; List[str]:\n        \"\"\"Extract the query value from a dict.\"\"\"\n        dict_key = self.config.get(\"dict_key\")\n        if dict_key:\n            queries_list = [data[dict_key] for data in queries]\n            return queries_list\n        else:\n            raise AttributeError(\n                \"message that contain `List[Dict[str, Any]]` \"\n                \"require a `dict_key` to select the key for retrieval\"\n            )\n\n    def inspect_embedder_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n        \"\"\"Debug embedder input parameters.\n\n        Returns the parameters that would be passed to the embedder module.\n        \"\"\"\n        if self.embedder:\n            inputs = self._prepare_task(*args, **kwargs)\n            return {\n                \"queries\": inputs[\"queries\"],\n                \"model_preference\": inputs.get(\"model_preference\"),\n            }\n        return {}\n\n    def _set_retriever(self, retriever: RETRIVERS):\n        if isinstance(\n            retriever, (WebRetriever, LexicalRetriever, SemanticRetriever, VectorDB)\n        ):\n            self.register_buffer(\"retriever\", retriever)\n        else:\n            raise TypeError(\n                \"`retriever` requires `HybridRetriever`, `LexicalRetriever`, \"\n                f\"`SemanticRetriever` or `VectorDB` instance given `{type(retriever)}`\"\n            )\n\n    def _set_model(self, model: Optional[Union[EMBEDDER_MODELS, Embedder]] = None):\n        if model is None:\n            self.embedder = None\n            return\n\n        if isinstance(model, Embedder):  # If already Embedder, use directly\n            self.embedder = model\n        else:  # Auto-wrap in Embedder\n            self.embedder = Embedder(model=model)\n\n    @property\n    def model(self):\n        \"\"\"Access underlying model for convenience.\n\n        Returns:\n            The wrapped model instance, or None if no embedder\n        \"\"\"\n        if self.embedder is None:\n            return None\n        return self.embedder.model\n\n    @model.setter\n    def model(self, value: Optional[Union[EMBEDDER_MODELS, Embedder]]):\n        \"\"\"Update the retriever's model.\"\"\"\n        self._set_model(value)\n\n    def _set_config(self, config: Optional[Dict[str, Any]] = None):\n        if config is None:\n            self.register_buffer(\"config\", {})\n            return\n\n        if not isinstance(config, dict):\n            raise TypeError(f\"`config` must be a dict or None, given `{type(config)}`\")\n\n        self.register_buffer(\"config\", config.copy())\n</code></pre>"},{"location":"api-reference/nn/modules/retriever/#msgflux.nn.modules.retriever.Retriever.model","title":"model  <code>property</code> <code>writable</code>","text":"<pre><code>model\n</code></pre> <p>Access underlying model for convenience.</p> <p>Returns:</p> Type Description <p>The wrapped model instance, or None if no embedder</p>"},{"location":"api-reference/nn/modules/retriever/#msgflux.nn.modules.retriever.Retriever.__init__","title":"__init__","text":"<pre><code>__init__(\n    retriever,\n    *,\n    model=None,\n    message_fields=None,\n    response_mode=\"plain_response\",\n    templates=None,\n    config=None,\n    name=None,\n)\n</code></pre> <p>Initialize the Retriever module.</p> <p>retriever:     Retriever client. model:     Embedding model for converting queries to embeddings. Can be either:     - Embedder: Custom Embedder instance (for advanced usage with hooks)     - EmbedderModel/ModelGateway: Will be auto-wrapped in Embedder     Optional - only needed for semantic retrieval. message_fields:     Dictionary mapping Message field names to their paths in the Message object.     Valid keys: \"task_inputs\"     !!! example         message_fields={\"task_inputs\": \"query.user\"}</p> <pre><code>Field description:\n- task_inputs: Field path for query input (str or dict)\n</code></pre> <p>response_mode:     What the response should be.     * <code>plain_response</code> (default): Returns the final agent response directly.     * other: Write on field in Message object. templates:     Dictionary mapping template types to Jinja template strings.     Valid keys: \"response\"     !!! example         templates={\"response\": \"Results: {{ content }}\"} config:     Dictionary with configuration options. Accepts any keys without validation.     Common options: \"top_k\", \"threshold\", \"return_score\", \"dict_key\"     !!! example         config={             \"top_k\": 4,             \"threshold\": 0.0,             \"return_score\": False,             \"dict_key\": \"name\"         }</p> <pre><code>Configuration options:\n- top_k: Maximum return of similar points (int)\n- threshold: Retriever threshold (float)\n- return_score: If True, return similarity score (bool)\n- dict_key: Help to extract a value from task_inputs if dict (str)\n</code></pre> <p>name:     Retriever name in snake case format.</p> Source code in <code>src/msgflux/nn/modules/retriever.py</code> <pre><code>def __init__(\n    self,\n    retriever: RETRIVERS,\n    *,\n    model: Optional[Union[EMBEDDER_MODELS, Embedder]] = None,\n    message_fields: Optional[Dict[str, Any]] = None,\n    response_mode: Optional[str] = \"plain_response\",\n    templates: Optional[Dict[str, str]] = None,\n    config: Optional[Dict[str, Any]] = None,\n    name: Optional[str] = None,\n):\n    \"\"\"Initialize the Retriever module.\n\n    Args:\n    retriever:\n        Retriever client.\n    model:\n        Embedding model for converting queries to embeddings. Can be either:\n        - Embedder: Custom Embedder instance (for advanced usage with hooks)\n        - EmbedderModel/ModelGateway: Will be auto-wrapped in Embedder\n        Optional - only needed for semantic retrieval.\n    message_fields:\n        Dictionary mapping Message field names to their paths in the Message object.\n        Valid keys: \"task_inputs\"\n        !!! example\n            message_fields={\"task_inputs\": \"query.user\"}\n\n        Field description:\n        - task_inputs: Field path for query input (str or dict)\n    response_mode:\n        What the response should be.\n        * `plain_response` (default): Returns the final agent response directly.\n        * other: Write on field in Message object.\n    templates:\n        Dictionary mapping template types to Jinja template strings.\n        Valid keys: \"response\"\n        !!! example\n            templates={\"response\": \"Results: {{ content }}\"}\n    config:\n        Dictionary with configuration options. Accepts any keys without validation.\n        Common options: \"top_k\", \"threshold\", \"return_score\", \"dict_key\"\n        !!! example\n            config={\n                \"top_k\": 4,\n                \"threshold\": 0.0,\n                \"return_score\": False,\n                \"dict_key\": \"name\"\n            }\n\n        Configuration options:\n        - top_k: Maximum return of similar points (int)\n        - threshold: Retriever threshold (float)\n        - return_score: If True, return similarity score (bool)\n        - dict_key: Help to extract a value from task_inputs if dict (str)\n    name:\n        Retriever name in snake case format.\n    \"\"\"\n    super().__init__()\n    self._set_retriever(retriever)\n    self._set_model(model)\n    self._set_message_fields(message_fields)\n    self._set_response_mode(response_mode)\n    self._set_templates(templates)\n    self._set_config(config)\n    if name:\n        self.set_name(name)\n</code></pre>"},{"location":"api-reference/nn/modules/retriever/#msgflux.nn.modules.retriever.Retriever.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(message, **kwargs)\n</code></pre> <p>Async version of forward. Execute the retriever asynchronously.</p> Source code in <code>src/msgflux/nn/modules/retriever.py</code> <pre><code>async def aforward(\n    self, message: Union[str, List[str], List[Dict[str, Any]], Message], **kwargs\n) -&gt; Union[str, Dict[str, str], Message]:\n    \"\"\"Async version of forward. Execute the retriever asynchronously.\"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    retriever_response = await self._aexecute_retriever(**inputs)\n    response = self._prepare_response(retriever_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/retriever/#msgflux.nn.modules.retriever.Retriever.forward","title":"forward","text":"<pre><code>forward(message, **kwargs)\n</code></pre> <p>Execute the retriever with the given message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Union[str, List[str], List[Dict[str, Any]], Message]</code> <p>The input message, which can be: - str: Direct query string for retrieval - List[str]: List of query strings - List[Dict[str, Any]]: List of query dictionaries - Message: Message object with fields mapped via message_fields</p> required <code>**kwargs</code> <p>Runtime overrides for message_fields. Can include: - task_inputs: Override field path or direct value</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, Dict[str, str], Message]</code> <p>Retrieved results (str, dict, or Message depending on response_mode)</p> Source code in <code>src/msgflux/nn/modules/retriever.py</code> <pre><code>def forward(\n    self, message: Union[str, List[str], List[Dict[str, Any]], Message], **kwargs\n) -&gt; Union[str, Dict[str, str], Message]:\n    \"\"\"Execute the retriever with the given message.\n\n    Args:\n        message: The input message, which can be:\n            - str: Direct query string for retrieval\n            - List[str]: List of query strings\n            - List[Dict[str, Any]]: List of query dictionaries\n            - Message: Message object with fields mapped via message_fields\n        **kwargs: Runtime overrides for message_fields. Can include:\n            - task_inputs: Override field path or direct value\n\n    Returns:\n        Retrieved results (str, dict, or Message depending on response_mode)\n    \"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    retriever_response = self._execute_retriever(**inputs)\n    response = self._prepare_response(retriever_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/retriever/#msgflux.nn.modules.retriever.Retriever.inspect_embedder_params","title":"inspect_embedder_params","text":"<pre><code>inspect_embedder_params(*args, **kwargs)\n</code></pre> <p>Debug embedder input parameters.</p> <p>Returns the parameters that would be passed to the embedder module.</p> Source code in <code>src/msgflux/nn/modules/retriever.py</code> <pre><code>def inspect_embedder_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n    \"\"\"Debug embedder input parameters.\n\n    Returns the parameters that would be passed to the embedder module.\n    \"\"\"\n    if self.embedder:\n        inputs = self._prepare_task(*args, **kwargs)\n        return {\n            \"queries\": inputs[\"queries\"],\n            \"model_preference\": inputs.get(\"model_preference\"),\n        }\n    return {}\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/","title":"Sequential","text":""},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential","title":"Sequential","text":"<p>               Bases: <code>Module</code></p> <p>A sequential container.</p> <p>Modules will be added to it in the order they are passed in the constructor. Alternatively, an <code>OrderedDict</code> of modules can be passed in. The <code>forward()</code> method of <code>Sequential</code> accepts any input and forwards it to the first module it contains. It then \"chains\" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.</p> <p>The value a <code>Sequential</code> provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the <code>Sequential</code> applies to each of the modules it stores (which are each a registered submodule of the <code>Sequential</code>).</p> <p>What's the difference between a <code>Sequential</code> and a <code>msgflux.nn.ModuleList</code>? A <code>ModuleList</code> is exactly what it sounds like--a list for storing <code>Module</code>s! On the other hand, the layers in a <code>Sequential</code> are connected in a cascading way.</p> <p>Example</p> <pre><code>from collections import OrderedDict\nimport msgflux.nn as nn\nclass ExpertSales(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass ExpertSupport(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, call 190\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\n# Using Sequential to create a small workflow. When **expert** is run,\n# input will first be passed to **ExpertSales**. The output of\n# **ExpertSales** will be used as the input to the first\n# **ExpertSupport**; Finally, the output of\n# **ExpertSupport** will be the experts response.\nexperts = nn.Sequential(ExpertSales(), ExpertSupport())\nexperts(\"I need help with my tv.\")\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nexperts_dict = nn.Sequential(OrderedDict([\n    (\"expert_sales\", ExpertSales()),\n    (\"expert_support\", ExpertSupport())\n]))\nexperts_dict(\"I need help with my tv.\")\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>class Sequential(Module):\n    \"\"\"A sequential container.\n\n    Modules will be added to it in the order they are passed in the\n    constructor. Alternatively, an `OrderedDict` of modules can be\n    passed in. The `forward()` method of `Sequential` accepts any\n    input and forwards it to the first module it contains. It then\n    \"chains\" outputs to inputs sequentially for each subsequent module,\n    finally returning the output of the last module.\n\n    The value a `Sequential` provides over manually calling a sequence\n    of modules is that it allows treating the whole container as a\n    single module, such that performing a transformation on the\n    `Sequential` applies to each of the modules it stores (which are\n    each a registered submodule of the `Sequential`).\n\n    What's the difference between a `Sequential` and a\n    `msgflux.nn.ModuleList`? A `ModuleList` is exactly what it\n    sounds like--a list for storing `Module`s! On the other hand,\n    the layers in a `Sequential` are connected in a cascading way.\n\n    !!! example\n        ```python\n        from collections import OrderedDict\n        import msgflux.nn as nn\n        class ExpertSales(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        class ExpertSupport(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, call 190\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        # Using Sequential to create a small workflow. When **expert** is run,\n        # input will first be passed to **ExpertSales**. The output of\n        # **ExpertSales** will be used as the input to the first\n        # **ExpertSupport**; Finally, the output of\n        # **ExpertSupport** will be the experts response.\n        experts = nn.Sequential(ExpertSales(), ExpertSupport())\n        experts(\"I need help with my tv.\")\n\n        # Using Sequential with OrderedDict. This is functionally the\n        # same as the above code\n        experts_dict = nn.Sequential(OrderedDict([\n            (\"expert_sales\", ExpertSales()),\n            (\"expert_support\", ExpertSupport())\n        ]))\n        experts_dict(\"I need help with my tv.\")\n        ```\n    \"\"\"\n\n    _modules: Dict[str, Module] = OrderedDict()\n\n    def __init__(self, *args: Union[Module, OrderedDict[str, Module]]):\n        super().__init__()\n        if len(args) == 1 and isinstance(args[0], OrderedDict):\n            for key, module in args[0].items():\n                self.add_module(key, module)\n        else:\n            for idx, module in enumerate(args):\n                self.add_module(str(idx), module)\n\n    def forward(self, *args, **kwargs) -&gt; Any:\n        modules_iter = iter(self._modules.values())\n        first_module = next(modules_iter)\n        output = first_module(*args, **kwargs)\n\n        for module in modules_iter:\n            output = module(output)\n\n        return output\n\n    async def aforward(self, *args, **kwargs) -&gt; Any:\n        \"\"\"Async version of forward. Executes modules sequentially with async\n        support.\n        \"\"\"\n        modules_iter = iter(self._modules.values())\n        first_module = next(modules_iter)\n\n        # Check for acall method first, then coroutine function\n        if hasattr(first_module, \"acall\"):\n            output = await first_module.acall(*args, **kwargs)\n        elif asyncio.iscoroutinefunction(first_module):\n            output = await first_module(*args, **kwargs)\n        else:\n            # Fallback to sync call\n            output = first_module(*args, **kwargs)\n\n        for module in modules_iter:\n            # Check for acall method first, then coroutine function\n            if hasattr(module, \"acall\"):\n                output = await module.acall(output)\n            elif asyncio.iscoroutinefunction(module):\n                output = await module(output)\n            else:\n                # Fallback to sync call\n                output = module(output)\n\n        return output\n\n    def _get_mermaid(\n        self,\n        title: Optional[str] = None,\n        orientation: Optional[str] = \"TD\",\n    ) -&gt; str:\n        mermaid_code = [\n            \"%%{\",\n            \"        init: {\",\n            \"            'theme': 'base',\",\n            \"            'themeVariables': {\",\n            \"            'primaryColor': '#E9E7E7',\",\n            \"            'primaryTextColor': '#000000',\",\n            \"            'primaryBorderColor': '#C0000',\",\n            \"            'lineColor': '#F8B229',\",\n            \"            'secondaryColor': '#91939C',\",\n            \"            'tertiaryColor': '#fff'\",\n            \"            }\",\n            \"        }\",\n            \"    }%%\",\n            f\"flowchart {orientation}\",\n        ]\n\n        if title:\n            mermaid_code.insert(0, f\"%% Title: {title}\")\n\n        mermaid_code.append(\"subgraph PARAMETERS\")\n        mermaid_code.append(\"direction LR\")\n        mermaid_code.append(\"param_msg([**msg**])\")\n        mermaid_code.append(\"end\")\n\n        first_node = None\n        for i, module_name in enumerate(self._modules.keys()):\n            node_id = f\"node_{i}\"\n            if i == 0:\n                first_node = node_id\n            mermaid_code.append(f\"{node_id}[ **msg = {module_name}\ufe59msg\ufe5a** ]\")\n\n        for i in range(len(self._modules) - 1):\n            mermaid_code.append(f\"node_{i} --&gt; node_{i + 1}\")\n\n        last_node = f\"node_{len(self._modules) - 1}\"\n        mermaid_code.append(f\"{last_node} --&gt; node_return\")\n        mermaid_code.append(\"node_return([**return msg**])\")\n\n        if first_node:\n            mermaid_code.append(f\"PARAMETERS --&gt; {first_node}\")\n\n        mermaid_code.append(\"%% Styles\")\n        mermaid_code.append(\n            \"classDef terminal fill:#FFF4DD,stroke:#333,stroke-width:2px;\"\n        )\n        mermaid_code.append(\n            \"classDef parameter fill:#FFF4DD,stroke:#333,stroke-width:px;\"\n        )\n\n        for i in range(len(self._modules)):\n            mermaid_code.append(f\"class node_{i} default;\")\n        mermaid_code.append(\"class node_return terminal;\")\n\n        mermaid_code.append(\"class param_msg parameter;\")\n\n        return \"\\n\".join(mermaid_code)\n\n    def _get_item_by_idx(self, iterator, idx) -&gt; T:\n        \"\"\"Get the idx-th item of the iterator.\"\"\"\n        size = len(self)\n        idx = operator.index(idx)\n        if not -size &lt;= idx &lt; size:\n            raise IndexError(f\"index {idx} is out of range\")\n        idx %= size\n        return next(islice(iterator, idx, None))\n\n    def __getitem__(self, idx: Union[slice, int]) -&gt; Union[\"Sequential\", T]:\n        if isinstance(idx, slice):\n            return self.__class__(OrderedDict(list(self._modules.items())[idx]))\n        else:\n            return self._get_item_by_idx(self._modules.values(), idx)\n\n    def __setitem__(self, idx: int, module: Module) -&gt; None:\n        key: str = self._get_item_by_idx(self._modules.keys(), idx)\n        return setattr(self, key, module)\n\n    def __delitem__(self, idx: Union[slice, int]) -&gt; None:\n        if isinstance(idx, slice):\n            for key in list(self._modules.keys())[idx]:\n                delattr(self, key)\n        else:\n            key = self._get_item_by_idx(self._modules.keys(), idx)\n            delattr(self, key)\n        # To preserve numbering\n        str_indices = [str(i) for i in range(len(self._modules))]\n        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n\n    def __len__(self) -&gt; int:\n        return len(self._modules)\n\n    def __add__(self, other) -&gt; \"Sequential\":\n        if isinstance(other, Sequential):\n            ret = Sequential()\n            for layer in self:\n                ret.append(layer)\n            for layer in other:\n                ret.append(layer)\n            return ret\n        else:\n            raise ValueError(\n                \"add operator supports only objects \"\n                f\"of Sequential class, but {type(other)!s} is given.\"\n            )\n\n    def pop(self, key: Union[int, slice]) -&gt; Module:\n        v = self[key]\n        del self[key]\n        return v\n\n    def __iadd__(self, other) -&gt; Self:\n        if isinstance(other, Sequential):\n            offset = len(self)\n            for i, module in enumerate(other):\n                self.add_module(str(i + offset), module)\n            return self\n        else:\n            raise ValueError(\n                \"add operator supports only objects \"\n                f\"of Sequential class, but {type(other)!s} is given.\"\n            )\n\n    def __dir__(self):\n        keys = super().__dir__()\n        keys = [key for key in keys if not key.isdigit()]\n        return keys\n\n    def __iter__(self) -&gt; Iterator[Module]:\n        return iter(self._modules.values())\n\n    def append(self, module: Module) -&gt; \"Sequential\":\n        r\"\"\"Append a given module to the end.\n\n        Args:\n            module (nn.Module): module to append\n        \"\"\"\n        self.add_module(str(len(self)), module)\n        return self\n\n    def insert(self, index: int, module: Module) -&gt; \"Sequential\":\n        if not isinstance(module, Module):\n            raise AssertionError(f\"module should be of type: {Module}\")\n        n = len(self._modules)\n        if not (-n &lt;= index &lt;= n):\n            raise IndexError(f\"Index out of range: {index}\")\n        if index &lt; 0:\n            index += n\n        for i in range(n, index, -1):\n            self._modules[str(i)] = self._modules[str(i - 1)]\n        self._modules[str(index)] = module\n        return self\n\n    def extend(self, sequential) -&gt; \"Sequential\":\n        for layer in sequential:\n            self.append(layer)\n        return self\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __add__(self, other) -&gt; \"Sequential\":\n    if isinstance(other, Sequential):\n        ret = Sequential()\n        for layer in self:\n            ret.append(layer)\n        for layer in other:\n            ret.append(layer)\n        return ret\n    else:\n        raise ValueError(\n            \"add operator supports only objects \"\n            f\"of Sequential class, but {type(other)!s} is given.\"\n        )\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(idx)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __delitem__(self, idx: Union[slice, int]) -&gt; None:\n    if isinstance(idx, slice):\n        for key in list(self._modules.keys())[idx]:\n            delattr(self, key)\n    else:\n        key = self._get_item_by_idx(self._modules.keys(), idx)\n        delattr(self, key)\n    # To preserve numbering\n    str_indices = [str(i) for i in range(len(self._modules))]\n    self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__dir__","title":"__dir__","text":"<pre><code>__dir__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __dir__(self):\n    keys = super().__dir__()\n    keys = [key for key in keys if not key.isdigit()]\n    return keys\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __getitem__(self, idx: Union[slice, int]) -&gt; Union[\"Sequential\", T]:\n    if isinstance(idx, slice):\n        return self.__class__(OrderedDict(list(self._modules.items())[idx]))\n    else:\n        return self._get_item_by_idx(self._modules.values(), idx)\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__iadd__","title":"__iadd__","text":"<pre><code>__iadd__(other)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iadd__(self, other) -&gt; Self:\n    if isinstance(other, Sequential):\n        offset = len(self)\n        for i, module in enumerate(other):\n            self.add_module(str(i + offset), module)\n        return self\n    else:\n        raise ValueError(\n            \"add operator supports only objects \"\n            f\"of Sequential class, but {type(other)!s} is given.\"\n        )\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__init__","title":"__init__","text":"<pre><code>__init__(*args)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __init__(self, *args: Union[Module, OrderedDict[str, Module]]):\n    super().__init__()\n    if len(args) == 1 and isinstance(args[0], OrderedDict):\n        for key, module in args[0].items():\n            self.add_module(key, module)\n    else:\n        for idx, module in enumerate(args):\n            self.add_module(str(idx), module)\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Module]:\n    return iter(self._modules.values())\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._modules)\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(idx, module)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __setitem__(self, idx: int, module: Module) -&gt; None:\n    key: str = self._get_item_by_idx(self._modules.keys(), idx)\n    return setattr(self, key, module)\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(*args, **kwargs)\n</code></pre> <p>Async version of forward. Executes modules sequentially with async support.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>async def aforward(self, *args, **kwargs) -&gt; Any:\n    \"\"\"Async version of forward. Executes modules sequentially with async\n    support.\n    \"\"\"\n    modules_iter = iter(self._modules.values())\n    first_module = next(modules_iter)\n\n    # Check for acall method first, then coroutine function\n    if hasattr(first_module, \"acall\"):\n        output = await first_module.acall(*args, **kwargs)\n    elif asyncio.iscoroutinefunction(first_module):\n        output = await first_module(*args, **kwargs)\n    else:\n        # Fallback to sync call\n        output = first_module(*args, **kwargs)\n\n    for module in modules_iter:\n        # Check for acall method first, then coroutine function\n        if hasattr(module, \"acall\"):\n            output = await module.acall(output)\n        elif asyncio.iscoroutinefunction(module):\n            output = await module(output)\n        else:\n            # Fallback to sync call\n            output = module(output)\n\n    return output\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.append","title":"append","text":"<pre><code>append(module)\n</code></pre> <p>Append a given module to the end.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module to append</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def append(self, module: Module) -&gt; \"Sequential\":\n    r\"\"\"Append a given module to the end.\n\n    Args:\n        module (nn.Module): module to append\n    \"\"\"\n    self.add_module(str(len(self)), module)\n    return self\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.extend","title":"extend","text":"<pre><code>extend(sequential)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def extend(self, sequential) -&gt; \"Sequential\":\n    for layer in sequential:\n        self.append(layer)\n    return self\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; Any:\n    modules_iter = iter(self._modules.values())\n    first_module = next(modules_iter)\n    output = first_module(*args, **kwargs)\n\n    for module in modules_iter:\n        output = module(output)\n\n    return output\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.insert","title":"insert","text":"<pre><code>insert(index, module)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def insert(self, index: int, module: Module) -&gt; \"Sequential\":\n    if not isinstance(module, Module):\n        raise AssertionError(f\"module should be of type: {Module}\")\n    n = len(self._modules)\n    if not (-n &lt;= index &lt;= n):\n        raise IndexError(f\"Index out of range: {index}\")\n    if index &lt; 0:\n        index += n\n    for i in range(n, index, -1):\n        self._modules[str(i)] = self._modules[str(i - 1)]\n    self._modules[str(index)] = module\n    return self\n</code></pre>"},{"location":"api-reference/nn/modules/sequential/#msgflux.nn.modules.container.Sequential.pop","title":"pop","text":"<pre><code>pop(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def pop(self, key: Union[int, slice]) -&gt; Module:\n    v = self[key]\n    del self[key]\n    return v\n</code></pre>"},{"location":"api-reference/nn/modules/speaker/","title":"Speaker","text":""},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker","title":"Speaker","text":"<p>               Bases: <code>Module</code></p> <p>Speaker is a Module type that uses language models to transform text in speak.</p> Source code in <code>src/msgflux/nn/modules/speaker.py</code> <pre><code>class Speaker(Module, metaclass=AutoParams):\n    \"\"\"Speaker is a Module type that uses language models to transform text in speak.\"\"\"\n\n    def __init__(\n        self,\n        model: Union[TextToSpeechModel, ModelGateway],\n        *,\n        guardrails: Optional[Dict[str, Callable]] = None,\n        message_fields: Optional[Dict[str, Any]] = None,\n        response_mode: Optional[str] = \"plain_response\",\n        response_format: Optional[\n            Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"]\n        ] = \"opus\",\n        prompt: Optional[str] = None,\n        config: Optional[Dict[str, Any]] = None,\n        name: Optional[str] = None,\n    ):\n        \"\"\"Initialize the Speaker module.\n\n        Args:\n        model:\n            Transcriber Model client.\n        guardrails:\n            Dictionary mapping guardrail types to callables.\n            Valid keys: \"input\" (output not supported for Speaker).\n            !!! example\n                guardrails={\"input\": input_checker}\n        message_fields:\n            Dictionary mapping Message field names to their paths in the Message object.\n            Valid keys: \"task_inputs\" (other fields not supported for Speaker).\n            !!! example\n                message_fields={\"task_inputs\": \"input.user\"}\n\n            Field description:\n            - task_inputs: Field path for task input (str)\n        response_mode:\n            What the response should be.\n            * `plain_response` (default): Returns the final agent response directly.\n            * other: Write on field in Message object.\n        response_format:\n            The format to audio in.\n        prompt:\n            Useful for instructing the model to follow some speak generation pattern.\n        config:\n            Dictionary with configuration options. Accepts any keys without validation.\n            Common option: \"stream\"\n            !!! example\n                config={\"stream\": False}\n\n            Configuration option:\n            - stream: Transmit response on-the-fly (bool)\n        name:\n            Transcriber name in snake case format.\n        \"\"\"\n        super().__init__()\n        self._set_guardrails(guardrails)\n        self._set_model(model)\n        self._set_prompt(prompt)\n        self._set_response_format(response_format)\n        self._set_response_mode(response_mode)\n        self._set_message_fields(message_fields)\n        self._set_config(config)\n        if name:\n            self.set_name(name)\n\n    def forward(\n        self, message: Union[str, Message], **kwargs\n    ) -&gt; Union[bytes, ModelStreamResponse]:\n        \"\"\"Execute the speaker with the given message.\n\n        Args:\n            message: The input message, which can be:\n                - str: Direct text input to convert to speech\n                - Message: Message object with fields mapped via message_fields\n            **kwargs: Runtime overrides for message_fields. Can include:\n                - task_inputs: Override field path or direct value\n\n        Returns:\n            Audio bytes or ModelStreamResponse if stream=True\n\n        Examples:\n            # Direct string input\n            speaker(\"Hello world\")\n\n            # Using Message object with message_fields\n            msg = Message(text=\"Hello world\")\n            speaker(msg)\n\n            # Runtime override\n            speaker(msg, task_inputs=\"custom.path\")\n        \"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = self._execute_model(**inputs)\n        response = self._process_model_response(model_response, message)\n        return response\n\n    async def aforward(\n        self, message: Union[str, Message], **kwargs\n    ) -&gt; Union[bytes, ModelStreamResponse]:\n        \"\"\"Async version of forward. Execute the speaker asynchronously.\"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = await self._aexecute_model(**inputs)\n        response = self._process_model_response(model_response, message)\n        return response\n\n    def _execute_model(\n        self, data: str, model_preference: Optional[str] = None\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        model_execution_params = self._prepare_model_execution(data, model_preference)\n        if self.guardrails.get(\"input\"):\n            self._execute_input_guardrail(model_execution_params)\n        model_response = self.model(**model_execution_params)\n        return model_response\n\n    async def _aexecute_model(\n        self, data: str, model_preference: Optional[str] = None\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        model_execution_params = self._prepare_model_execution(data, model_preference)\n        if self.guardrails.get(\"input\"):\n            await self._aexecute_input_guardrail(model_execution_params)\n        model_response = await self.model.acall(**model_execution_params)\n        return model_response\n\n    def _prepare_model_execution(\n        self, data: str, model_preference: Optional[str] = None\n    ) -&gt; Dict[str, Union[str, bool]]:\n        model_execution_params = dotdict(\n            data=data, response_format=self.response_format, prompt=self.prompt\n        )\n        stream = self.config.get(\"stream\", False)\n        if stream:\n            model_execution_params.stream = stream\n        if isinstance(self.model, ModelGateway) and model_preference is not None:\n            model_execution_params.model_preference = model_preference\n        return model_execution_params\n\n    def _prepare_guardrail_execution(\n        self, model_execution_params: Dict[str, Union[str, bool]]\n    ) -&gt; Dict[str, str]:\n        guardrail_params = {\"data\": model_execution_params.data}\n        return guardrail_params\n\n    def _process_model_response(\n        self,\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        message: Union[str, Message],\n    ) -&gt; Union[bytes, Message, ModelStreamResponse]:\n        if model_response.response_type == \"audio_generation\":\n            raw_response = self._extract_raw_response(model_response)\n            response = self._prepare_response(raw_response, message)\n            return response\n        else:\n            raise ValueError(\n                f\"Unsupported model response type `{model_response.response_type}`\"\n            )\n\n    def _prepare_task(self, message: Union[str, Message], **kwargs) -&gt; Dict[str, str]:\n        if isinstance(message, Message):\n            data = self._extract_message_values(self.task_inputs, message)\n            if data is None:\n                raise ValueError(f\"No text found in paths: `{self.task_inputs}`\")\n        elif isinstance(message, str):\n            data = message\n        else:\n            raise ValueError(f\"Unsupported message type: `{type(message)}`\")\n\n        model_preference = kwargs.pop(\"model_preference\", None)\n        if model_preference is None and isinstance(message, Message):\n            model_preference = self.get_model_preference_from_message(message)\n\n        return {\"data\": data, \"model_preference\": model_preference}\n\n    def inspect_model_execution_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n        \"\"\"Debug model input parameters.\"\"\"\n        inputs = self._prepare_task(*args, **kwargs)\n        model_execution_params = self._prepare_model_execution(**inputs)\n        return model_execution_params\n\n    def _set_model(self, model: Union[TextToSpeechModel, ModelGateway]):\n        if model.model_type == \"text_to_speech\":\n            self.register_buffer(\"model\", model)\n        else:\n            raise TypeError(\n                f\"`model` need be a `text_to_speech` model, given `{type(model)}`\"\n            )\n\n    def _set_response_format(self, response_format: str):\n        supported_formats = [\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"]\n        if isinstance(response_format, str):\n            if response_format in supported_formats:\n                self.register_buffer(\"response_format\", response_format)\n            else:\n                raise ValueError(\n                    f\"`response_format` can be `{supported_formats}` \"\n                    f\"given `{response_format}\"\n                )\n        else:\n            raise TypeError(\n                f\"`response_format` need be a str or given `{type(response_format)}\"\n            )\n\n    def _set_config(self, config: Optional[Dict[str, Any]] = None):\n        if config is None:\n            self.register_buffer(\"config\", {})\n            return\n\n        if not isinstance(config, dict):\n            raise TypeError(f\"`config` must be a dict or None, given `{type(config)}`\")\n\n        self.register_buffer(\"config\", config.copy())\n</code></pre>"},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker.__init__","title":"__init__","text":"<pre><code>__init__(\n    model,\n    *,\n    guardrails=None,\n    message_fields=None,\n    response_mode=\"plain_response\",\n    response_format=\"opus\",\n    prompt=None,\n    config=None,\n    name=None,\n)\n</code></pre> <p>Initialize the Speaker module.</p> <p>model:     Transcriber Model client. guardrails:     Dictionary mapping guardrail types to callables.     Valid keys: \"input\" (output not supported for Speaker).     !!! example         guardrails={\"input\": input_checker} message_fields:     Dictionary mapping Message field names to their paths in the Message object.     Valid keys: \"task_inputs\" (other fields not supported for Speaker).     !!! example         message_fields={\"task_inputs\": \"input.user\"}</p> <pre><code>Field description:\n- task_inputs: Field path for task input (str)\n</code></pre> <p>response_mode:     What the response should be.     * <code>plain_response</code> (default): Returns the final agent response directly.     * other: Write on field in Message object. response_format:     The format to audio in. prompt:     Useful for instructing the model to follow some speak generation pattern. config:     Dictionary with configuration options. Accepts any keys without validation.     Common option: \"stream\"     !!! example         config={\"stream\": False}</p> <pre><code>Configuration option:\n- stream: Transmit response on-the-fly (bool)\n</code></pre> <p>name:     Transcriber name in snake case format.</p> Source code in <code>src/msgflux/nn/modules/speaker.py</code> <pre><code>def __init__(\n    self,\n    model: Union[TextToSpeechModel, ModelGateway],\n    *,\n    guardrails: Optional[Dict[str, Callable]] = None,\n    message_fields: Optional[Dict[str, Any]] = None,\n    response_mode: Optional[str] = \"plain_response\",\n    response_format: Optional[\n        Literal[\"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"]\n    ] = \"opus\",\n    prompt: Optional[str] = None,\n    config: Optional[Dict[str, Any]] = None,\n    name: Optional[str] = None,\n):\n    \"\"\"Initialize the Speaker module.\n\n    Args:\n    model:\n        Transcriber Model client.\n    guardrails:\n        Dictionary mapping guardrail types to callables.\n        Valid keys: \"input\" (output not supported for Speaker).\n        !!! example\n            guardrails={\"input\": input_checker}\n    message_fields:\n        Dictionary mapping Message field names to their paths in the Message object.\n        Valid keys: \"task_inputs\" (other fields not supported for Speaker).\n        !!! example\n            message_fields={\"task_inputs\": \"input.user\"}\n\n        Field description:\n        - task_inputs: Field path for task input (str)\n    response_mode:\n        What the response should be.\n        * `plain_response` (default): Returns the final agent response directly.\n        * other: Write on field in Message object.\n    response_format:\n        The format to audio in.\n    prompt:\n        Useful for instructing the model to follow some speak generation pattern.\n    config:\n        Dictionary with configuration options. Accepts any keys without validation.\n        Common option: \"stream\"\n        !!! example\n            config={\"stream\": False}\n\n        Configuration option:\n        - stream: Transmit response on-the-fly (bool)\n    name:\n        Transcriber name in snake case format.\n    \"\"\"\n    super().__init__()\n    self._set_guardrails(guardrails)\n    self._set_model(model)\n    self._set_prompt(prompt)\n    self._set_response_format(response_format)\n    self._set_response_mode(response_mode)\n    self._set_message_fields(message_fields)\n    self._set_config(config)\n    if name:\n        self.set_name(name)\n</code></pre>"},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(message, **kwargs)\n</code></pre> <p>Async version of forward. Execute the speaker asynchronously.</p> Source code in <code>src/msgflux/nn/modules/speaker.py</code> <pre><code>async def aforward(\n    self, message: Union[str, Message], **kwargs\n) -&gt; Union[bytes, ModelStreamResponse]:\n    \"\"\"Async version of forward. Execute the speaker asynchronously.\"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = await self._aexecute_model(**inputs)\n    response = self._process_model_response(model_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker.forward","title":"forward","text":"<pre><code>forward(message, **kwargs)\n</code></pre> <p>Execute the speaker with the given message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Union[str, Message]</code> <p>The input message, which can be: - str: Direct text input to convert to speech - Message: Message object with fields mapped via message_fields</p> required <code>**kwargs</code> <p>Runtime overrides for message_fields. Can include: - task_inputs: Override field path or direct value</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[bytes, ModelStreamResponse]</code> <p>Audio bytes or ModelStreamResponse if stream=True</p> <p>Examples:</p>"},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker.forward--direct-string-input","title":"Direct string input","text":"<p>speaker(\"Hello world\")</p>"},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker.forward--using-message-object-with-message_fields","title":"Using Message object with message_fields","text":"<p>msg = Message(text=\"Hello world\") speaker(msg)</p>"},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker.forward--runtime-override","title":"Runtime override","text":"<p>speaker(msg, task_inputs=\"custom.path\")</p> Source code in <code>src/msgflux/nn/modules/speaker.py</code> <pre><code>def forward(\n    self, message: Union[str, Message], **kwargs\n) -&gt; Union[bytes, ModelStreamResponse]:\n    \"\"\"Execute the speaker with the given message.\n\n    Args:\n        message: The input message, which can be:\n            - str: Direct text input to convert to speech\n            - Message: Message object with fields mapped via message_fields\n        **kwargs: Runtime overrides for message_fields. Can include:\n            - task_inputs: Override field path or direct value\n\n    Returns:\n        Audio bytes or ModelStreamResponse if stream=True\n\n    Examples:\n        # Direct string input\n        speaker(\"Hello world\")\n\n        # Using Message object with message_fields\n        msg = Message(text=\"Hello world\")\n        speaker(msg)\n\n        # Runtime override\n        speaker(msg, task_inputs=\"custom.path\")\n    \"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = self._execute_model(**inputs)\n    response = self._process_model_response(model_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/speaker/#msgflux.nn.modules.speaker.Speaker.inspect_model_execution_params","title":"inspect_model_execution_params","text":"<pre><code>inspect_model_execution_params(*args, **kwargs)\n</code></pre> <p>Debug model input parameters.</p> Source code in <code>src/msgflux/nn/modules/speaker.py</code> <pre><code>def inspect_model_execution_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n    \"\"\"Debug model input parameters.\"\"\"\n    inputs = self._prepare_task(*args, **kwargs)\n    model_execution_params = self._prepare_model_execution(**inputs)\n    return model_execution_params\n</code></pre>"},{"location":"api-reference/nn/modules/tool/","title":"Tool","text":""},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.Tool","title":"Tool","text":"<p>               Bases: <code>Module</code></p> <p>Tool is Module type that provide a json schema to tools.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>class Tool(Module):\n    \"\"\"Tool is Module type that provide a json schema to tools.\"\"\"\n\n    def get_json_schema(self):\n        return generate_tool_json_schema(self)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.Tool.get_json_schema","title":"get_json_schema","text":"<pre><code>get_json_schema()\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def get_json_schema(self):\n    return generate_tool_json_schema(self)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.LocalTool","title":"LocalTool","text":"<p>               Bases: <code>Tool</code></p> <p>Local tool implementation.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>class LocalTool(Tool):\n    \"\"\"Local tool implementation.\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        description: str,\n        annotations: Dict[str, Any],\n        tool_config: Dict[str, Any],\n        impl: Callable,\n    ):\n        super().__init__()\n        self.set_name(name)\n        self.set_description(description)\n        self.set_annotations(annotations)\n        self.register_buffer(\"tool_config\", tool_config)\n        self.impl = impl  # Not a buffer for now\n\n    @tool_retry\n    @set_tool_attributes(execution_type=\"local\")\n    def forward(self, **kwargs):\n        if inspect.iscoroutinefunction(self.impl):\n            return F.wait_for(self.impl, **kwargs)\n        return self.impl(**kwargs)\n\n    @tool_retry\n    @aset_tool_attributes(execution_type=\"local\")\n    async def aforward(self, *args, **kwargs):\n        if hasattr(self.impl, \"acall\"):\n            return await self.impl.acall(*args, **kwargs)\n        elif inspect.iscoroutinefunction(self.impl):\n            return await self.impl(*args, **kwargs)\n        # Fall back to sync call in executor to avoid blocking event loop\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(None, lambda: self.impl(*args, **kwargs))\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.LocalTool.impl","title":"impl  <code>instance-attribute</code>","text":"<pre><code>impl = impl\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.LocalTool.__init__","title":"__init__","text":"<pre><code>__init__(name, description, annotations, tool_config, impl)\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    description: str,\n    annotations: Dict[str, Any],\n    tool_config: Dict[str, Any],\n    impl: Callable,\n):\n    super().__init__()\n    self.set_name(name)\n    self.set_description(description)\n    self.set_annotations(annotations)\n    self.register_buffer(\"tool_config\", tool_config)\n    self.impl = impl  # Not a buffer for now\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.LocalTool.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(*args, **kwargs)\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>@tool_retry\n@aset_tool_attributes(execution_type=\"local\")\nasync def aforward(self, *args, **kwargs):\n    if hasattr(self.impl, \"acall\"):\n        return await self.impl.acall(*args, **kwargs)\n    elif inspect.iscoroutinefunction(self.impl):\n        return await self.impl(*args, **kwargs)\n    # Fall back to sync call in executor to avoid blocking event loop\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(None, lambda: self.impl(*args, **kwargs))\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.LocalTool.forward","title":"forward","text":"<pre><code>forward(**kwargs)\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>@tool_retry\n@set_tool_attributes(execution_type=\"local\")\ndef forward(self, **kwargs):\n    if inspect.iscoroutinefunction(self.impl):\n        return F.wait_for(self.impl, **kwargs)\n    return self.impl(**kwargs)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.MCPTool","title":"MCPTool","text":"<p>               Bases: <code>Tool</code></p> <p>MCP Tool Proxy - wraps remote MCP tool as a Tool object.</p> <p>This allows MCP tools to be treated exactly like local tools, enabling polymorphism and unified telemetry.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Tool name (without namespace prefix)</p> required <code>mcp_client</code> <code>Any</code> <p>Connected MCP client</p> required <code>mcp_tool_info</code> <code>Any</code> <p>MCP tool metadata</p> required <code>namespace</code> <code>str</code> <p>MCP server namespace</p> required <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Optional tool configuration</p> <code>None</code> Example <p>mcp_tool = MCPTool( ...     name=\"read_file\", ...     mcp_client=client, ...     mcp_tool_info=tool_info, ...     namespace=\"filesystem\" ... ) result = mcp_tool(path=\"/file.txt\")</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>class MCPTool(Tool):\n    \"\"\"MCP Tool Proxy - wraps remote MCP tool as a Tool object.\n\n    This allows MCP tools to be treated exactly like local tools,\n    enabling polymorphism and unified telemetry.\n\n    Args:\n        name: Tool name (without namespace prefix)\n        mcp_client: Connected MCP client\n        mcp_tool_info: MCP tool metadata\n        namespace: MCP server namespace\n        config: Optional tool configuration\n\n    Example:\n        &gt;&gt;&gt; mcp_tool = MCPTool(\n        ...     name=\"read_file\",\n        ...     mcp_client=client,\n        ...     mcp_tool_info=tool_info,\n        ...     namespace=\"filesystem\"\n        ... )\n        &gt;&gt;&gt; result = mcp_tool(path=\"/file.txt\")\n    \"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        mcp_client: Any,  # MCPClient type\n        mcp_tool_info: Any,  # MCPToolInfo type\n        namespace: str,\n        config: Optional[Dict[str, Any]] = None,\n    ):\n        super().__init__()\n\n        # Set full tool name with namespace\n        full_name = f\"{namespace}__{name}\"\n        self.set_name(full_name)\n\n        # Store MCP-specific data\n        self._mcp_client = mcp_client\n        self._mcp_tool_info = mcp_tool_info\n        self._namespace = namespace\n        self._mcp_tool_name = name\n\n        # Set description from MCP tool info\n        if hasattr(mcp_tool_info, \"description\"):\n            self.set_description(mcp_tool_info.description)\n\n        # Store config\n        self.register_buffer(\"tool_config\", config or {})\n\n    def get_json_schema(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert MCP tool schema to standard tool JSON schema.\"\"\"\n        return convert_mcp_schema_to_tool_schema(self._mcp_tool_info, self._namespace)\n\n    @set_tool_attributes(execution_type=\"remote\", protocol=\"mcp\")\n    def forward(self, **kwargs) -&gt; Any:\n        \"\"\"Execute MCP tool call.\"\"\"\n        # Call MCP tool (wrap async in sync)\n        result = F.wait_for(self._mcp_client.call_tool, self._mcp_tool_name, kwargs)\n\n        # Handle errors\n        if result.isError:\n            error_text = extract_tool_result_text(result)\n            raise RuntimeError(f\"MCP tool error: {error_text}\")\n\n        # Extract and return result\n        return extract_tool_result_text(result)\n\n    @aset_tool_attributes(execution_type=\"remote\", protocol=\"mcp\")\n    async def aforward(self, **kwargs) -&gt; Any:\n        \"\"\"Execute MCP tool call asynchronously.\"\"\"\n        # Call MCP tool\n        result = await self._mcp_client.call_tool(self._mcp_tool_name, kwargs)\n\n        # Handle errors\n        if result.isError:\n            error_text = extract_tool_result_text(result)\n            raise RuntimeError(f\"MCP tool error: {error_text}\")\n\n        # Extract and return result\n        return extract_tool_result_text(result)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.MCPTool.__init__","title":"__init__","text":"<pre><code>__init__(\n    name, mcp_client, mcp_tool_info, namespace, config=None\n)\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    mcp_client: Any,  # MCPClient type\n    mcp_tool_info: Any,  # MCPToolInfo type\n    namespace: str,\n    config: Optional[Dict[str, Any]] = None,\n):\n    super().__init__()\n\n    # Set full tool name with namespace\n    full_name = f\"{namespace}__{name}\"\n    self.set_name(full_name)\n\n    # Store MCP-specific data\n    self._mcp_client = mcp_client\n    self._mcp_tool_info = mcp_tool_info\n    self._namespace = namespace\n    self._mcp_tool_name = name\n\n    # Set description from MCP tool info\n    if hasattr(mcp_tool_info, \"description\"):\n        self.set_description(mcp_tool_info.description)\n\n    # Store config\n    self.register_buffer(\"tool_config\", config or {})\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.MCPTool.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(**kwargs)\n</code></pre> <p>Execute MCP tool call asynchronously.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>@aset_tool_attributes(execution_type=\"remote\", protocol=\"mcp\")\nasync def aforward(self, **kwargs) -&gt; Any:\n    \"\"\"Execute MCP tool call asynchronously.\"\"\"\n    # Call MCP tool\n    result = await self._mcp_client.call_tool(self._mcp_tool_name, kwargs)\n\n    # Handle errors\n    if result.isError:\n        error_text = extract_tool_result_text(result)\n        raise RuntimeError(f\"MCP tool error: {error_text}\")\n\n    # Extract and return result\n    return extract_tool_result_text(result)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.MCPTool.forward","title":"forward","text":"<pre><code>forward(**kwargs)\n</code></pre> <p>Execute MCP tool call.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>@set_tool_attributes(execution_type=\"remote\", protocol=\"mcp\")\ndef forward(self, **kwargs) -&gt; Any:\n    \"\"\"Execute MCP tool call.\"\"\"\n    # Call MCP tool (wrap async in sync)\n    result = F.wait_for(self._mcp_client.call_tool, self._mcp_tool_name, kwargs)\n\n    # Handle errors\n    if result.isError:\n        error_text = extract_tool_result_text(result)\n        raise RuntimeError(f\"MCP tool error: {error_text}\")\n\n    # Extract and return result\n    return extract_tool_result_text(result)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.MCPTool.get_json_schema","title":"get_json_schema","text":"<pre><code>get_json_schema()\n</code></pre> <p>Convert MCP tool schema to standard tool JSON schema.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def get_json_schema(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert MCP tool schema to standard tool JSON schema.\"\"\"\n    return convert_mcp_schema_to_tool_schema(self._mcp_tool_info, self._namespace)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary","title":"ToolLibrary","text":"<p>               Bases: <code>Module</code></p> <p>ToolLibrary is a Module type that manage tool calls over the tool library.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>class ToolLibrary(Module, metaclass=AutoParams):\n    \"\"\"ToolLibrary is a Module type that manage tool calls over the tool library.\"\"\"\n\n    def __init__(\n        self,\n        name: str,\n        tools: List[Callable],\n        special_tools: Optional[List[str]] = None,\n        mcp_servers: Optional[List[Dict[str, Any]]] = None,\n    ):\n        \"\"\"Initialize the ToolLibrary.\n\n        Args:\n        name:\n            Library name.\n        tools:\n            A list of callables.\n        special_tools:\n            Autonomy tools for the model.\n        mcp_servers:\n            List of MCP server configurations. Each config should contain:\n            - name: Namespace for tools from this server\n            - transport: \"stdio\" or \"http\"\n            - For stdio: command, args, cwd, env\n            - For http: base_url, headers\n            - Optional: include_tools, exclude_tools, tool_config\n        \"\"\"\n        super().__init__()\n        self.set_name(f\"{name}_tool_library\")\n        self.library = ModuleDict()\n        self.register_buffer(\"special_library\", [])\n        self.register_buffer(\"tool_configs\", {})\n        self.register_buffer(\"mcp_clients\", {})\n        for tool in tools:\n            self.add(tool)\n        if special_tools:\n            for special_tool in special_tools:\n                self.special_add(special_tool)\n        if mcp_servers:\n            self._initialize_mcp_clients(mcp_servers)\n\n    def add(self, tool: Union[str, Callable]):\n        \"\"\"Add a local tool in library.\"\"\"\n        if isinstance(tool, str):\n            if tool in self.special_library.keys():\n                raise ValueError(\n                    f\"The special tool name `{tool}` is already in special tool library\"\n                )\n            self.special_library.append(tool)\n        else:\n            name = getattr(tool, \"name\", None) or getattr(tool, \"__name__\", None)\n            if name in self.library.keys():\n                raise ValueError(f\"The tool name `{name}` is already in tool library\")\n            if not isinstance(tool, Tool):\n                tool = _convert_module_to_nn_tool(tool)\n\n            # Store tool config (may be empty dict for local tools)\n            self.tool_configs[tool.name] = getattr(tool, \"tool_config\", {})\n\n            self.library.update({tool.name: tool})\n\n    def remove(self, tool_name: str):\n        if tool_name in self.library.keys():\n            self.library.pop(tool_name)\n            self.tool_configs.pop(tool_name, None)\n        elif tool_name in self.special_library:\n            self.special_library.remove(tool_name)\n        else:\n            raise ValueError(f\"The tool name `{tool_name}` is not in tool library\")\n\n    def clear(self):\n        self.library.clear()\n        self.special_library.clear()\n        # TODO: clean mcp\n\n    def _initialize_mcp_clients(self, mcp_servers: List[Dict[str, Any]]):\n        \"\"\"Initialize MCP clients from server configurations.\"\"\"\n        for server_config in mcp_servers:\n            namespace = server_config.get(\"name\")\n            if not namespace:\n                raise ValueError(\"MCP server config must include 'name' field\")\n\n            transport_type = server_config.get(\"transport\", \"stdio\")\n\n            # Create client based on transport type\n            if transport_type == \"stdio\":\n                client = MCPClient.from_stdio(\n                    command=server_config.get(\"command\"),\n                    args=server_config.get(\"args\"),\n                    cwd=server_config.get(\"cwd\"),\n                    env=server_config.get(\"env\"),\n                    timeout=server_config.get(\"timeout\", 30.0),\n                )\n            elif transport_type == \"http\":\n                client = MCPClient.from_http(\n                    base_url=server_config.get(\"base_url\"),\n                    timeout=server_config.get(\"timeout\", 30.0),\n                    headers=server_config.get(\"headers\"),\n                )\n            else:\n                raise ValueError(\n                    f\"Unknown transport type: {transport_type}. \"\n                    \"Supported types: 'stdio', 'http'\"\n                )\n\n            # Connect and list tools with error handling\n            try:\n                F.wait_for(client.connect)\n                all_tools = F.wait_for(client.list_tools, use_cache=False)\n\n                # Apply filters\n                include_tools = server_config.get(\"include_tools\")\n                exclude_tools = server_config.get(\"exclude_tools\")\n                filtered_tools = filter_tools(all_tools, include_tools, exclude_tools)\n\n                # Create MCPTool for each remote tool\n                tool_configs = server_config.get(\"tool_config\", {})\n                for mcp_tool_info in filtered_tools:\n                    tool_config = tool_configs.get(mcp_tool_info.name, {})\n\n                    # Create MCPTool instance\n                    mcp_tool = MCPTool(\n                        name=mcp_tool_info.name,\n                        mcp_client=client,\n                        mcp_tool_info=mcp_tool_info,\n                        namespace=namespace,\n                        config=tool_config,\n                    )\n\n                    # Add to library (will have name like \"namespace__tool_name\")\n                    self.library.update({mcp_tool.name: mcp_tool})\n                    self.tool_configs[mcp_tool.name] = mcp_tool.tool_config\n\n                self.mcp_clients[namespace] = {\n                    \"client\": client,\n                    \"tools\": filtered_tools,\n                    \"tool_config\": tool_configs,\n                }\n\n                logger.debug(\n                    f\"Successfully connected to MCP server `{namespace}` \"\n                    f\"with {len(filtered_tools)} tools\"\n                )\n            except Exception as e:\n                logger.error(\n                    f\"Failed to initialize MCP server '{namespace}': {e!s}\",\n                    exc_info=True,\n                )\n                # Continue with other servers instead of failing completely\n\n    def get_tools(self) -&gt; Iterator[Dict[str, Tool]]:\n        return self.library.items()\n\n    def get_tool_names(self) -&gt; List[str]:\n        \"\"\"Get names of all tools.\"\"\"\n        return list(self.library.keys())\n\n    def get_mcp_tool_names(self) -&gt; List[str]:\n        \"\"\"Get names of all MCP tools (with namespace).\"\"\"\n        tool_names = []\n        for namespace, mcp_data in self.mcp_clients.items():\n            for tool in mcp_data[\"tools\"]:\n                tool_names.append(f\"{namespace}__{tool.name}\")\n        return tool_names\n\n    def get_tool_json_schemas(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Returns a list of JSON schemas from local and MCP tools.\"\"\"\n        schemas = []\n\n        # Local tools\n        for tool_name in self.library:\n            schemas.append(self.library[tool_name].get_json_schema())\n\n        # MCP tools\n        if self.mcp_clients:\n            for namespace, mcp_data in self.mcp_clients.items():\n                for mcp_tool in mcp_data[\"tools\"]:\n                    schema = convert_mcp_schema_to_tool_schema(mcp_tool, namespace)\n                    schemas.append(schema)\n\n        return schemas\n\n    def forward(  # noqa: C901\n        self,\n        tool_callings: List[Tuple[str, str, Any]],\n        model_state: Optional[List[Dict[str, Any]]] = None,\n        vars: Optional[Mapping[str, Any]] = None,\n    ) -&gt; ToolResponses:\n        \"\"\"Executes tool calls with tool config logic.\n\n        Args:\n            tool_callings:\n                A list of tuples containing the tool id, name and parameters.\n                !!! example\n                    [('123121', 'tool_name1', {'parameter1': 'value1'}),\n                    ('322', 'tool_name2', '')]\n            model_state:\n                The current state of the Agent for the `handoff` functionality.\n            vars:\n                Extra kwargs to be used in tools.\n\n        Returns:\n            ToolResponses:\n                Structured object containing all tool call results.\n        \"\"\"\n        # TODO capturar no trace quando o modelo erra algo na tool\n        # TODO capturar no trace o tool config\n        if model_state is None:\n            model_state = {}\n\n        if vars is None:\n            vars = {}\n\n        prepared_calls = []\n        call_metadata = []\n        tool_calls: List[ToolCall] = []\n        return_directly = True if tool_callings else False\n\n        for tool_id, tool_name, tool_params in tool_callings:\n            if tool_name not in self.library:\n                tool_calls.append(\n                    ToolCall(\n                        id=tool_id,\n                        name=tool_name,\n                        parameters=tool_params,\n                        error=f\"Error: Tool `{tool_name}` not found.\",\n                    )\n                )\n                return_directly = False\n                continue\n\n            # Get tool\n            tool = self.library[tool_name]\n            config = self.tool_configs.get(tool_name, {})\n\n            # Handle inject_vars\n            inject_vars = config.get(\"inject_vars\", False)\n            if inject_vars:\n                if isinstance(inject_vars, list):\n                    for key in inject_vars:\n                        if key not in vars:\n                            raise ValueError(\n                                f\"The tool `{tool_name}` requires the injected \"\n                                f\"parameter `{key}`, but it was not found.\"\n                            )\n                        tool_params[key] = vars[key]\n                elif inject_vars is True:\n                    tool_params[\"vars\"] = vars\n\n            if config.get(\"background\", False):\n                return_directly = False\n                F.background_task(tool, **(tool_params or {}))\n                tool_calls.append(\n                    ToolCall(\n                        id=tool_id,\n                        name=tool_name,\n                        parameters=tool_params,\n                        result=f\"\"\"The `{tool_name}` tool was started in the background.\n                        This tool will not generate a return\"\"\",\n                    )\n                )\n                continue\n\n            if config.get(\n                \"call_as_response\", False\n            ):  # return function call as response\n                tool_calls.append(\n                    ToolCall(id=tool_id, name=tool_name, parameters=tool_params)\n                )\n                return_directly = True\n                continue\n\n            if config.get(\"inject_model_state\", False):  # Add model_state\n                tool_params[\"model_state\"] = model_state\n\n            if not config.get(\"return_direct\", False):\n                return_directly = False\n\n            final_tool_params = tool_params or {}\n            # Add tool_call_id for telemetry\n            final_tool_params[\"tool_call_id\"] = tool_id\n            prepared_calls.append(partial(tool, **final_tool_params))\n\n            call_metadata.append(\n                dotdict(\n                    id=tool_id,\n                    name=tool_name,\n                    config=config,\n                    params=final_tool_params,\n                )\n            )\n\n        if prepared_calls:\n            results = F.scatter_gather(prepared_calls)\n            for meta, result in zip(call_metadata, results):\n                if isinstance(meta.params, dict):\n                    parameters = meta.params.to_dict()\n                    parameters.pop(\"vars\", None)\n                    parameters.pop(\"tool_call_id\", None)\n                else:\n                    parameters = None\n                tool_calls.append(\n                    ToolCall(\n                        id=meta.id,\n                        name=meta.name,\n                        parameters=parameters,\n                        result=result,\n                    )\n                )\n\n        return ToolResponses(return_directly=return_directly, tool_calls=tool_calls)\n\n    async def aforward(  # noqa: C901\n        self,\n        tool_callings: List[Tuple[str, str, Any]],\n        model_state: Optional[List[Dict[str, Any]]] = None,\n        vars: Optional[Mapping[str, Any]] = None,\n    ) -&gt; ToolResponses:\n        \"\"\"Async version of forward. Executes tool calls with logic for\n        `handoff`, `return_direct`.\n\n        Args:\n            tool_callings:\n                A list of tuples containing the tool id, name and parameters.\n                !!! example\n                    [('123121', 'tool_name1', {'parameter1': 'value1'}),\n                    ('322', 'tool_name2', '')]\n            model_state:\n                The current state of the Agent for the `handoff` functionality.\n            vars:\n                Extra kwargs to be used in tools.\n\n        Returns:\n            ToolResponses:\n                Structured object containing all tool call results.\n        \"\"\"\n        if model_state is None:\n            model_state = {}\n\n        if vars is None:\n            vars = {}\n\n        prepared_calls = []\n        call_metadata = []\n        tool_calls: List[ToolCall] = []\n        return_directly = True if tool_callings else False\n\n        for tool_id, tool_name, tool_params in tool_callings:\n            if tool_name not in self.library:\n                tool_calls.append(\n                    ToolCall(\n                        id=tool_id,\n                        name=tool_name,\n                        parameters=tool_params,\n                        error=f\"Error: Tool `{tool_name}` not found.\",\n                    )\n                )\n                return_directly = False\n                continue\n\n            # Get tool\n            tool = self.library[tool_name]\n            config = self.tool_configs.get(tool_name, {})\n\n            # Handle inject_vars\n            inject_vars = config.get(\"inject_vars\", False)\n            if inject_vars:\n                if isinstance(inject_vars, list):\n                    for key in inject_vars:\n                        if key not in vars:\n                            raise ValueError(\n                                f\"The tool `{tool_name}` requires the injected \"\n                                f\"parameter `{key}`, but it was not found.\"\n                            )\n                        tool_params[key] = vars[key]\n                elif inject_vars is True:\n                    tool_params[\"vars\"] = vars\n\n            if config.get(\"background\", False):\n                return_directly = False\n                await F.abackground_task(tool.acall, **(tool_params or {}))\n                tool_calls.append(\n                    ToolCall(\n                        id=tool_id,\n                        name=tool_name,\n                        parameters=tool_params,\n                        result=f\"\"\"The `{tool_name}` tool was started in the background.\n                        This tool will not generate a return\"\"\",\n                    )\n                )\n                continue\n\n            if config.get(\n                \"call_as_response\", False\n            ):  # return function call as response\n                tool_calls.append(\n                    ToolCall(id=tool_id, name=tool_name, parameters=tool_params)\n                )\n                return_directly = True\n                continue\n\n            if config.get(\"inject_model_state\", False):  # Add model_state\n                tool_params[\"model_state\"] = model_state\n\n            if not config.get(\"return_direct\", False):\n                return_directly = False\n\n            final_tool_params = tool_params or {}\n            # Add tool_call_id for telemetry\n            final_tool_params[\"tool_call_id\"] = tool_id\n            prepared_calls.append(partial(tool.acall, **final_tool_params))\n\n            call_metadata.append(\n                dotdict(\n                    id=tool_id,\n                    name=tool_name,\n                    config=config,\n                    params=final_tool_params,\n                )\n            )\n\n        if prepared_calls:\n            results = await F.ascatter_gather(prepared_calls)\n            for meta, result in zip(call_metadata, results):\n                if isinstance(meta.params, dict):\n                    parameters = meta.params.to_dict()\n                    parameters.pop(\"vars\", None)\n                    parameters.pop(\"tool_call_id\", None)\n                else:\n                    parameters = None\n                tool_calls.append(\n                    ToolCall(\n                        id=meta.id,\n                        name=meta.name,\n                        parameters=parameters,\n                        result=result,\n                    )\n                )\n\n        return ToolResponses(return_directly=return_directly, tool_calls=tool_calls)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.library","title":"library  <code>instance-attribute</code>","text":"<pre><code>library = ModuleDict()\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.__init__","title":"__init__","text":"<pre><code>__init__(name, tools, special_tools=None, mcp_servers=None)\n</code></pre> <p>Initialize the ToolLibrary.</p> <p>name:     Library name. tools:     A list of callables. special_tools:     Autonomy tools for the model. mcp_servers:     List of MCP server configurations. Each config should contain:     - name: Namespace for tools from this server     - transport: \"stdio\" or \"http\"     - For stdio: command, args, cwd, env     - For http: base_url, headers     - Optional: include_tools, exclude_tools, tool_config</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    tools: List[Callable],\n    special_tools: Optional[List[str]] = None,\n    mcp_servers: Optional[List[Dict[str, Any]]] = None,\n):\n    \"\"\"Initialize the ToolLibrary.\n\n    Args:\n    name:\n        Library name.\n    tools:\n        A list of callables.\n    special_tools:\n        Autonomy tools for the model.\n    mcp_servers:\n        List of MCP server configurations. Each config should contain:\n        - name: Namespace for tools from this server\n        - transport: \"stdio\" or \"http\"\n        - For stdio: command, args, cwd, env\n        - For http: base_url, headers\n        - Optional: include_tools, exclude_tools, tool_config\n    \"\"\"\n    super().__init__()\n    self.set_name(f\"{name}_tool_library\")\n    self.library = ModuleDict()\n    self.register_buffer(\"special_library\", [])\n    self.register_buffer(\"tool_configs\", {})\n    self.register_buffer(\"mcp_clients\", {})\n    for tool in tools:\n        self.add(tool)\n    if special_tools:\n        for special_tool in special_tools:\n            self.special_add(special_tool)\n    if mcp_servers:\n        self._initialize_mcp_clients(mcp_servers)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.add","title":"add","text":"<pre><code>add(tool)\n</code></pre> <p>Add a local tool in library.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def add(self, tool: Union[str, Callable]):\n    \"\"\"Add a local tool in library.\"\"\"\n    if isinstance(tool, str):\n        if tool in self.special_library.keys():\n            raise ValueError(\n                f\"The special tool name `{tool}` is already in special tool library\"\n            )\n        self.special_library.append(tool)\n    else:\n        name = getattr(tool, \"name\", None) or getattr(tool, \"__name__\", None)\n        if name in self.library.keys():\n            raise ValueError(f\"The tool name `{name}` is already in tool library\")\n        if not isinstance(tool, Tool):\n            tool = _convert_module_to_nn_tool(tool)\n\n        # Store tool config (may be empty dict for local tools)\n        self.tool_configs[tool.name] = getattr(tool, \"tool_config\", {})\n\n        self.library.update({tool.name: tool})\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(tool_callings, model_state=None, vars=None)\n</code></pre> <p>Async version of forward. Executes tool calls with logic for <code>handoff</code>, <code>return_direct</code>.</p> <p>Parameters:</p> Name Type Description Default <code>tool_callings</code> <code>List[Tuple[str, str, Any]]</code> <p>A list of tuples containing the tool id, name and parameters.</p> <p>Example</p> <p>[('123121', 'tool_name1', {'parameter1': 'value1'}), ('322', 'tool_name2', '')]</p> required <code>model_state</code> <code>Optional[List[Dict[str, Any]]]</code> <p>The current state of the Agent for the <code>handoff</code> functionality.</p> <code>None</code> <code>vars</code> <code>Optional[Mapping[str, Any]]</code> <p>Extra kwargs to be used in tools.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ToolResponses</code> <code>ToolResponses</code> <p>Structured object containing all tool call results.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>async def aforward(  # noqa: C901\n    self,\n    tool_callings: List[Tuple[str, str, Any]],\n    model_state: Optional[List[Dict[str, Any]]] = None,\n    vars: Optional[Mapping[str, Any]] = None,\n) -&gt; ToolResponses:\n    \"\"\"Async version of forward. Executes tool calls with logic for\n    `handoff`, `return_direct`.\n\n    Args:\n        tool_callings:\n            A list of tuples containing the tool id, name and parameters.\n            !!! example\n                [('123121', 'tool_name1', {'parameter1': 'value1'}),\n                ('322', 'tool_name2', '')]\n        model_state:\n            The current state of the Agent for the `handoff` functionality.\n        vars:\n            Extra kwargs to be used in tools.\n\n    Returns:\n        ToolResponses:\n            Structured object containing all tool call results.\n    \"\"\"\n    if model_state is None:\n        model_state = {}\n\n    if vars is None:\n        vars = {}\n\n    prepared_calls = []\n    call_metadata = []\n    tool_calls: List[ToolCall] = []\n    return_directly = True if tool_callings else False\n\n    for tool_id, tool_name, tool_params in tool_callings:\n        if tool_name not in self.library:\n            tool_calls.append(\n                ToolCall(\n                    id=tool_id,\n                    name=tool_name,\n                    parameters=tool_params,\n                    error=f\"Error: Tool `{tool_name}` not found.\",\n                )\n            )\n            return_directly = False\n            continue\n\n        # Get tool\n        tool = self.library[tool_name]\n        config = self.tool_configs.get(tool_name, {})\n\n        # Handle inject_vars\n        inject_vars = config.get(\"inject_vars\", False)\n        if inject_vars:\n            if isinstance(inject_vars, list):\n                for key in inject_vars:\n                    if key not in vars:\n                        raise ValueError(\n                            f\"The tool `{tool_name}` requires the injected \"\n                            f\"parameter `{key}`, but it was not found.\"\n                        )\n                    tool_params[key] = vars[key]\n            elif inject_vars is True:\n                tool_params[\"vars\"] = vars\n\n        if config.get(\"background\", False):\n            return_directly = False\n            await F.abackground_task(tool.acall, **(tool_params or {}))\n            tool_calls.append(\n                ToolCall(\n                    id=tool_id,\n                    name=tool_name,\n                    parameters=tool_params,\n                    result=f\"\"\"The `{tool_name}` tool was started in the background.\n                    This tool will not generate a return\"\"\",\n                )\n            )\n            continue\n\n        if config.get(\n            \"call_as_response\", False\n        ):  # return function call as response\n            tool_calls.append(\n                ToolCall(id=tool_id, name=tool_name, parameters=tool_params)\n            )\n            return_directly = True\n            continue\n\n        if config.get(\"inject_model_state\", False):  # Add model_state\n            tool_params[\"model_state\"] = model_state\n\n        if not config.get(\"return_direct\", False):\n            return_directly = False\n\n        final_tool_params = tool_params or {}\n        # Add tool_call_id for telemetry\n        final_tool_params[\"tool_call_id\"] = tool_id\n        prepared_calls.append(partial(tool.acall, **final_tool_params))\n\n        call_metadata.append(\n            dotdict(\n                id=tool_id,\n                name=tool_name,\n                config=config,\n                params=final_tool_params,\n            )\n        )\n\n    if prepared_calls:\n        results = await F.ascatter_gather(prepared_calls)\n        for meta, result in zip(call_metadata, results):\n            if isinstance(meta.params, dict):\n                parameters = meta.params.to_dict()\n                parameters.pop(\"vars\", None)\n                parameters.pop(\"tool_call_id\", None)\n            else:\n                parameters = None\n            tool_calls.append(\n                ToolCall(\n                    id=meta.id,\n                    name=meta.name,\n                    parameters=parameters,\n                    result=result,\n                )\n            )\n\n    return ToolResponses(return_directly=return_directly, tool_calls=tool_calls)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def clear(self):\n    self.library.clear()\n    self.special_library.clear()\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.forward","title":"forward","text":"<pre><code>forward(tool_callings, model_state=None, vars=None)\n</code></pre> <p>Executes tool calls with tool config logic.</p> <p>Parameters:</p> Name Type Description Default <code>tool_callings</code> <code>List[Tuple[str, str, Any]]</code> <p>A list of tuples containing the tool id, name and parameters.</p> <p>Example</p> <p>[('123121', 'tool_name1', {'parameter1': 'value1'}), ('322', 'tool_name2', '')]</p> required <code>model_state</code> <code>Optional[List[Dict[str, Any]]]</code> <p>The current state of the Agent for the <code>handoff</code> functionality.</p> <code>None</code> <code>vars</code> <code>Optional[Mapping[str, Any]]</code> <p>Extra kwargs to be used in tools.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ToolResponses</code> <code>ToolResponses</code> <p>Structured object containing all tool call results.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def forward(  # noqa: C901\n    self,\n    tool_callings: List[Tuple[str, str, Any]],\n    model_state: Optional[List[Dict[str, Any]]] = None,\n    vars: Optional[Mapping[str, Any]] = None,\n) -&gt; ToolResponses:\n    \"\"\"Executes tool calls with tool config logic.\n\n    Args:\n        tool_callings:\n            A list of tuples containing the tool id, name and parameters.\n            !!! example\n                [('123121', 'tool_name1', {'parameter1': 'value1'}),\n                ('322', 'tool_name2', '')]\n        model_state:\n            The current state of the Agent for the `handoff` functionality.\n        vars:\n            Extra kwargs to be used in tools.\n\n    Returns:\n        ToolResponses:\n            Structured object containing all tool call results.\n    \"\"\"\n    # TODO capturar no trace quando o modelo erra algo na tool\n    # TODO capturar no trace o tool config\n    if model_state is None:\n        model_state = {}\n\n    if vars is None:\n        vars = {}\n\n    prepared_calls = []\n    call_metadata = []\n    tool_calls: List[ToolCall] = []\n    return_directly = True if tool_callings else False\n\n    for tool_id, tool_name, tool_params in tool_callings:\n        if tool_name not in self.library:\n            tool_calls.append(\n                ToolCall(\n                    id=tool_id,\n                    name=tool_name,\n                    parameters=tool_params,\n                    error=f\"Error: Tool `{tool_name}` not found.\",\n                )\n            )\n            return_directly = False\n            continue\n\n        # Get tool\n        tool = self.library[tool_name]\n        config = self.tool_configs.get(tool_name, {})\n\n        # Handle inject_vars\n        inject_vars = config.get(\"inject_vars\", False)\n        if inject_vars:\n            if isinstance(inject_vars, list):\n                for key in inject_vars:\n                    if key not in vars:\n                        raise ValueError(\n                            f\"The tool `{tool_name}` requires the injected \"\n                            f\"parameter `{key}`, but it was not found.\"\n                        )\n                    tool_params[key] = vars[key]\n            elif inject_vars is True:\n                tool_params[\"vars\"] = vars\n\n        if config.get(\"background\", False):\n            return_directly = False\n            F.background_task(tool, **(tool_params or {}))\n            tool_calls.append(\n                ToolCall(\n                    id=tool_id,\n                    name=tool_name,\n                    parameters=tool_params,\n                    result=f\"\"\"The `{tool_name}` tool was started in the background.\n                    This tool will not generate a return\"\"\",\n                )\n            )\n            continue\n\n        if config.get(\n            \"call_as_response\", False\n        ):  # return function call as response\n            tool_calls.append(\n                ToolCall(id=tool_id, name=tool_name, parameters=tool_params)\n            )\n            return_directly = True\n            continue\n\n        if config.get(\"inject_model_state\", False):  # Add model_state\n            tool_params[\"model_state\"] = model_state\n\n        if not config.get(\"return_direct\", False):\n            return_directly = False\n\n        final_tool_params = tool_params or {}\n        # Add tool_call_id for telemetry\n        final_tool_params[\"tool_call_id\"] = tool_id\n        prepared_calls.append(partial(tool, **final_tool_params))\n\n        call_metadata.append(\n            dotdict(\n                id=tool_id,\n                name=tool_name,\n                config=config,\n                params=final_tool_params,\n            )\n        )\n\n    if prepared_calls:\n        results = F.scatter_gather(prepared_calls)\n        for meta, result in zip(call_metadata, results):\n            if isinstance(meta.params, dict):\n                parameters = meta.params.to_dict()\n                parameters.pop(\"vars\", None)\n                parameters.pop(\"tool_call_id\", None)\n            else:\n                parameters = None\n            tool_calls.append(\n                ToolCall(\n                    id=meta.id,\n                    name=meta.name,\n                    parameters=parameters,\n                    result=result,\n                )\n            )\n\n    return ToolResponses(return_directly=return_directly, tool_calls=tool_calls)\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.get_mcp_tool_names","title":"get_mcp_tool_names","text":"<pre><code>get_mcp_tool_names()\n</code></pre> <p>Get names of all MCP tools (with namespace).</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def get_mcp_tool_names(self) -&gt; List[str]:\n    \"\"\"Get names of all MCP tools (with namespace).\"\"\"\n    tool_names = []\n    for namespace, mcp_data in self.mcp_clients.items():\n        for tool in mcp_data[\"tools\"]:\n            tool_names.append(f\"{namespace}__{tool.name}\")\n    return tool_names\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.get_tool_json_schemas","title":"get_tool_json_schemas","text":"<pre><code>get_tool_json_schemas()\n</code></pre> <p>Returns a list of JSON schemas from local and MCP tools.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def get_tool_json_schemas(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Returns a list of JSON schemas from local and MCP tools.\"\"\"\n    schemas = []\n\n    # Local tools\n    for tool_name in self.library:\n        schemas.append(self.library[tool_name].get_json_schema())\n\n    # MCP tools\n    if self.mcp_clients:\n        for namespace, mcp_data in self.mcp_clients.items():\n            for mcp_tool in mcp_data[\"tools\"]:\n                schema = convert_mcp_schema_to_tool_schema(mcp_tool, namespace)\n                schemas.append(schema)\n\n    return schemas\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.get_tool_names","title":"get_tool_names","text":"<pre><code>get_tool_names()\n</code></pre> <p>Get names of all tools.</p> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def get_tool_names(self) -&gt; List[str]:\n    \"\"\"Get names of all tools.\"\"\"\n    return list(self.library.keys())\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.get_tools","title":"get_tools","text":"<pre><code>get_tools()\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def get_tools(self) -&gt; Iterator[Dict[str, Tool]]:\n    return self.library.items()\n</code></pre>"},{"location":"api-reference/nn/modules/tool/#msgflux.nn.modules.tool.ToolLibrary.remove","title":"remove","text":"<pre><code>remove(tool_name)\n</code></pre> Source code in <code>src/msgflux/nn/modules/tool.py</code> <pre><code>def remove(self, tool_name: str):\n    if tool_name in self.library.keys():\n        self.library.pop(tool_name)\n        self.tool_configs.pop(tool_name, None)\n    elif tool_name in self.special_library:\n        self.special_library.remove(tool_name)\n    else:\n        raise ValueError(f\"The tool name `{tool_name}` is not in tool library\")\n</code></pre>"},{"location":"api-reference/nn/modules/transcriber/","title":"Transcriber","text":""},{"location":"api-reference/nn/modules/transcriber/#msgflux.nn.modules.transcriber.Transcriber","title":"Transcriber","text":"<p>               Bases: <code>Module</code></p> <p>Transcriber is a Module type that uses language models to transcribe audios.</p> Source code in <code>src/msgflux/nn/modules/transcriber.py</code> <pre><code>class Transcriber(Module, metaclass=AutoParams):\n    \"\"\"Transcriber is a Module type that uses language models to transcribe audios.\"\"\"\n\n    def __init__(\n        self,\n        model: Union[SpeechToTextModel, ModelGateway],\n        *,\n        message_fields: Optional[Dict[str, Any]] = None,\n        response_mode: Optional[str] = \"plain_response\",\n        response_template: Optional[str] = None,\n        response_format: Optional[str] = \"text\",\n        prompt: Optional[str] = None,\n        config: Optional[Dict[str, Any]] = None,\n        name: Optional[str] = None,\n    ):\n        \"\"\"Initialize the Transcriber module.\n\n        Args:\n        model:\n            Transcriber Model client.\n        message_fields:\n            Dictionary mapping Message field names to their paths in the Message object.\n            Valid keys: \"task_multimodal_inputs\", \"model_preference\"\n            !!! example\n                message_fields={\n                    \"task_multimodal_inputs\": \"audio.user\",\n                    # or dict-based: \"task_multimodal_inputs\": {\"audio\": \"audio.user\"}\n                    \"model_preference\": \"model.preference\"\n                }\n\n            Field descriptions:\n            - task_multimodal_inputs: Field path for audio input (str or dict)\n            - model_preference: Field path for model preference (str, only valid\n              with ModelGateway)\n        response_mode:\n            What the response should be.\n            * `plain_response` (default): Returns the final agent response directly.\n            * other: Write on field in Message object.\n        response_format: How the model should format the output. Options:\n            * text (default)\n            * json\n            * srt\n            * verbose_json\n            * vtt\n        prompt:\n            Useful for instructing the model to follow some transcript\n            generation pattern.\n        config:\n            Dictionary with configuration options. Accepts any keys without validation.\n            Common options: \"language\", \"stream\", \"timestamp_granularities\"\n            !!! example\n                config={\n                    \"language\": \"en\",\n                    \"stream\": False,\n                    \"timestamp_granularities\": \"word\"\n                }\n\n            Configuration options:\n            - language: Spoken language acronym (str)\n            - stream: Transmit response on-the-fly (bool)\n            - timestamp_granularities: Enable timestamp granularities - \"word\",\n              \"segment\", or None\n              (requires response_format=verbose_json)\n        name:\n            Transcriber name in snake case format.\n        \"\"\"\n        super().__init__()\n        self._set_model(model)\n        self._set_prompt(prompt)\n        self._set_message_fields(message_fields)\n        self._set_response_format(response_format)\n        self._set_response_mode(response_mode)\n        self._set_response_template(response_template)\n        self._set_config(config)\n        if name:\n            self.set_name(name)\n\n    def forward(\n        self, message: Union[bytes, str, Dict[str, str], Message], **kwargs\n    ) -&gt; Union[str, Dict[str, str], Message, ModelStreamResponse]:\n        \"\"\"Execute the transcriber with the given message.\n\n        Args:\n            message: The input message, which can be:\n                - bytes: Direct audio bytes to transcribe\n                - str: Audio file path or URL\n                - dict: Audio input as dictionary\n                - Message: Message object with fields mapped via message_fields\n            **kwargs: Runtime overrides for message_fields. Can include:\n                - task_multimodal_inputs: Override multimodal inputs\n                  (e.g., \"audio.path\" or {\"audio\": \"audio.path\"})\n                - model_preference: Override model preference\n\n        Returns:\n            Transcribed text (str, dict, Message, or ModelStreamResponse depending\n            on configuration).\n        \"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = self._execute_model(**inputs)\n        response = self._process_model_response(model_response, message)\n        return response\n\n    async def aforward(\n        self, message: Union[bytes, str, Dict[str, str], Message], **kwargs\n    ) -&gt; Union[str, Dict[str, str], Message, ModelStreamResponse]:\n        \"\"\"Async version of forward. Execute the transcriber asynchronously.\"\"\"\n        inputs = self._prepare_task(message, **kwargs)\n        model_response = await self._aexecute_model(**inputs)\n        response = self._process_model_response(model_response, message)\n        return response\n\n    def _execute_model(\n        self, data: Union[str, bytes], model_preference: Optional[str] = None\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        model_execution_params = self._prepare_model_execution(data, model_preference)\n        model_response = self.model(**model_execution_params)\n        return model_response\n\n    async def _aexecute_model(\n        self, data: Union[str, bytes], model_preference: Optional[str] = None\n    ) -&gt; Union[ModelResponse, ModelStreamResponse]:\n        model_execution_params = self._prepare_model_execution(data, model_preference)\n        model_response = await self.model.acall(**model_execution_params)\n        return model_response\n\n    def _prepare_model_execution(\n        self, data: Union[str, bytes], model_preference: Optional[str] = None\n    ) -&gt; Dict[str, Any]:\n        model_execution_params = dotdict(\n            data=data,\n            language=self.config.get(\"language\"),\n            response_format=self.response_format,\n            timestamp_granularities=self.config.get(\"timestamp_granularities\"),\n            prompt=self.prompt,\n            stream=self.config.get(\"stream\", False),\n        )\n        if isinstance(self.model, ModelGateway) and model_preference is not None:\n            model_execution_params.model_preference = model_preference\n        return model_execution_params\n\n    def _process_model_response(\n        self,\n        model_response: Union[ModelResponse, ModelStreamResponse],\n        message: Union[str, Message],\n    ) -&gt; Union[str, Dict[str, str], Message, ModelStreamResponse]:\n        if model_response.response_type == \"transcript\":\n            raw_response = self._extract_raw_response(model_response)\n            response = self._prepare_response(raw_response, message)\n            return response\n        else:\n            raise ValueError(\n                f\"Unsupported model response type `{model_response.response_type}`\"\n            )\n\n    def _prepare_task(\n        self, message: Union[bytes, str, Dict[str, str], Message], **kwargs\n    ) -&gt; Dict[str, Union[bytes, str]]:\n        data = self._process_task_multimodal_inputs(message)\n\n        model_preference = kwargs.pop(\"model_preference\", None)\n        if model_preference is None and isinstance(message, Message):\n            model_preference = self.get_model_preference_from_message(message)\n\n        return {\"data\": data, \"model_preference\": model_preference}\n\n    def _process_task_multimodal_inputs(\n        self, message: Union[bytes, str, Dict[str, str], Message]\n    ) -&gt; bytes:\n        if isinstance(message, Message):\n            audio_content = self._extract_message_values(\n                self.task_multimodal_inputs, message\n            )\n        else:\n            audio_content = message\n\n        if isinstance(audio_content, dict):\n            audio = audio_content.get(\"audio\", None)\n            if audio:\n                audio_content = audio\n            else:\n                raise ValueError(\n                    \"`task_multimodal_inputs` path based-on dict requires \"\n                    f\"an `audio` key given {audio_content}\"\n                )\n\n        return audio_content\n\n    def inspect_model_execution_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n        \"\"\"Debug model input parameters.\"\"\"\n        inputs = self._prepare_task(*args, **kwargs)\n        model_execution_params = self._prepare_model_execution(**inputs)\n        return model_execution_params\n\n    def _set_model(self, model: Union[SpeechToTextModel, ModelGateway]):\n        if model.model_type == \"speech_to_text\":\n            self.register_buffer(\"model\", model)\n        else:\n            raise TypeError(\n                f\"`model` need be a `speech_to_text` model, given `{type(model)}`\"\n            )\n\n    def _set_config(self, config: Optional[Dict[str, Any]] = None):\n        if config is None:\n            self.register_buffer(\"config\", {})\n            return\n\n        if not isinstance(config, dict):\n            raise TypeError(f\"`config` must be a dict or None, given `{type(config)}`\")\n\n        self.register_buffer(\"config\", config.copy())\n\n    def _set_response_format(self, response_format: str):\n        supported_formats = [\"json\", \"text\", \"srt\", \"verbose_json\", \"vtt\"]\n        if isinstance(response_format, str):\n            if response_format in supported_formats:\n                self.register_buffer(\"response_format\", response_format)\n            else:\n                raise ValueError(\n                    f\"`response_format` can be `{supported_formats}` \"\n                    f\"given `{response_format}\"\n                )\n        else:\n            raise TypeError(\n                f\"`response_format` need be a str or given `{type(response_format)}\"\n            )\n</code></pre>"},{"location":"api-reference/nn/modules/transcriber/#msgflux.nn.modules.transcriber.Transcriber.__init__","title":"__init__","text":"<pre><code>__init__(\n    model,\n    *,\n    message_fields=None,\n    response_mode=\"plain_response\",\n    response_template=None,\n    response_format=\"text\",\n    prompt=None,\n    config=None,\n    name=None,\n)\n</code></pre> <p>Initialize the Transcriber module.</p> <p>model:     Transcriber Model client. message_fields:     Dictionary mapping Message field names to their paths in the Message object.     Valid keys: \"task_multimodal_inputs\", \"model_preference\"     !!! example         message_fields={             \"task_multimodal_inputs\": \"audio.user\",             # or dict-based: \"task_multimodal_inputs\": {\"audio\": \"audio.user\"}             \"model_preference\": \"model.preference\"         }</p> <pre><code>Field descriptions:\n- task_multimodal_inputs: Field path for audio input (str or dict)\n- model_preference: Field path for model preference (str, only valid\n  with ModelGateway)\n</code></pre> <p>response_mode:     What the response should be.     * <code>plain_response</code> (default): Returns the final agent response directly.     * other: Write on field in Message object. response_format: How the model should format the output. Options:     * text (default)     * json     * srt     * verbose_json     * vtt prompt:     Useful for instructing the model to follow some transcript     generation pattern. config:     Dictionary with configuration options. Accepts any keys without validation.     Common options: \"language\", \"stream\", \"timestamp_granularities\"     !!! example         config={             \"language\": \"en\",             \"stream\": False,             \"timestamp_granularities\": \"word\"         }</p> <pre><code>Configuration options:\n- language: Spoken language acronym (str)\n- stream: Transmit response on-the-fly (bool)\n- timestamp_granularities: Enable timestamp granularities - \"word\",\n  \"segment\", or None\n  (requires response_format=verbose_json)\n</code></pre> <p>name:     Transcriber name in snake case format.</p> Source code in <code>src/msgflux/nn/modules/transcriber.py</code> <pre><code>def __init__(\n    self,\n    model: Union[SpeechToTextModel, ModelGateway],\n    *,\n    message_fields: Optional[Dict[str, Any]] = None,\n    response_mode: Optional[str] = \"plain_response\",\n    response_template: Optional[str] = None,\n    response_format: Optional[str] = \"text\",\n    prompt: Optional[str] = None,\n    config: Optional[Dict[str, Any]] = None,\n    name: Optional[str] = None,\n):\n    \"\"\"Initialize the Transcriber module.\n\n    Args:\n    model:\n        Transcriber Model client.\n    message_fields:\n        Dictionary mapping Message field names to their paths in the Message object.\n        Valid keys: \"task_multimodal_inputs\", \"model_preference\"\n        !!! example\n            message_fields={\n                \"task_multimodal_inputs\": \"audio.user\",\n                # or dict-based: \"task_multimodal_inputs\": {\"audio\": \"audio.user\"}\n                \"model_preference\": \"model.preference\"\n            }\n\n        Field descriptions:\n        - task_multimodal_inputs: Field path for audio input (str or dict)\n        - model_preference: Field path for model preference (str, only valid\n          with ModelGateway)\n    response_mode:\n        What the response should be.\n        * `plain_response` (default): Returns the final agent response directly.\n        * other: Write on field in Message object.\n    response_format: How the model should format the output. Options:\n        * text (default)\n        * json\n        * srt\n        * verbose_json\n        * vtt\n    prompt:\n        Useful for instructing the model to follow some transcript\n        generation pattern.\n    config:\n        Dictionary with configuration options. Accepts any keys without validation.\n        Common options: \"language\", \"stream\", \"timestamp_granularities\"\n        !!! example\n            config={\n                \"language\": \"en\",\n                \"stream\": False,\n                \"timestamp_granularities\": \"word\"\n            }\n\n        Configuration options:\n        - language: Spoken language acronym (str)\n        - stream: Transmit response on-the-fly (bool)\n        - timestamp_granularities: Enable timestamp granularities - \"word\",\n          \"segment\", or None\n          (requires response_format=verbose_json)\n    name:\n        Transcriber name in snake case format.\n    \"\"\"\n    super().__init__()\n    self._set_model(model)\n    self._set_prompt(prompt)\n    self._set_message_fields(message_fields)\n    self._set_response_format(response_format)\n    self._set_response_mode(response_mode)\n    self._set_response_template(response_template)\n    self._set_config(config)\n    if name:\n        self.set_name(name)\n</code></pre>"},{"location":"api-reference/nn/modules/transcriber/#msgflux.nn.modules.transcriber.Transcriber.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(message, **kwargs)\n</code></pre> <p>Async version of forward. Execute the transcriber asynchronously.</p> Source code in <code>src/msgflux/nn/modules/transcriber.py</code> <pre><code>async def aforward(\n    self, message: Union[bytes, str, Dict[str, str], Message], **kwargs\n) -&gt; Union[str, Dict[str, str], Message, ModelStreamResponse]:\n    \"\"\"Async version of forward. Execute the transcriber asynchronously.\"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = await self._aexecute_model(**inputs)\n    response = self._process_model_response(model_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/transcriber/#msgflux.nn.modules.transcriber.Transcriber.forward","title":"forward","text":"<pre><code>forward(message, **kwargs)\n</code></pre> <p>Execute the transcriber with the given message.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Union[bytes, str, Dict[str, str], Message]</code> <p>The input message, which can be: - bytes: Direct audio bytes to transcribe - str: Audio file path or URL - dict: Audio input as dictionary - Message: Message object with fields mapped via message_fields</p> required <code>**kwargs</code> <p>Runtime overrides for message_fields. Can include: - task_multimodal_inputs: Override multimodal inputs   (e.g., \"audio.path\" or {\"audio\": \"audio.path\"}) - model_preference: Override model preference</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, Dict[str, str], Message, ModelStreamResponse]</code> <p>Transcribed text (str, dict, Message, or ModelStreamResponse depending</p> <code>Union[str, Dict[str, str], Message, ModelStreamResponse]</code> <p>on configuration).</p> Source code in <code>src/msgflux/nn/modules/transcriber.py</code> <pre><code>def forward(\n    self, message: Union[bytes, str, Dict[str, str], Message], **kwargs\n) -&gt; Union[str, Dict[str, str], Message, ModelStreamResponse]:\n    \"\"\"Execute the transcriber with the given message.\n\n    Args:\n        message: The input message, which can be:\n            - bytes: Direct audio bytes to transcribe\n            - str: Audio file path or URL\n            - dict: Audio input as dictionary\n            - Message: Message object with fields mapped via message_fields\n        **kwargs: Runtime overrides for message_fields. Can include:\n            - task_multimodal_inputs: Override multimodal inputs\n              (e.g., \"audio.path\" or {\"audio\": \"audio.path\"})\n            - model_preference: Override model preference\n\n    Returns:\n        Transcribed text (str, dict, Message, or ModelStreamResponse depending\n        on configuration).\n    \"\"\"\n    inputs = self._prepare_task(message, **kwargs)\n    model_response = self._execute_model(**inputs)\n    response = self._process_model_response(model_response, message)\n    return response\n</code></pre>"},{"location":"api-reference/nn/modules/transcriber/#msgflux.nn.modules.transcriber.Transcriber.inspect_model_execution_params","title":"inspect_model_execution_params","text":"<pre><code>inspect_model_execution_params(*args, **kwargs)\n</code></pre> <p>Debug model input parameters.</p> Source code in <code>src/msgflux/nn/modules/transcriber.py</code> <pre><code>def inspect_model_execution_params(self, *args, **kwargs) -&gt; Mapping[str, Any]:\n    \"\"\"Debug model input parameters.\"\"\"\n    inputs = self._prepare_task(*args, **kwargs)\n    model_execution_params = self._prepare_model_execution(**inputs)\n    return model_execution_params\n</code></pre>"},{"location":"blog/","title":"Posts","text":""},{"location":"blog/2023/02/03/english-search-support/","title":"English Support in msgflux","text":"<p>Neste post, vamos explorar as incr\u00edveis possibilidades dos agentes inteligentes.</p>"},{"location":"blog/2023/02/03/english-search-support/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O conte\u00fado completo do post come\u00e7a aqui...</p>"},{"location":"blog/2025/01/03/chinese-search-support/","title":"Chinese Support in msgflux","text":"<p>Neste post, vamos explorar as incr\u00edveis possibilidades dos agentes inteligentes.</p>"},{"location":"blog/2025/01/03/chinese-search-support/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O conte\u00fado completo do post come\u00e7a aqui...</p>"},{"location":"blog/2025/12/03/agent-support/","title":"Amazing: Uma Nova Era","text":"<p>Neste post, vamos as incr\u00edveis possibilidades dos agentes inteligentes.</p>"},{"location":"blog/2025/12/03/agent-support/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>O conte\u00fado completo do post come\u00e7a aqui...</p>"},{"location":"guides/sequential/","title":"Sequential","text":"<p>import msgflux.nn as nn</p>"},{"location":"learn/autoparams/","title":"AutoParams","text":"<p>AutoParams is a metaclass that enables elegant, dataclass-style module definitions in msgflux. It automatically captures class attributes as default parameters, eliminating boilerplate code while maintaining full OOP capabilities.</p>"},{"location":"learn/autoparams/#overview","title":"Overview","text":"<p>AutoParams transforms verbose module definitions into clean, declarative configurations:</p> <p>Traditional Approach (verbose): <pre><code>class DataProcessor:\n    def __init__(self, batch_size, timeout, max_retries):\n        self.batch_size = batch_size\n        self.timeout = timeout\n        self.max_retries = max_retries\n\n    def process(self, data):\n        # Processing logic here\n        pass\n\n# Every instance needs all parameters\nfast_processor = DataProcessor(batch_size=100, timeout=10, max_retries=3)\nslow_processor = DataProcessor(batch_size=10, timeout=60, max_retries=5)\n</code></pre></p> <p>With AutoParams (clean): <pre><code>import msgflux as mf\n\n# Define base class with logic once\nclass DataProcessor(metaclass=mf.AutoParams):\n    def __init__(self, batch_size, timeout, max_retries):\n        self.batch_size = batch_size\n        self.timeout = timeout\n        self.max_retries = max_retries\n\n    def process(self, data):\n        # Processing logic here\n        pass\n\n# Create variants by just setting defaults\nclass FastProcessor(DataProcessor):\n    batch_size = 100\n    timeout = 10\n    max_retries = 3\n\nclass SlowProcessor(DataProcessor):\n    batch_size = 10\n    timeout = 60\n    max_retries = 5\n\n# Clean instantiation\nfast = FastProcessor()  # Uses defaults\nslow = SlowProcessor()  # Uses defaults\ncustom = FastProcessor(batch_size=200)  # Override specific params\n</code></pre></p>"},{"location":"learn/autoparams/#why-autoparams","title":"Why AutoParams?","text":""},{"location":"learn/autoparams/#benefits","title":"Benefits","text":"<ol> <li>Eliminates Boilerplate - No repetitive <code>__init__</code> assignments</li> <li>Declarative Configuration - Parameters as class attributes</li> <li>Flexible Defaults - Easy to override during instantiation</li> <li>Inheritance Support - Build module hierarchies naturally</li> <li>Documentation Integration - Docstrings and class names as parameters</li> <li>PyTorch-Like - Familiar pattern for PyTorch users</li> </ol>"},{"location":"learn/autoparams/#when-to-use","title":"When to Use","text":"<ul> <li>\u2705 Defining <code>nn.Module</code> subclasses (Agent, Retriever, etc.)</li> <li>\u2705 Creating configurable modules with many parameters</li> <li>\u2705 Building module families with shared defaults</li> <li>\u2705 When you want clean, readable code</li> </ul>"},{"location":"learn/autoparams/#quick-start","title":"Quick Start","text":""},{"location":"learn/autoparams/#basic-usage","title":"Basic Usage","text":"<pre><code>import msgflux as mf\n\nclass Configuration(metaclass=mf.AutoParams):\n    \"\"\"Simple configuration class.\"\"\"\n\n    def __init__(self, host, port, debug):\n        self.host = host\n        self.port = port\n        self.debug = debug\n\nclass DevelopmentConfig(Configuration):\n    \"\"\"Development configuration.\"\"\"\n    host = \"localhost\"\n    port = 8000\n    debug = True\n\nclass ProductionConfig(Configuration):\n    \"\"\"Production configuration.\"\"\"\n    host = \"0.0.0.0\"\n    port = 443\n    debug = False\n\n# Use defaults\ndev = DevelopmentConfig()\nprint(dev.host)  # \"localhost\"\nprint(dev.debug)  # True\n\n# Override specific parameters\ncustom_dev = DevelopmentConfig(port=3000)\nprint(custom_dev.port)  # 3000\nprint(custom_dev.host)  # \"localhost\" (still default)\n\n# Production instance\nprod = ProductionConfig()\nprint(prod.debug)  # False\n</code></pre>"},{"location":"learn/autoparams/#core-features","title":"Core Features","text":""},{"location":"learn/autoparams/#1-docstring-as-parameter","title":"1. Docstring as Parameter","text":"<p>Use the class docstring as a parameter value:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass Agent(nn.Module, metaclass=mf.AutoParams):\n    \"\"\"Configurable agent base class.\"\"\"\n\n    _autoparams_use_docstring_for = \"description\"\n\n    def __init__(self, name, description):\n        super().__init__()\n        self.name = name\n        self.description = description\n\nclass ResearchAgent(Agent):\n    \"\"\"An AI agent specialized in research tasks with web search capabilities.\"\"\"\n    name = \"research_assistant\"\n\nagent = ResearchAgent()\nprint(agent.name)  # \"research_assistant\"\nprint(agent.description)  # \"An AI agent specialized in research tasks...\"\n</code></pre>"},{"location":"learn/autoparams/#2-class-name-as-parameter","title":"2. Class Name as Parameter","text":"<p>Automatically use the class name as a parameter:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass Agent(nn.Module, metaclass=mf.AutoParams):\n    \"\"\"Base agent with automatic naming.\"\"\"\n\n    _autoparams_use_classname_for = \"name\"\n\n    def __init__(self, name):\n        super().__init__()\n        self.name = name\n\nclass CustomerSupportAgent(Agent):\n    pass\n\nclass SalesAgent(Agent):\n    pass\n\nsupport = CustomerSupportAgent()\nprint(support.name)  # \"CustomerSupportAgent\"\n\nsales = SalesAgent()\nprint(sales.name)  # \"SalesAgent\"\n</code></pre>"},{"location":"learn/autoparams/#3-combined-docstring-class-name","title":"3. Combined: Docstring + Class Name","text":"<p>Use both features together:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass Agent(nn.Module, metaclass=mf.AutoParams):\n    \"\"\"Base agent class.\"\"\"\n\n    _autoparams_use_docstring_for = \"description\"\n    _autoparams_use_classname_for = \"name\"\n\n    temperature = 0.7\n    max_tokens = 2000\n\n    def __init__(self, name, description, temperature, max_tokens):\n        super().__init__()\n        self.name = name\n        self.description = description\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n\nclass DataAnalyst(Agent):\n    \"\"\"An agent that analyzes data and generates insights.\"\"\"\n    temperature = 0.2  # Override: more deterministic for analysis\n\nanalyst = DataAnalyst()\nprint(analyst.name)  # \"DataAnalyst\"\nprint(analyst.description)  # \"An agent that analyzes data...\"\nprint(analyst.temperature)  # 0.2\nprint(analyst.max_tokens)  # 2000 (inherited)\n</code></pre>"},{"location":"learn/autoparams/#integration-with-nnmodule","title":"Integration with nn.Module","text":"<p>AutoParams is designed to work seamlessly with msgflux neural network modules:</p>"},{"location":"learn/autoparams/#basic-module-example","title":"Basic Module Example","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\n# Define base module with initialization logic\nclass CustomModule(nn.Module, metaclass=mf.AutoParams):\n    \"\"\"Base custom module.\"\"\"\n\n    def __init__(self, temperature, max_tokens, enable_cache):\n        super().__init__()\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.enable_cache = enable_cache\n\n    def forward(self, msg):\n        # Module logic here\n        return msg\n\n# Create variants with different defaults\nclass ConservativeModule(CustomModule):\n    \"\"\"Conservative settings for production.\"\"\"\n    temperature = 0.2\n    max_tokens = 1000\n    enable_cache = True\n\nclass CreativeModule(CustomModule):\n    \"\"\"Creative settings for exploration.\"\"\"\n    temperature = 0.9\n    max_tokens = 2000\n    enable_cache = False\n\n# Use with defaults\nconservative = ConservativeModule()\nprint(conservative.temperature)  # 0.2\nprint(conservative.enable_cache)  # True\n\n# Override specific parameters\ncustom = CreativeModule(temperature=0.5)\nprint(custom.temperature)  # 0.5\nprint(custom.enable_cache)  # False (still default)\n</code></pre>"},{"location":"learn/autoparams/#retriever-example","title":"Retriever Example","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass DocumentRetriever(nn.Retriever):\n    \"\"\"Semantic retriever for technical documentation.\"\"\"\n\n    response_mode = \"plain_response\"\n    top_k = 5\n    threshold = 0.7\n\n# Create vector database and embedder\nvector_db = mf.data.VectorDB.qdrant(\n    collection_name=\"docs\",\n    url=\"http://localhost:6333\"\n)\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\n# Use with defaults\nretriever = DocumentRetriever(\n    retriever=vector_db,\n    model=embedder\n)\n\n# Or customize\nstrict_retriever = DocumentRetriever(\n    retriever=vector_db,\n    model=embedder,\n    top_k=10,      # More results\n    threshold=0.8  # Higher similarity required\n)\n</code></pre>"},{"location":"learn/autoparams/#speaker-example","title":"Speaker Example","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass NaturalSpeaker(nn.Speaker):\n    \"\"\"Natural-sounding voice for user-facing applications.\"\"\"\n\n    response_format = \"mp3\"\n    response_mode = \"plain_response\"\n    voice = \"alloy\"\n    speed = 1.0\n\nclass FastSpeaker(NaturalSpeaker):\n    \"\"\"Faster speech for time-constrained scenarios.\"\"\"\n    speed = 1.25\n\ntts = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# Normal speed\nnormal = NaturalSpeaker(model=tts)\naudio1 = normal(\"Hello, how can I help you?\")\n\n# Fast speed\nfast = FastSpeaker(model=tts)\naudio2 = fast(\"Hello, how can I help you?\")\n</code></pre>"},{"location":"learn/autoparams/#inheritance-patterns","title":"Inheritance Patterns","text":""},{"location":"learn/autoparams/#building-module-hierarchies","title":"Building Module Hierarchies","text":"<p>Create families of related modules:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\n# Base module with common configuration\nclass BaseModule(nn.Module, metaclass=mf.AutoParams):\n    \"\"\"Base module with standard configuration.\"\"\"\n\n    def __init__(self, temperature, max_tokens, enable_cache):\n        super().__init__()\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.enable_cache = enable_cache\n\n    def forward(self, msg):\n        # Processing logic\n        return msg\n\n# Specialized for different use cases\nclass CreativeModule(BaseModule):\n    \"\"\"Module optimized for creative tasks.\"\"\"\n    temperature = 0.9  # More random/creative\n    max_tokens = 3000  # Longer outputs\n    enable_cache = False\n\nclass AnalyticalModule(BaseModule):\n    \"\"\"Module optimized for analytical tasks.\"\"\"\n    temperature = 0.2  # More deterministic\n    max_tokens = 1500  # Focused responses\n    enable_cache = True\n\nclass CodingModule(AnalyticalModule):\n    \"\"\"Module specialized in coding tasks.\"\"\"\n    max_tokens = 4000  # Longer code blocks\n\n# Create instances\ncreative = CreativeModule()\nprint(creative.temperature)  # 0.9\n\nanalytical = AnalyticalModule()\nprint(analytical.temperature)  # 0.2\n\ncoder = CodingModule()\nprint(coder.temperature)  # 0.2 (inherited from AnalyticalModule)\nprint(coder.max_tokens)  # 4000 (overridden)\n</code></pre>"},{"location":"learn/autoparams/#multi-level-inheritance","title":"Multi-Level Inheritance","text":"<pre><code>import msgflux as mf\n\nclass BaseConfig(metaclass=mf.AutoParams):\n    \"\"\"Base configuration.\"\"\"\n\n    def __init__(self, timeout, retry_attempts):\n        self.timeout = timeout\n        self.retry_attempts = retry_attempts\n\nclass NetworkConfig(BaseConfig):\n    \"\"\"Network configuration.\"\"\"\n    timeout = 30\n    retry_attempts = 3\n\nclass DatabaseConfig(NetworkConfig):\n    \"\"\"Database configuration inherits network settings.\"\"\"\n    connection_pool_size = 10\n\nclass ProductionDatabaseConfig(DatabaseConfig):\n    \"\"\"Production database with stricter settings.\"\"\"\n    retry_attempts = 5  # Override\n    connection_pool_size = 50  # Override\n\n# All parameters available\nprod_db = ProductionDatabaseConfig()\nprint(prod_db.timeout)  # 30 (from NetworkConfig)\nprint(prod_db.retry_attempts)  # 5 (overridden)\nprint(prod_db.connection_pool_size)  # 50 (overridden)\n</code></pre>"},{"location":"learn/autoparams/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"learn/autoparams/#dynamic-defaults","title":"Dynamic Defaults","text":"<p>Use class methods or properties for computed defaults:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\nimport os\n\nclass APIModule(nn.Module, metaclass=mf.AutoParams):\n    \"\"\"Module that calls external APIs.\"\"\"\n\n    def __init__(self, api_key, timeout, max_retries):\n        super().__init__()\n        self.api_key = api_key\n        self.timeout = timeout\n        self.max_retries = max_retries\n\n    def forward(self, msg):\n        # API call logic\n        return msg\n\nclass OpenAIModule(APIModule):\n    \"\"\"OpenAI-specific API module.\"\"\"\n    api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n    timeout = 30\n    max_retries = 3\n\n# api_key is automatically loaded from environment\nmodule = OpenAIModule()\nprint(module.api_key)  # Value from OPENAI_API_KEY env var\n</code></pre>"},{"location":"learn/autoparams/#conditional-configuration","title":"Conditional Configuration","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass Environment:\n    DEVELOPMENT = \"development\"\n    PRODUCTION = \"production\"\n\nclass ConfigurableModule(nn.Module, metaclass=mf.AutoParams):\n    \"\"\"Module with environment-specific defaults.\"\"\"\n\n    def __init__(self, environment, debug_mode, error_reporting):\n        super().__init__()\n        self.environment = environment\n        self.debug_mode = debug_mode\n        self.error_reporting = error_reporting\n\n    def forward(self, msg):\n        if self.debug_mode:\n            print(f\"Debug: Processing in {self.environment} mode\")\n        return msg\n\nclass DevelopmentModule(ConfigurableModule):\n    \"\"\"Development module.\"\"\"\n    environment = Environment.DEVELOPMENT\n    debug_mode = True\n    error_reporting = False\n\nclass ProductionModule(ConfigurableModule):\n    \"\"\"Production-ready module.\"\"\"\n    environment = Environment.PRODUCTION\n    debug_mode = False\n    error_reporting = True\n\ndev = DevelopmentModule()\nprint(dev.debug_mode)  # True\n\nprod = ProductionModule()\nprint(prod.debug_mode)  # False\nprint(prod.error_reporting)  # True\n</code></pre>"},{"location":"learn/autoparams/#real-world-examples","title":"Real-World Examples","text":""},{"location":"learn/autoparams/#multi-language-support-system","title":"Multi-Language Support System","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass TranscriptionModule(nn.Module, metaclass=mf.AutoParams):\n    \"\"\"Base transcription module.\"\"\"\n\n    def __init__(self, language, prompt, temperature):\n        super().__init__()\n        self.language = language\n        self.prompt = prompt\n        self.temperature = temperature\n\n    def forward(self, msg):\n        # Transcription logic\n        return msg\n\nclass EnglishTranscriber(TranscriptionModule):\n    \"\"\"Optimized for English transcription.\"\"\"\n    language = \"en\"\n    prompt = \"Transcribe with proper punctuation and capitalization.\"\n    temperature = 0.0\n\nclass SpanishTranscriber(TranscriptionModule):\n    \"\"\"Optimized for Spanish transcription.\"\"\"\n    language = \"es\"\n    prompt = \"Transcribir con puntuaci\u00f3n y capitalizaci\u00f3n apropiadas.\"\n    temperature = 0.0\n\nclass MultilingualTranscriber(TranscriptionModule):\n    \"\"\"Auto-detects language.\"\"\"\n    language = None\n    prompt = \"Transcribe in the detected language.\"\n    temperature = 0.0\n\n# Create language-specific instances\nen_transcriber = EnglishTranscriber()\nes_transcriber = SpanishTranscriber()\nmulti_transcriber = MultilingualTranscriber()\n</code></pre>"},{"location":"learn/autoparams/#content-moderation-pipeline","title":"Content Moderation Pipeline","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass ModerationModule(nn.Module, metaclass=mf.AutoParams):\n    \"\"\"Base moderation module.\"\"\"\n\n    def __init__(self, system_message, threshold, temperature):\n        super().__init__()\n        self.system_message = system_message\n        self.threshold = threshold\n        self.temperature = temperature\n\n    def forward(self, msg):\n        # Moderation logic\n        return msg\n\nclass StrictModerator(ModerationModule):\n    \"\"\"Strict moderation for public content.\"\"\"\n    system_message = \"You are a strict content moderator. Flag any potentially harmful content.\"\n    threshold = 0.3  # Low threshold = more strict\n    temperature = 0.1\n\nclass LenientModerator(ModerationModule):\n    \"\"\"Lenient moderation for private communities.\"\"\"\n    system_message = \"You are a lenient content moderator. Only flag clearly harmful content.\"\n    threshold = 0.7  # High threshold = less strict\n    temperature = 0.1\n\nclass ChildSafeModerator(StrictModerator):\n    \"\"\"Ultra-strict moderation for child-safe content.\"\"\"\n    threshold = 0.1  # Very low threshold\n\n# Different moderation levels\nstrict = StrictModerator()\nlenient = LenientModerator()\nchild_safe = ChildSafeModerator()\n</code></pre>"},{"location":"learn/autoparams/#rag-system-with-different-retrieval-strategies","title":"RAG System with Different Retrieval Strategies","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass SemanticRetriever(nn.Retriever):\n    \"\"\"Base semantic retriever.\"\"\"\n    response_mode = \"plain_response\"\n    top_k = 5\n\nclass PreciseRetriever(SemanticRetriever):\n    \"\"\"High-precision retrieval.\"\"\"\n    top_k = 3\n    threshold = 0.85  # Only very similar results\n\nclass BroadRetriever(SemanticRetriever):\n    \"\"\"Broad retrieval for exploration.\"\"\"\n    top_k = 10\n    threshold = 0.6  # More permissive\n\nclass HybridRetriever(SemanticRetriever):\n    \"\"\"Combines semantic and keyword search.\"\"\"\n    top_k = 7\n    threshold = 0.7\n    use_keyword_boost = True\n\nvector_db = mf.data.VectorDB.qdrant(\n    collection_name=\"knowledge_base\",\n    url=\"http://localhost:6333\"\n)\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\n# Different retrieval strategies\nprecise = PreciseRetriever(retriever=vector_db, model=embedder)\nbroad = BroadRetriever(retriever=vector_db, model=embedder)\nhybrid = HybridRetriever(retriever=vector_db, model=embedder)\n</code></pre>"},{"location":"learn/autoparams/#best-practices","title":"Best Practices","text":""},{"location":"learn/autoparams/#1-use-descriptive-class-names","title":"1. Use Descriptive Class Names","text":"<pre><code>import msgflux.nn as nn\n\n# Good - Clear purpose\nclass CustomerSupportModule(nn.Module):\n    \"\"\"Module for customer support inquiries.\"\"\"\n    temperature = 0.7\n\n# Avoid - Vague names\nclass Module1(nn.Module):\n    temperature = 0.7\n</code></pre>"},{"location":"learn/autoparams/#2-document-with-docstrings","title":"2. Document with Docstrings","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\n# Good - Clear documentation\nclass AnalyticalModule(nn.Module, metaclass=mf.AutoParams):\n    \"\"\"\n    Module optimized for analytical and data-driven tasks.\n\n    Uses low temperature for consistent, factual responses.\n    Suitable for data analysis, reporting, and calculations.\n    \"\"\"\n\n    def __init__(self, temperature, max_tokens):\n        super().__init__()\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n\nclass DataAnalyzer(AnalyticalModule):\n    temperature = 0.2\n    max_tokens = 2000\n\n# Avoid - No documentation\nclass MyModule(nn.Module, metaclass=mf.AutoParams):\n    def __init__(self, temperature, max_tokens):\n        super().__init__()\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n\nclass Analyzer(MyModule):\n    temperature = 0.2\n    max_tokens = 2000\n</code></pre>"},{"location":"learn/autoparams/#3-group-related-configurations","title":"3. Group Related Configurations","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\n# Good - Logical groupings\nclass BaseModule(nn.Module, metaclass=mf.AutoParams):\n    def __init__(self, temperature, max_tokens, enable_cache, verbose):\n        super().__init__()\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.enable_cache = enable_cache\n        self.verbose = verbose\n\nclass ProductionModule(BaseModule):\n    # Model parameters\n    temperature = 0.7\n    max_tokens = 2000\n\n    # Behavior parameters\n    enable_cache = True\n    verbose = False\n</code></pre>"},{"location":"learn/autoparams/#4-use-inheritance-for-variants","title":"4. Use Inheritance for Variants","text":"<pre><code># Good - Base + variants\nclass BaseRetriever(nn.Retriever):\n    response_mode = \"plain_response\"\n    top_k = 5\n\nclass FastRetriever(BaseRetriever):\n    top_k = 3  # Fewer results for speed\n\nclass ThoroughRetriever(BaseRetriever):\n    top_k = 10  # More results for coverage\n\n# Avoid - Duplicating everything\nclass FastRetriever(nn.Retriever):\n    response_mode = \"plain_response\"  # Duplicated\n    top_k = 3\n\nclass ThoroughRetriever(nn.Retriever):\n    response_mode = \"plain_response\"  # Duplicated\n    top_k = 10\n</code></pre>"},{"location":"learn/autoparams/#5-provide-sensible-defaults","title":"5. Provide Sensible Defaults","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass WebSearchModule(nn.Module, metaclass=mf.AutoParams):\n    \"\"\"Base web search module.\"\"\"\n\n    def __init__(self, max_results, timeout, retry_attempts):\n        super().__init__()\n        self.max_results = max_results\n        self.timeout = timeout\n        self.retry_attempts = retry_attempts\n\n# Good - Reasonable defaults\nclass StandardSearch(WebSearchModule):\n    max_results = 5  # Reasonable number\n    timeout = 30     # Reasonable timeout\n    retry_attempts = 3  # Reasonable retries\n\n# Avoid - Extreme or unclear defaults\nclass BadSearch(WebSearchModule):\n    max_results = 1000  # Too many\n    timeout = 1      # Too short\n    retry_attempts = 100  # Too many\n</code></pre>"},{"location":"learn/autoparams/#api-reference","title":"API Reference","text":""},{"location":"learn/autoparams/#special-attributes","title":"Special Attributes","text":"Attribute Type Description <code>_autoparams_use_docstring_for</code> str Use class docstring as value for this parameter <code>_autoparams_use_classname_for</code> str Use class name as value for this parameter"},{"location":"learn/autoparams/#how-it-works","title":"How It Works","text":"<ol> <li>Class Definition: AutoParams captures class attributes (except methods and special attributes)</li> <li>Initialization: When instantiated, defaults from class attributes are used</li> <li>Override: Parameters passed to <code>__init__</code> override defaults</li> <li>Inheritance: Child classes inherit parent defaults and can override them</li> </ol>"},{"location":"learn/autoparams/#comparison-traditional-vs-autoparams","title":"Comparison: Traditional vs AutoParams","text":""},{"location":"learn/autoparams/#traditional-approach","title":"Traditional Approach","text":"<pre><code>class TraditionalModule:\n    def __init__(self, temperature=0.7, max_tokens=2000, enable_cache=True, verbose=False):\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.enable_cache = enable_cache\n        self.verbose = verbose\n\n    def process(self, data):\n        # Processing logic\n        pass\n\n# Every variant needs all parameters repeated\nfast = TraditionalModule(temperature=0.8, max_tokens=1500, enable_cache=True, verbose=False)\nslow = TraditionalModule(temperature=0.2, max_tokens=3000, enable_cache=True, verbose=True)\n</code></pre> <p>Issues: - Repetitive parameter assignments - Constructor becomes long with many parameters - Defaults buried in function signature - Hard to create variants without repeating all parameters - Configuration not visible at class level</p>"},{"location":"learn/autoparams/#with-autoparams","title":"With AutoParams","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass BaseModule(nn.Module, metaclass=mf.AutoParams):\n    \"\"\"Base module with AutoParams.\"\"\"\n\n    def __init__(self, temperature, max_tokens, enable_cache, verbose):\n        super().__init__()\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.enable_cache = enable_cache\n        self.verbose = verbose\n\n    def process(self, data):\n        # Processing logic\n        pass\n\n# Create variants by just setting defaults\nclass FastModule(BaseModule):\n    temperature = 0.8\n    max_tokens = 1500\n    enable_cache = True\n    verbose = False\n\nclass SlowModule(BaseModule):\n    temperature = 0.2\n    max_tokens = 3000\n    enable_cache = True\n    verbose = True\n\n# Clean instantiation\nfast = FastModule()\nslow = SlowModule()\n</code></pre>"},{"location":"learn/dotdict/","title":"dotdict","text":"<p>The <code>dotdict</code> class provides an enhanced dictionary with attribute-style access and nested path support, making it easier to work with complex nested data structures.</p>"},{"location":"learn/dotdict/#overview","title":"Overview","text":"<p><code>dotdict</code> extends Python's built-in <code>dict</code> with convenient features:</p> <ul> <li>Dot notation access: <code>obj.key</code> instead of <code>obj['key']</code></li> <li>Nested path operations: <code>obj.get(\"user.profile.name\")</code></li> <li>List index support: <code>obj.get(\"items.0.title\")</code></li> <li>Immutability option: Create frozen dictionaries</li> <li>Hidden keys: Protect sensitive data from being displayed</li> <li>Type preservation: Automatic wrapping of nested dicts and lists</li> </ul>"},{"location":"learn/dotdict/#quick-start","title":"Quick Start","text":""},{"location":"learn/dotdict/#basic-usage","title":"Basic Usage","text":"<pre><code>import msgflux as mf\n\n# Create a dotdict\nuser = mf.dotdict({\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"email\": \"alice@example.com\"\n})\n\n# Access with dot notation\nprint(user.name)   # \"Alice\"\nprint(user.age)    # 30\n\n# Also works with traditional bracket notation\nprint(user[\"email\"])  # \"alice@example.com\"\n\n# Set values with dot notation\nuser.location = \"New York\"\nprint(user.location)  # \"New York\"\n</code></pre>"},{"location":"learn/dotdict/#nested-structures","title":"Nested Structures","text":"<pre><code>import msgflux as mf\n\n# Create nested structure\ndata = mf.dotdict({\n    \"user\": {\n        \"profile\": {\n            \"name\": \"Bob\",\n            \"age\": 25\n        },\n        \"settings\": {\n            \"theme\": \"dark\",\n            \"notifications\": True\n        }\n    }\n})\n\n# Access nested values\nprint(data.user.profile.name)  # \"Bob\"\nprint(data.user.settings.theme)  # \"dark\"\n\n# Nested values are automatically wrapped as dotdict\nprint(type(data.user))  # &lt;class 'msgflux.dotdict.dotdict'&gt;\n</code></pre>"},{"location":"learn/dotdict/#path-based-access","title":"Path-Based Access","text":""},{"location":"learn/dotdict/#reading-with-get","title":"Reading with <code>get()</code>","text":"<p>Access deeply nested values using dot-separated paths:</p> <pre><code>import msgflux as mf\n\ndata = mf.dotdict({\n    \"api\": {\n        \"endpoints\": {\n            \"users\": \"/api/v1/users\",\n            \"posts\": \"/api/v1/posts\"\n        }\n    }\n})\n\n# Get with path\nendpoint = data.get(\"api.endpoints.users\")\nprint(endpoint)  # \"/api/v1/users\"\n\n# With default value\nmissing = data.get(\"api.endpoints.comments\", \"/api/v1/comments\")\nprint(missing)  # \"/api/v1/comments\"\n</code></pre>"},{"location":"learn/dotdict/#writing-with-set","title":"Writing with <code>set()</code>","text":"<p>Create or modify nested values using paths:</p> <pre><code>import msgflux as mf\n\nconfig = mf.dotdict()\n\n# Set nested values (creates intermediate dotdicts automatically)\nconfig.set(\"database.host\", \"localhost\")\nconfig.set(\"database.port\", 5432)\nconfig.set(\"database.credentials.username\", \"admin\")\n\nprint(config.database.host)  # \"localhost\"\nprint(config.database.credentials.username)  # \"admin\"\n\n# View structure\nprint(config)\n# {'database': {'host': 'localhost', 'port': 5432, 'credentials': {'username': 'admin'}}}\n</code></pre>"},{"location":"learn/dotdict/#understanding-access-methods-get-vs-dot-notation","title":"Understanding Access Methods: <code>get()</code> vs Dot Notation","text":"<p>IMPORTANT: There are two ways to access values in a dotdict, and understanding when to use each is critical.</p>"},{"location":"learn/dotdict/#when-dot-notation-works","title":"When Dot Notation Works","text":"<p>Dot notation (e.g., <code>config.database.host</code>) only works when the nested structure already exists. The structure must be created first via:</p> <ul> <li>Initial dictionary in constructor</li> <li><code>set()</code> method calls</li> <li><code>update()</code> method calls</li> </ul> <pre><code>import msgflux as mf\n\n# \u2713 WORKS: Structure created in constructor\ndata = mf.dotdict({\"user\": {\"name\": \"Alice\"}})\nprint(data.user.name)  # \"Alice\" - works!\n\n# \u2713 WORKS: Structure created via set()\nconfig = mf.dotdict()\nconfig.set(\"database.host\", \"localhost\")\nprint(config.database.host)  # \"localhost\" - works!\n\n# \u2713 WORKS: Structure created via update()\nsettings = mf.dotdict()\nsettings.update({\"server.port\": 8080})\nprint(settings.server.port)  # 8080 - works!\n</code></pre>"},{"location":"learn/dotdict/#when-dot-notation-fails","title":"When Dot Notation Fails","text":"<p>Dot notation fails with AttributeError when trying to access non-existent intermediate paths:</p> <pre><code>import msgflux as mf\n\n# \u2717 FAILS: Trying to access non-existent path\nconfig = mf.dotdict()\ntry:\n    port = config.database.port  # AttributeError!\nexcept AttributeError as e:\n    print(f\"Error: {e}\")  # Error: 'dotdict' object has no attribute 'database'\n</code></pre>"},{"location":"learn/dotdict/#use-get-for-safe-access","title":"Use <code>get()</code> for Safe Access","text":"<p>The <code>get()</code> method with path strings always works safely, even for non-existent paths:</p> <pre><code>import msgflux as mf\n\nconfig = mf.dotdict()\n\n# \u2713 WORKS: get() with default value\nport = config.get(\"database.port\", 5432)\nprint(port)  # 5432 (default value)\n\n# \u2713 WORKS: get() returns None for missing paths\nhost = config.get(\"database.host\")\nprint(host)  # None\n\n# After creating the structure, both methods work\nconfig.set(\"database.port\", 3306)\nprint(config.get(\"database.port\"))  # 3306\nprint(config.database.port)  # 3306 - now this works too!\n</code></pre>"},{"location":"learn/dotdict/#best-practice-guidelines","title":"Best Practice Guidelines","text":"<p>Use <code>get()</code> when: - Accessing potentially non-existent paths - You want a default value if the path doesn't exist - Working with dynamic or uncertain data structures - You need safe access without try/except blocks</p> <p>Use dot notation when: - The structure is guaranteed to exist (created in constructor or via set/update) - Accessing top-level keys that you know exist - Code readability is important and you're certain about the structure</p> <pre><code>import msgflux as mf\n\n# Example: Safe API response handling\nresponse = mf.dotdict()\n\n# Use get() for potentially missing fields\nuser_id = response.get(\"data.user.id\", None)\nif user_id:\n    # Now safe to use dot notation on known structure\n    print(f\"User ID: {user_id}\")\n\n# Or combine both approaches\nconfig = mf.dotdict()\nconfig.set(\"app.name\", \"MyApp\")\nconfig.set(\"app.version\", \"1.0\")\n\n# Safe: structure was created above\nprint(f\"Running {config.app.name} v{config.app.version}\")\n\n# Safe: using get() for optional settings\ndebug = config.get(\"app.debug\", False)\nprint(f\"Debug mode: {debug}\")\n</code></pre>"},{"location":"learn/dotdict/#working-with-lists","title":"Working with Lists","text":"<p><code>dotdict</code> supports accessing list items using numeric indices in paths:</p> <pre><code>import msgflux as mf\n\ndata = mf.dotdict({\n    \"users\": [\n        {\"name\": \"Alice\", \"role\": \"admin\"},\n        {\"name\": \"Bob\", \"role\": \"user\"},\n        {\"name\": \"Charlie\", \"role\": \"moderator\"}\n    ]\n})\n\n# Access list items by index\nfirst_user = data.get(\"users.0.name\")\nprint(first_user)  # \"Alice\"\n\nsecond_role = data.get(\"users.1.role\")\nprint(second_role)  # \"user\"\n\n# Set values in lists\ndata.set(\"users.0.status\", \"active\")\nprint(data.users[0].status)  # \"active\"\n\n# Note: dot notation doesn't work for numeric indices\n# data.users.0.name  # SyntaxError\n# Use get() or bracket notation instead:\nprint(data.users[0].name)  # \"Alice\"\n</code></pre>"},{"location":"learn/dotdict/#immutability-with-frozen","title":"Immutability with <code>frozen</code>","text":"<p>Create read-only dictionaries:</p> <pre><code>import msgflux as mf\n\n# Create frozen dotdict\nconstants = mf.dotdict(\n    {\"PI\": 3.14159, \"E\": 2.71828},\n    frozen=True\n)\n\nprint(constants.PI)  # 3.14159\n\n# Attempting to modify raises an error\ntry:\n    constants.PI = 3.14\nexcept AttributeError as e:\n    print(e)  # \"Cannot modify frozen dotdict\"\n\ntry:\n    constants.set(\"GOLDEN_RATIO\", 1.618)\nexcept AttributeError as e:\n    print(e)  # \"Cannot modify frozen dotdict\"\n</code></pre>"},{"location":"learn/dotdict/#hidden-keys","title":"Hidden Keys","text":"<p>Protect sensitive data from being displayed or accessed via <code>get()</code>:</p> <pre><code>import msgflux as mf\n\n# Create dotdict with hidden keys\nconfig = mf.dotdict(\n    {\n        \"api_key\": \"sk-secret-key-12345\",\n        \"api_url\": \"https://api.example.com\",\n        \"username\": \"admin\",\n        \"password\": \"super-secret\"\n    },\n    hidden_keys=[\"api_key\", \"password\"]\n)\n\n# Hidden keys return None when accessed via get()\nprint(config.get(\"api_key\"))  # None\nprint(config.get(\"password\"))  # None\n\n# Non-hidden keys work normally\nprint(config.get(\"api_url\"))  # \"https://api.example.com\"\nprint(config.get(\"username\"))  # \"admin\"\n\n# Hidden keys are still accessible via dot/bracket notation\nprint(config.api_key)  # \"sk-secret-key-12345\" (direct access still works)\nprint(config[\"password\"])  # \"super-secret\"\n\n# Hidden keys don't appear in string representations\nprint(config)  # {'api_url': 'https://api.example.com', 'username': 'admin'}\n</code></pre> <p>Use Cases for Hidden Keys: - API keys and secrets - Passwords and tokens - Sensitive user data - Internal configuration that shouldn't be logged</p>"},{"location":"learn/dotdict/#advanced-features","title":"Advanced Features","text":""},{"location":"learn/dotdict/#update-with-nested-paths","title":"Update with Nested Paths","text":"<p>Use <code>update()</code> with dot-separated keys:</p> <pre><code>import msgflux as mf\n\nconfig = mf.dotdict({\n    \"server\": {\"host\": \"localhost\"}\n})\n\n# Update with nested keys\nconfig.update({\n    \"server.port\": 8080,\n    \"server.ssl\": True,\n    \"database.type\": \"postgresql\"\n})\n\nprint(config.server.port)  # 8080\nprint(config.database.type)  # \"postgresql\"\n\n# Merge existing nested structures\nconfig.update({\n    \"server\": {\"workers\": 4}\n})\n\nprint(config.server.host)  # \"localhost\" (preserved)\nprint(config.server.workers)  # 4 (added)\n</code></pre>"},{"location":"learn/dotdict/#convert-to-regular-dict","title":"Convert to Regular Dict","text":"<p>Convert back to a standard Python dictionary:</p> <pre><code>import msgflux as mf\n\ndata = mf.dotdict({\n    \"user\": {\n        \"name\": \"Alice\",\n        \"settings\": {\"theme\": \"dark\"}\n    }\n})\n\n# Convert to regular dict\nregular_dict = data.to_dict()\nprint(type(regular_dict))  # &lt;class 'dict'&gt;\nprint(type(regular_dict[\"user\"]))  # &lt;class 'dict'&gt; (not dotdict)\n\n# All nested dotdicts are converted to regular dicts\nprint(regular_dict)\n# {'user': {'name': 'Alice', 'settings': {'theme': 'dark'}}}\n</code></pre>"},{"location":"learn/dotdict/#json-serialization","title":"JSON Serialization","text":"<p>Export as JSON:</p> <pre><code>import msgflux as mf\n\ndata = mf.dotdict({\n    \"name\": \"Product\",\n    \"price\": 29.99,\n    \"tags\": [\"electronics\", \"gadget\"]\n})\n\n# Serialize to JSON bytes\njson_bytes = data.to_json()\nprint(json_bytes)\n# b'{\"name\":\"Product\",\"price\":29.99,\"tags\":[\"electronics\",\"gadget\"]}'\n\n# Decode to string if needed\njson_str = json_bytes.decode('utf-8')\nprint(json_str)\n# '{\"name\":\"Product\",\"price\":29.99,\"tags\":[\"electronics\",\"gadget\"]}'\n</code></pre>"},{"location":"learn/dotdict/#common-use-cases","title":"Common Use Cases","text":""},{"location":"learn/dotdict/#configuration-management","title":"Configuration Management","text":"<pre><code>import msgflux as mf\n\n# Application configuration\nconfig = mf.dotdict()\n\n# Set configuration values\nconfig.set(\"app.name\", \"My Application\")\nconfig.set(\"app.version\", \"1.0.0\")\nconfig.set(\"database.host\", \"localhost\")\nconfig.set(\"database.port\", 5432)\nconfig.set(\"redis.host\", \"localhost\")\nconfig.set(\"redis.port\", 6379)\n\n# Access configuration easily\nprint(f\"Starting {config.app.name} v{config.app.version}\")\nprint(f\"Database: {config.database.host}:{config.database.port}\")\n</code></pre>"},{"location":"learn/dotdict/#api-response-handling","title":"API Response Handling","text":"<pre><code>import msgflux as mf\n\n# Simulate API response\napi_response = mf.dotdict({\n    \"status\": \"success\",\n    \"data\": {\n        \"user\": {\n            \"id\": 123,\n            \"username\": \"johndoe\",\n            \"profile\": {\n                \"full_name\": \"John Doe\",\n                \"avatar_url\": \"https://example.com/avatar.jpg\"\n            }\n        },\n        \"permissions\": [\"read\", \"write\", \"admin\"]\n    }\n})\n\n# Easy access to nested data\nif api_response.status == \"success\":\n    user_id = api_response.data.user.id\n    full_name = api_response.data.user.profile.full_name\n    permissions = api_response.data.permissions\n\n    print(f\"User {full_name} (ID: {user_id})\")\n    print(f\"Permissions: {', '.join(permissions)}\")\n</code></pre>"},{"location":"learn/dotdict/#message-passing","title":"Message Passing","text":"<pre><code>import msgflux as mf\n\n# Create message with dotdict\nmessage = mf.dotdict()\n\n# Add data at different stages\nmessage.set(\"request.id\", \"req-12345\")\nmessage.set(\"request.timestamp\", \"2024-01-15T10:30:00\")\n\n# Processing stage\nmessage.set(\"processing.model\", \"gpt-5\")\nmessage.set(\"processing.tokens\", 150)\n\n# Response stage\nmessage.set(\"response.status\", \"completed\")\nmessage.set(\"response.data\", \"Hello, world!\")\n\n# Access full context\nprint(f\"Request {message.request.id} processed with {message.processing.model}\")\nprint(f\"Result: {message.response.data}\")\n</code></pre>"},{"location":"learn/dotdict/#integration-with-message","title":"Integration with Message","text":"<p><code>dotdict</code> works seamlessly with msgflux's <code>Message</code> class:</p> <pre><code>import msgflux as mf\n\n# Message internally uses dotdict\nmsg = mf.Message()\nmsg.set(\"user.id\", 123)\nmsg.set(\"user.name\", \"Alice\")\n\n# Access like dotdict\nprint(msg.user.name)  # \"Alice\"\n\n# Or create dotdict from message data\ndata = mf.dotdict(msg.to_dict())\nprint(data.user.id)  # 123\n</code></pre>"},{"location":"learn/dotdict/#api-reference","title":"API Reference","text":""},{"location":"learn/dotdict/#constructor","title":"Constructor","text":"<pre><code>mf.dotdict(data=None, *, frozen=False, hidden_keys=None, **kwargs)\n</code></pre> <p>Parameters: - <code>data</code> (dict, optional): Initial dictionary data - <code>frozen</code> (bool): If <code>True</code>, creates immutable dotdict - <code>hidden_keys</code> (list[str]): Keys to hide from <code>get()</code> and string representations - <code>**kwargs</code>: Additional key-value pairs</p>"},{"location":"learn/dotdict/#methods","title":"Methods","text":"Method Description <code>get(path, default=None)</code> Get value using dot-separated path <code>set(path, value)</code> Set value using dot-separated path <code>update(*args, **kwargs)</code> Update with dict or key-value pairs (supports nested paths) <code>to_dict()</code> Convert to regular Python dict <code>to_json()</code> Serialize to JSON bytes"},{"location":"learn/dotdict/#attributes","title":"Attributes","text":"<p>All standard <code>dict</code> methods are available (<code>keys()</code>, <code>values()</code>, <code>items()</code>, etc.)</p>"},{"location":"learn/dotdict/#best-practices","title":"Best Practices","text":""},{"location":"learn/dotdict/#1-use-for-complex-nested-data","title":"1. Use for Complex Nested Data","text":"<pre><code># Good - Complex nested structures\nconfig = mf.dotdict({\n    \"services\": {\n        \"api\": {\"host\": \"api.example.com\"},\n        \"db\": {\"host\": \"db.example.com\"}\n    }\n})\n\n# Not ideal - Simple flat data (regular dict is fine)\nsimple = mf.dotdict({\"name\": \"Alice\", \"age\": 30})\n</code></pre>"},{"location":"learn/dotdict/#2-protect-sensitive-data","title":"2. Protect Sensitive Data","text":"<pre><code># Good - Hide sensitive keys\ncredentials = mf.dotdict(\n    {\"username\": \"admin\", \"password\": \"secret\"},\n    hidden_keys=[\"password\"]\n)\n</code></pre>"},{"location":"learn/dotdict/#3-use-frozen-for-constants","title":"3. Use Frozen for Constants","text":"<pre><code># Good - Immutable configuration\nCONSTANTS = mf.dotdict(\n    {\"MAX_RETRIES\": 3, \"TIMEOUT\": 30},\n    frozen=True\n)\n</code></pre>"},{"location":"learn/dotdict/#4-use-get-for-uncertain-paths-dot-notation-for-known-structures","title":"4. Use <code>get()</code> for Uncertain Paths, Dot Notation for Known Structures","text":"<pre><code>import msgflux as mf\n\n# Good - Use get() for potentially non-existent paths\nvalue = data.get(\"level1.level2.level3.value\", \"default\")\n\n# Good - Use dot notation when structure is guaranteed to exist\nconfig = mf.dotdict({\"app\": {\"name\": \"MyApp\", \"version\": \"1.0\"}})\nprint(config.app.name)  # Safe because structure exists\n\n# Avoid - Dot notation on uncertain paths without error handling\ntry:\n    value = data.level1.level2.level3.value  # Risky!\nexcept AttributeError:\n    value = \"default\"  # Better to use get() instead\n\n# Best - Combine both approaches appropriately\nconfig = mf.dotdict()\nconfig.set(\"server.host\", \"localhost\")  # Creates structure\nhost = config.server.host  # Safe: structure exists\nport = config.get(\"server.port\", 8080)  # Safe: use get() for optional value\n</code></pre>"},{"location":"learn/inline/","title":"inline","text":"<p>The <code>inline</code> function provides a simple, declarative language for orchestrating module workflows at runtime. Define complex pipelines using intuitive string syntax without writing explicit Python code for control flow.</p>"},{"location":"learn/inline/#overview","title":"Overview","text":"<p><code>inline</code> allows you to:</p> <ul> <li>Define workflows declaratively using string syntax</li> <li>Change execution flow at runtime without modifying code</li> <li>Combine sequential and parallel execution</li> <li>Add conditional branching with if-else logic</li> <li>Implement loops for iterative processing</li> <li>Keep workflow logic separate from module implementation</li> </ul>"},{"location":"learn/inline/#quick-start","title":"Quick Start","text":""},{"location":"learn/inline/#basic-sequential-pipeline","title":"Basic Sequential Pipeline","text":"<pre><code>import msgflux as mf\n\n# Define simple modules\ndef prep(msg):\n    msg.data_ready = True\n    return msg\n\ndef process(msg):\n    if msg.get(\"data_ready\"):\n        msg.processed = \"completed\"\n    return msg\n\ndef output(msg):\n    print(f\"Result: {msg.processed}\")\n    return msg\n\n# Define workflow with inline\nmodules = {\n    \"prep\": prep,\n    \"process\": process,\n    \"output\": output\n}\n\nmessage = mf.dotdict()\nmf.inline(\"prep -&gt; process -&gt; output\", modules, message)\n# Output: Result: completed\n</code></pre>"},{"location":"learn/inline/#syntax-reference","title":"Syntax Reference","text":""},{"location":"learn/inline/#1-sequential-execution-","title":"1. Sequential Execution (<code>-&gt;</code>)","text":"<p>Execute modules in order:</p> <pre><code>\"module1 -&gt; module2 -&gt; module3\"\n</code></pre> <pre><code>import msgflux as mf\n\ndef step1(msg):\n    msg.count = 1\n    return msg\n\ndef step2(msg):\n    msg.count += 1\n    return msg\n\ndef step3(msg):\n    msg.count += 1\n    return msg\n\nmodules = {\"step1\": step1, \"step2\": step2, \"step3\": step3}\nmsg = mf.dotdict()\n\nmf.inline(\"step1 -&gt; step2 -&gt; step3\", modules, msg)\nprint(msg.count)  # 3\n</code></pre>"},{"location":"learn/inline/#2-parallel-execution","title":"2. Parallel Execution (<code>[...]</code>)","text":"<p>Execute modules concurrently:</p> <pre><code>\"[module1, module2, module3]\"\n</code></pre> <p>Important: Parallel modules receive the same message and should modify it directly. Return values from parallel modules are ignored - use side effects to update the message.</p> <pre><code>import msgflux as mf\n\ndef setup(msg):\n    msg.set(\"input\", 100)\n    return msg\n\ndef compute_double(msg):\n    # Modifies message directly\n    msg.set(\"results.doubled\", msg.input * 2)\n    return msg\n\ndef compute_square(msg):\n    # Modifies message directly\n    msg.set(\"results.squared\", msg.input ** 2)\n    return msg\n\ndef combine(msg):\n    # Access results from parallel modifications\n    total = msg.results.doubled + msg.results.squared\n    msg.final_result = total\n    return msg\n\nmodules = {\n    \"setup\": setup,\n    \"compute_double\": compute_double,\n    \"compute_square\": compute_square,\n    \"combine\": combine\n}\n\nmsg = mf.dotdict()\nmf.inline(\"setup -&gt; [compute_double, compute_square] -&gt; combine\", modules, msg)\nprint(msg.final_result)  # 10200 (200 + 10000)\n</code></pre>"},{"location":"learn/inline/#3-conditional-execution-condition-true-false","title":"3. Conditional Execution (<code>{condition ? true, false}</code>)","text":"<p>Branch based on conditions:</p> <pre><code>\"{condition ? true_module, false_module}\"\n\"{condition ? only_if_true}\"  # false branch is optional\n</code></pre> <p>Supported Operators:</p> Operator Description Example <code>==</code> Equal <code>status == \"active\"</code> <code>!=</code> Not equal <code>type != \"guest\"</code> <code>&gt;</code> Greater than <code>age &gt; 18</code> <code>&lt;</code> Less than <code>count &lt; 10</code> <code>&gt;=</code> Greater or equal <code>score &gt;= 80</code> <code>&lt;=</code> Less or equal <code>level &lt;= 5</code> <code>is None</code> Check if None <code>user.name is None</code> <code>is not None</code> Check if not None <code>data is not None</code> <p>Logical Operators:</p> Operator Description Example <code>&amp;</code> AND <code>age &gt;= 18 &amp; verified == true</code> <code>\\|\\|</code> OR <code>is_premium == true \\|\\| credits &gt; 100</code> <code>!</code> NOT <code>!(banned == true)</code> <pre><code>import msgflux as mf\n\ndef adult_flow(msg):\n    msg.access = \"granted\"\n    msg.content = \"adult content\"\n    return msg\n\ndef child_flow(msg):\n    msg.access = \"restricted\"\n    msg.content = \"family-friendly content\"\n    return msg\n\nmodules = {\n    \"adult_flow\": adult_flow,\n    \"child_flow\": child_flow\n}\n\n# Age check\nmsg = mf.dotdict()\nmsg.set(\"user.age\", 25)\n\nmf.inline(\"{user.age &gt;= 18 ? adult_flow, child_flow}\", modules, msg)\nprint(msg.content)  # \"adult content\"\n\n# Try with younger user\nmsg2 = mf.dotdict()\nmsg2.set(\"user.age\", 15)\n\nmf.inline(\"{user.age &gt;= 18 ? adult_flow, child_flow}\", modules, msg2)\nprint(msg2.content)  # \"family-friendly content\"\n</code></pre>"},{"location":"learn/inline/#4-while-loops-condition-actions","title":"4. While Loops (<code>@{condition}: actions;</code>)","text":"<p>Execute actions repeatedly while condition is true:</p> <pre><code>\"@{condition}: module1 -&gt; module2;\"\n</code></pre> <p>Safety: Loops have a maximum iteration limit (default: 1000) to prevent infinite loops.</p> <pre><code>import msgflux as mf\n\ndef increment(msg):\n    msg.counter = msg.get(\"counter\", 0) + 1\n    return msg\n\nmodules = {\"increment\": increment}\n\nmsg = mf.dotdict()\nmsg.counter = 0\n\n# Loop while counter &lt; 5\nmf.inline(\"@{counter &lt; 5}: increment;\", modules, msg)\nprint(msg.counter)  # 5\n</code></pre>"},{"location":"learn/inline/#complex-examples","title":"Complex Examples","text":""},{"location":"learn/inline/#combined-sequential-and-parallel","title":"Combined Sequential and Parallel","text":"<pre><code>import msgflux as mf\n\ndef prepare(msg):\n    msg.status = \"ready\"\n    msg.data = [1, 2, 3, 4, 5]\n    return msg\n\ndef filter_even(msg):\n    even = [x for x in msg.data if x % 2 == 0]\n    msg.set(\"filtered.even\", even)\n    return msg\n\ndef filter_odd(msg):\n    odd = [x for x in msg.data if x % 2 != 0]\n    msg.set(\"filtered.odd\", odd)\n    return msg\n\ndef sum_all(msg):\n    total_even = sum(msg.filtered.even)\n    total_odd = sum(msg.filtered.odd)\n    msg.results = {\n        \"even_sum\": total_even,\n        \"odd_sum\": total_odd,\n        \"total\": total_even + total_odd\n    }\n    return msg\n\nmodules = {\n    \"prepare\": prepare,\n    \"filter_even\": filter_even,\n    \"filter_odd\": filter_odd,\n    \"sum_all\": sum_all\n}\n\nmsg = mf.dotdict()\nmf.inline(\"prepare -&gt; [filter_even, filter_odd] -&gt; sum_all\", modules, msg)\n\nprint(msg.results)\n# {'even_sum': 6, 'odd_sum': 9, 'total': 15}\n</code></pre>"},{"location":"learn/inline/#multi-condition-logic","title":"Multi-Condition Logic","text":"<pre><code>import msgflux as mf\n\ndef premium_access(msg):\n    msg.features = [\"basic\", \"advanced\", \"premium\", \"priority_support\"]\n    return msg\n\ndef basic_access(msg):\n    msg.features = [\"basic\"]\n    return msg\n\ndef denied_access(msg):\n    msg.features = []\n    msg.reason = \"account suspended or inactive\"\n    return msg\n\nmodules = {\n    \"premium_access\": premium_access,\n    \"basic_access\": basic_access,\n    \"denied_access\": denied_access\n}\n\n# Complex condition with AND and OR\nmsg = mf.dotdict()\nmsg.set(\"user.is_premium\", True)\nmsg.set(\"user.is_active\", True)\nmsg.set(\"user.is_suspended\", False)\n\nworkflow = \"{user.is_active == true &amp; !user.is_suspended == true ? \" \\\n           \"{user.is_premium == true ? premium_access, basic_access}, \" \\\n           \"denied_access}\"\n\nmf.inline(workflow, modules, msg)\nprint(msg.features)  # ['basic', 'advanced', 'premium', 'priority_support']\n</code></pre>"},{"location":"learn/inline/#loop-with-parallel-processing","title":"Loop with Parallel Processing","text":"<pre><code>import msgflux as mf\nimport asyncio\n\nasync def init_batch(msg):\n    msg.batch_count = 0\n    msg.processed_items = []\n    return msg\n\nasync def process_a(msg):\n    msg.set(\"temp.result_a\", f\"A-{msg.batch_count}\")\n    return msg\n\nasync def process_b(msg):\n    msg.set(\"temp.result_b\", f\"B-{msg.batch_count}\")\n    return msg\n\nasync def collect_results(msg):\n    msg.processed_items.append({\n        \"a\": msg.temp.result_a,\n        \"b\": msg.temp.result_b\n    })\n    msg.batch_count += 1\n    return msg\n\nmodules = {\n    \"init_batch\": init_batch,\n    \"process_a\": process_a,\n    \"process_b\": process_b,\n    \"collect_results\": collect_results\n}\n\nmsg = mf.dotdict()\n\n# Loop with parallel processing\nresult = await mf.ainline(\n    \"init_batch -&gt; @{batch_count &lt; 3}: [process_a, process_b] -&gt; collect_results;\",\n    modules,\n    msg\n)\n\nprint(result.processed_items)\n# [\n#   {'a': 'A-0', 'b': 'B-0'},\n#   {'a': 'A-1', 'b': 'B-1'},\n#   {'a': 'A-2', 'b': 'B-2'}\n# ]\n</code></pre>"},{"location":"learn/inline/#none-checks","title":"None Checks","text":"<pre><code>import msgflux as mf\n\ndef request_name(msg):\n    msg.prompt = \"Please enter your name\"\n    return msg\n\ndef greet_user(msg):\n    msg.greeting = f\"Hello, {msg.user.name}!\"\n    return msg\n\ndef validate(msg):\n    if msg.get(\"user.name\"):\n        msg.name_provided = True\n    return msg\n\nmodules = {\n    \"request_name\": request_name,\n    \"greet_user\": greet_user,\n    \"validate\": validate\n}\n\n# Case 1: Name is None\nmsg1 = mf.dotdict()\nmsg1.set(\"user.name\", None)\n\nmf.inline(\"{user.name is None ? request_name, greet_user}\", modules, msg1)\nprint(msg1.prompt)  # \"Please enter your name\"\n\n# Case 2: Name is provided\nmsg2 = mf.dotdict()\nmsg2.set(\"user.name\", \"Alice\")\n\nmf.inline(\"{user.name is None ? request_name, greet_user}\", modules, msg2)\nprint(msg2.greeting)  # \"Hello, Alice!\"\n\n# Case 3: Check not None with validation\nmsg3 = mf.dotdict()\nmsg3.set(\"user.name\", \"Bob\")\n\nmf.inline(\"{user.name is not None ? validate}\", modules, msg3)\nprint(msg3.name_provided)  # True\n</code></pre>"},{"location":"learn/inline/#async-support","title":"Async Support","text":"<p>Use <code>ainline</code> for asynchronous workflows:</p> <pre><code>import msgflux as mf\nimport asyncio\n\nasync def async_fetch(msg):\n    await asyncio.sleep(0.1)  # Simulate async operation\n    msg.data = \"fetched\"\n    return msg\n\nasync def async_process(msg):\n    await asyncio.sleep(0.1)\n    msg.processed = f\"{msg.data}_processed\"\n    return msg\n\nmodules = {\n    \"fetch\": async_fetch,\n    \"process\": async_process\n}\n\nmsg = mf.dotdict()\n\n# Use ainline for async workflows\nresult = await mf.ainline(\"fetch -&gt; process\", modules, msg)\nprint(result.processed)  # \"fetched_processed\"\n</code></pre>"},{"location":"learn/inline/#best-practices","title":"Best Practices","text":""},{"location":"learn/inline/#1-keep-modules-pure","title":"1. Keep Modules Pure","text":"<pre><code># Good - Pure function, returns modified message\ndef process(msg):\n    msg.result = msg.value * 2\n    return msg\n\n# Avoid - Side effects\ndef process_bad(msg):\n    global_var = msg.value  # Don't use global state\n    write_to_file(msg)      # Don't perform I/O\n    return msg\n</code></pre>"},{"location":"learn/inline/#2-use-descriptive-module-names","title":"2. Use Descriptive Module Names","text":"<pre><code># Good - Clear, descriptive names\nmodules = {\n    \"validate_input\": validate_fn,\n    \"fetch_user_data\": fetch_fn,\n    \"send_notification\": notify_fn\n}\n\n# Avoid - Vague names\nmodules = {\n    \"a\": fn1,\n    \"do_stuff\": fn2,\n    \"handler\": fn3\n}\n</code></pre>"},{"location":"learn/inline/#3-handle-parallel-execution-correctly","title":"3. Handle Parallel Execution Correctly","text":"<pre><code># Good - Modify message directly (return values are ignored)\ndef parallel_task(msg):\n    value = msg.get(\"input_value\")\n    result = compute(value)\n    msg.set(\"results.task_output\", result)  # Store in message\n    return msg\n\n# Bad - Returning dict doesn't work (return values are ignored!)\ndef parallel_task_bad(msg):\n    value = msg.get(\"input_value\")\n    result = compute(value)\n    return {\"output\": result}  # This is IGNORED in parallel execution!\n</code></pre>"},{"location":"learn/inline/#4-design-for-readability","title":"4. Design for Readability","text":"<pre><code># Good - Workflow is easy to understand\nworkflow = \"validate -&gt; fetch_data -&gt; [process_a, process_b] -&gt; combine -&gt; output\"\n\n# Acceptable but harder to read\nworkflow = \"{valid ? {premium ? [a,b,c], [d,e]}, fail}\"\n\n# Better - Break complex logic into steps\nworkflow = \"validate -&gt; {premium ? premium_flow, basic_flow} -&gt; output\"\n</code></pre>"},{"location":"learn/inline/#5-set-iteration-limits-for-loops","title":"5. Set Iteration Limits for Loops","text":"<pre><code># Good - Bounded loops\nworkflow = \"@{counter &lt; 10}: process;\"\n\n# Risky - Could run indefinitely\nworkflow = \"@{status != 'done'}: process;\"  # Make sure 'done' is eventually set\n</code></pre>"},{"location":"learn/inline/#integration-with-modules","title":"Integration with Modules","text":""},{"location":"learn/inline/#using-with-nnmodule","title":"Using with nn.Module","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass DataProcessor(nn.Module):\n    def forward(self, msg):\n        msg.processed = True\n        return msg\n\nclass Validator(nn.Module):\n    def forward(self, msg):\n        msg.valid = msg.get(\"data\") is not None\n        return msg\n\n# Use modules in inline workflow\nprocessor = DataProcessor()\nvalidator = Validator()\n\nmodules = {\n    \"validate\": validator,\n    \"process\": processor\n}\n\nmsg = mf.dotdict({\"data\": [1, 2, 3]})\nmf.inline(\"validate -&gt; {valid ? process}\", modules, msg)\n</code></pre>"},{"location":"learn/inline/#dynamic-workflow-selection","title":"Dynamic Workflow Selection","text":"<pre><code>import msgflux as mf\n\ndef get_workflow(user_type):\n    \"\"\"Select workflow based on user type.\"\"\"\n    workflows = {\n        \"admin\": \"validate -&gt; process_admin -&gt; audit -&gt; notify\",\n        \"user\": \"validate -&gt; process_user -&gt; notify\",\n        \"guest\": \"rate_limit -&gt; validate -&gt; {allowed ? process_guest}\"\n    }\n    return workflows.get(user_type, \"reject\")\n\n# Use dynamically selected workflow\nuser_type = \"admin\"\nworkflow = get_workflow(user_type)\n\nmodules = {\n    \"validate\": validate_fn,\n    \"process_admin\": admin_fn,\n    \"process_user\": user_fn,\n    \"process_guest\": guest_fn,\n    \"audit\": audit_fn,\n    \"notify\": notify_fn,\n    \"rate_limit\": rate_limit_fn,\n    \"reject\": reject_fn\n}\n\nmsg = mf.dotdict({\"user_type\": user_type})\nmf.inline(workflow, modules, msg)\n</code></pre>"},{"location":"learn/inline/#syntax-summary","title":"Syntax Summary","text":"Feature Syntax Example Sequential <code>-&gt;</code> <code>\"a -&gt; b -&gt; c\"</code> Parallel <code>[...]</code> <code>\"[a, b, c]\"</code> Conditional <code>{cond ? t, f}</code> <code>\"{x &gt; 5 ? yes, no}\"</code> While Loop <code>@{cond}: actions;</code> <code>\"@{count &lt; 10}: inc;\"</code> AND <code>&amp;</code> <code>\"{a &amp; b ? yes}\"</code> OR <code>\\|\\|</code> <code>\"{a \\|\\| b ? yes}\"</code> NOT <code>!</code> <code>\"{!a ? yes}\"</code> Is None <code>is None</code> <code>\"{x is None ? ask}\"</code> Not None <code>is not None</code> <code>\"{x is not None ? use}\"</code>"},{"location":"learn/installation/","title":"Installation","text":"<p>This guide covers how to install msgFlux using different package managers.</p>"},{"location":"learn/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.10+</li> <li>Operating System: Linux, macOS or Windows</li> </ul>"},{"location":"learn/installation/#quick-installation","title":"Quick Installation","text":"uv (recommended)pip <p>uv is a fast Python package installer and resolver, significantly faster than pip.</p> <p>Install uv:</p> Linux/macOSWindowsWith pip <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code>powershell -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <pre><code>pip install uv\n</code></pre> <p>Install msgFlux:</p> <pre><code># Basic installation\nuv pip install msgflux\n\n# With OpenAI support\nuv pip install msgflux[openai]\n\n# With multiple providers\nuv pip install \"msgflux[openai,google,anthropic]\"\n</code></pre> <p>Standard installation using pip:</p> <pre><code># Basic installation\npip install msgflux\n\n# With OpenAI support\npip install msgflux[openai]\n\n# With multiple providers\npip install \"msgflux[openai,google,anthropic]\"\n</code></pre>"},{"location":"learn/installation/#provider-support","title":"Provider Support","text":"<p>Install extras for specific AI providers:</p> uvpip <pre><code># OpenAI (GPT, DALL-E, Whisper, TTS)\nuv pip install msgflux[openai]\n\n# Google (Gemini)\nuv pip install msgflux[google]\n\n# Anthropic (Claude)\nuv pip install msgflux[anthropic]\n\n# Groq\nuv pip install msgflux[groq]\n\n# Together AI\nuv pip install msgflux[together]\n\n# Ollama (local models)\nuv pip install msgflux[ollama]\n\n# Multiple providers\nuv pip install \"msgflux[openai,google,anthropic]\"\n</code></pre> <pre><code># OpenAI (GPT, DALL-E, Whisper, TTS)\npip install msgflux[openai]\n\n# Google (Gemini)\npip install msgflux[google]\n\n# Anthropic (Claude)\npip install msgflux[anthropic]\n\n# Groq\npip install msgflux[groq]\n\n# Together AI\npip install msgflux[together]\n\n# Ollama (local models)\npip install msgflux[ollama]\n\n# Multiple providers\npip install \"msgflux[openai,google,anthropic]\"\n</code></pre>"},{"location":"learn/installation/#installation-from-github","title":"Installation from GitHub","text":"<p>Install directly from source without cloning:</p> uvpip <pre><code># Latest from main branch\nuv pip install git+https://github.com/msgflux/msgflux.git\n\n# With extras\nuv pip install \"git+https://github.com/msgflux/msgflux.git#egg=msgflux[openai]\"\n\n# Specific branch\nuv pip install git+https://github.com/msgflux/msgflux.git@branch-name\n\n# Specific tag/release\nuv pip install git+https://github.com/msgflux/msgflux.git@v0.1.0\n\n# Specific commit\nuv pip install git+https://github.com/msgflux/msgflux.git@abc123def\n\n# From a fork\nuv pip install git+https://github.com/your-username/msgflux.git@feature-branch\n</code></pre> <pre><code># Latest from main branch\npip install git+https://github.com/msgflux/msgflux.git\n\n# With extras\npip install \"git+https://github.com/msgflux/msgflux.git#egg=msgflux[openai]\"\n\n# Specific branch\npip install git+https://github.com/msgflux/msgflux.git@branch-name\n\n# Specific tag/release\npip install git+https://github.com/msgflux/msgflux.git@v0.1.0\n\n# Specific commit\npip install git+https://github.com/msgflux/msgflux.git@abc123def\n\n# From a fork\npip install git+https://github.com/your-username/msgflux.git@feature-branch\n</code></pre>"},{"location":"learn/installation/#clone-and-install-development","title":"Clone and Install (Development)","text":"<p>For local development with editable installation:</p> uvpip <pre><code>git clone https://github.com/msgflux/msgflux.git\ncd msgflux\nuv pip install -e \".[dev]\"\n</code></pre> <pre><code>git clone https://github.com/msgflux/msgflux.git\ncd msgflux\npip install -e \".[dev]\"\n</code></pre>"},{"location":"learn/installation/#configuration","title":"Configuration","text":""},{"location":"learn/installation/#set-api-keys","title":"Set API Keys","text":"Environment Variables.env FileIn Code <pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport GOOGLE_API_KEY=\"...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\n</code></pre> <p>Create a <code>.env</code> file in your project:</p> <pre><code>OPENAI_API_KEY=sk-...\nGOOGLE_API_KEY=...\nANTHROPIC_API_KEY=sk-ant-...\n</code></pre> <pre><code>import msgflux as mf\n\nmf.set_envs(\n    OPENAI_API_KEY=\"sk-...\",\n    GOOGLE_API_KEY=\"...\",\n    ANTHROPIC_API_KEY=\"sk-ant-...\"\n)\n</code></pre>"},{"location":"learn/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import msgflux as mf\n\n# Check version\nprint(mf.__version__)\n\n# List available providers\nproviders = mf.Model.providers()\nprint(providers['chat_completion'])\n</code></pre>"},{"location":"learn/installation/#quick-example","title":"Quick Example","text":"<pre><code>import msgflux as mf\n\n# Set API key\nmf.set_envs(OPENAI_API_KEY=\"sk-...\")\n\n# Create model\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Generate response\nresponse = model(messages=[\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n])\n\nprint(response.consume())\n</code></pre>"},{"location":"learn/installation/#upgrade","title":"Upgrade","text":"uvpip <pre><code>uv pip install --upgrade msgflux\nuv pip install --upgrade \"msgflux[openai,google]\"\n</code></pre> <pre><code>pip install --upgrade msgflux\npip install --upgrade \"msgflux[openai,google]\"\n</code></pre>"},{"location":"learn/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"learn/installation/#import-error","title":"Import Error","text":"<pre><code># Check installation\npip list | grep msgflux\n</code></pre>"},{"location":"learn/installation/#provider-not-available","title":"Provider Not Available","text":"<pre><code># Install provider extra\nuv pip install msgflux[openai]\n</code></pre>"},{"location":"learn/installation/#api-key-not-found","title":"API Key Not Found","text":"<pre><code># Set environment variable\nexport OPENAI_API_KEY=\"sk-...\"\n</code></pre>"},{"location":"learn/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Get started with examples</li> <li>Models - Learn about model types</li> <li>Chat Completion - Build conversational AI</li> </ul>"},{"location":"learn/message/","title":"Message","text":"<p>The <code>Message</code> class is a structured data container designed to facilitate information flow in computational graphs created with <code>nn</code> modules. Inspired by <code>torch.Tensor</code> and built on top of <code>dotdict</code>, it provides an organized way to pass data between different components of a system.</p>"},{"location":"learn/message/#overview","title":"Overview","text":"<p><code>Message</code> implements a permission-based system where each Module can have specific read and write access to predefined fields. This creates a clean separation of concerns and makes data flow explicit and traceable.</p>"},{"location":"learn/message/#key-features","title":"Key Features","text":"<ul> <li>Structured Fields: Predefined fields for different data types (text, audio, images, videos)</li> <li>Flexible Access: Set and get data using dot notation or string paths</li> <li>Automatic Metadata: Auto-generated execution IDs and user tracking</li> <li>Module Integration: Built-in support for <code>nn.Module</code> components</li> <li>Type Safety: Organized structure for multimodal data</li> </ul>"},{"location":"learn/message/#quick-start","title":"Quick Start","text":""},{"location":"learn/message/#basic-usage","title":"Basic Usage","text":"<pre><code>import msgflux as mf\n\n# Create empty message\nmsg = mf.Message()\n\n# Create with content\nmsg = mf.Message(content=\"Hello, world!\")\n\n# Create with metadata\nmsg = mf.Message(\n    content=\"Hello!\",\n    user_id=\"user_123\",\n    user_name=\"Alice\",\n    chat_id=\"chat_456\"\n)\n\nprint(msg)\n</code></pre>"},{"location":"learn/message/#setting-data","title":"Setting Data","text":"<pre><code>import msgflux as mf\n\nmsg = mf.Message()\n\n# Using dot notation\nmsg.content = \"Hello, world!\"\nmsg.texts.input = \"User question\"\nmsg.texts.output = \"AI response\"\n\n# Using set method\nmsg.set(\"context.history\", [\"previous message\"])\nmsg.set(\"extra.custom_field\", {\"key\": \"value\"})\n\nprint(msg.content)  # \"Hello, world!\"\nprint(msg.texts.input)  # \"User question\"\n</code></pre>"},{"location":"learn/message/#getting-data","title":"Getting Data","text":"<pre><code>import msgflux as mf\n\nmsg = mf.Message(content=\"Hello!\")\nmsg.texts.input = \"Question\"\n\n# Using dot notation\ncontent = msg.content\ntext_input = msg.texts.input\n\n# Using get method\ncontent = msg.get(\"content\")\ntext_input = msg.get(\"texts.input\")\n\nprint(content)  # \"Hello!\"\nprint(text_input)  # \"Question\"\n</code></pre>"},{"location":"learn/message/#message-structure","title":"Message Structure","text":""},{"location":"learn/message/#default-fields","title":"Default Fields","text":"<p>Every <code>Message</code> instance has these predefined fields:</p> <pre><code>import msgflux as mf\n\nmsg = mf.Message()\n\n# Available fields\nmsg.metadata    # Execution ID, user info\nmsg.content     # Main content (text, dict, etc.)\nmsg.texts       # Text-based data\nmsg.context     # Contextual information\nmsg.audios      # Audio data\nmsg.images      # Image data\nmsg.videos      # Video data\nmsg.extra       # Custom/extra data\nmsg.outputs     # Module outputs\nmsg.response    # Final response data\n</code></pre>"},{"location":"learn/message/#metadata","title":"Metadata","text":"<p>Metadata is automatically populated:</p> <pre><code>import msgflux as mf\n\nmsg = mf.Message(\n    user_id=\"user_123\",\n    user_name=\"Alice\",\n    chat_id=\"chat_456\"\n)\n\nprint(msg.metadata)\n# {\n#     'execution_id': 'a1b2c3d4-e5f6-7890-abcd-ef1234567890',\n#     'user_id': 'user_123',\n#     'user_name': 'Alice',\n#     'chat_id': 'chat_456'\n# }\n\n# Execution ID is auto-generated\nprint(msg.metadata.execution_id)\n</code></pre>"},{"location":"learn/message/#working-with-fields","title":"Working with Fields","text":""},{"location":"learn/message/#text-fields","title":"Text Fields","text":"<p>Store text-based data:</p> <pre><code>import msgflux as mf\n\nmsg = mf.Message()\n\n# Store different text types\nmsg.texts.input = \"User input\"\nmsg.texts.output = \"Model output\"\nmsg.texts.system = \"System message\"\nmsg.texts.history = [\"msg1\", \"msg2\"]\n\nprint(msg.texts.input)  # \"User input\"\n</code></pre>"},{"location":"learn/message/#context-fields","title":"Context Fields","text":"<p>Store contextual information:</p> <pre><code>import msgflux as mf\n\nmsg = mf.Message()\n\n# Store context\nmsg.context.history = [\n    {\"role\": \"user\", \"content\": \"Hi\"},\n    {\"role\": \"assistant\", \"content\": \"Hello!\"}\n]\nmsg.context.settings = {\"temperature\": 0.7}\nmsg.context.metadata = {\"source\": \"web\"}\n\nprint(msg.context.history)\n</code></pre>"},{"location":"learn/message/#media-fields","title":"Media Fields","text":"<p>Store multimedia data:</p> <pre><code>import msgflux as mf\n\nmsg = mf.Message()\n\n# Images\nmsg.images.input = \"path/to/image.jpg\"\nmsg.images.processed = \"path/to/processed.jpg\"\n\n# Audios\nmsg.audios.recording = \"path/to/audio.mp3\"\nmsg.audios.transcription = \"Transcribed text\"\n\n# Videos\nmsg.videos.input = \"path/to/video.mp4\"\nmsg.videos.frames = [\"frame1.jpg\", \"frame2.jpg\"]\n\nprint(msg.images.input)\n</code></pre>"},{"location":"learn/message/#extra-fields","title":"Extra Fields","text":"<p>Store custom data:</p> <pre><code>import msgflux as mf\n\nmsg = mf.Message()\n\n# Store any custom data\nmsg.extra.custom_field = \"custom value\"\nmsg.extra.metrics = {\"accuracy\": 0.95}\nmsg.extra.flags = {\"is_verified\": True}\n\nprint(msg.extra.custom_field)\n</code></pre>"},{"location":"learn/message/#nested-access","title":"Nested Access","text":"<p>Access nested data using dot notation or paths:</p> <pre><code>import msgflux as mf\n\nmsg = mf.Message()\n\n# Set nested data\nmsg.set(\"context.user.preferences.language\", \"en\")\nmsg.set(\"extra.settings.model.temperature\", 0.7)\n\n# Get nested data\nlanguage = msg.get(\"context.user.preferences.language\")\ntemp = msg.get(\"extra.settings.model.temperature\")\n\nprint(language)  # \"en\"\nprint(temp)  # 0.7\n\n# Using dot notation\nlanguage = msg.context.user.preferences.language\ntemp = msg.extra.settings.model.temperature\n</code></pre>"},{"location":"learn/message/#responses","title":"Responses","text":""},{"location":"learn/message/#setting-responses","title":"Setting Responses","text":"<pre><code>import msgflux as mf\n\nmsg = mf.Message()\n\n# Set response\nmsg.response.agent_output = \"AI generated response\"\nmsg.response.confidence = 0.95\nmsg.response.metadata = {\"tokens\": 150}\n\nprint(msg.response.agent_output)\n</code></pre>"},{"location":"learn/message/#getting-response","title":"Getting Response","text":"<pre><code>import msgflux as mf\n\nmsg = mf.Message()\nmsg.response.result = \"Final answer\"\n\n# Get first response value\nresponse = msg.get_response()\nprint(response)  # \"Final answer\"\n\n# Or access directly\nresponse = msg.response.result\nprint(response)  # \"Final answer\"\n</code></pre>"},{"location":"learn/message/#integration-with-modules","title":"Integration with Modules","text":"<p><code>Message</code> is designed to work seamlessly with <code>nn.Module</code>:</p> <pre><code>import msgflux as mf\n\nclass MyAgent(mf.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n    def forward(self, msg: mf.Message):\n        # Read from message\n        user_input = msg.texts.input\n\n        # Process\n        response = self.model(messages=[\n            {\"role\": \"user\", \"content\": user_input}\n        ])\n\n        # Write to message\n        msg.texts.output = response.consume()\n        msg.response.agent = msg.texts.output\n\n        return msg\n\n# Usage\nagent = MyAgent()\nmsg = mf.Message()\nmsg.texts.input = \"Hello, AI!\"\n\n# Process message\nresult = agent(msg)\nprint(result.texts.output)\n</code></pre>"},{"location":"learn/message/#common-patterns","title":"Common Patterns","text":""},{"location":"learn/message/#conversation-pipeline","title":"Conversation Pipeline","text":"<pre><code>import msgflux as mf\n\nclass ConversationPipeline(mf.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n    def forward(self, msg: mf.Message):\n        # Get conversation history\n        history = msg.context.history or []\n\n        # Add current input\n        history.append({\n            \"role\": \"user\",\n            \"content\": msg.texts.input\n        })\n\n        # Generate response\n        response = self.model(messages=history)\n        ai_response = response.consume()\n\n        # Update message\n        msg.texts.output = ai_response\n        history.append({\n            \"role\": \"assistant\",\n            \"content\": ai_response\n        })\n\n        # Store updated history\n        msg.context.history = history\n        msg.response.conversation = ai_response\n\n        return msg\n\n# Usage\npipeline = ConversationPipeline()\nmsg = mf.Message(\n    texts={\"input\": \"What is AI?\"},\n    context={\"history\": []}\n)\n\nresult = pipeline(msg)\nprint(result.texts.output)\nprint(result.context.history)\n</code></pre>"},{"location":"learn/message/#multi-stage-processing","title":"Multi-Stage Processing","text":"<pre><code>import msgflux as mf\n\nclass Preprocessor(mf.nn.Module):\n    def forward(self, msg: mf.Message):\n        # Clean input\n        text = msg.texts.input.strip().lower()\n        msg.texts.processed = text\n        return msg\n\nclass Analyzer(mf.nn.Module):\n    def forward(self, msg: mf.Message):\n        # Analyze processed text\n        text = msg.texts.processed\n        msg.outputs.analysis = {\n            \"length\": len(text),\n            \"words\": len(text.split())\n        }\n        return msg\n\nclass Responder(mf.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n    def forward(self, msg: mf.Message):\n        # Generate response\n        text = msg.texts.processed\n        response = self.model(messages=[\n            {\"role\": \"user\", \"content\": text}\n        ])\n        msg.response.final = response.consume()\n        return msg\n\n# Chain modules\npreprocessor = Preprocessor()\nanalyzer = Analyzer()\nresponder = Responder()\n\nmsg = mf.Message()\nmsg.texts.input = \"  HELLO WORLD  \"\n\n# Process through pipeline\nmsg = preprocessor(msg)\nmsg = analyzer(msg)\nmsg = responder(msg)\n\nprint(msg.texts.processed)  # \"hello world\"\nprint(msg.outputs.analysis)  # {'length': 11, 'words': 2}\nprint(msg.response.final)\n</code></pre>"},{"location":"learn/message/#rag-system-with-message","title":"RAG System with Message","text":"<pre><code>import msgflux as mf\n\nclass RAGSystem(mf.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n        self.model = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n    def forward(self, msg: mf.Message):\n        query = msg.texts.input\n        documents = msg.context.documents or []\n\n        # Embed query\n        query_emb = self.embedder(query).consume()\n        msg.extra.query_embedding = query_emb\n\n        # Simple retrieval (in practice, use vector DB)\n        relevant_docs = documents[:3]  # Top 3\n        msg.context.retrieved = relevant_docs\n\n        # Build context\n        context = \"\\n\\n\".join(relevant_docs)\n        prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n\n        # Generate response\n        response = self.model(messages=[\n            {\"role\": \"user\", \"content\": prompt}\n        ])\n\n        msg.texts.output = response.consume()\n        msg.response.rag = msg.texts.output\n\n        return msg\n\n# Usage\nrag = RAGSystem()\nmsg = mf.Message(\n    texts={\"input\": \"What is machine learning?\"},\n    context={\"documents\": [\n        \"ML is a subset of AI...\",\n        \"It involves training models...\",\n        \"Common techniques include...\"\n    ]}\n)\n\nresult = rag(msg)\nprint(result.texts.output)\nprint(result.context.retrieved)\n</code></pre>"},{"location":"learn/message/#multimodal-processing","title":"Multimodal Processing","text":"<pre><code>import msgflux as mf\n\nclass MultimodalAgent(mf.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.vision_model = mf.Model.chat_completion(\"openai/gpt-4o\")\n        self.tts_model = mf.Model.text_to_speech(\"openai/tts-1\")\n\n    def forward(self, msg: mf.Message):\n        # Process image + text\n        image_path = msg.images.input\n        question = msg.texts.input\n\n        # Vision analysis\n        response = self.vision_model(messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": question},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_path}}\n                ]\n            }\n        ])\n\n        text_response = response.consume()\n        msg.texts.output = text_response\n\n        # Generate audio\n        audio_response = self.tts_model(text_response)\n        msg.audios.output = audio_response.consume()\n\n        msg.response.multimodal = {\n            \"text\": text_response,\n            \"audio\": msg.audios.output\n        }\n\n        return msg\n\n# Usage\nagent = MultimodalAgent()\nmsg = mf.Message(\n    texts={\"input\": \"What's in this image?\"},\n    images={\"input\": \"path/to/image.jpg\"}\n)\n\nresult = agent(msg)\nprint(result.texts.output)\nprint(result.audios.output)\n</code></pre>"},{"location":"learn/message/#best-practices","title":"Best Practices","text":""},{"location":"learn/message/#1-use-appropriate-fields","title":"1. Use Appropriate Fields","text":"<pre><code>import msgflux as mf\n\n# Good - Use semantic field names\nmsg = mf.Message()\nmsg.texts.user_question = \"What is AI?\"\nmsg.texts.ai_answer = \"AI is...\"\nmsg.context.conversation_id = \"conv_123\"\n\n# Less clear - Everything in extra\nmsg.extra.data1 = \"What is AI?\"\nmsg.extra.data2 = \"AI is...\"\nmsg.extra.data3 = \"conv_123\"\n</code></pre>"},{"location":"learn/message/#2-preserve-message-history","title":"2. Preserve Message History","text":"<pre><code>import msgflux as mf\n\nclass Agent(mf.nn.Module):\n    def forward(self, msg: mf.Message):\n        # Good - Preserve original input\n        original_input = msg.texts.input\n        msg.texts.original = original_input\n\n        # Process\n        processed = original_input.lower()\n        msg.texts.processed = processed\n\n        return msg\n</code></pre>"},{"location":"learn/message/#3-use-metadata-for-tracking","title":"3. Use Metadata for Tracking","text":"<pre><code>import msgflux as mf\n\nmsg = mf.Message(\n    user_id=\"user_123\",\n    user_name=\"Alice\",\n    chat_id=\"chat_456\"\n)\n\n# Later in pipeline\ndef track_message(msg: mf.Message):\n    print(f\"Processing message {msg.metadata.execution_id}\")\n    print(f\"User: {msg.metadata.user_name}\")\n    print(f\"Chat: {msg.metadata.chat_id}\")\n</code></pre>"},{"location":"learn/message/#4-document-field-usage","title":"4. Document Field Usage","text":"<pre><code>import msgflux as mf\n\nclass DocumentedAgent(mf.nn.Module):\n    \"\"\"\n    Agent that processes text input.\n\n    Message fields used:\n    - Input: msg.texts.input (str) - User question\n    - Output: msg.texts.output (str) - Agent response\n    - Context: msg.context.history (list) - Conversation history\n    \"\"\"\n\n    def forward(self, msg: mf.Message):\n        # Implementation\n        pass\n</code></pre>"},{"location":"learn/message/#advanced-usage","title":"Advanced Usage","text":""},{"location":"learn/message/#cloning-messages","title":"Cloning Messages","text":"<pre><code>import msgflux as mf\n\nmsg = mf.Message(content=\"Original\")\nmsg.texts.data = \"Important data\"\n\n# Create a copy (dotdict supports this)\nmsg_copy = mf.Message(**dict(msg))\nmsg_copy.texts.data = \"Modified data\"\n\nprint(msg.texts.data)  # \"Important data\"\nprint(msg_copy.texts.data)  # \"Modified data\"\n</code></pre>"},{"location":"learn/message/#conditional-processing","title":"Conditional Processing","text":"<pre><code>import msgflux as mf\n\nclass ConditionalAgent(mf.nn.Module):\n    def forward(self, msg: mf.Message):\n        # Check if field exists\n        if msg.get(\"context.history\"):\n            # Process with history\n            history = msg.context.history\n            msg.texts.mode = \"conversation\"\n        else:\n            # Process without history\n            msg.texts.mode = \"standalone\"\n\n        return msg\n</code></pre>"},{"location":"learn/message/#merging-messages","title":"Merging Messages","text":"<pre><code>import msgflux as mf\n\ndef merge_messages(msg1: mf.Message, msg2: mf.Message) -&gt; mf.Message:\n    \"\"\"Merge two messages.\"\"\"\n    merged = mf.Message()\n\n    # Combine texts\n    merged.texts.input1 = msg1.texts.input\n    merged.texts.input2 = msg2.texts.input\n\n    # Combine context\n    merged.context.from_msg1 = msg1.context\n    merged.context.from_msg2 = msg2.context\n\n    return merged\n\nmsg1 = mf.Message(texts={\"input\": \"First\"})\nmsg2 = mf.Message(texts={\"input\": \"Second\"})\nmerged = merge_messages(msg1, msg2)\n\nprint(merged.texts.input1)  # \"First\"\nprint(merged.texts.input2)  # \"Second\"\n</code></pre>"},{"location":"learn/message/#see-also","title":"See Also","text":"<ul> <li>Module - Build custom modules with Message</li> <li>Agent - Agent implementation using Message</li> <li>dotdict - Underlying data structure</li> <li>Functional - Functional operations with Message</li> </ul>"},{"location":"learn/data/databases/vector/","title":"faiss","text":"<p>vector</p>"},{"location":"learn/models/chat_completion/","title":"Chat Completion","text":"<p>The <code>chat_completion</code> model is the most versatile model type for natural language interactions. It processes messages in conversational format and supports advanced features like multimodal input/output, structured generation, and tool calling.</p> <p>All code examples use the recommended import pattern:</p> <pre><code>import msgflux as mf\n</code></pre>"},{"location":"learn/models/chat_completion/#overview","title":"Overview","text":"<p>Chat completion models are stateless - they don't maintain conversation history between calls. You must provide all context (previous messages, system prompt, etc.) in each request.</p>"},{"location":"learn/models/chat_completion/#quick-start","title":"Quick Start","text":"<pre><code>import msgflux as mf\n\n# Create model\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Simple completion\nresponse = model(messages=[{\"role\": \"user\", \"content\": \"Hello!\"}])\nprint(response.consume())  # \"Hello! How can I help you today?\"\n\n# With system prompt\nresponse = model(\n    messages=[{\"role\": \"user\", \"content\": \"What is AI?\"}],\n    system_prompt=\"You are a helpful assistant.\"\n)\nprint(response.consume())\n</code></pre>"},{"location":"learn/models/chat_completion/#model-initialization","title":"Model Initialization","text":""},{"location":"learn/models/chat_completion/#basic-parameters","title":"Basic Parameters","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\n    \"openai/gpt-4o\",\n    temperature=0.7,        # Randomness (0-2)\n    max_tokens=1000,        # Maximum output tokens\n    top_p=0.9,              # Nucleus sampling\n    enable_cache=True,      # Enable response caching\n    cache_size=128          # Cache size\n)\n</code></pre>"},{"location":"learn/models/chat_completion/#model-information","title":"Model Information","text":"<pre><code># Get model metadata\nprint(model.get_model_info())\n# {'model_id': 'gpt-4o', 'provider': 'openai'}\n\nprint(model.instance_type())\n# {'model_type': 'chat_completion'}\n\n# Serialize model state\nstate = model.serialize()\nprint(state)\n# {\n#     'msgflux_type': 'model',\n#     'provider': 'openai',\n#     'model_type': 'chat_completion',\n#     'state': {...}\n# }\n</code></pre>"},{"location":"learn/models/chat_completion/#model-profiles","title":"Model Profiles","text":"<p>Model profiles provide metadata about capabilities, pricing, and limits from models.dev:</p>"},{"location":"learn/models/chat_completion/#getting-profile-information","title":"Getting Profile Information","text":"<pre><code>import msgflux as mf\nfrom msgflux.models.profiles import get_model_profile\n\n# Get profile for a model\nprofile = get_model_profile(\"gpt-4o\", provider_id=\"openai\")\n\nif profile:\n    # Check capabilities\n    print(f\"Tool calling: {profile.capabilities.tool_call}\")\n    print(f\"Structured output: {profile.capabilities.structured_output}\")\n    print(f\"Reasoning: {profile.capabilities.reasoning}\")\n\n    # Check modalities\n    print(f\"Input: {profile.modalities.input}\")   # ['text', 'image']\n    print(f\"Output: {profile.modalities.output}\") # ['text']\n\n    # Check limits\n    print(f\"Context window: {profile.limits.context}\")  # 128000\n    print(f\"Max output: {profile.limits.output}\")       # 16384\n\n    # Check pricing\n    print(f\"Input: ${profile.cost.input_per_million}/M tokens\")\n    print(f\"Output: ${profile.cost.output_per_million}/M tokens\")\n</code></pre>"},{"location":"learn/models/chat_completion/#cost-calculation","title":"Cost Calculation","text":"<pre><code>from msgflux.models.profiles import get_model_profile\n\nprofile = get_model_profile(\"gpt-4o\", provider_id=\"openai\")\n\nif profile:\n    # Calculate cost for a request\n    cost = profile.cost.calculate(\n        input_tokens=1000,\n        output_tokens=500\n    )\n    print(f\"Estimated cost: ${cost:.4f}\")\n</code></pre>"},{"location":"learn/models/chat_completion/#profile-based-model-selection","title":"Profile-Based Model Selection","text":"<pre><code>import msgflux as mf\nfrom msgflux.models.profiles import get_model_profile\n\ndef select_model_by_budget(max_cost_per_million_output):\n    \"\"\"Select cheapest model within budget that supports tools.\"\"\"\n    models = [\n        (\"gpt-4o\", \"openai\"),\n        (\"claude-3-5-sonnet-20241022\", \"anthropic\"),\n        (\"gemini-2.0-flash-exp\", \"google\")\n    ]\n\n    for model_id, provider in models:\n        profile = get_model_profile(model_id, provider_id=provider)\n        if profile:\n            if (profile.capabilities.tool_call and\n                profile.cost.output_per_million &lt;= max_cost_per_million_output):\n                return f\"{provider}/{model_id}\"\n\n    return None\n\n# Select model under $5 per million output tokens\nmodel_path = select_model_by_budget(5.0)\nif model_path:\n    model = mf.Model.chat_completion(model_path)\n</code></pre>"},{"location":"learn/models/chat_completion/#response-caching","title":"Response Caching","text":"<p>Response caching avoids redundant API calls by caching identical requests:</p>"},{"location":"learn/models/chat_completion/#enabling-cache","title":"Enabling Cache","text":"<pre><code>import msgflux as mf\n\n# Enable cache on initialization\nmodel = mf.Model.chat_completion(\n    \"openai/gpt-4o\",\n    enable_cache=True,   # Enable caching\n    cache_size=128       # Cache up to 128 responses\n)\n\n# First call - hits API\nresponse1 = model(messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\nprint(response1.consume())\n\n# Second identical call - returns cached response (no API call)\nresponse2 = model(messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\nprint(response2.consume())\n\n# Different call - hits API again\nresponse3 = model(messages=[{\"role\": \"user\", \"content\": \"Hi\"}])\nprint(response3.consume())\n</code></pre>"},{"location":"learn/models/chat_completion/#cache-statistics","title":"Cache Statistics","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\n    \"openai/gpt-4o\",\n    enable_cache=True,\n    cache_size=128\n)\n\n# Make some calls\nmodel(messages=[{\"role\": \"user\", \"content\": \"Test 1\"}])\nmodel(messages=[{\"role\": \"user\", \"content\": \"Test 1\"}])  # Cache hit\nmodel(messages=[{\"role\": \"user\", \"content\": \"Test 2\"}])\n\n# Check cache stats\nif model._response_cache:\n    stats = model._response_cache.cache_info()\n    print(stats)\n    # {\n    #     'hits': 1,\n    #     'misses': 2,\n    #     'maxsize': 128,\n    #     'currsize': 2\n    # }\n\n    # Clear cache\n    model._response_cache.cache_clear()\n</code></pre>"},{"location":"learn/models/chat_completion/#cache-behavior","title":"Cache Behavior","text":"<p>The cache is sensitive to: - Message content - System prompt - Temperature and sampling parameters - Generation schema - Tool schemas</p> <p>Changing any of these creates a new cache entry.</p>"},{"location":"learn/models/chat_completion/#message-formats","title":"Message Formats","text":""},{"location":"learn/models/chat_completion/#simple-string","title":"Simple String","text":"<pre><code>response = model(\n    messages=\"What is Python?\",\n    system_prompt=\"You are a programming expert.\"\n)\n</code></pre>"},{"location":"learn/models/chat_completion/#chatml-format","title":"ChatML Format","text":"<pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"},\n    {\"role\": \"assistant\", \"content\": \"Hi! How can I help?\"},\n    {\"role\": \"user\", \"content\": \"Tell me a joke.\"}\n]\n\nresponse = model(messages=messages)\n</code></pre>"},{"location":"learn/models/chat_completion/#chatblock-format","title":"ChatBlock Format","text":"<pre><code>import msgflux as mf\n\n# Text only\nmessages = [\n    mf.ChatBlock.user(\"What's in this image?\")\n]\n\n# With images\nmessages = [\n    mf.ChatBlock.user(\n        \"Describe this image\",\n        media=mf.ChatBlock.image(\"https://example.com/image.jpg\")\n    )\n]\n\n# Multiple media\nmessages = [\n    mf.ChatBlock.user(\n        \"Compare these images\",\n        media=[\n            mf.ChatBlock.image(\"https://example.com/image1.jpg\"),\n            mf.ChatBlock.image(\"https://example.com/image2.jpg\")\n        ]\n    )\n]\n\nresponse = model(messages=messages)\n</code></pre>"},{"location":"learn/models/chat_completion/#streaming","title":"Streaming","text":"<p>Stream tokens as they're generated:</p>"},{"location":"learn/models/chat_completion/#basic-streaming","title":"Basic Streaming","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nresponse = model(\n    messages=[{\"role\": \"user\", \"content\": \"Count to 10\"}],\n    stream=True\n)\n\n# Consume stream\nasync for chunk in response.consume():\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"learn/models/chat_completion/#streaming-with-fastapi","title":"Streaming with FastAPI","text":"<pre><code>from fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nimport msgflux as mf\n\napp = FastAPI()\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n@app.get(\"/chat\")\nasync def chat(query: str):\n    response = model(\n        messages=[{\"role\": \"user\", \"content\": query}],\n        stream=True\n    )\n\n    return StreamingResponse(\n        response.consume(),\n        media_type=\"text/plain\"\n    )\n</code></pre>"},{"location":"learn/models/chat_completion/#async-support","title":"Async Support","text":"<p>Async version for concurrent operations:</p> <pre><code>import msgflux as mf\nimport asyncio\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nasync def get_completion(prompt):\n    response = await model.acall(messages=[{\"role\": \"user\", \"content\": prompt}])\n    return response.consume()\n\n# Run multiple completions concurrently\nasync def main():\n    results = await asyncio.gather(\n        get_completion(\"What is AI?\"),\n        get_completion(\"What is ML?\"),\n        get_completion(\"What is DL?\")\n    )\n    for result in results:\n        print(result)\n\nasyncio.run(main())\n</code></pre>"},{"location":"learn/models/chat_completion/#multimodal-inputs","title":"Multimodal Inputs","text":"<p>Modern models support multiple input modalities:</p>"},{"location":"learn/models/chat_completion/#image-understanding","title":"Image Understanding","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nmessages = [{\n    \"role\": \"user\",\n    \"content\": [\n        {\"type\": \"text\", \"text\": \"What's in this image?\"},\n        {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": \"https://example.com/image.jpg\"\n            }\n        }\n    ]\n}]\n\nresponse = model(messages=messages)\nprint(response.consume())\n</code></pre>"},{"location":"learn/models/chat_completion/#using-chatblock-helper","title":"Using ChatBlock Helper","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nmessages = [\n    mf.ChatBlock.user(\n        \"Describe this image\",\n        media=mf.ChatBlock.image(\"https://example.com/image.jpg\")\n    )\n]\n\nresponse = model(messages=messages)\nprint(response.consume())\n</code></pre>"},{"location":"learn/models/chat_completion/#base64-images","title":"Base64 Images","text":"<pre><code>import msgflux as mf\nimport base64\n\n# Read and encode image\nwith open(\"image.jpg\", \"rb\") as f:\n    image_data = base64.b64encode(f.read()).decode()\n\nmessages = [{\n    \"role\": \"user\",\n    \"content\": [\n        {\"type\": \"text\", \"text\": \"What's in this image?\"},\n        {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": f\"data:image/jpeg;base64,{image_data}\"\n            }\n        }\n    ]\n}]\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\nresponse = model(messages=messages)\n</code></pre>"},{"location":"learn/models/chat_completion/#structured-generation","title":"Structured Generation","text":"<p>Generate structured data conforming to a schema:</p>"},{"location":"learn/models/chat_completion/#basic-schema","title":"Basic Schema","text":"<pre><code>import msgflux as mf\nfrom msgspec import Struct\n\nclass CalendarEvent(Struct):\n    name: str\n    date: str\n    participants: list[str]\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nresponse = model(\n    messages=\"Alice and Bob are going to a science fair on Friday.\",\n    system_prompt=\"Extract the event information.\",\n    generation_schema=CalendarEvent\n)\n\nevent = response.consume()\nprint(event)\n# {'name': 'science fair', 'date': 'Friday', 'participants': ['Alice', 'Bob']}\n</code></pre>"},{"location":"learn/models/chat_completion/#nested-schemas","title":"Nested Schemas","text":"<pre><code>import msgflux as mf\nfrom msgspec import Struct\n\nclass Address(Struct):\n    street: str\n    city: str\n    country: str\n\nclass Person(Struct):\n    name: str\n    age: int\n    address: Address\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nresponse = model(\n    messages=\"John Doe, 30 years old, lives at 123 Main St, New York, USA.\",\n    system_prompt=\"Extract person information.\",\n    generation_schema=Person\n)\n\nperson = response.consume()\nprint(person)\n# {\n#     'name': 'John Doe',\n#     'age': 30,\n#     'address': {\n#         'street': '123 Main St',\n#         'city': 'New York',\n#         'country': 'USA'\n#     }\n# }\n</code></pre>"},{"location":"learn/models/chat_completion/#generation-schemas-for-planning","title":"Generation Schemas for Planning","text":"<pre><code>import msgflux as mf\n\n# Access built-in planning schemas\nChainOfThoughts = mf.generation.plan.ChainOfThoughts\nReAct = mf.generation.plan.ReAct\nSelfConsistency = mf.generation.plan.SelfConsistency\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Use Chain of Thoughts\nresponse = model(\n    messages=\"What is 25 * 4 + 17?\",\n    generation_schema=ChainOfThoughts\n)\n\nresult = response.consume()\nprint(result)\n</code></pre>"},{"location":"learn/models/chat_completion/#typed-parsers-xml","title":"Typed Parsers (XML)","text":"<p>Alternative to JSON for structured output using typed XML:</p>"},{"location":"learn/models/chat_completion/#setup","title":"Setup","text":"<pre><code>pip install msgflux[xml]\n</code></pre>"},{"location":"learn/models/chat_completion/#using-typed-xml","title":"Using Typed XML","text":"<pre><code>import msgflux as mf\nfrom jinja2 import Template\n\n# Get typed XML parser\ntyped_xml = mf.dsl.typed_parsers.typed_parser_registry[\"typed_xml\"]\n\n# Define instructions\ninstructions = \"\"\"Extract the event information:\nname: str\ndate: str\nparticipants: list[str]\n\"\"\"\n\n# Create system prompt\ntemplate = Template(typed_xml.template)\nsystem_prompt = template.render({\"instructions\": instructions}).strip()\n\n# Generate with XML output\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nresponse = model(\n    messages=\"Alice and Bob are going to a science fair on Friday.\",\n    system_prompt=system_prompt,\n    typed_parser=\"typed_xml\"\n)\n\n# Automatically parsed to dict\nresult = response.consume()\nprint(result)\n# {'name': 'science fair', 'date': 'Friday', 'participants': ['Alice', 'Bob']}\n</code></pre>"},{"location":"learn/models/chat_completion/#tool-calling","title":"Tool Calling","text":"<p>Models can suggest calling functions (tools) to gather information:</p>"},{"location":"learn/models/chat_completion/#defining-tools","title":"Defining Tools","text":"<pre><code>import msgflux as mf\n\n# Define tool schema\ntools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get current weather for a location.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"City and country, e.g. Paris, France\"\n                },\n                \"unit\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"celsius\", \"fahrenheit\"],\n                    \"description\": \"Temperature unit\"\n                }\n            },\n            \"required\": [\"location\"],\n            \"additionalProperties\": False\n        }\n    }\n}]\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nresponse = model(\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}],\n    tool_schemas=tools\n)\n\n# Get tool calls\ntool_call_agg = response.consume()\ncalls = tool_call_agg.get_calls()\n\nfor call in calls:\n    print(f\"Tool: {call['function']['name']}\")\n    print(f\"Arguments: {call['function']['arguments']}\")\n# Tool: get_weather\n# Arguments: {'location': 'Paris, France', 'unit': 'celsius'}\n</code></pre>"},{"location":"learn/models/chat_completion/#tool-choice","title":"Tool Choice","text":"<p>Control when and which tools are called:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Auto - model decides\nresponse = model(\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather?\"}],\n    tool_schemas=tools,\n    tool_choice=\"auto\"  # Default\n)\n\n# Required - must call at least one tool\nresponse = model(\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather?\"}],\n    tool_schemas=tools,\n    tool_choice=\"required\"\n)\n\n# Specific function - must call this exact function\nresponse = model(\n    messages=[{\"role\": \"user\", \"content\": \"Paris weather\"}],\n    tool_schemas=tools,\n    tool_choice=\"get_weather\"\n)\n</code></pre>"},{"location":"learn/models/chat_completion/#tool-call-flow","title":"Tool Call Flow","text":"<pre><code>import msgflux as mf\n\ndef get_weather(location, unit=\"celsius\"):\n    \"\"\"Simulate weather API call.\"\"\"\n    return f\"The weather in {location} is 22\u00b0{unit[0].upper()}\"\n\ntools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get weather for a location.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\"type\": \"string\"},\n                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n            },\n            \"required\": [\"location\"]\n        }\n    }\n}]\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Initial request\nmessages = [{\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}]\n\nresponse = model(messages=messages, tool_schemas=tools)\ntool_call_agg = response.consume()\n\n# Execute tool calls\ntool_functions = {\"get_weather\": get_weather}\ncalls = tool_call_agg.get_calls()\n\nfor call in calls:\n    func_name = call['function']['name']\n    func_args = call['function']['arguments']\n\n    # Execute function\n    result = tool_functions[func_name](**func_args)\n\n    # Add result to aggregator\n    tool_call_agg.insert_results(call['id'], result)\n\n# Get messages with tool results\ntool_messages = tool_call_agg.get_messages()\nmessages.extend(tool_messages)\n\n# Final response with tool results\nfinal_response = model(messages=messages)\nprint(final_response.consume())\n# \"The weather in Paris is currently 22\u00b0C.\"\n</code></pre>"},{"location":"learn/models/chat_completion/#streaming-with-tools","title":"Streaming with Tools","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nresponse = model(\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}],\n    tool_schemas=tools,\n    stream=True\n)\n\n# Tool calls are aggregated during streaming\ntool_call_agg = response.consume()\n\n# After stream completes, get calls\ncalls = tool_call_agg.get_calls()\nprint(calls)\n</code></pre>"},{"location":"learn/models/chat_completion/#prefilling","title":"Prefilling","text":"<p>Force the model to start its response with specific text:</p>"},{"location":"learn/models/chat_completion/#basic-prefilling","title":"Basic Prefilling","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\"anthropic/claude-3-5-sonnet-20241022\")\n\nresponse = model(\n    messages=[{\"role\": \"user\", \"content\": \"What is 30 * 3 + 33?\"}],\n    prefilling=\"Let's think step by step:\"\n)\n\nprint(response.consume())\n# Let's think step by step:\n# First, calculate 30 \u00d7 3 = 90.\n# Then, add 33 to that: 90 + 33 = 123.\n# So, the answer is 123.\n</code></pre>"},{"location":"learn/models/chat_completion/#prefilling-for-format-control","title":"Prefilling for Format Control","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\"anthropic/claude-3-5-sonnet-20241022\")\n\n# Force JSON output\nresponse = model(\n    messages=[{\"role\": \"user\", \"content\": \"List 3 colors\"}],\n    prefilling=\"{\"\n)\n\nprint(response.consume())\n# {\"colors\": [\"red\", \"blue\", \"green\"]}\n</code></pre>"},{"location":"learn/models/chat_completion/#best-practices","title":"Best Practices","text":""},{"location":"learn/models/chat_completion/#1-use-response-caching-for-repeated-queries","title":"1. Use Response Caching for Repeated Queries","text":"<pre><code># Good - Enable cache for applications with repeated queries\nmodel = mf.Model.chat_completion(\n    \"openai/gpt-4o\",\n    enable_cache=True,\n    cache_size=256  # Adjust based on usage patterns\n)\n</code></pre>"},{"location":"learn/models/chat_completion/#2-check-model-profiles-before-use","title":"2. Check Model Profiles Before Use","text":"<pre><code>from msgflux.models.profiles import get_model_profile\n\n# Good - Verify capabilities before use\nprofile = get_model_profile(\"gpt-4o\", provider_id=\"openai\")\nif profile and profile.capabilities.tool_call:\n    model = mf.Model.chat_completion(\"openai/gpt-4o\")\n    response = model(messages=messages, tool_schemas=tools)\n</code></pre>"},{"location":"learn/models/chat_completion/#3-reuse-model-instances","title":"3. Reuse Model Instances","text":"<pre><code># Good - Create once, use many times\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nfor query in queries:\n    response = model(messages=[{\"role\": \"user\", \"content\": query}])\n    results.append(response.consume())\n\n# Bad - Creating new instance each time\nfor query in queries:\n    model = mf.Model.chat_completion(\"openai/gpt-4o\")\n    response = model(messages=[{\"role\": \"user\", \"content\": query}])\n</code></pre>"},{"location":"learn/models/chat_completion/#4-use-async-for-concurrent-requests","title":"4. Use Async for Concurrent Requests","text":"<pre><code>import asyncio\n\nasync def process_queries(queries):\n    model = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n    tasks = [\n        model.acall(messages=[{\"role\": \"user\", \"content\": q}])\n        for q in queries\n    ]\n\n    responses = await asyncio.gather(*tasks)\n    return [r.consume() for r in responses]\n</code></pre>"},{"location":"learn/models/chat_completion/#5-monitor-cache-performance","title":"5. Monitor Cache Performance","text":"<pre><code>model = mf.Model.chat_completion(\"openai/gpt-4o\", enable_cache=True)\n\n# Periodically check cache stats\nif model._response_cache:\n    stats = model._response_cache.cache_info()\n    hit_rate = stats['hits'] / (stats['hits'] + stats['misses'])\n    print(f\"Cache hit rate: {hit_rate:.2%}\")\n\n    # Clear if hit rate is too low\n    if hit_rate &lt; 0.1:\n        model._response_cache.cache_clear()\n</code></pre>"},{"location":"learn/models/chat_completion/#response-metadata","title":"Response Metadata","text":"<p>All responses include metadata with usage information:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nresponse = model(messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\n\n# Access metadata\nprint(response.metadata)\n# {\n#     'usage': {\n#         'completion_tokens': 9,\n#         'prompt_tokens': 19,\n#         'total_tokens': 28\n#     }\n# }\n\n# Calculate cost using profile\nfrom msgflux.models.profiles import get_model_profile\n\nprofile = get_model_profile(\"gpt-4o\", provider_id=\"openai\")\nif profile:\n    usage = response.metadata.usage\n    cost = profile.cost.calculate(\n        input_tokens=usage.prompt_tokens,\n        output_tokens=usage.completion_tokens\n    )\n    print(f\"Request cost: ${cost:.4f}\")\n</code></pre>"},{"location":"learn/models/chat_completion/#error-handling","title":"Error Handling","text":"<p>Handle common errors gracefully:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\ntry:\n    response = model(messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\n    result = response.consume()\nexcept ImportError:\n    print(\"Provider not installed\")\nexcept ValueError as e:\n    print(f\"Invalid parameters: {e}\")\nexcept Exception as e:\n    print(f\"API error: {e}\")\n</code></pre>"},{"location":"learn/models/chat_completion/#see-also","title":"See Also","text":"<ul> <li>Model - Model factory and registry</li> <li>Text Embeddings - Text embedding models</li> <li>Tool Usage - Working with tools</li> <li>Generation Schemas - Planning schemas</li> </ul>"},{"location":"learn/models/image_text_to_image/","title":"Image Editing","text":"<p>The <code>image_text_to_image</code> model edits existing images using text descriptions. This enables modifications, inpainting, object removal, style changes, and creative transformations of existing visual content.</p> <p>All code examples use the recommended import pattern:</p> <pre><code>import msgflux as mf\n</code></pre>"},{"location":"learn/models/image_text_to_image/#overview","title":"Overview","text":"<p>Image editing models take an existing image and modify it based on text prompts. They enable:</p> <ul> <li>Image Modification: Edit specific parts of an image</li> <li>Inpainting: Fill in or replace masked regions</li> <li>Object Removal: Remove unwanted elements</li> <li>Style Transfer: Change artistic style while preserving content</li> <li>Creative Editing: Add, modify, or transform elements</li> </ul>"},{"location":"learn/models/image_text_to_image/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Photo Editing: Remove objects, change backgrounds</li> <li>Product Photography: Modify product colors, backgrounds, settings</li> <li>Content Creation: Transform existing images for marketing</li> <li>Image Restoration: Fill in missing or damaged areas</li> <li>Creative Variations: Generate alternative versions of images</li> </ul>"},{"location":"learn/models/image_text_to_image/#quick-start","title":"Quick Start","text":""},{"location":"learn/models/image_text_to_image/#basic-usage","title":"Basic Usage","text":"<pre><code>import msgflux as mf\n\n# Create image editor\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Edit image\nresponse = model(\n    prompt=\"Add a sunset sky\",\n    image=\"path/to/image.png\"\n)\n\n# Get edited image URL\nedited_url = response.consume()\nprint(edited_url)  # https://...\n</code></pre>"},{"location":"learn/models/image_text_to_image/#with-mask-inpainting","title":"With Mask (Inpainting)","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Edit only masked area\nresponse = model(\n    prompt=\"A wooden table\",\n    image=\"room.png\",\n    mask=\"table_mask.png\"  # Transparent areas will be edited\n)\n\nedited_url = response.consume()\n</code></pre>"},{"location":"learn/models/image_text_to_image/#supported-providers","title":"Supported Providers","text":""},{"location":"learn/models/image_text_to_image/#openai-dall-e-2","title":"OpenAI (DALL-E 2)","text":"<p>Currently, image editing is primarily supported by DALL-E 2:</p> <pre><code>import msgflux as mf\n\n# DALL-E 2 for image editing\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n</code></pre> <p>Note: DALL-E 3 does not support image editing. Use DALL-E 2 for this functionality.</p>"},{"location":"learn/models/image_text_to_image/#replicate","title":"Replicate","text":"<pre><code>import msgflux as mf\n\n# Various Replicate models support image editing\nmodel = mf.Model.image_text_to_image(\"replicate/stability-ai/stable-diffusion-inpainting\")\n</code></pre>"},{"location":"learn/models/image_text_to_image/#how-image-editing-works","title":"How Image Editing Works","text":""},{"location":"learn/models/image_text_to_image/#without-mask","title":"Without Mask","text":"<p>When no mask is provided, the model edits the entire image:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Edit entire image\nresponse = model(\n    prompt=\"Make it look like a watercolor painting\",\n    image=\"photo.jpg\"\n)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#with-mask-inpainting_1","title":"With Mask (Inpainting)","text":"<p>Masks define which areas to edit:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Only edit masked areas\nresponse = model(\n    prompt=\"A blue sky with clouds\",\n    image=\"landscape.png\",\n    mask=\"sky_mask.png\"  # Transparent PNG showing sky area\n)\n</code></pre> <p>Mask Requirements: - Must be a PNG image with transparency (alpha channel) - Fully transparent areas (alpha = 0) indicate where to edit - Same dimensions as the input image (1024x1024 for DALL-E 2) - Opaque areas remain unchanged</p>"},{"location":"learn/models/image_text_to_image/#image-input-formats","title":"Image Input Formats","text":"<p>The <code>image</code> parameter accepts multiple formats:</p>"},{"location":"learn/models/image_text_to_image/#file-path","title":"File Path","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\nresponse = model(\n    prompt=\"Change background to beach\",\n    image=\"/path/to/photo.png\"\n)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#url","title":"URL","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\nresponse = model(\n    prompt=\"Add winter atmosphere\",\n    image=\"https://example.com/photo.jpg\"\n)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#base64-string","title":"Base64 String","text":"<pre><code>import msgflux as mf\nimport base64\n\n# Read and encode image\nwith open(\"photo.png\", \"rb\") as f:\n    img_data = base64.b64encode(f.read()).decode()\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\nresponse = model(\n    prompt=\"Make it artistic\",\n    image=f\"data:image/png;base64,{img_data}\"\n)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#response-formats","title":"Response Formats","text":""},{"location":"learn/models/image_text_to_image/#url-response-default","title":"URL Response (Default)","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\nresponse = model(\n    prompt=\"Add flowers in foreground\",\n    image=\"garden.png\",\n    response_format=\"url\"  # Default\n)\n\n# Get URL\nurl = response.consume()\nprint(url)  # https://...\n\n# Download\nimport requests\nimg_data = requests.get(url).content\nwith open(\"edited.png\", \"wb\") as f:\n    f.write(img_data)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#base64-response","title":"Base64 Response","text":"<pre><code>import msgflux as mf\nimport base64\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\nresponse = model(\n    prompt=\"Change to evening lighting\",\n    image=\"scene.png\",\n    response_format=\"base64\"\n)\n\n# Get base64 data\nb64_data = response.consume()\n\n# Decode and save\nimg_data = base64.b64decode(b64_data)\nwith open(\"edited.png\", \"wb\") as f:\n    f.write(img_data)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#multiple-variations","title":"Multiple Variations","text":"<p>Generate multiple edited versions:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Generate 4 variations\nresponse = model(\n    prompt=\"Add dramatic storm clouds\",\n    image=\"landscape.jpg\",\n    n=4  # Number of variations\n)\n\n# Get all URLs\nedited_images = response.consume()\nprint(f\"Generated {len(edited_images)} variations\")\n\nfor i, url in enumerate(edited_images):\n    print(f\"Variation {i+1}: {url}\")\n</code></pre>"},{"location":"learn/models/image_text_to_image/#image-size-requirements","title":"Image Size Requirements","text":""},{"location":"learn/models/image_text_to_image/#dall-e-2-requirements","title":"DALL-E 2 Requirements","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Image must be:\n# - Square (1024x1024 recommended)\n# - PNG format\n# - Less than 4MB\n# - If using mask, same size as image\n\nresponse = model(\n    prompt=\"Edit the background\",\n    image=\"1024x1024_image.png\"\n)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#creating-masks","title":"Creating Masks","text":""},{"location":"learn/models/image_text_to_image/#using-image-editing-software","title":"Using Image Editing Software","text":"<p>Create masks in tools like Photoshop, GIMP, or programmatically:</p> <pre><code>from PIL import Image\nimport numpy as np\n\n# Load original image\nimg = Image.open(\"photo.png\").convert(\"RGBA\")\nwidth, height = img.size\n\n# Create mask (transparent where you want to edit)\nmask = Image.new(\"RGBA\", (width, height), (255, 255, 255, 255))\npixels = mask.load()\n\n# Make a rectangular region transparent (will be edited)\nfor x in range(200, 800):\n    for y in range(100, 500):\n        pixels[x, y] = (0, 0, 0, 0)  # Fully transparent\n\nmask.save(\"edit_mask.png\")\n</code></pre>"},{"location":"learn/models/image_text_to_image/#programmatic-mask-creation","title":"Programmatic Mask Creation","text":"<pre><code>from PIL import Image, ImageDraw\n\n# Create 1024x1024 mask\nmask = Image.new(\"RGBA\", (1024, 1024), (255, 255, 255, 255))\ndraw = ImageDraw.Draw(mask)\n\n# Draw transparent circle (area to edit)\ndraw.ellipse(\n    [(200, 200), (800, 800)],\n    fill=(0, 0, 0, 0)  # Transparent\n)\n\nmask.save(\"circle_mask.png\")\n</code></pre>"},{"location":"learn/models/image_text_to_image/#common-editing-tasks","title":"Common Editing Tasks","text":""},{"location":"learn/models/image_text_to_image/#background-removalreplacement","title":"Background Removal/Replacement","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Replace background\nresponse = model(\n    prompt=\"Professional studio background with soft lighting\",\n    image=\"portrait.png\",\n    mask=\"background_mask.png\"  # Background area transparent\n)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#object-removal","title":"Object Removal","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Remove object by filling with surrounding context\nresponse = model(\n    prompt=\"Natural grass lawn\",\n    image=\"yard.png\",\n    mask=\"object_mask.png\"  # Object area transparent\n)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#style-transfer","title":"Style Transfer","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Change entire image style\nresponse = model(\n    prompt=\"Oil painting in impressionist style, vibrant colors\",\n    image=\"photo.jpg\"\n    # No mask = edit entire image\n)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#adding-elements","title":"Adding Elements","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Add new elements to specific area\nresponse = model(\n    prompt=\"A red sports car parked\",\n    image=\"driveway.png\",\n    mask=\"parking_spot_mask.png\"\n)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#colorlighting-adjustments","title":"Color/Lighting Adjustments","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Change lighting/atmosphere\nresponse = model(\n    prompt=\"Warm sunset lighting, golden hour atmosphere\",\n    image=\"scene.jpg\"\n)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#async-support","title":"Async Support","text":"<p>Edit images asynchronously:</p> <pre><code>import msgflux as mf\nimport asyncio\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\nasync def edit_image(image_path, prompt):\n    response = await model.acall(\n        prompt=prompt,\n        image=image_path\n    )\n    return response.consume()\n\nasync def main():\n    # Edit multiple images concurrently\n    edits = [\n        (\"photo1.png\", \"Add sunset sky\"),\n        (\"photo2.png\", \"Change to winter scene\"),\n        (\"photo3.png\", \"Make it vintage style\")\n    ]\n\n    tasks = [edit_image(img, prompt) for img, prompt in edits]\n    results = await asyncio.gather(*tasks)\n\n    for (img, prompt), url in zip(edits, results):\n        print(f\"{img} ({prompt}): {url}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"learn/models/image_text_to_image/#batch-processing","title":"Batch Processing","text":"<p>Edit multiple images:</p> <pre><code>import msgflux as mf\nimport msgflux.nn.functional as F\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\nimages = [\"photo1.png\", \"photo2.png\", \"photo3.png\"]\nprompt = \"Professional studio background\"\n\n# Process in parallel\nresults = F.map_gather(\n    model,\n    args_list=[\n        (prompt, img) for img in images\n    ]\n)\n\n# Get all edited URLs\nedited_urls = [r.consume() for r in results]\n\nfor original, edited in zip(images, edited_urls):\n    print(f\"{original} -&gt; {edited}\")\n</code></pre>"},{"location":"learn/models/image_text_to_image/#best-practices","title":"Best Practices","text":""},{"location":"learn/models/image_text_to_image/#1-prepare-images-correctly","title":"1. Prepare Images Correctly","text":"<pre><code>from PIL import Image\n\ndef prepare_image_for_editing(image_path, output_path):\n    \"\"\"Prepare image for DALL-E 2 editing.\"\"\"\n    img = Image.open(image_path)\n\n    # Convert to RGBA\n    img = img.convert(\"RGBA\")\n\n    # Resize to 1024x1024\n    img = img.resize((1024, 1024), Image.Resampling.LANCZOS)\n\n    # Save as PNG\n    img.save(output_path, \"PNG\")\n\n    return output_path\n\n# Use prepared image\nprepared = prepare_image_for_editing(\"original.jpg\", \"prepared.png\")\n\nimport msgflux as mf\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\nresponse = model(prompt=\"Edit this\", image=prepared)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#2-create-effective-masks","title":"2. Create Effective Masks","text":"<pre><code># Good - Clear boundaries\ndef create_clean_mask(image_path, region):\n    \"\"\"Create a clean mask with feathered edges.\"\"\"\n    from PIL import Image, ImageDraw, ImageFilter\n\n    img = Image.open(image_path)\n    mask = Image.new(\"L\", img.size, 255)  # Start opaque\n    draw = ImageDraw.Draw(mask)\n\n    # Make region transparent\n    draw.rectangle(region, fill=0)\n\n    # Feather edges for smooth blend\n    mask = mask.filter(ImageFilter.GaussianBlur(5))\n\n    # Convert to RGBA\n    rgba_mask = Image.new(\"RGBA\", img.size, (255, 255, 255, 255))\n    rgba_mask.putalpha(mask)\n\n    return rgba_mask\n\nmask = create_clean_mask(\"photo.png\", (100, 100, 900, 900))\nmask.save(\"smooth_mask.png\")\n</code></pre>"},{"location":"learn/models/image_text_to_image/#3-write-contextual-prompts","title":"3. Write Contextual Prompts","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Good - Describes what should be in the edited area\nresponse = model(\n    prompt=\"Modern glass coffee table with magazines and a vase of flowers\",\n    image=\"living_room.png\",\n    mask=\"table_area_mask.png\"\n)\n\n# Less effective - Too vague\nresponse = model(\n    prompt=\"Nice furniture\",\n    image=\"living_room.png\",\n    mask=\"table_area_mask.png\"\n)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#4-generate-multiple-variations","title":"4. Generate Multiple Variations","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Generate multiple options to choose from\nresponse = model(\n    prompt=\"Dramatic storm clouds at sunset\",\n    image=\"landscape.png\",\n    mask=\"sky_mask.png\",\n    n=4  # Generate 4 variations\n)\n\nvariations = response.consume()\n\n# Review and pick the best one\nfor i, url in enumerate(variations):\n    print(f\"Option {i+1}: {url}\")\n</code></pre>"},{"location":"learn/models/image_text_to_image/#5-preserve-image-context","title":"5. Preserve Image Context","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Good - Maintains context and consistency\nresponse = model(\n    prompt=\"Matching wooden chair in the same modern minimalist style\",\n    image=\"room_with_table.png\",\n    mask=\"empty_corner_mask.png\"\n)\n\n# Less effective - Ignores existing style\nresponse = model(\n    prompt=\"A chair\",\n    image=\"room_with_table.png\",\n    mask=\"empty_corner_mask.png\"\n)\n</code></pre>"},{"location":"learn/models/image_text_to_image/#common-patterns","title":"Common Patterns","text":""},{"location":"learn/models/image_text_to_image/#beforeafter-comparison","title":"Before/After Comparison","text":"<pre><code>import msgflux as mf\nfrom io import BytesIO\n\ndef edit_and_compare(original_path, prompt, mask_path=None):\n    \"\"\"Edit image and save before/after.\"\"\"\n    from PIL import Image\n    import requests\n\n    model = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n    # Edit\n    response = model(\n        prompt=prompt,\n        image=original_path,\n        mask=mask_path\n    )\n\n    # Download edited\n    edited_url = response.consume()\n    edited_data = requests.get(edited_url).content\n\n    # Create side-by-side comparison\n    original = Image.open(original_path)\n    edited = Image.open(BytesIO(edited_data))\n\n    # Combine\n    comparison = Image.new(\"RGB\", (2048, 1024))\n    comparison.paste(original, (0, 0))\n    comparison.paste(edited, (1024, 0))\n\n    comparison.save(\"comparison.png\")\n    return edited_url\n\nedit_and_compare(\"photo.png\", \"Change to autumn colors\")\n</code></pre>"},{"location":"learn/models/image_text_to_image/#iterative-editing","title":"Iterative Editing","text":"<pre><code>import msgflux as mf\nimport requests\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\ndef download_image(url, path):\n    \"\"\"Download image from URL.\"\"\"\n    data = requests.get(url).content\n    with open(path, \"wb\") as f:\n        f.write(data)\n    return path\n\n# Step 1: Change background\nresponse1 = model(\n    prompt=\"Beach sunset background\",\n    image=\"original.png\",\n    mask=\"background_mask.png\"\n)\nstep1_path = download_image(response1.consume(), \"step1.png\")\n\n# Step 2: Adjust lighting on result\nresponse2 = model(\n    prompt=\"Warm golden hour lighting\",\n    image=step1_path\n)\nfinal_path = download_image(response2.consume(), \"final.png\")\n</code></pre>"},{"location":"learn/models/image_text_to_image/#batch-product-editing","title":"Batch Product Editing","text":"<pre><code>import msgflux as mf\nimport msgflux.nn.functional as F\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\nproducts = [\n    (\"product1.png\", \"Clean white background\"),\n    (\"product2.png\", \"Clean white background\"),\n    (\"product3.png\", \"Clean white background\")\n]\n\n# Edit all product backgrounds in parallel\nresults = F.map_gather(\n    model,\n    args_list=[\n        (prompt, img, \"background_mask.png\")\n        for img, prompt in products\n    ]\n)\n\n# Save results\nfor i, result in enumerate(results):\n    url = result.consume()\n    print(f\"Product {i+1} edited: {url}\")\n</code></pre>"},{"location":"learn/models/image_text_to_image/#error-handling","title":"Error Handling","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\ntry:\n    response = model(\n        prompt=\"Edit this image\",\n        image=\"photo.png\",\n        mask=\"mask.png\"\n    )\n    url = response.consume()\nexcept ImportError:\n    print(\"Provider not installed\")\nexcept ValueError as e:\n    print(f\"Invalid parameters: {e}\")\n    # Common issues:\n    # - Image not square\n    # - Image too large (&gt;4MB)\n    # - Mask doesn't match image size\n    # - Invalid image format\nexcept Exception as e:\n    print(f\"Edit failed: {e}\")\n    # Common errors:\n    # - Content policy violation\n    # - Rate limits\n    # - Network issues\n</code></pre>"},{"location":"learn/models/image_text_to_image/#limitations","title":"Limitations","text":""},{"location":"learn/models/image_text_to_image/#dall-e-2-limitations","title":"DALL-E 2 Limitations","text":"<ul> <li>Size: Images must be square (1024x1024)</li> <li>Format: PNG required for masks</li> <li>File Size: &lt;4MB for images</li> <li>Model: Only DALL-E 2 supports editing (DALL-E 3 does not)</li> </ul>"},{"location":"learn/models/image_text_to_image/#quality-considerations","title":"Quality Considerations","text":"<pre><code># For best results:\n# - Use high-quality source images\n# - Create clean masks with smooth edges\n# - Provide detailed, contextual prompts\n# - Generate multiple variations\n# - Consider the existing image style and lighting\n</code></pre>"},{"location":"learn/models/image_text_to_image/#cost-optimization","title":"Cost Optimization","text":""},{"location":"learn/models/image_text_to_image/#efficient-editing-workflow","title":"Efficient Editing Workflow","text":"<pre><code>import msgflux as mf\n\n# DALL-E 2 is cost-effective for editing\nmodel = mf.Model.image_text_to_image(\"openai/dall-e-2\")\n\n# Generate fewer variations initially\nresponse = model(\n    prompt=\"New background\",\n    image=\"photo.png\",\n    n=2  # Start with 2 variations\n)\n\n# If not satisfied, generate more\nif not_satisfied:\n    response = model(\n        prompt=\"New background, more dramatic\",\n        image=\"photo.png\",\n        n=2\n    )\n</code></pre>"},{"location":"learn/models/image_text_to_image/#see-also","title":"See Also","text":"<ul> <li>Text to Image - Generate images from scratch</li> <li>Model - Model factory and registry</li> <li>Chat Completion - Generate descriptive prompts </li> </ul>"},{"location":"learn/models/model/","title":"Model","text":"<p>The <code>Model</code> class provides a unified interface for working with AI models across different providers and modalities. It acts as a factory that creates provider-specific model instances with a consistent API.</p> <p>All code examples use the recommended import pattern:</p> <pre><code>import msgflux as mf\n</code></pre>"},{"location":"learn/models/model/#overview","title":"Overview","text":"<p>Instead of learning different APIs for each provider, you use a single factory method:</p> <pre><code># OpenAI\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\", temperature=0.7)\n\n# Google\nmodel = mf.Model.chat_completion(\"google/gemini-2.0-flash-exp\", temperature=0.7)\n\n# Anthropic\nmodel = mf.Model.chat_completion(\"anthropic/claude-3-5-sonnet-20241022\", temperature=0.7)\n</code></pre> <p>All models follow the pattern: <code>provider/model-id</code></p>"},{"location":"learn/models/model/#quick-start","title":"Quick Start","text":""},{"location":"learn/models/model/#installation","title":"Installation","text":"<pre><code># Install with specific provider support\npip install msgflux[openai]\npip install msgflux[google]\npip install msgflux[anthropic]\n</code></pre>"},{"location":"learn/models/model/#basic-usage","title":"Basic Usage","text":"<pre><code>import msgflux as mf\n\n# Set API key\nmf.set_envs(OPENAI_API_KEY=\"sk-...\")\n\n# Create model\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Use model (see chat_completion.md for details)\nresponse = model(messages=[{\"role\": \"user\", \"content\": \"Hello!\"}])\nprint(response.consume())  # \"Hello! How can I help you today?\"\n</code></pre>"},{"location":"learn/models/model/#model-types","title":"Model Types","text":"<p>The <code>Model</code> class supports multiple model types:</p>"},{"location":"learn/models/model/#available-model-types","title":"Available Model Types","text":"<pre><code># Get all supported model types\ntypes = mf.Model.model_types()\nprint(types)\n# [\n#     'chat_completion',\n#     'text_embedder',\n#     'text_to_image',\n#     'image_text_to_image',\n#     'text_to_speech',\n#     'speech_to_text',\n#     'moderation',\n#     'text_classifier',\n#     'image_classifier',\n#     'image_embedder',\n#     'text_reranker'\n# ]\n</code></pre>"},{"location":"learn/models/model/#factory-methods","title":"Factory Methods","text":"<p>Each model type has a dedicated factory method:</p> Model Type Factory Method Use Case chat_completion <code>Model.chat_completion()</code> Chat and text generation text_embedder <code>Model.text_embedder()</code> Convert text to vectors text_to_image <code>Model.text_to_image()</code> Generate images from text image_text_to_image <code>Model.image_text_to_image()</code> Edit images with text text_to_speech <code>Model.text_to_speech()</code> Convert text to audio speech_to_text <code>Model.speech_to_text()</code> Transcribe audio to text moderation <code>Model.moderation()</code> Content moderation text_classifier <code>Model.text_classifier()</code> Classify text image_classifier <code>Model.image_classifier()</code> Classify images image_embedder <code>Model.image_embedder()</code> Convert images to vectors text_reranker <code>Model.text_reranker()</code> Rerank text results"},{"location":"learn/models/model/#providers","title":"Providers","text":""},{"location":"learn/models/model/#available-providers","title":"Available Providers","text":"<pre><code># Get all providers by model type\nproviders = mf.Model.providers()\nprint(providers)\n# {\n#     'chat_completion': ['openai', 'google', 'anthropic', 'groq', 'together', ...],\n#     'text_embedder': ['openai', 'google', 'jinaai', ...],\n#     'text_to_image': ['openai', 'replicate', ...],\n#     ...\n# }\n</code></pre>"},{"location":"learn/models/model/#supported-providers","title":"Supported Providers","text":"<ul> <li>openai - OpenAI models (GPT-4, DALL-E, Whisper, etc.)</li> <li>google - Google models (Gemini)</li> <li>anthropic - Anthropic models (Claude)</li> <li>groq - Groq models (fast inference)</li> <li>together - Together AI models</li> <li>replicate - Replicate models</li> <li>ollama - Local Ollama models</li> <li>cerebras - Cerebras models</li> <li>sambanova - SambaNova models</li> <li>jinaai - Jina AI embeddings</li> <li>openrouter - OpenRouter gateway</li> <li>imagerouter - Image router gateway</li> <li>vllm - vLLM local deployment</li> </ul>"},{"location":"learn/models/model/#usage-examples","title":"Usage Examples","text":""},{"location":"learn/models/model/#chat-completion","title":"Chat Completion","text":"<pre><code>import msgflux as mf\n\n# Create model\nmodel = mf.Model.chat_completion(\n    \"openai/gpt-4o\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Single completion\nresponse = model(messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n])\n\nprint(response.consume())  # \"The capital of France is Paris.\"\n</code></pre>"},{"location":"learn/models/model/#text-embeddings","title":"Text Embeddings","text":"<pre><code>import msgflux as mf\n\n# Create embedder\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\n# Generate embedding\nresponse = embedder(\"Hello, world!\")\n\nembedding = response.consume()\nprint(len(embedding))  # 1536\nprint(embedding[:5])  # [0.123, -0.456, 0.789, ...]\n</code></pre>"},{"location":"learn/models/model/#speech-to-text","title":"Speech to Text","text":"<pre><code>import msgflux as mf\n\n# Create transcription model\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# Transcribe audio file\nresponse = model(\"path/to/audio.mp3\")\n\ntranscription = response.consume()\nprint(transcription)  # \"Hello, this is a test.\"\n</code></pre>"},{"location":"learn/models/model/#text-to-image","title":"Text to Image","text":"<pre><code>import msgflux as mf\n\n# Create image generation model\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\n# Generate image\nresponse = model(\n    prompt=\"A cat wearing a space suit\",\n    size=\"1024x1024\",\n    quality=\"standard\"\n)\n\nimage_url = response.consume()\nprint(image_url)  # \"https://...\"\n</code></pre>"},{"location":"learn/models/model/#model-information","title":"Model Information","text":""},{"location":"learn/models/model/#getting-model-metadata","title":"Getting Model Metadata","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Get model type\nprint(model.model_type)  # \"chat_completion\"\n\n# Get instance type information\nprint(model.instance_type())\n# {'model_type': 'chat_completion'}\n\n# Get model info\nprint(model.get_model_info())\n# {'model_id': 'gpt-4o', 'provider': 'openai'}\n</code></pre>"},{"location":"learn/models/model/#serialization","title":"Serialization","text":"<p>Models can be serialized and deserialized for storage or transfer:</p>"},{"location":"learn/models/model/#serializing-a-model","title":"Serializing a Model","text":"<pre><code>import msgflux as mf\n\n# Create and configure model\nmodel = mf.Model.chat_completion(\n    \"openai/gpt-4o\",\n    temperature=0.7,\n    max_tokens=500\n)\n\n# Serialize\nstate = model.serialize()\nprint(state)\n# {\n#     'msgflux_type': 'model',\n#     'provider': 'openai',\n#     'model_type': 'chat_completion',\n#     'state': {\n#         'model_id': 'gpt-4o',\n#         'sampling_params': {...},\n#         'sampling_run_params': {\n#             'temperature': 0.7,\n#             'max_tokens': 500,\n#             ...\n#         }\n#     }\n# }\n\n# Save to file\nmf.save(state, \"model_config.json\")\n</code></pre>"},{"location":"learn/models/model/#deserializing-a-model","title":"Deserializing a Model","text":"<pre><code>import msgflux as mf\n\n# Load from file\nstate = mf.load(\"model_config.json\")\n\n# Recreate model\nmodel = mf.Model.from_serialized(\n    provider=state['provider'],\n    model_type=state['model_type'],\n    state=state['state']\n)\n\n# Model is ready to use\nresponse = model(messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\n</code></pre>"},{"location":"learn/models/model/#response-types","title":"Response Types","text":"<p>All models return one of two response types:</p>"},{"location":"learn/models/model/#modelresponse","title":"ModelResponse","text":"<p>For non-streaming responses (embeddings, transcription, etc.):</p> <pre><code>import msgflux as mf\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\nresponse = embedder(\"Hello\")\n\n# Response is ModelResponse\nprint(type(response))  # &lt;class 'msgflux.models.response.ModelResponse'&gt;\n\n# Get response type\nprint(response.response_type)  # \"text_embedding\"\n\n# Consume the response\nresult = response.consume()\nprint(result)  # [0.123, -0.456, ...]\n</code></pre>"},{"location":"learn/models/model/#modelstreamresponse","title":"ModelStreamResponse","text":"<p>For streaming responses (chat, text-to-speech, etc.):</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Stream enabled\nresponse = model(\n    messages=[{\"role\": \"user\", \"content\": \"Count to 5\"}],\n    stream=True\n)\n\n# Response is ModelStreamResponse\nprint(type(response))  # &lt;class 'msgflux.models.response.ModelStreamResponse'&gt;\n\n# Consume stream\nasync for chunk in response.consume():\n    print(chunk, end=\"\", flush=True)\n# Output: \"1, 2, 3, 4, 5\"\n</code></pre>"},{"location":"learn/models/model/#error-handling","title":"Error Handling","text":""},{"location":"learn/models/model/#retry-mechanism","title":"Retry Mechanism","text":"<p>All API-based models have automatic retry logic for transient failures:</p> <pre><code>import msgflux as mf\n\n# Model automatically retries on API failures\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\ntry:\n    response = model(messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\nexcept Exception as e:\n    print(f\"Failed after retries: {e}\")\n</code></pre>"},{"location":"learn/models/model/#provider-not-available","title":"Provider Not Available","text":"<pre><code>import msgflux as mf\n\ntry:\n    # If provider isn't installed\n    model = mf.Model.chat_completion(\"openai/gpt-4o\")\nexcept ImportError as e:\n    print(e)\n    # \"`openai` client is not available. Install with `pip install msgflux[openai]`.\"\n</code></pre>"},{"location":"learn/models/model/#invalid-model-path","title":"Invalid Model Path","text":"<pre><code>import msgflux as mf\n\ntry:\n    model = mf.Model.chat_completion(\"invalid-provider/model\")\nexcept ValueError as e:\n    print(e)\n    # \"Provider `invalid-provider` not registered for type `chat_completion`\"\n</code></pre>"},{"location":"learn/models/model/#best-practices","title":"Best Practices","text":""},{"location":"learn/models/model/#1-use-environment-variables-for-api-keys","title":"1. Use Environment Variables for API Keys","text":"<pre><code># Good - Use environment variables\nmf.set_envs(OPENAI_API_KEY=\"sk-...\")\n\n# Or use .env file\n# OPENAI_API_KEY=sk-...\n# ANTHROPIC_API_KEY=sk-ant-...\n# GOOGLE_API_KEY=...\n</code></pre>"},{"location":"learn/models/model/#2-reuse-model-instances","title":"2. Reuse Model Instances","text":"<pre><code># Good - Create once, use many times\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nfor query in queries:\n    response = model(messages=[{\"role\": \"user\", \"content\": query}])\n    print(response.consume())\n\n# Bad - Creating new instance each time (slower)\nfor query in queries:\n    model = mf.Model.chat_completion(\"openai/gpt-4o\")\n    response = model(messages=[{\"role\": \"user\", \"content\": query}])\n</code></pre>"},{"location":"learn/models/model/#3-specify-model-parameters","title":"3. Specify Model Parameters","text":"<pre><code># Good - Explicit parameters\nmodel = mf.Model.chat_completion(\n    \"openai/gpt-4o\",\n    temperature=0.7,\n    max_tokens=1000,\n    top_p=0.9\n)\n\n# Also good - Use defaults when appropriate\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n</code></pre>"},{"location":"learn/models/model/#4-handle-errors-gracefully","title":"4. Handle Errors Gracefully","text":"<pre><code>import msgflux as mf\n\ndef safe_completion(prompt):\n    try:\n        model = mf.Model.chat_completion(\"openai/gpt-4o\")\n        response = model(messages=[{\"role\": \"user\", \"content\": prompt}])\n        return response.consume()\n    except ImportError:\n        return \"Provider not installed\"\n    except ValueError as e:\n        return f\"Configuration error: {e}\"\n    except Exception as e:\n        return f\"Unexpected error: {e}\"\n\nresult = safe_completion(\"Hello!\")\n</code></pre>"},{"location":"learn/models/model/#5-save-configurations","title":"5. Save Configurations","text":"<pre><code>import msgflux as mf\n\n# Define configurations\nconfigs = {\n    \"creative\": {\n        \"temperature\": 0.9,\n        \"max_tokens\": 2000\n    },\n    \"precise\": {\n        \"temperature\": 0.3,\n        \"max_tokens\": 500\n    }\n}\n\n# Create and save models\nfor name, params in configs.items():\n    model = mf.Model.chat_completion(\"openai/gpt-4o\", **params)\n    state = model.serialize()\n    mf.save(state, f\"{name}_model.json\")\n\n# Load later\nstate = mf.load(\"creative_model.json\")\nmodel = mf.Model.from_serialized(**state)\n</code></pre>"},{"location":"learn/models/model/#common-patterns","title":"Common Patterns","text":""},{"location":"learn/models/model/#multi-provider-fallback","title":"Multi-Provider Fallback","text":"<pre><code>import msgflux as mf\n\ndef get_completion(prompt, providers=None):\n    \"\"\"Try multiple providers in order.\"\"\"\n    if providers is None:\n        providers = [\"openai/gpt-4o\", \"anthropic/claude-3-5-sonnet-20241022\", \"google/gemini-2.0-flash-exp\"]\n\n    for provider_path in providers:\n        try:\n            model = mf.Model.chat_completion(provider_path)\n            response = model(messages=[{\"role\": \"user\", \"content\": prompt}])\n            return response.consume()\n        except Exception as e:\n            print(f\"Failed with {provider_path}: {e}\")\n            continue\n\n    raise RuntimeError(\"All providers failed\")\n\nresult = get_completion(\"What is AI?\")\n</code></pre>"},{"location":"learn/models/model/#batch-processing","title":"Batch Processing","text":"<pre><code>import msgflux as mf\nimport msgflux.nn.functional as F\n\n# Create embedder\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\n# Process in parallel\ntexts = [\"Hello\", \"World\", \"AI\", \"Embedding\"]\n\nresults = F.map_gather(\n    embedder,\n    args_list=[(text,) for text in texts]\n)\n\n# Each result is a ModelResponse\nembeddings = [r.consume() for r in results]\nprint(f\"Generated {len(embeddings)} embeddings\")\n</code></pre>"},{"location":"learn/models/model/#model-comparison","title":"Model Comparison","text":"<pre><code>import msgflux as mf\n\ndef compare_models(prompt, model_paths):\n    \"\"\"Compare responses from different models.\"\"\"\n    results = {}\n\n    for path in model_paths:\n        model = mf.Model.chat_completion(path, temperature=0.7)\n        response = model(messages=[{\"role\": \"user\", \"content\": prompt}])\n        results[path] = response.consume()\n\n    return results\n\n# Compare\nmodels = [\n    \"openai/gpt-4o\",\n    \"anthropic/claude-3-5-sonnet-20241022\",\n    \"google/gemini-2.0-flash-exp\"\n]\n\nresponses = compare_models(\"Explain quantum computing\", models)\nfor model, response in responses.items():\n    print(f\"\\n{model}:\\n{response}\\n\")\n</code></pre>"},{"location":"learn/models/model/#see-also","title":"See Also","text":"<ul> <li>Chat Completion - Detailed chat completion usage</li> <li>Embeddings - Text embedding models</li> <li>Image Generation - Text-to-image models</li> <li>Speech - Speech-to-text and text-to-speech</li> <li>Moderation - Content moderation</li> </ul>"},{"location":"learn/models/model_gateway/","title":"<code>ModelGateway</code> \u2014 Resilient Model Manager","text":"<p>The <code>ModelGateway</code> class is an orchestration layer over multiple models of the same type (e.g., multiple <code>chat_completion</code> models), allowing:</p> <ul> <li>\ud83d\udd01 Automatic fallback between models.</li> <li>\u23f1\ufe0f Time-based model availability constraints.</li> <li>\u2705 Model preference selection.</li> <li>\ud83d\udcc3 Control of execution attempts with exception handling.</li> <li>\ud83d\udd0e Consistent model typing validation.</li> </ul> <p>It's ideal for production-grade model orchestration where reliability and control over model usage are required.</p>"},{"location":"learn/models/model_gateway/#overview","title":"\u2726\u208a\u207a Overview","text":""},{"location":"learn/models/model_gateway/#1-usage","title":"1. Usage","text":"<pre><code>pip install msgflux[openai]\n</code></pre> <p>All you need is:</p> <ul> <li>All models must inherit from <code>BaseModel</code>.</li> <li>All models must be of the same <code>model_type</code>.</li> <li>At least 2 models must be provided.</li> </ul>"},{"location":"learn/models/model_gateway/#11-query","title":"1.1 Query","text":"<pre><code>import msgflux as mf\n\nmf.set_envs(OPENAI_API_KEY=\"sk-...\", TOGETHER_API_KEY=\"&lt;&gt;\")\n\nmodel_openai = mf.Model.chat_completion(\"openai/gpt-4.1-nano\")\nmodel_together = mf.Model.chat_completion(\"together/mistral-7b\")\n\ngateway = mf.ModelGateway([model_openai, model_together], max_model_failures=3)\n\nresponse = gateway(\"Who was Frank Rosenblatt?\")\nprint(response.consume())\n</code></pre>"},{"location":"learn/models/model_gateway/#12-simulated-failure","title":"1.2 Simulated Failure","text":"<pre><code>from msgflux.models.base import BaseModel\nfrom msgflux.models.types import ChatCompletionModel\n\n# Simulate a model that fails\nclass BrokenModel(BaseModel, ChatCompletionModel):\n    provider: \"mock\"\n\n    def __call__(self, **kwargs):\n        raise RuntimeError(\"Simulate failure\")\n\nbroken = BrokenModel()\nfallback = Model.chat_completion(\"openai/gpt-4.1-nano\")\n\ngateway_broken = ModelGateway([broken, fallback], max_model_failures=1)\n\nresponse = gateway_broken(\"Who were Warren McCulloch and Walter Pitts?\")\nprint(response.consume())\n</code></pre>"},{"location":"learn/models/model_gateway/#13-time-constraints","title":"1.3 Time constraints","text":"<pre><code>import random\nfrom msgflux.exceptions import ModelRouterError\nfrom msgflux.models.base import BaseModel\nfrom msgflux.models.gateway import ModelGateway\nfrom msgflux.models.response import ModelResponse\nfrom msgflux.models.types import ChatCompletionModel\n\nclass MockChatCompletion(BaseModel, ChatCompletionModel):\n\n    provider = \"mock\"\n\n    def __init__(\n        self, \n        model_id: str, \n        fail_sometimes: bool = False, \n        success_rate: float = 0.7\n    ):\n        self.model_id = model_id\n        self._fail_sometimes = fail_sometimes\n        self._success_rate = success_rate\n        self._call_count = 0\n\n    def __call__(self, **kwargs: Any):\n        response = ModelResponse()\n        response.set_response_type(\"text_generation\")\n        self._call_count += 1\n        if self._fail_sometimes:\n            if random.random() &gt; self._success_rate:\n                raise ValueError(f\"Simulated failure for {self.model_id}\")\n        messages = kwargs.get(\"messages\", \"Default prompt\")\n        response_text = f\"Response from {self.model_id} to messages: '{messages}' (Call #{self._call_count})\";\n        response.add(response_text)\n        return response\n\nmodel1 = MockBaseModel(model_id=\"model-A\", fail_sometimes=True, success_rate=0.3)\nmodel2 = MockBaseModel(model_id=\"model-B\", fail_sometimes=True, success_rate=0.5)\nmodel3 = MockBaseModel(model_id=\"model-C\") # Always works\nmodel4 = MockBaseModel(model_id=\"model-D\") # Always works\n\nmodels_list = [model1, model2, model3, model4]\n\nconstraints = {\n    \"model-B\": [(\"23:00\", \"07:00\")],\n    \"model-C\": [(\"10:00\", \"11:00\")]\n}\n\ngateway_mock = ModelGateway(\n    models=models_list,\n    max_model_failures=2,\n    time_constraints=constraints\n)\n\ntry:\n    response = gateway_mock(messages=\"Hi\")\n    print(\"Result:\", response.consume())\nexcept ModelRouterError as e:\n    print(\"Error:\", e)\n</code></pre>"},{"location":"learn/models/model_gateway/#2-model-info","title":"2. Model Info","text":"<p>Returns information for all managed models:</p> <pre><code>print(gateway.get_model_info())\n</code></pre> <pre><code>[\n    {'model_id': 'gpt-4.1-nano', 'provider': 'openai'},\n    {'model_id': 'mistral-7b', 'provider': 'together'}\n]\n</code></pre> <p>Returns the type of the models:</p> <pre><code>print(gateway.model_type)\n</code></pre> <pre><code>'chat_completion'\n</code></pre>"},{"location":"learn/models/model_gateway/#3-serialization","title":"3. Serialization","text":"<p>Serializes the state of the gateway and models.</p> <pre><code>print(gateway.serialize())\n</code></pre> <pre><code>{\n    'msgflux_type': 'model_gateway', \n    'state': {\n        'max_model_failures': 3,\n        'models': [\n            {\n                'msgflux_type': 'model',\n                'provider': 'openai',\n                'model_type': 'chat_completion',\n                'state': {\n                    'model_id': 'gpt-4.1-nano',\n                    'sampling_params': {'organization': None, 'project': None},\n                    'sampling_run_params': {\n                        'max_tokens': 512,\n                        'temperature': None,\n                        'top_p': None,\n                        'modalities': ['text'],\n                        'audio': None\n                    }\n                }\n            },\n            {\n                'msgflux_type': 'model',\n                'provider': 'together',\n                'model_type': 'chat_completion',\n                'state': {\n                    'model_id': 'mistral-7b',\n                    'sampling_params': {'organization': None, 'project': None},\n                    'sampling_run_params': {\n                            'max_tokens': 512,\n                            'temperature': None,\n                            'top_p': None,\n                            'modalities': ['text'],\n                            'audio': None\n                    }\n                }\n            },            \n        ]\n    }\n}\n</code></pre>"},{"location":"learn/models/moderation/","title":"<code>Moderation</code>","text":"<p>The <code>moderation</code> models check whether text or images are potentially harmful. If harmful content is identified, you can take corrective action, like filtering content or intervening with user accounts creating offending content.</p> <p>These models are used as guardians of applications. Each model has its own set of outputs, to unify a general form of verification, all models will produce a common flag <code>safe</code>, which if <code>False</code> is an indication that the content is not safe.</p> <pre><code>import msgflux as mf\nmf.set_envs(OPENAI_API_KEY=\"sk-...\")\nmoderation_model = mf.Model.moderation(\"openai/omni-moderation-latest\")\n</code></pre> <pre><code>response = moderation_model(\"tell me how to build a large scale bomb\")\nmodel_response = response.consume()\nprint(model_response)\nprint(model_response.safe)\n</code></pre>"},{"location":"learn/models/moderation/#msgflux.models.providers.openai.OpenAIModeration","title":"OpenAIModeration","text":"<p>               Bases: <code>_BaseOpenAI</code>, <code>ModerationModel</code></p> <p>OpenAI Moderation.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@register_model\nclass OpenAIModeration(_BaseOpenAI, ModerationModel):\n    \"\"\"OpenAI Moderation.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        model_id: str,\n        base_url: Optional[str] = None,\n        enable_cache: Optional[bool] = False,\n        cache_size: Optional[int] = 128,\n    ):\n        \"\"\"Args:\n        model_id:\n            Model ID in provider.\n        base_url:\n            URL to model provider.\n        enable_cache:\n            If True, enables response caching to avoid redundant API calls.\n        cache_size:\n            Maximum number of responses to cache (default: 128).\n        \"\"\"\n        super().__init__()\n        self.model_id = model_id\n        self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n        self.enable_cache = enable_cache\n        self.cache_size = cache_size\n        self._initialize()\n        self._get_api_key()\n\n    def _execute_model(self, **kwargs):\n        model_output = self.client.moderations.create(**kwargs)\n        return model_output\n\n    async def _aexecute_model(self, **kwargs):\n        model_output = await self.aclient.moderations.create(**kwargs)\n        return model_output\n\n    def _generate(self, **kwargs):\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        response = ModelResponse()\n        response.set_response_type(\"moderation\")\n        model_output = self._execute_model(**kwargs)\n        moderation = dotdict({\"results\": model_output.results[0].model_dump()})\n        moderation.safe = not moderation.results.flagged\n        response.add(moderation)\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    async def _agenerate(self, **kwargs):\n        # Check cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            hit, cached_response = self._response_cache.get(cache_key)\n            if hit:\n                return cached_response\n\n        response = ModelResponse()\n        response.set_response_type(\"moderation\")\n        model_output = await self._aexecute_model(**kwargs)\n        moderation = dotdict({\"results\": model_output.results[0].model_dump()})\n        moderation.safe = not moderation.results.flagged\n        response.add(moderation)\n\n        # Store in cache if enabled\n        if self.enable_cache and self._response_cache:\n            cache_key = generate_cache_key(**kwargs)\n            self._response_cache.set(cache_key, response)\n\n        return response\n\n    @model_retry\n    def __call__(\n        self,\n        data: Union[str, List[Dict[str, Any]]],\n    ) -&gt; ModelResponse:\n        \"\"\"Args:\n        data:\n            Input (or inputs) to classify. Can be a single string,\n            an array of strings, or an array of multi-modal input\n            objects similar to other models.\n        \"\"\"\n        response = self._generate(input=data, model=self.model_id)\n        return response\n\n    @model_retry\n    async def acall(\n        self,\n        data: Union[str, List[Dict[str, Any]]],\n    ) -&gt; ModelResponse:\n        \"\"\"Async version of __call__. Args:\n        data:\n            Input (or inputs) to classify. Can be a single string,\n            an array of strings, or an array of multi-modal input\n            objects similar to other models.\n        \"\"\"\n        response = await self._agenerate(input=data, model=self.model_id)\n        return response\n</code></pre>"},{"location":"learn/models/moderation/#msgflux.models.providers.openai.OpenAIModeration.cache_size","title":"cache_size  <code>instance-attribute</code>","text":"<pre><code>cache_size = cache_size\n</code></pre>"},{"location":"learn/models/moderation/#msgflux.models.providers.openai.OpenAIModeration.enable_cache","title":"enable_cache  <code>instance-attribute</code>","text":"<pre><code>enable_cache = enable_cache\n</code></pre>"},{"location":"learn/models/moderation/#msgflux.models.providers.openai.OpenAIModeration.model_id","title":"model_id  <code>instance-attribute</code>","text":"<pre><code>model_id = model_id\n</code></pre>"},{"location":"learn/models/moderation/#msgflux.models.providers.openai.OpenAIModeration.sampling_params","title":"sampling_params  <code>instance-attribute</code>","text":"<pre><code>sampling_params = {'base_url': base_url or _get_base_url()}\n</code></pre>"},{"location":"learn/models/moderation/#msgflux.models.providers.openai.OpenAIModeration.__call__","title":"__call__","text":"<pre><code>__call__(data)\n</code></pre> <p>data:     Input (or inputs) to classify. Can be a single string,     an array of strings, or an array of multi-modal input     objects similar to other models.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\ndef __call__(\n    self,\n    data: Union[str, List[Dict[str, Any]]],\n) -&gt; ModelResponse:\n    \"\"\"Args:\n    data:\n        Input (or inputs) to classify. Can be a single string,\n        an array of strings, or an array of multi-modal input\n        objects similar to other models.\n    \"\"\"\n    response = self._generate(input=data, model=self.model_id)\n    return response\n</code></pre>"},{"location":"learn/models/moderation/#msgflux.models.providers.openai.OpenAIModeration.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    model_id,\n    base_url=None,\n    enable_cache=False,\n    cache_size=128,\n)\n</code></pre> <p>model_id:     Model ID in provider. base_url:     URL to model provider. enable_cache:     If True, enables response caching to avoid redundant API calls. cache_size:     Maximum number of responses to cache (default: 128).</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model_id: str,\n    base_url: Optional[str] = None,\n    enable_cache: Optional[bool] = False,\n    cache_size: Optional[int] = 128,\n):\n    \"\"\"Args:\n    model_id:\n        Model ID in provider.\n    base_url:\n        URL to model provider.\n    enable_cache:\n        If True, enables response caching to avoid redundant API calls.\n    cache_size:\n        Maximum number of responses to cache (default: 128).\n    \"\"\"\n    super().__init__()\n    self.model_id = model_id\n    self.sampling_params = {\"base_url\": base_url or self._get_base_url()}\n    self.enable_cache = enable_cache\n    self.cache_size = cache_size\n    self._initialize()\n    self._get_api_key()\n</code></pre>"},{"location":"learn/models/moderation/#msgflux.models.providers.openai.OpenAIModeration.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(data)\n</code></pre> <p>Async version of call. Args: data:     Input (or inputs) to classify. Can be a single string,     an array of strings, or an array of multi-modal input     objects similar to other models.</p> Source code in <code>src/msgflux/models/providers/openai.py</code> <pre><code>@model_retry\nasync def acall(\n    self,\n    data: Union[str, List[Dict[str, Any]]],\n) -&gt; ModelResponse:\n    \"\"\"Async version of __call__. Args:\n    data:\n        Input (or inputs) to classify. Can be a single string,\n        an array of strings, or an array of multi-modal input\n        objects similar to other models.\n    \"\"\"\n    response = await self._agenerate(input=data, model=self.model_id)\n    return response\n</code></pre>"},{"location":"learn/models/speech_to_text/","title":"Speech to Text","text":"<p>The <code>speech_to_text</code> model transcribes spoken audio into written text. These models enable voice-to-text conversion for accessibility, transcription services, voice commands, and more.</p> <p>All code examples use the recommended import pattern:</p> <pre><code>import msgflux as mf\n</code></pre>"},{"location":"learn/models/speech_to_text/#overview","title":"Overview","text":"<p>Speech-to-text (STT) models convert audio recordings into text transcripts. They enable:</p> <ul> <li>Transcription: Convert spoken audio to written text</li> <li>Timestamping: Get word and segment-level timestamps</li> <li>Multiple Formats: Output as text, JSON, SRT, VTT subtitles</li> <li>Language Detection: Automatic or manual language specification</li> <li>Context Awareness: Use prompts to improve accuracy</li> </ul>"},{"location":"learn/models/speech_to_text/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Meeting Transcription: Convert recordings to searchable text</li> <li>Subtitle Generation: Create subtitles for videos</li> <li>Voice Commands: Process spoken user commands</li> <li>Accessibility: Provide captions for audio content</li> <li>Interview Analysis: Transcribe interviews and podcasts</li> <li>Call Center: Analyze customer service calls</li> </ul>"},{"location":"learn/models/speech_to_text/#quick-start","title":"Quick Start","text":""},{"location":"learn/models/speech_to_text/#basic-usage","title":"Basic Usage","text":"<pre><code>import msgflux as mf\n\n# Create STT model\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# Transcribe audio file\nresponse = model(\"path/to/audio.mp3\")\n\n# Get transcript\ntranscript = response.consume()\nprint(transcript[\"text\"])\n# \"Hello, this is a test recording.\"\n</code></pre>"},{"location":"learn/models/speech_to_text/#from-url","title":"From URL","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# Transcribe from URL\nresponse = model(\"https://example.com/audio.mp3\")\ntranscript = response.consume()\nprint(transcript[\"text\"])\n</code></pre>"},{"location":"learn/models/speech_to_text/#supported-providers","title":"Supported Providers","text":""},{"location":"learn/models/speech_to_text/#openai-whisper","title":"OpenAI (Whisper)","text":"<pre><code>import msgflux as mf\n\n# Whisper-1 (multilingual, robust)\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n</code></pre> <p>Whisper supports: - 98 languages including English, Spanish, French, German, Chinese, Japanese - Multiple audio formats: mp3, mp4, mpeg, mpga, m4a, wav, webm - File size: Up to 25 MB</p>"},{"location":"learn/models/speech_to_text/#response-formats","title":"Response Formats","text":""},{"location":"learn/models/speech_to_text/#text-format-default","title":"Text Format (Default)","text":"<p>Simple text output:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\nresponse = model(\n    \"audio.mp3\",\n    response_format=\"text\"\n)\n\ntranscript = response.consume()\nprint(transcript[\"text\"])\n# \"This is the transcribed text.\"\n</code></pre>"},{"location":"learn/models/speech_to_text/#json-format","title":"JSON Format","text":"<p>Structured output:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\nresponse = model(\n    \"audio.mp3\",\n    response_format=\"json\"\n)\n\ntranscript = response.consume()\nprint(transcript)\n# {\"text\": \"This is the transcribed text.\"}\n</code></pre>"},{"location":"learn/models/speech_to_text/#verbose-json","title":"Verbose JSON","text":"<p>Detailed output with metadata:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\nresponse = model(\n    \"audio.mp3\",\n    response_format=\"verbose_json\"\n)\n\ntranscript = response.consume()\nprint(transcript)\n# {\n#     \"text\": \"This is the transcribed text.\",\n#     \"language\": \"en\",\n#     \"duration\": 5.2,\n#     \"segments\": [...]\n# }\n</code></pre>"},{"location":"learn/models/speech_to_text/#srt-subrip-format","title":"SRT (SubRip) Format","text":"<p>Subtitle format for videos:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\nresponse = model(\n    \"audio.mp3\",\n    response_format=\"srt\"\n)\n\ntranscript = response.consume()\nprint(transcript[\"text\"])\n# 1\n# 00:00:00,000 --&gt; 00:00:02,000\n# This is the first subtitle\n#\n# 2\n# 00:00:02,000 --&gt; 00:00:05,000\n# This is the second subtitle\n</code></pre>"},{"location":"learn/models/speech_to_text/#vtt-webvtt-format","title":"VTT (WebVTT) Format","text":"<p>Web-friendly subtitle format:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\nresponse = model(\n    \"audio.mp3\",\n    response_format=\"vtt\"\n)\n\ntranscript = response.consume()\nprint(transcript[\"text\"])\n# WEBVTT\n#\n# 00:00:00.000 --&gt; 00:00:02.000\n# This is the first subtitle\n#\n# 00:00:02.000 --&gt; 00:00:05.000\n# This is the second subtitle\n</code></pre>"},{"location":"learn/models/speech_to_text/#timestamp-granularities","title":"Timestamp Granularities","text":""},{"location":"learn/models/speech_to_text/#word-level-timestamps","title":"Word-Level Timestamps","text":"<p>Get timestamp for each word:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\nresponse = model(\n    \"audio.mp3\",\n    response_format=\"verbose_json\",\n    timestamp_granularities=[\"word\"]\n)\n\ntranscript = response.consume()\nprint(transcript[\"words\"])\n# [\n#     {\"word\": \"Hello\", \"start\": 0.0, \"end\": 0.5},\n#     {\"word\": \"world\", \"start\": 0.6, \"end\": 1.1}\n# ]\n</code></pre>"},{"location":"learn/models/speech_to_text/#segment-level-timestamps","title":"Segment-Level Timestamps","text":"<p>Get timestamps for phrases/segments:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\nresponse = model(\n    \"audio.mp3\",\n    response_format=\"verbose_json\",\n    timestamp_granularities=[\"segment\"]\n)\n\ntranscript = response.consume()\nprint(transcript[\"segments\"])\n# [\n#     {\"id\": 0, \"start\": 0.0, \"end\": 2.5, \"text\": \"Hello world.\"},\n#     {\"id\": 1, \"start\": 2.5, \"end\": 5.0, \"text\": \"How are you?\"}\n# ]\n</code></pre>"},{"location":"learn/models/speech_to_text/#both-word-and-segment-timestamps","title":"Both Word and Segment Timestamps","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\nresponse = model(\n    \"audio.mp3\",\n    response_format=\"verbose_json\",\n    timestamp_granularities=[\"word\", \"segment\"]\n)\n\ntranscript = response.consume()\nprint(\"Words:\", transcript[\"words\"])\nprint(\"Segments:\", transcript[\"segments\"])\n</code></pre>"},{"location":"learn/models/speech_to_text/#language-specification","title":"Language Specification","text":""},{"location":"learn/models/speech_to_text/#automatic-detection","title":"Automatic Detection","text":"<p>By default, Whisper auto-detects the language:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\nresponse = model(\"audio.mp3\")\ntranscript = response.consume()\n# Language automatically detected\n</code></pre>"},{"location":"learn/models/speech_to_text/#manual-language-specification","title":"Manual Language Specification","text":"<p>Improve accuracy and speed by specifying the language:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# English\nresponse = model(\"audio.mp3\", language=\"en\")\n\n# Spanish\nresponse = model(\"audio.mp3\", language=\"es\")\n\n# French\nresponse = model(\"audio.mp3\", language=\"fr\")\n\n# Japanese\nresponse = model(\"audio.mp3\", language=\"ja\")\n\n# Chinese\nresponse = model(\"audio.mp3\", language=\"zh\")\n</code></pre>"},{"location":"learn/models/speech_to_text/#iso-639-1-language-codes","title":"ISO 639-1 Language Codes","text":"<p>Common language codes: - <code>en</code> - English - <code>es</code> - Spanish - <code>fr</code> - French - <code>de</code> - German - <code>it</code> - Italian - <code>pt</code> - Portuguese - <code>ru</code> - Russian - <code>ja</code> - Japanese - <code>ko</code> - Korean - <code>zh</code> - Chinese - <code>ar</code> - Arabic - <code>hi</code> - Hindi</p>"},{"location":"learn/models/speech_to_text/#context-and-prompts","title":"Context and Prompts","text":"<p>Improve transcription accuracy with context:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# Technical content\nresponse = model(\n    \"meeting.mp3\",\n    prompt=\"This is a technical discussion about machine learning, neural networks, and AI\"\n)\n\n# Names and terminology\nresponse = model(\n    \"interview.mp3\",\n    prompt=\"Interview with Dr. Smith about quantum computing\"\n)\n\n# Continuing previous segment\nresponse = model(\n    \"part2.mp3\",\n    prompt=\"Previous text ended with: ...and that's how we solved the problem.\"\n)\n</code></pre>"},{"location":"learn/models/speech_to_text/#temperature-control","title":"Temperature Control","text":"<p>Control transcription randomness:</p> <pre><code>import msgflux as mf\n\n# Deterministic (temperature=0)\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\", temperature=0.0)\n\n# More creative (higher temperature)\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\", temperature=0.3)\n</code></pre> <p>Note: Lower temperature = more conservative/repetitive, Higher temperature = more creative but potentially less accurate.</p>"},{"location":"learn/models/speech_to_text/#streaming","title":"Streaming","text":"<p>Process transcription in real-time:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# Stream transcription\nresponse = model(\"long_audio.mp3\", stream=True)\n\n# Process chunks as they arrive\nfor chunk in response.consume():\n    if chunk is None:\n        break\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"learn/models/speech_to_text/#async-support","title":"Async Support","text":"<p>Transcribe audio asynchronously:</p> <pre><code>import msgflux as mf\nimport asyncio\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\nasync def transcribe_audio(audio_path):\n    response = await model.acall(audio_path)\n    return response.consume()\n\nasync def main():\n    # Transcribe multiple files concurrently\n    audio_files = [\"audio1.mp3\", \"audio2.mp3\", \"audio3.mp3\"]\n\n    tasks = [transcribe_audio(f) for f in audio_files]\n    transcripts = await asyncio.gather(*tasks)\n\n    for file, transcript in zip(audio_files, transcripts):\n        print(f\"{file}: {transcript['text']}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"learn/models/speech_to_text/#batch-processing","title":"Batch Processing","text":"<p>Transcribe multiple files:</p> <pre><code>import msgflux as mf\nimport msgflux.nn.functional as F\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\naudio_files = [\n    \"meeting1.mp3\",\n    \"meeting2.mp3\",\n    \"meeting3.mp3\"\n]\n\n# Process in parallel\nresults = F.map_gather(\n    model,\n    args_list=[(f,) for f in audio_files]\n)\n\n# Get all transcripts\nfor file, result in zip(audio_files, results):\n    transcript = result.consume()\n    print(f\"{file}:\")\n    print(transcript[\"text\"])\n    print()\n</code></pre>"},{"location":"learn/models/speech_to_text/#common-patterns","title":"Common Patterns","text":""},{"location":"learn/models/speech_to_text/#meeting-transcription","title":"Meeting Transcription","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ndef transcribe_meeting(audio_path, attendees=None):\n    \"\"\"Transcribe meeting with context.\"\"\"\n    prompt = \"\"\n    if attendees:\n        prompt = f\"Meeting with {', '.join(attendees)}\"\n\n    response = model(\n        audio_path,\n        prompt=prompt,\n        response_format=\"verbose_json\",\n        timestamp_granularities=[\"segment\"]\n    )\n\n    transcript = response.consume()\n\n    # Format output\n    output = f\"Meeting Transcript\\n{'='*50}\\n\\n\"\n\n    for segment in transcript.get(\"segments\", []):\n        timestamp = f\"[{segment['start']:.1f}s - {segment['end']:.1f}s]\"\n        output += f\"{timestamp}\\n{segment['text']}\\n\\n\"\n\n    return output\n\n# Use it\ntranscript = transcribe_meeting(\n    \"meeting.mp3\",\n    attendees=[\"Alice\", \"Bob\", \"Carol\"]\n)\nprint(transcript)\n</code></pre>"},{"location":"learn/models/speech_to_text/#subtitle-generation","title":"Subtitle Generation","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ndef generate_subtitles(video_audio_path, output_path):\n    \"\"\"Generate SRT subtitles for video.\"\"\"\n    response = model(\n        video_audio_path,\n        response_format=\"srt\",\n        language=\"en\"\n    )\n\n    transcript = response.consume()\n\n    # Save subtitles\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(transcript[\"text\"])\n\n    return output_path\n\n# Generate subtitles\nsubtitle_file = generate_subtitles(\"video_audio.mp3\", \"subtitles.srt\")\nprint(f\"Subtitles saved to: {subtitle_file}\")\n</code></pre>"},{"location":"learn/models/speech_to_text/#podcast-transcription","title":"Podcast Transcription","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ndef transcribe_podcast(audio_path, hosts=None, topic=None):\n    \"\"\"Transcribe podcast with metadata.\"\"\"\n    # Build context prompt\n    prompt_parts = []\n    if hosts:\n        prompt_parts.append(f\"Podcast hosts: {', '.join(hosts)}\")\n    if topic:\n        prompt_parts.append(f\"Topic: {topic}\")\n\n    prompt = \". \".join(prompt_parts) if prompt_parts else None\n\n    response = model(\n        audio_path,\n        prompt=prompt,\n        response_format=\"verbose_json\",\n        timestamp_granularities=[\"word\", \"segment\"]\n    )\n\n    return response.consume()\n\n# Transcribe\ntranscript = transcribe_podcast(\n    \"podcast.mp3\",\n    hosts=[\"Alice\", \"Bob\"],\n    topic=\"Artificial Intelligence\"\n)\n\nprint(\"Full text:\", transcript[\"text\"])\nprint(f\"Duration: {transcript.get('duration', 'N/A')} seconds\")\nprint(f\"Language: {transcript.get('language', 'N/A')}\")\n</code></pre>"},{"location":"learn/models/speech_to_text/#multi-language-support","title":"Multi-Language Support","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ndef transcribe_multilingual(audio_files_with_langs):\n    \"\"\"Transcribe multiple files in different languages.\"\"\"\n    results = {}\n\n    for audio_file, language in audio_files_with_langs:\n        response = model(audio_file, language=language)\n        transcript = response.consume()\n        results[audio_file] = {\n            \"language\": language,\n            \"text\": transcript[\"text\"]\n        }\n\n    return results\n\n# Transcribe files in different languages\nfiles_langs = [\n    (\"english.mp3\", \"en\"),\n    (\"spanish.mp3\", \"es\"),\n    (\"french.mp3\", \"fr\")\n]\n\ntranscripts = transcribe_multilingual(files_langs)\n\nfor file, data in transcripts.items():\n    print(f\"{file} ({data['language']}):\")\n    print(data['text'])\n    print()\n</code></pre>"},{"location":"learn/models/speech_to_text/#voice-command-processing","title":"Voice Command Processing","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ndef process_voice_command(audio_path):\n    \"\"\"Process voice command.\"\"\"\n    response = model(\n        audio_path,\n        prompt=\"Voice command for controlling smart home devices\",\n        language=\"en\",\n        temperature=0.0  # Deterministic\n    )\n\n    transcript = response.consume()\n    command_text = transcript[\"text\"].lower().strip()\n\n    # Parse command\n    if \"turn on\" in command_text:\n        device = command_text.replace(\"turn on\", \"\").strip()\n        return {\"action\": \"turn_on\", \"device\": device}\n    elif \"turn off\" in command_text:\n        device = command_text.replace(\"turn off\", \"\").strip()\n        return {\"action\": \"turn_off\", \"device\": device}\n    else:\n        return {\"action\": \"unknown\", \"text\": command_text}\n\n# Process command\ncommand = process_voice_command(\"command.mp3\")\nprint(command)\n# {\"action\": \"turn_on\", \"device\": \"the lights\"}\n</code></pre>"},{"location":"learn/models/speech_to_text/#search-transcripts","title":"Search Transcripts","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ndef search_in_audio(audio_path, search_term):\n    \"\"\"Search for specific content in audio.\"\"\"\n    response = model(\n        audio_path,\n        response_format=\"verbose_json\",\n        timestamp_granularities=[\"word\", \"segment\"]\n    )\n\n    transcript = response.consume()\n\n    # Search segments\n    results = []\n    for segment in transcript.get(\"segments\", []):\n        if search_term.lower() in segment[\"text\"].lower():\n            results.append({\n                \"timestamp\": f\"{segment['start']:.1f}s - {segment['end']:.1f}s\",\n                \"text\": segment[\"text\"]\n            })\n\n    return results\n\n# Search\nmatches = search_in_audio(\"meeting.mp3\", \"budget\")\nfor match in matches:\n    print(f\"[{match['timestamp']}] {match['text']}\")\n</code></pre>"},{"location":"learn/models/speech_to_text/#best-practices","title":"Best Practices","text":""},{"location":"learn/models/speech_to_text/#1-specify-language-when-known","title":"1. Specify Language When Known","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# Good - Faster and more accurate\nresponse = model(\"english_audio.mp3\", language=\"en\")\n\n# Less optimal - Requires language detection\nresponse = model(\"english_audio.mp3\")\n</code></pre>"},{"location":"learn/models/speech_to_text/#2-use-prompts-for-context","title":"2. Use Prompts for Context","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# Good - Provides context\nresponse = model(\n    \"tech_talk.mp3\",\n    prompt=\"Technical presentation about Kubernetes, Docker, and microservices\"\n)\n\n# Less optimal - No context\nresponse = model(\"tech_talk.mp3\")\n</code></pre>"},{"location":"learn/models/speech_to_text/#3-choose-appropriate-response-format","title":"3. Choose Appropriate Response Format","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# For simple transcription\nresponse = model(\"audio.mp3\", response_format=\"text\")\n\n# For subtitles\nresponse = model(\"video.mp3\", response_format=\"srt\")\n\n# For detailed analysis\nresponse = model(\"interview.mp3\", response_format=\"verbose_json\",\n                 timestamp_granularities=[\"word\", \"segment\"])\n</code></pre>"},{"location":"learn/models/speech_to_text/#4-handle-long-audio-files","title":"4. Handle Long Audio Files","text":"<pre><code>import msgflux as mf\nfrom pydub import AudioSegment\n\ndef split_audio(audio_path, chunk_length_ms=30000):\n    \"\"\"Split long audio into chunks.\"\"\"\n    audio = AudioSegment.from_file(audio_path)\n    chunks = []\n\n    for i in range(0, len(audio), chunk_length_ms):\n        chunk = audio[i:i + chunk_length_ms]\n        chunk_path = f\"/tmp/chunk_{i}.mp3\"\n        chunk.export(chunk_path, format=\"mp3\")\n        chunks.append(chunk_path)\n\n    return chunks\n\ndef transcribe_long_audio(audio_path):\n    \"\"\"Transcribe long audio file.\"\"\"\n    model = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n    # Split into chunks\n    chunks = split_audio(audio_path)\n\n    # Transcribe each chunk\n    full_transcript = \"\"\n    previous_text = \"\"\n\n    for chunk_path in chunks:\n        # Use previous text for context\n        response = model(\n            chunk_path,\n            prompt=previous_text[-500:] if previous_text else None\n        )\n        transcript = response.consume()\n        chunk_text = transcript[\"text\"]\n\n        full_transcript += chunk_text + \" \"\n        previous_text = chunk_text\n\n    return full_transcript.strip()\n\ntranscript = transcribe_long_audio(\"long_audio.mp3\")\n</code></pre>"},{"location":"learn/models/speech_to_text/#5-save-transcripts","title":"5. Save Transcripts","text":"<pre><code>import msgflux as mf\nimport json\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ndef save_transcript(audio_path, output_path):\n    \"\"\"Transcribe and save with metadata.\"\"\"\n    response = model(\n        audio_path,\n        response_format=\"verbose_json\",\n        timestamp_granularities=[\"word\", \"segment\"]\n    )\n\n    transcript = response.consume()\n\n    # Save to JSON\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(transcript, f, indent=2, ensure_ascii=False)\n\n    # Also save plain text\n    text_path = output_path.replace(\".json\", \".txt\")\n    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(transcript[\"text\"])\n\n    return output_path\n\nsave_transcript(\"meeting.mp3\", \"meeting_transcript.json\")\n</code></pre>"},{"location":"learn/models/speech_to_text/#audio-format-support","title":"Audio Format Support","text":""},{"location":"learn/models/speech_to_text/#supported-formats","title":"Supported Formats","text":"<ul> <li>MP3 (.mp3)</li> <li>MP4 (.mp4, audio track)</li> <li>MPEG (.mpeg)</li> <li>MPGA (.mpga)</li> <li>M4A (.m4a)</li> <li>WAV (.wav)</li> <li>WEBM (.webm)</li> </ul>"},{"location":"learn/models/speech_to_text/#file-size-limits","title":"File Size Limits","text":"<ul> <li>Maximum file size: 25 MB</li> <li>For larger files, split into chunks or compress</li> </ul>"},{"location":"learn/models/speech_to_text/#audio-preprocessing","title":"Audio Preprocessing","text":"<pre><code>from pydub import AudioSegment\n\ndef prepare_audio(input_path, output_path):\n    \"\"\"Prepare audio for transcription.\"\"\"\n    audio = AudioSegment.from_file(input_path)\n\n    # Normalize volume\n    audio = audio.normalize()\n\n    # Convert to mono if stereo\n    if audio.channels &gt; 1:\n        audio = audio.set_channels(1)\n\n    # Set sample rate to 16kHz (optimal for Whisper)\n    audio = audio.set_frame_rate(16000)\n\n    # Export as MP3\n    audio.export(output_path, format=\"mp3\", bitrate=\"64k\")\n\n    return output_path\n\n# Prepare and transcribe\nprepared = prepare_audio(\"raw_audio.wav\", \"prepared.mp3\")\n\nimport msgflux as mf\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\nresponse = model(prepared)\n</code></pre>"},{"location":"learn/models/speech_to_text/#error-handling","title":"Error Handling","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ntry:\n    response = model(\"audio.mp3\")\n    transcript = response.consume()\nexcept ImportError:\n    print(\"Provider not installed\")\nexcept ValueError as e:\n    print(f\"Invalid parameters: {e}\")\n    # Common issues:\n    # - Invalid language code\n    # - Invalid response_format\n    # - File too large (&gt;25MB)\nexcept FileNotFoundError:\n    print(\"Audio file not found\")\nexcept Exception as e:\n    print(f\"Transcription failed: {e}\")\n    # Common errors:\n    # - Unsupported audio format\n    # - Corrupted audio file\n    # - Network issues\n    # - Rate limits\n</code></pre>"},{"location":"learn/models/speech_to_text/#cost-optimization","title":"Cost Optimization","text":""},{"location":"learn/models/speech_to_text/#efficient-transcription","title":"Efficient Transcription","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# Use language specification to reduce processing time\nresponse = model(\"audio.mp3\", language=\"en\")\n\n# Use simple format when timestamps not needed\nresponse = model(\"audio.mp3\", response_format=\"text\")\n\n# Compress audio before uploading\nfrom pydub import AudioSegment\n\naudio = AudioSegment.from_file(\"original.wav\")\naudio.export(\"compressed.mp3\", format=\"mp3\", bitrate=\"64k\")\nresponse = model(\"compressed.mp3\")\n</code></pre>"},{"location":"learn/models/speech_to_text/#see-also","title":"See Also","text":"<ul> <li>Text to Speech - Convert text to audio</li> <li>Chat Completion - Process transcripts with LLMs</li> <li>Model - Model factory and registry</li> </ul>"},{"location":"learn/models/text_embedder/","title":"Text Embedder","text":"<p>The <code>text_embedder</code> model transforms text into dense vector representations (embeddings) that capture semantic meaning. These vectors enable similarity search, semantic retrieval, clustering, and classification tasks.</p> <p>All code examples use the recommended import pattern:</p> <pre><code>import msgflux as mf\n</code></pre>"},{"location":"learn/models/text_embedder/#overview","title":"Overview","text":"<p>Text embeddings convert sentences, paragraphs, or documents into numerical vectors that encode their semantic meaning. Unlike simple word counts or TF-IDF, embeddings capture:</p> <ul> <li>Semantic similarity: Similar meanings have similar vectors</li> <li>Contextual understanding: Same words in different contexts get different embeddings</li> <li>Dimensionality reduction: High-dimensional text \u2192 fixed-size vectors</li> </ul>"},{"location":"learn/models/text_embedder/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Semantic Search: Find documents similar to a query</li> <li>RAG (Retrieval-Augmented Generation): Retrieve relevant context for LLMs</li> <li>Clustering: Group similar documents</li> <li>Classification: Train classifiers on embeddings</li> <li>Recommendation: Find similar items</li> </ul>"},{"location":"learn/models/text_embedder/#quick-start","title":"Quick Start","text":""},{"location":"learn/models/text_embedder/#basic-usage","title":"Basic Usage","text":"<pre><code>import msgflux as mf\n\n# Create embedder\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\n# Generate embedding\nresponse = embedder(\"Hello, world!\")\n\n# Get vector\nembedding = response.consume()\nprint(len(embedding))  # 1536\nprint(embedding[:5])   # [0.123, -0.456, 0.789, -0.234, 0.567]\n</code></pre>"},{"location":"learn/models/text_embedder/#with-custom-dimensions","title":"With Custom Dimensions","text":"<pre><code>import msgflux as mf\n\n# OpenAI models support custom dimensions\nembedder = mf.Model.text_embedder(\n    \"openai/text-embedding-3-small\",\n    dimensions=256  # Reduce from 1536 to 256\n)\n\nresponse = embedder(\"Compact embedding\")\nembedding = response.consume()\nprint(len(embedding))  # 256\n</code></pre>"},{"location":"learn/models/text_embedder/#supported-providers","title":"Supported Providers","text":""},{"location":"learn/models/text_embedder/#openai","title":"OpenAI","text":"<pre><code>import msgflux as mf\n\n# text-embedding-3-small (1536 dims, $0.02/1M tokens)\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\n# text-embedding-3-large (3072 dims, $0.13/1M tokens)\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-large\")\n\n# Legacy ada-002 (1536 dims, $0.10/1M tokens)\nembedder = mf.Model.text_embedder(\"openai/text-embedding-ada-002\")\n</code></pre>"},{"location":"learn/models/text_embedder/#jina-ai","title":"Jina AI","text":"<pre><code>import msgflux as mf\n\n# Specialized embedding models\nembedder = mf.Model.text_embedder(\"jinaai/jina-embeddings-v3\")\n</code></pre>"},{"location":"learn/models/text_embedder/#together-ai","title":"Together AI","text":"<pre><code>import msgflux as mf\n\n# Open-source embeddings\nembedder = mf.Model.text_embedder(\"together/togethercomputer/m2-bert-80M-8k-retrieval\")\n</code></pre>"},{"location":"learn/models/text_embedder/#local-ollama","title":"Local (Ollama)","text":"<pre><code>import msgflux as mf\n\n# Local embeddings with Ollama\nembedder = mf.Model.text_embedder(\"ollama/nomic-embed-text\")\n</code></pre>"},{"location":"learn/models/text_embedder/#local-vllm","title":"Local (vLLM)","text":"<pre><code>import msgflux as mf\n\n# Self-hosted with vLLM\nembedder = mf.Model.text_embedder(\n    \"vllm/BAAI/bge-small-en-v1.5\",\n    base_url=\"http://localhost:8000\"\n)\n</code></pre>"},{"location":"learn/models/text_embedder/#batch-processing","title":"Batch Processing","text":"<p>Process multiple texts efficiently:</p>"},{"location":"learn/models/text_embedder/#sequential-processing","title":"Sequential Processing","text":"<pre><code>import msgflux as mf\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\ntexts = [\n    \"The quick brown fox\",\n    \"jumps over the lazy dog\",\n    \"Machine learning is amazing\"\n]\n\nembeddings = []\nfor text in texts:\n    response = embedder(text)\n    embeddings.append(response.consume())\n\nprint(f\"Generated {len(embeddings)} embeddings\")\n</code></pre>"},{"location":"learn/models/text_embedder/#parallel-processing","title":"Parallel Processing","text":"<pre><code>import msgflux as mf\nimport msgflux.nn.functional as F\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\ntexts = [\n    \"Text 1\",\n    \"Text 2\",\n    \"Text 3\",\n    \"Text 4\"\n]\n\n# Process in parallel\nresults = F.map_gather(\n    embedder,\n    args_list=[(text,) for text in texts]\n)\n\n# Extract embeddings\nembeddings = [r.consume() for r in results]\nprint(f\"Generated {len(embeddings)} embeddings in parallel\")\n</code></pre>"},{"location":"learn/models/text_embedder/#async-batch-processing","title":"Async Batch Processing","text":"<pre><code>import msgflux as mf\nimport asyncio\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\nasync def embed_batch(texts):\n    tasks = [embedder.acall(text) for text in texts]\n    responses = await asyncio.gather(*tasks)\n    return [r.consume() for r in responses]\n\ntexts = [\"Text 1\", \"Text 2\", \"Text 3\"]\nembeddings = asyncio.run(embed_batch(texts))\n</code></pre>"},{"location":"learn/models/text_embedder/#response-caching","title":"Response Caching","text":"<p>Cache embeddings to avoid redundant API calls:</p>"},{"location":"learn/models/text_embedder/#enabling-cache","title":"Enabling Cache","text":"<pre><code>import msgflux as mf\n\n# Enable cache (highly recommended for embeddings)\nembedder = mf.Model.text_embedder(\n    \"openai/text-embedding-3-small\",\n    enable_cache=True,\n    cache_size=1000  # Cache up to 1000 embeddings\n)\n\n# First call - hits API\nresponse1 = embedder(\"machine learning\")\nprint(response1.consume()[:5])\n\n# Second call - returns cached result\nresponse2 = embedder(\"machine learning\")\nprint(response2.consume()[:5])  # Same result, no API call\n\n# Check cache stats\nif embedder._response_cache:\n    stats = embedder._response_cache.cache_info()\n    print(f\"Cache hits: {stats['hits']}\")\n    print(f\"Cache misses: {stats['misses']}\")\n</code></pre>"},{"location":"learn/models/text_embedder/#working-with-embeddings","title":"Working with Embeddings","text":""},{"location":"learn/models/text_embedder/#cosine-similarity","title":"Cosine Similarity","text":"<pre><code>import msgflux as mf\nimport numpy as np\n\ndef cosine_similarity(a, b):\n    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\n# Generate embeddings\ntext1 = \"I love machine learning\"\ntext2 = \"Machine learning is great\"\ntext3 = \"The weather is nice today\"\n\nemb1 = embedder(text1).consume()\nemb2 = embedder(text2).consume()\nemb3 = embedder(text3).consume()\n\n# Calculate similarities\nsim_1_2 = cosine_similarity(emb1, emb2)\nsim_1_3 = cosine_similarity(emb1, emb3)\n\nprint(f\"Similarity (text1, text2): {sim_1_2:.4f}\")  # ~0.85\nprint(f\"Similarity (text1, text3): {sim_1_3:.4f}\")  # ~0.30\n</code></pre>"},{"location":"learn/models/text_embedder/#semantic-search","title":"Semantic Search","text":"<pre><code>import msgflux as mf\nimport numpy as np\n\ndef semantic_search(query, documents, embedder, top_k=3):\n    \"\"\"Find most similar documents to query.\"\"\"\n\n    # Embed query\n    query_emb = np.array(embedder(query).consume())\n\n    # Embed all documents\n    doc_embs = [np.array(embedder(doc).consume()) for doc in documents]\n\n    # Calculate similarities\n    similarities = [\n        np.dot(query_emb, doc_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(doc_emb))\n        for doc_emb in doc_embs\n    ]\n\n    # Get top-k\n    top_indices = np.argsort(similarities)[-top_k:][::-1]\n\n    return [(documents[i], similarities[i]) for i in top_indices]\n\n# Example usage\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\", enable_cache=True)\n\ndocuments = [\n    \"Python is a programming language\",\n    \"Machine learning uses algorithms\",\n    \"The weather is sunny today\",\n    \"Neural networks are powerful\",\n    \"I like to eat pizza\"\n]\n\nquery = \"Tell me about AI and ML\"\nresults = semantic_search(query, documents, embedder)\n\nfor doc, score in results:\n    print(f\"{score:.4f}: {doc}\")\n# 0.7234: Machine learning uses algorithms\n# 0.6891: Neural networks are powerful\n# 0.4123: Python is a programming language\n</code></pre>"},{"location":"learn/models/text_embedder/#rag-integration","title":"RAG Integration","text":"<p>Embeddings are essential for Retrieval-Augmented Generation:</p>"},{"location":"learn/models/text_embedder/#building-a-simple-rag-system","title":"Building a Simple RAG System","text":"<pre><code>import msgflux as mf\nimport numpy as np\n\nclass SimpleRAG:\n    def __init__(self, embedder_model, chat_model):\n        self.embedder = embedder_model\n        self.chat = chat_model\n        self.documents = []\n        self.embeddings = []\n\n    def add_documents(self, docs):\n        \"\"\"Add documents to knowledge base.\"\"\"\n        self.documents.extend(docs)\n\n        # Generate embeddings\n        for doc in docs:\n            emb = self.embedder(doc).consume()\n            self.embeddings.append(np.array(emb))\n\n    def retrieve(self, query, top_k=3):\n        \"\"\"Retrieve most relevant documents.\"\"\"\n        query_emb = np.array(self.embedder(query).consume())\n\n        similarities = [\n            np.dot(query_emb, doc_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(doc_emb))\n            for doc_emb in self.embeddings\n        ]\n\n        top_indices = np.argsort(similarities)[-top_k:][::-1]\n        return [self.documents[i] for i in top_indices]\n\n    def query(self, question):\n        \"\"\"Ask question with RAG.\"\"\"\n        # Retrieve relevant docs\n        context_docs = self.retrieve(question)\n        context = \"\\n\\n\".join(context_docs)\n\n        # Generate answer with context\n        prompt = f\"\"\"Answer the question based on the context below.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n\n        response = self.chat(messages=[{\"role\": \"user\", \"content\": prompt}])\n        return response.consume()\n\n# Usage\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\", enable_cache=True)\nchat = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nrag = SimpleRAG(embedder, chat)\n\n# Add knowledge\nrag.add_documents([\n    \"msgflux is a Python library for building AI systems.\",\n    \"The Model class provides unified access to different AI providers.\",\n    \"AutoParams allows dataclass-style module definitions.\",\n    \"msgflux supports OpenAI, Anthropic, and Google models.\"\n])\n\n# Ask questions\nanswer = rag.query(\"What is msgflux?\")\nprint(answer)\n</code></pre>"},{"location":"learn/models/text_embedder/#dimensions-and-performance","title":"Dimensions and Performance","text":""},{"location":"learn/models/text_embedder/#choosing-dimensions","title":"Choosing Dimensions","text":"<pre><code>import msgflux as mf\n\n# Higher dimensions = better accuracy, more storage/compute\nembedder_large = mf.Model.text_embedder(\n    \"openai/text-embedding-3-large\",\n    dimensions=3072  # Full size\n)\n\n# Lower dimensions = faster, less storage, slightly lower accuracy\nembedder_small = mf.Model.text_embedder(\n    \"openai/text-embedding-3-small\",\n    dimensions=256   # Reduced from 1536\n)\n\n# Trade-off example:\n# - 3072 dims: 99% accuracy, 12x storage\n# - 1536 dims: 98% accuracy, 6x storage\n# - 512 dims:  95% accuracy, 2x storage\n# - 256 dims:  92% accuracy, 1x storage\n</code></pre>"},{"location":"learn/models/text_embedder/#performance-comparison","title":"Performance Comparison","text":"<pre><code>import time\nimport msgflux as mf\n\ntext = \"Sample text for embedding\"\n\n# Full dimensions\nembedder_full = mf.Model.text_embedder(\n    \"openai/text-embedding-3-small\",\n    dimensions=1536\n)\n\nstart = time.time()\nemb_full = embedder_full(text).consume()\ntime_full = time.time() - start\n\n# Reduced dimensions\nembedder_reduced = mf.Model.text_embedder(\n    \"openai/text-embedding-3-small\",\n    dimensions=256\n)\n\nstart = time.time()\nemb_reduced = embedder_reduced(text).consume()\ntime_reduced = time.time() - start\n\nprint(f\"Full (1536d): {time_full:.4f}s, {len(emb_full)} dims\")\nprint(f\"Reduced (256d): {time_reduced:.4f}s, {len(emb_reduced)} dims\")\nprint(f\"Size reduction: {len(emb_reduced)/len(emb_full)*100:.1f}%\")\n</code></pre>"},{"location":"learn/models/text_embedder/#response-metadata","title":"Response Metadata","text":"<p>Access usage and cost information:</p> <pre><code>import msgflux as mf\nfrom msgflux.models.profiles import get_model_profile\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\nresponse = embedder(\"This is a test sentence\")\n\n# Check metadata\nprint(response.metadata)\n# {'usage': {'prompt_tokens': 5, 'total_tokens': 5}}\n\n# Calculate cost\nprofile = get_model_profile(\"text-embedding-3-small\", provider_id=\"openai\")\nif profile:\n    tokens = response.metadata.usage.total_tokens\n    cost = tokens * profile.cost.input_per_token\n    print(f\"Cost: ${cost:.6f}\")\n</code></pre>"},{"location":"learn/models/text_embedder/#best-practices","title":"Best Practices","text":""},{"location":"learn/models/text_embedder/#1-enable-caching","title":"1. Enable Caching","text":"<pre><code># Good - Cache embeddings (they're deterministic)\nembedder = mf.Model.text_embedder(\n    \"openai/text-embedding-3-small\",\n    enable_cache=True,\n    cache_size=5000  # Adjust based on corpus size\n)\n</code></pre>"},{"location":"learn/models/text_embedder/#2-batch-similar-texts","title":"2. Batch Similar Texts","text":"<pre><code># Good - Process similar workloads together\nimport msgflux.nn.functional as F\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\n# Batch all product descriptions\nproduct_embeddings = F.map_gather(\n    embedder,\n    args_list=[(desc,) for desc in product_descriptions]\n)\n\n# Batch all user queries separately\nquery_embeddings = F.map_gather(\n    embedder,\n    args_list=[(query,) for query in user_queries]\n)\n</code></pre>"},{"location":"learn/models/text_embedder/#3-choose-appropriate-dimensions","title":"3. Choose Appropriate Dimensions","text":"<pre><code># Good - Balance accuracy vs performance/cost\n# For production search with millions of vectors:\nembedder = mf.Model.text_embedder(\n    \"openai/text-embedding-3-small\",\n    dimensions=512  # Good balance\n)\n\n# For high-accuracy semantic tasks:\nembedder = mf.Model.text_embedder(\n    \"openai/text-embedding-3-large\",\n    dimensions=3072  # Maximum accuracy\n)\n</code></pre>"},{"location":"learn/models/text_embedder/#4-normalize-for-cosine-similarity","title":"4. Normalize for Cosine Similarity","text":"<pre><code>import numpy as np\n\ndef normalize_embedding(embedding):\n    \"\"\"Normalize embedding for cosine similarity.\"\"\"\n    embedding = np.array(embedding)\n    return embedding / np.linalg.norm(embedding)\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\n# Normalize embeddings once\nemb = normalize_embedding(embedder(\"text\").consume())\n\n# Now dot product = cosine similarity\nsimilarity = np.dot(emb1, emb2)  # Faster than full cosine formula\n</code></pre>"},{"location":"learn/models/text_embedder/#5-handle-long-texts","title":"5. Handle Long Texts","text":"<pre><code>import msgflux as mf\n\ndef chunk_text(text, max_tokens=8000):\n    \"\"\"Split text into chunks under token limit.\"\"\"\n    # Simple word-based chunking (use tiktoken for accurate count)\n    words = text.split()\n    chunks = []\n    current_chunk = []\n\n    for word in words:\n        current_chunk.append(word)\n        if len(current_chunk) &gt;= max_tokens // 4:  # Rough estimate\n            chunks.append(\" \".join(current_chunk))\n            current_chunk = []\n\n    if current_chunk:\n        chunks.append(\" \".join(current_chunk))\n\n    return chunks\n\ndef embed_long_text(text, embedder):\n    \"\"\"Embed long text by chunking and averaging.\"\"\"\n    chunks = chunk_text(text)\n\n    embeddings = []\n    for chunk in chunks:\n        emb = embedder(chunk).consume()\n        embeddings.append(np.array(emb))\n\n    # Average embeddings\n    return np.mean(embeddings, axis=0).tolist()\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\nlong_text = \"...\" * 10000  # Very long text\n\nembedding = embed_long_text(long_text, embedder)\n</code></pre>"},{"location":"learn/models/text_embedder/#common-patterns","title":"Common Patterns","text":""},{"location":"learn/models/text_embedder/#document-deduplication","title":"Document Deduplication","text":"<pre><code>import msgflux as mf\nimport numpy as np\n\ndef find_duplicates(documents, embedder, threshold=0.95):\n    \"\"\"Find duplicate documents based on embedding similarity.\"\"\"\n    embeddings = [\n        np.array(embedder(doc).consume())\n        for doc in documents\n    ]\n\n    duplicates = []\n    for i in range(len(documents)):\n        for j in range(i + 1, len(documents)):\n            similarity = np.dot(embeddings[i], embeddings[j]) / (\n                np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j])\n            )\n\n            if similarity &gt;= threshold:\n                duplicates.append((i, j, similarity))\n\n    return duplicates\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\", enable_cache=True)\n\ndocs = [\n    \"Python is great\",\n    \"Python is awesome\",  # Very similar\n    \"Java is also good\",\n    \"The weather is nice\"\n]\n\ndupes = find_duplicates(docs, embedder)\nfor i, j, sim in dupes:\n    print(f\"Documents {i} and {j} are {sim:.2%} similar\")\n</code></pre>"},{"location":"learn/models/text_embedder/#clustering","title":"Clustering","text":"<pre><code>import msgflux as mf\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\ndef cluster_documents(documents, embedder, n_clusters=3):\n    \"\"\"Cluster documents using K-means on embeddings.\"\"\"\n    # Generate embeddings\n    embeddings = [\n        embedder(doc).consume()\n        for doc in documents\n    ]\n\n    # Cluster\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    labels = kmeans.fit_predict(embeddings)\n\n    # Group by cluster\n    clusters = {i: [] for i in range(n_clusters)}\n    for doc, label in zip(documents, labels):\n        clusters[label].append(doc)\n\n    return clusters\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\ndocs = [\n    \"Python programming\",\n    \"Machine learning with Python\",\n    \"Cooking recipes\",\n    \"Italian cuisine\",\n    \"Deep learning tutorial\",\n    \"Baking bread\"\n]\n\nclusters = cluster_documents(docs, embedder, n_clusters=2)\nfor cluster_id, docs in clusters.items():\n    print(f\"\\nCluster {cluster_id}:\")\n    for doc in docs:\n        print(f\"  - {doc}\")\n</code></pre>"},{"location":"learn/models/text_embedder/#error-handling","title":"Error Handling","text":"<pre><code>import msgflux as mf\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\ntry:\n    response = embedder(\"Some text\")\n    embedding = response.consume()\nexcept ImportError:\n    print(\"Provider not installed\")\nexcept ValueError as e:\n    print(f\"Invalid input: {e}\")\nexcept Exception as e:\n    print(f\"API error: {e}\")\n</code></pre>"},{"location":"learn/models/text_embedder/#see-also","title":"See Also","text":"<ul> <li>Model - Model factory and registry</li> <li>Chat Completion - Chat models</li> <li>Data Retrievers - Vector databases and retrievers</li> <li>RAG Patterns - RAG implementation patterns</li> </ul>"},{"location":"learn/models/text_to_image/","title":"Text to Image","text":"<p>The <code>text_to_image</code> model generates images from text descriptions. These models can create photorealistic images, artwork, illustrations, and more from natural language prompts.</p> <p>All code examples use the recommended import pattern:</p> <pre><code>import msgflux as mf\n</code></pre>"},{"location":"learn/models/text_to_image/#overview","title":"Overview","text":"<p>Text-to-image models transform textual descriptions into visual content. They enable:</p> <ul> <li>Image Generation: Create images from scratch using text prompts</li> <li>Style Control: Generate images in specific artistic styles</li> <li>Variations: Create multiple variations of the same concept</li> <li>Quality Control: Adjust output quality and resolution</li> </ul>"},{"location":"learn/models/text_to_image/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Content Creation: Marketing materials, social media posts</li> <li>Prototyping: Visual concepts for design projects</li> <li>Art Generation: Digital artwork and illustrations</li> <li>Product Visualization: Product mockups and concepts</li> <li>Education: Visual aids and explanations</li> </ul>"},{"location":"learn/models/text_to_image/#quick-start","title":"Quick Start","text":""},{"location":"learn/models/text_to_image/#basic-usage","title":"Basic Usage","text":"<pre><code>import msgflux as mf\n\n# Create image generator\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\n# Generate image\nresponse = model(prompt=\"A serene lake at sunset with mountains\")\n\n# Get image URL\nimage_url = response.consume()\nprint(image_url)  # https://...\n</code></pre>"},{"location":"learn/models/text_to_image/#with-custom-parameters","title":"With Custom Parameters","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\nresponse = model(\n    prompt=\"A futuristic city with flying cars\",\n    size=\"1792x1024\",      # Landscape\n    quality=\"hd\",           # High quality\n    n=1                     # Number of images\n)\n\nimage_url = response.consume()\n</code></pre>"},{"location":"learn/models/text_to_image/#supported-providers","title":"Supported Providers","text":""},{"location":"learn/models/text_to_image/#openai-dall-e","title":"OpenAI (DALL-E)","text":"<pre><code>import msgflux as mf\n\n# DALL-E 3 (highest quality)\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\n# DALL-E 2 (faster, cheaper)\nmodel = mf.Model.text_to_image(\"openai/dall-e-2\")\n</code></pre>"},{"location":"learn/models/text_to_image/#replicate","title":"Replicate","text":"<pre><code>import msgflux as mf\n\n# Stable Diffusion and other models\nmodel = mf.Model.text_to_image(\"replicate/stability-ai/sdxl\")\n</code></pre>"},{"location":"learn/models/text_to_image/#imagerouter","title":"ImageRouter","text":"<pre><code>import msgflux as mf\n\n# Router to multiple image generation providers\nmodel = mf.Model.text_to_image(\"imagerouter/default\")\n</code></pre>"},{"location":"learn/models/text_to_image/#image-sizes","title":"Image Sizes","text":""},{"location":"learn/models/text_to_image/#dall-e-3-sizes","title":"DALL-E 3 Sizes","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\n# Square (default)\nresponse = model(\n    prompt=\"A cat wearing sunglasses\",\n    size=\"1024x1024\"\n)\n\n# Landscape\nresponse = model(\n    prompt=\"A panoramic mountain view\",\n    size=\"1792x1024\"\n)\n\n# Portrait\nresponse = model(\n    prompt=\"A full-length portrait\",\n    size=\"1024x1792\"\n)\n</code></pre>"},{"location":"learn/models/text_to_image/#dall-e-2-sizes","title":"DALL-E 2 Sizes","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-2\")\n\n# Available sizes for DALL-E 2\nsizes = [\"256x256\", \"512x512\", \"1024x1024\"]\n\nresponse = model(\n    prompt=\"A robot playing piano\",\n    size=\"1024x1024\"\n)\n</code></pre>"},{"location":"learn/models/text_to_image/#quality-settings","title":"Quality Settings","text":""},{"location":"learn/models/text_to_image/#hd-quality","title":"HD Quality","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\n# Standard quality (faster, cheaper)\nresponse = model(\n    prompt=\"A detailed landscape\",\n    quality=\"standard\"\n)\n\n# HD quality (more detail, higher cost)\nresponse = model(\n    prompt=\"A detailed landscape\",\n    quality=\"hd\"\n)\n</code></pre>"},{"location":"learn/models/text_to_image/#response-formats","title":"Response Formats","text":""},{"location":"learn/models/text_to_image/#url-response-default","title":"URL Response (Default)","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\nresponse = model(\n    prompt=\"A sunset over the ocean\",\n    response_format=\"url\"  # Default\n)\n\n# Get URL\nimage_url = response.consume()\nprint(image_url)  # https://...\n\n# Download image\nimport requests\nimg_data = requests.get(image_url).content\nwith open(\"image.png\", \"wb\") as f:\n    f.write(img_data)\n</code></pre>"},{"location":"learn/models/text_to_image/#base64-response","title":"Base64 Response","text":"<pre><code>import msgflux as mf\nimport base64\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\nresponse = model(\n    prompt=\"A colorful abstract painting\",\n    response_format=\"base64\"\n)\n\n# Get base64 data\nb64_data = response.consume()\n\n# Decode and save\nimg_data = base64.b64decode(b64_data)\nwith open(\"image.png\", \"wb\") as f:\n    f.write(img_data)\n</code></pre>"},{"location":"learn/models/text_to_image/#multiple-images","title":"Multiple Images","text":"<p>Generate multiple variations:</p> <pre><code>import msgflux as mf\n\n# DALL-E 2 supports multiple images\nmodel = mf.Model.text_to_image(\"openai/dall-e-2\")\n\nresponse = model(\n    prompt=\"A cute robot\",\n    n=4,  # Generate 4 variations\n    size=\"512x512\"\n)\n\n# Get all image URLs\nimages = response.consume()\nprint(f\"Generated {len(images)} images\")\n\nfor i, url in enumerate(images):\n    print(f\"Image {i+1}: {url}\")\n</code></pre>"},{"location":"learn/models/text_to_image/#background-control","title":"Background Control","text":"<p>Control background transparency (DALL-E 3):</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\n# Transparent background\nresponse = model(\n    prompt=\"A red apple\",\n    background=\"transparent\"\n)\n\n# Opaque background\nresponse = model(\n    prompt=\"A red apple\",\n    background=\"opaque\"\n)\n\n# Auto (model decides)\nresponse = model(\n    prompt=\"A red apple\",\n    background=\"auto\"  # Default\n)\n</code></pre>"},{"location":"learn/models/text_to_image/#content-moderation","title":"Content Moderation","text":"<p>Control content filtering:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\n    \"openai/dall-e-3\",\n    moderation=\"auto\"  # Auto moderation (default)\n)\n\n# Low moderation (more permissive)\nmodel_low = mf.Model.text_to_image(\n    \"openai/dall-e-3\",\n    moderation=\"low\"\n)\n\nresponse = model(prompt=\"Your prompt here\")\n</code></pre>"},{"location":"learn/models/text_to_image/#async-support","title":"Async Support","text":"<p>Generate images asynchronously:</p> <pre><code>import msgflux as mf\nimport asyncio\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\nasync def generate_image(prompt):\n    response = await model.acall(prompt=prompt, size=\"1024x1024\")\n    return response.consume()\n\n# Generate multiple images concurrently\nasync def main():\n    prompts = [\n        \"A serene lake\",\n        \"A bustling city\",\n        \"A quiet forest\"\n    ]\n\n    tasks = [generate_image(p) for p in prompts]\n    images = await asyncio.gather(*tasks)\n\n    for prompt, url in zip(prompts, images):\n        print(f\"{prompt}: {url}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"learn/models/text_to_image/#response-metadata","title":"Response Metadata","text":"<p>Access generation metadata:</p> <pre><code>import msgflux as mf\nfrom msgflux.models.profiles import get_model_profile\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\nresponse = model(\n    prompt=\"A beautiful sunset\",\n    size=\"1024x1024\",\n    quality=\"hd\"\n)\n\n# Access metadata\nprint(response.metadata)\n# {\n#     'created': 1234567890,\n#     'revised_prompt': 'A picturesque sunset over the ocean...',\n#     'content_filter_results': {...}\n# }\n\n# Get revised prompt (DALL-E often enhances your prompt)\nrevised = response.metadata.get('revised_prompt')\nprint(f\"Revised prompt: {revised}\")\n\n# Calculate cost\nprofile = get_model_profile(\"dall-e-3\", provider_id=\"openai\")\nif profile:\n    # Image generation pricing is per-image, not per-token\n    print(f\"Cost per image: ${profile.cost.input_per_million / 1000:.4f}\")\n</code></pre>"},{"location":"learn/models/text_to_image/#prompt-engineering","title":"Prompt Engineering","text":""},{"location":"learn/models/text_to_image/#effective-prompts","title":"Effective Prompts","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\n# Good - Specific and detailed\nresponse = model(\n    prompt=\"\"\"A professional photograph of a modern minimalist living room with:\n    - Large floor-to-ceiling windows\n    - Natural light streaming in\n    - Scandinavian furniture\n    - Indoor plants\n    - Neutral color palette\n    - Shot with a wide-angle lens\"\"\"\n)\n\n# Less effective - Too vague\nresponse = model(prompt=\"A nice room\")\n</code></pre>"},{"location":"learn/models/text_to_image/#style-specifications","title":"Style Specifications","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\n# Different art styles\nstyles = {\n    \"photorealistic\": \"A photorealistic portrait of a woman, studio lighting, 85mm lens\",\n    \"oil painting\": \"An oil painting of a countryside landscape in the style of Van Gogh\",\n    \"digital art\": \"A digital art illustration of a fantasy castle, vibrant colors\",\n    \"3d render\": \"A 3D render of a futuristic car, octane render, high detail\",\n    \"sketch\": \"A pencil sketch of a cat, detailed crosshatching\",\n}\n\nfor style_name, prompt in styles.items():\n    response = model(prompt=prompt)\n    print(f\"{style_name}: {response.consume()}\")\n</code></pre>"},{"location":"learn/models/text_to_image/#composition-control","title":"Composition Control","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\n# Control composition with detailed descriptions\nprompts = [\n    # Rule of thirds\n    \"A lone tree positioned on the right third of the image, sunset on the left\",\n\n    # Centered composition\n    \"A symmetrical view of a building, centered composition, front view\",\n\n    # Leading lines\n    \"A road leading into the distance toward mountains, vanishing point\",\n\n    # Foreground/background\n    \"A flower in sharp focus in the foreground, blurred forest in background\"\n]\n\nfor prompt in prompts:\n    response = model(prompt=prompt)\n    print(response.consume())\n</code></pre>"},{"location":"learn/models/text_to_image/#common-patterns","title":"Common Patterns","text":""},{"location":"learn/models/text_to_image/#batch-generation","title":"Batch Generation","text":"<pre><code>import msgflux as mf\nimport msgflux.nn.functional as F\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\nprompts = [\n    \"A red sports car\",\n    \"A blue mountain bike\",\n    \"A green sailboat\",\n    \"A yellow airplane\"\n]\n\n# Generate in parallel\nresults = F.map_gather(\n    model,\n    args_list=[(prompt,) for prompt in prompts]\n)\n\n# Get all URLs\nimage_urls = [r.consume() for r in results]\n\nfor prompt, url in zip(prompts, image_urls):\n    print(f\"{prompt}: {url}\")\n</code></pre>"},{"location":"learn/models/text_to_image/#download-and-save","title":"Download and Save","text":"<pre><code>import msgflux as mf\nimport requests\nfrom pathlib import Path\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\ndef generate_and_save(prompt, filename):\n    \"\"\"Generate image and save to file.\"\"\"\n    response = model(prompt=prompt, quality=\"hd\")\n    url = response.consume()\n\n    # Download\n    img_data = requests.get(url).content\n\n    # Save\n    output_path = Path(filename)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(output_path, \"wb\") as f:\n        f.write(img_data)\n\n    print(f\"Saved: {output_path}\")\n    return output_path\n\n# Usage\ngenerate_and_save(\"A sunset over the ocean\", \"outputs/sunset.png\")\ngenerate_and_save(\"A forest path\", \"outputs/forest.png\")\n</code></pre>"},{"location":"learn/models/text_to_image/#variation-generation","title":"Variation Generation","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-2\")  # DALL-E 2 for variations\n\nbase_prompt = \"A cozy coffee shop interior\"\n\n# Generate variations\nvariations = [\n    f\"{base_prompt}, morning light\",\n    f\"{base_prompt}, evening ambiance\",\n    f\"{base_prompt}, bustling with people\",\n    f\"{base_prompt}, empty and quiet\"\n]\n\nimages = []\nfor prompt in variations:\n    response = model(prompt=prompt, size=\"1024x1024\")\n    images.append(response.consume())\n\nprint(f\"Generated {len(images)} variations\")\n</code></pre>"},{"location":"learn/models/text_to_image/#iterative-refinement","title":"Iterative Refinement","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\n# Start with base concept\nprompt = \"A robot\"\n\n# Iteratively add details\nrefinements = [\n    \"A humanoid robot\",\n    \"A humanoid robot in a workshop\",\n    \"A humanoid robot in a workshop, holding tools\",\n    \"A humanoid robot in a workshop, holding tools, dramatic lighting\"\n]\n\nfor i, refined_prompt in enumerate(refinements):\n    response = model(prompt=refined_prompt)\n    print(f\"Iteration {i+1}: {response.consume()}\")\n    print(f\"Revised: {response.metadata.get('revised_prompt', 'N/A')}\\n\")\n</code></pre>"},{"location":"learn/models/text_to_image/#best-practices","title":"Best Practices","text":""},{"location":"learn/models/text_to_image/#1-be-specific-and-detailed","title":"1. Be Specific and Detailed","text":"<pre><code># Good - Specific details\nprompt = \"\"\"A professional food photograph of a gourmet burger:\n- Brioche bun with sesame seeds\n- Perfectly grilled beef patty\n- Fresh lettuce, tomato, and onion\n- Melted cheddar cheese\n- Wooden cutting board\n- Natural daylight from window\n- Shallow depth of field\"\"\"\n\n# Less effective - Vague\nprompt = \"A burger\"\n</code></pre>"},{"location":"learn/models/text_to_image/#2-specify-technical-aspects","title":"2. Specify Technical Aspects","text":"<pre><code># Include photography/art technical details\nprompts = [\n    # Photography\n    \"Portrait photo, 85mm lens, f/1.8, bokeh background, golden hour\",\n\n    # Digital art\n    \"Digital illustration, flat design, vibrant colors, vector style\",\n\n    # 3D render\n    \"3D render, physically based rendering, ray tracing, high detail\"\n]\n</code></pre>"},{"location":"learn/models/text_to_image/#3-use-quality-parameters-wisely","title":"3. Use Quality Parameters Wisely","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\n# For final/client work - HD quality\nclient_image = model(\n    prompt=\"Professional product photo\",\n    quality=\"hd\",\n    size=\"1792x1024\"\n)\n\n# For iterations/drafts - Standard quality\ndraft_image = model(\n    prompt=\"Quick concept sketch\",\n    quality=\"standard\",\n    size=\"1024x1024\"\n)\n</code></pre>"},{"location":"learn/models/text_to_image/#4-handle-errors-gracefully","title":"4. Handle Errors Gracefully","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\ndef safe_generate(prompt, max_retries=3):\n    \"\"\"Generate with error handling and retries.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            response = model(prompt=prompt)\n            return response.consume()\n        except Exception as e:\n            print(f\"Attempt {attempt + 1} failed: {e}\")\n            if attempt == max_retries - 1:\n                raise\n    return None\n\nimage_url = safe_generate(\"A beautiful landscape\")\n</code></pre>"},{"location":"learn/models/text_to_image/#5-respect-content-policies","title":"5. Respect Content Policies","text":"<pre><code># Good - Follows content policies\nsafe_prompts = [\n    \"A family having dinner together\",\n    \"Children playing in a park\",\n    \"A doctor examining a patient\"\n]\n\n# Avoid - May violate policies\n# - Violence, gore\n# - Explicit content\n# - Illegal activities\n# - Copyrighted characters\n# - Public figures (use generic descriptions)\n</code></pre>"},{"location":"learn/models/text_to_image/#cost-optimization","title":"Cost Optimization","text":""},{"location":"learn/models/text_to_image/#choose-appropriate-model","title":"Choose Appropriate Model","text":"<pre><code>import msgflux as mf\n\n# DALL-E 3: Higher quality, higher cost\n# Use for: Final outputs, client work, high-detail needs\nmodel_hd = mf.Model.text_to_image(\"openai/dall-e-3\")\n\n# DALL-E 2: Lower cost, good quality\n# Use for: Iterations, drafts, bulk generation\nmodel_standard = mf.Model.text_to_image(\"openai/dall-e-2\")\n</code></pre>"},{"location":"learn/models/text_to_image/#batch-similar-requests","title":"Batch Similar Requests","text":"<pre><code>import msgflux as mf\nimport msgflux.nn.functional as F\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-2\")\n\n# Generate multiple similar images in one batch\nprompts = [f\"Product photo of {item}\" for item in [\n    \"coffee mug\",\n    \"water bottle\",\n    \"notebook\",\n    \"pen\"\n]]\n\nresults = F.map_gather(\n    model,\n    args_list=[(p,) for p in prompts]\n)\n</code></pre>"},{"location":"learn/models/text_to_image/#error-handling","title":"Error Handling","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_image(\"openai/dall-e-3\")\n\ntry:\n    response = model(prompt=\"A landscape\")\n    image_url = response.consume()\nexcept ImportError:\n    print(\"Provider not installed\")\nexcept ValueError as e:\n    print(f\"Invalid parameters: {e}\")\nexcept Exception as e:\n    print(f\"Generation failed: {e}\")\n    # Common errors:\n    # - Content policy violation\n    # - Rate limits\n    # - Network issues\n</code></pre>"},{"location":"learn/models/text_to_image/#see-also","title":"See Also","text":"<ul> <li>Model - Model factory and registry</li> <li>Image Editing - Edit existing images with text</li> <li>Chat Completion - Generate image descriptions</li> <li>Moderation - Content moderation</li> </ul>"},{"location":"learn/models/text_to_speech/","title":"Text to Speech","text":"<p>The <code>text_to_speech</code> model converts text into natural-sounding spoken audio. These models enable voice generation for accessibility, content creation, virtual assistants, and more.</p> <p>All code examples use the recommended import pattern:</p> <pre><code>import msgflux as mf\n</code></pre>"},{"location":"learn/models/text_to_speech/#overview","title":"Overview","text":"<p>Text-to-speech (TTS) models transform written text into spoken audio. They enable:</p> <ul> <li>Voice Synthesis: Convert any text to natural-sounding speech</li> <li>Voice Selection: Choose from different voice profiles</li> <li>Speed Control: Adjust speaking rate</li> <li>Format Options: Generate audio in various formats</li> <li>Streaming: Real-time audio generation</li> </ul>"},{"location":"learn/models/text_to_speech/#common-use-cases","title":"Common Use Cases","text":"<ul> <li>Accessibility: Convert text to speech for visually impaired users</li> <li>Content Creation: Generate voiceovers for videos and podcasts</li> <li>Virtual Assistants: Add voice to chatbots and AI assistants</li> <li>Audio Books: Convert written content to audio format</li> <li>Language Learning: Pronunciation examples</li> <li>Notifications: Voice alerts and announcements</li> </ul>"},{"location":"learn/models/text_to_speech/#quick-start","title":"Quick Start","text":""},{"location":"learn/models/text_to_speech/#basic-usage","title":"Basic Usage","text":"<pre><code>import msgflux as mf\n\n# Create TTS model\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# Generate speech\nresponse = model(\"Hello, how are you today?\")\n\n# Get audio file path\naudio_path = response.consume()\nprint(audio_path)  # /tmp/tmpXXXXXX.opus\n</code></pre>"},{"location":"learn/models/text_to_speech/#with-custom-voice","title":"With Custom Voice","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_speech(\n    \"openai/tts-1\",\n    voice=\"nova\",  # Female voice\n    speed=1.0      # Normal speed\n)\n\nresponse = model(\"Welcome to our service!\")\naudio_path = response.consume()\n</code></pre>"},{"location":"learn/models/text_to_speech/#supported-providers","title":"Supported Providers","text":""},{"location":"learn/models/text_to_speech/#openai","title":"OpenAI","text":"<pre><code>import msgflux as mf\n\n# Standard quality (faster, cheaper)\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# HD quality (higher quality, slower)\nmodel = mf.Model.text_to_speech(\"openai/tts-1-hd\")\n</code></pre>"},{"location":"learn/models/text_to_speech/#together-ai","title":"Together AI","text":"<pre><code>import msgflux as mf\n\n# Together AI text-to-speech\nmodel = mf.Model.text_to_speech(\"together/tts-1\")\n</code></pre>"},{"location":"learn/models/text_to_speech/#voice-options","title":"Voice Options","text":""},{"location":"learn/models/text_to_speech/#available-voices-openai","title":"Available Voices (OpenAI)","text":"<pre><code>import msgflux as mf\n\n# Alloy (neutral)\nmodel = mf.Model.text_to_speech(\"openai/tts-1\", voice=\"alloy\")\n\n# Echo (male)\nmodel = mf.Model.text_to_speech(\"openai/tts-1\", voice=\"echo\")\n\n# Fable (neutral, expressive)\nmodel = mf.Model.text_to_speech(\"openai/tts-1\", voice=\"fable\")\n\n# Onyx (male, deeper)\nmodel = mf.Model.text_to_speech(\"openai/tts-1\", voice=\"onyx\")\n\n# Nova (female, energetic)\nmodel = mf.Model.text_to_speech(\"openai/tts-1\", voice=\"nova\")\n\n# Shimmer (female, warm)\nmodel = mf.Model.text_to_speech(\"openai/tts-1\", voice=\"shimmer\")\n</code></pre>"},{"location":"learn/models/text_to_speech/#voice-characteristics","title":"Voice Characteristics","text":"Voice Gender Characteristics alloy Neutral Balanced, general purpose echo Male Clear, professional fable Neutral Expressive, storytelling onyx Male Deep, authoritative nova Female Bright, energetic shimmer Female Warm, friendly"},{"location":"learn/models/text_to_speech/#speed-control","title":"Speed Control","text":"<p>Adjust the speaking rate:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# Slow (0.25x - 1.0x)\nresponse = model(\n    \"This will be spoken slowly.\",\n    speed=0.5\n)\n\n# Normal (default)\nresponse = model(\n    \"This will be spoken at normal speed.\",\n    speed=1.0\n)\n\n# Fast (1.0x - 4.0x)\nresponse = model(\n    \"This will be spoken quickly.\",\n    speed=2.0\n)\n</code></pre> <p>Note: Speed parameter accepts values from 0.25 to 4.0.</p>"},{"location":"learn/models/text_to_speech/#audio-formats","title":"Audio Formats","text":""},{"location":"learn/models/text_to_speech/#supported-formats","title":"Supported Formats","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# Opus (default, best for streaming)\nresponse = model(\n    \"Hello world\",\n    response_format=\"opus\"\n)\n\n# MP3 (universal compatibility)\nresponse = model(\n    \"Hello world\",\n    response_format=\"mp3\"\n)\n\n# AAC (good quality, small size)\nresponse = model(\n    \"Hello world\",\n    response_format=\"aac\"\n)\n\n# FLAC (lossless, large)\nresponse = model(\n    \"Hello world\",\n    response_format=\"flac\"\n)\n\n# WAV (uncompressed)\nresponse = model(\n    \"Hello world\",\n    response_format=\"wav\"\n)\n\n# PCM (raw audio)\nresponse = model(\n    \"Hello world\",\n    response_format=\"pcm\"\n)\n</code></pre>"},{"location":"learn/models/text_to_speech/#format-comparison","title":"Format Comparison","text":"Format Quality Size Use Case opus High Small Streaming, real-time mp3 Good Medium Universal playback aac High Small Mobile, web flac Lossless Large Archival, editing wav Lossless Large Professional audio pcm Raw Largest Audio processing"},{"location":"learn/models/text_to_speech/#voice-instructions","title":"Voice Instructions","text":"<p>Control voice characteristics with prompts:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# Add emotional tone\nresponse = model(\n    \"I'm so excited about this!\",\n    prompt=\"Speak with enthusiasm and energy\"\n)\n\n# Control pacing\nresponse = model(\n    \"This is an important announcement.\",\n    prompt=\"Speak slowly and clearly, emphasizing each word\"\n)\n\n# Set context\nresponse = model(\n    \"Welcome to the show!\",\n    prompt=\"Speak as a radio host, upbeat and friendly\"\n)\n</code></pre> <p>Note: Voice instructions work best with tts-1-hd model.</p>"},{"location":"learn/models/text_to_speech/#streaming-audio","title":"Streaming Audio","text":"<p>Generate and play audio in real-time:</p> <pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# Stream audio\nresponse = model(\n    \"This is a long text that will be streamed as audio chunks...\",\n    stream=True\n)\n\n# Process chunks as they arrive\nfor chunk in response.consume():\n    if chunk is None:  # End of stream\n        break\n    # chunk is bytes - play or save incrementally\n    process_audio_chunk(chunk)\n</code></pre>"},{"location":"learn/models/text_to_speech/#streaming-to-file","title":"Streaming to File","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\nresponse = model(\n    \"This will be streamed to a file.\",\n    stream=True,\n    response_format=\"mp3\"\n)\n\n# Write chunks to file\nwith open(\"output.mp3\", \"wb\") as f:\n    for chunk in response.consume():\n        if chunk is None:\n            break\n        f.write(chunk)\n\nprint(\"Audio saved to output.mp3\")\n</code></pre>"},{"location":"learn/models/text_to_speech/#streaming-playback","title":"Streaming Playback","text":"<pre><code>import msgflux as mf\nimport pyaudio  # pip install pyaudio\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# Setup audio playback\np = pyaudio.PyAudio()\nstream = p.open(\n    format=pyaudio.paInt16,\n    channels=1,\n    rate=24000,  # 24kHz for TTS\n    output=True\n)\n\n# Stream and play\nresponse = model(\n    \"This will be played in real-time.\",\n    stream=True,\n    response_format=\"pcm\"\n)\n\nfor chunk in response.consume():\n    if chunk is None:\n        break\n    stream.write(chunk)\n\nstream.stop_stream()\nstream.close()\np.terminate()\n</code></pre>"},{"location":"learn/models/text_to_speech/#async-support","title":"Async Support","text":"<p>Generate audio asynchronously:</p> <pre><code>import msgflux as mf\nimport asyncio\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\nasync def generate_speech(text):\n    response = await model.acall(text, voice=\"nova\")\n    return response.consume()\n\nasync def main():\n    # Generate multiple audio files concurrently\n    texts = [\n        \"First announcement\",\n        \"Second announcement\",\n        \"Third announcement\"\n    ]\n\n    tasks = [generate_speech(text) for text in texts]\n    audio_paths = await asyncio.gather(*tasks)\n\n    for text, path in zip(texts, audio_paths):\n        print(f\"{text}: {path}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"learn/models/text_to_speech/#async-streaming","title":"Async Streaming","text":"<pre><code>import msgflux as mf\nimport asyncio\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\nasync def stream_speech(text):\n    response = await model.acall(text, stream=True)\n\n    async for chunk in response.consume():\n        if chunk is None:\n            break\n        # Process chunk asynchronously\n        await process_chunk(chunk)\n\nasyncio.run(stream_speech(\"Hello world\"))\n</code></pre>"},{"location":"learn/models/text_to_speech/#batch-processing","title":"Batch Processing","text":"<p>Generate multiple audio files:</p> <pre><code>import msgflux as mf\nimport msgflux.nn.functional as F\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\", voice=\"nova\")\n\ntexts = [\n    \"Welcome to chapter one.\",\n    \"Welcome to chapter two.\",\n    \"Welcome to chapter three.\"\n]\n\n# Generate in parallel\nresults = F.map_gather(\n    model,\n    args_list=[(text,) for text in texts]\n)\n\n# Save all files\nfor i, result in enumerate(results):\n    audio_path = result.consume()\n    # Copy to permanent location\n    import shutil\n    shutil.copy(audio_path, f\"chapter_{i+1}.opus\")\n</code></pre>"},{"location":"learn/models/text_to_speech/#working-with-audio-files","title":"Working with Audio Files","text":""},{"location":"learn/models/text_to_speech/#save-to-specific-location","title":"Save to Specific Location","text":"<pre><code>import msgflux as mf\nimport shutil\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\nresponse = model(\n    \"Save this audio\",\n    response_format=\"mp3\"\n)\n\n# Get temporary file path\ntemp_path = response.consume()\n\n# Copy to desired location\nshutil.copy(temp_path, \"output.mp3\")\nprint(\"Saved to output.mp3\")\n</code></pre>"},{"location":"learn/models/text_to_speech/#play-audio","title":"Play Audio","text":"<pre><code>import msgflux as mf\nimport subprocess\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\nresponse = model(\"Play this message\")\naudio_path = response.consume()\n\n# Play with system player\nsubprocess.run([\"mpv\", audio_path])  # Or use \"afplay\" on macOS\n</code></pre>"},{"location":"learn/models/text_to_speech/#get-audio-info","title":"Get Audio Info","text":"<pre><code>import msgflux as mf\nfrom pydub import AudioSegment\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\nresponse = model(\n    \"Get information about this audio\",\n    response_format=\"mp3\"\n)\n\naudio_path = response.consume()\n\n# Load with pydub\naudio = AudioSegment.from_mp3(audio_path)\n\nprint(f\"Duration: {len(audio) / 1000:.2f} seconds\")\nprint(f\"Channels: {audio.channels}\")\nprint(f\"Frame rate: {audio.frame_rate} Hz\")\nprint(f\"Sample width: {audio.sample_width} bytes\")\n</code></pre>"},{"location":"learn/models/text_to_speech/#common-patterns","title":"Common Patterns","text":""},{"location":"learn/models/text_to_speech/#multi-voice-narration","title":"Multi-Voice Narration","text":"<pre><code>import msgflux as mf\nfrom pydub import AudioSegment\n\n# Create models with different voices\nnarrator = mf.Model.text_to_speech(\"openai/tts-1\", voice=\"fable\")\ncharacter1 = mf.Model.text_to_speech(\"openai/tts-1\", voice=\"nova\")\ncharacter2 = mf.Model.text_to_speech(\"openai/tts-1\", voice=\"onyx\")\n\n# Generate dialogue\nnarration = narrator(\"The story begins\").consume()\nline1 = character1(\"Hello there!\").consume()\nline2 = character2(\"Hi, how are you?\").consume()\n\n# Combine audio\nfrom pydub import AudioSegment\n\ncombined = AudioSegment.from_file(narration)\ncombined += AudioSegment.from_file(line1)\ncombined += AudioSegment.from_file(line2)\n\ncombined.export(\"dialogue.mp3\", format=\"mp3\")\n</code></pre>"},{"location":"learn/models/text_to_speech/#text-to-audio-book","title":"Text-to-Audio Book","text":"<pre><code>import msgflux as mf\nfrom pydub import AudioSegment\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1-hd\", voice=\"fable\")\n\n# Split book into chapters\nchapters = [\n    \"Chapter 1: Once upon a time...\",\n    \"Chapter 2: The adventure begins...\",\n    \"Chapter 3: A challenge appears...\"\n]\n\n# Generate audio for each chapter\naudio_segments = []\n\nfor i, chapter_text in enumerate(chapters):\n    print(f\"Generating chapter {i+1}...\")\n    response = model(chapter_text, response_format=\"mp3\")\n    audio_path = response.consume()\n    audio_segments.append(AudioSegment.from_mp3(audio_path))\n\n# Combine with silence between chapters\ncombined = audio_segments[0]\nsilence = AudioSegment.silent(duration=2000)  # 2 seconds\n\nfor segment in audio_segments[1:]:\n    combined += silence + segment\n\ncombined.export(\"audiobook.mp3\", format=\"mp3\")\nprint(\"Audiobook created!\")\n</code></pre>"},{"location":"learn/models/text_to_speech/#real-time-chat-tts","title":"Real-time Chat TTS","text":"<pre><code>import msgflux as mf\nimport subprocess\nimport tempfile\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\", voice=\"nova\")\n\ndef speak(text):\n    \"\"\"Convert text to speech and play immediately.\"\"\"\n    response = model(text, response_format=\"mp3\")\n    audio_path = response.consume()\n\n    # Play immediately\n    subprocess.run(\n        [\"mpv\", \"--really-quiet\", audio_path],\n        stdout=subprocess.DEVNULL,\n        stderr=subprocess.DEVNULL\n    )\n\n# Use in chat\nspeak(\"Hello! How can I help you today?\")\nuser_input = input(\"You: \")\nspeak(f\"You said: {user_input}\")\n</code></pre>"},{"location":"learn/models/text_to_speech/#language-learning","title":"Language Learning","text":"<pre><code>import msgflux as mf\n\nmodel_slow = mf.Model.text_to_speech(\"openai/tts-1\", voice=\"nova\", speed=0.7)\nmodel_normal = mf.Model.text_to_speech(\"openai/tts-1\", voice=\"nova\", speed=1.0)\n\nphrase = \"The quick brown fox jumps over the lazy dog\"\n\n# Slow version for learning\nslow_audio = model_slow(phrase, response_format=\"mp3\").consume()\n\n# Normal speed for practice\nnormal_audio = model_normal(phrase, response_format=\"mp3\").consume()\n\nprint(f\"Slow: {slow_audio}\")\nprint(f\"Normal: {normal_audio}\")\n</code></pre>"},{"location":"learn/models/text_to_speech/#podcast-generation","title":"Podcast Generation","text":"<pre><code>import msgflux as mf\nfrom pydub import AudioSegment\n\n# Create hosts with different voices\nhost1 = mf.Model.text_to_speech(\"openai/tts-1-hd\", voice=\"echo\")\nhost2 = mf.Model.text_to_speech(\"openai/tts-1-hd\", voice=\"shimmer\")\n\n# Podcast script\nintro = host1(\"Welcome to our podcast!\").consume()\nresponse1 = host2(\"Thanks for having me!\").consume()\ndiscussion = host1(\"Let's talk about AI...\").consume()\n\n# Combine with music/effects\npodcast = AudioSegment.from_file(intro)\npodcast += AudioSegment.from_file(response1)\npodcast += AudioSegment.from_file(discussion)\n\npodcast.export(\"podcast.mp3\", format=\"mp3\")\n</code></pre>"},{"location":"learn/models/text_to_speech/#best-practices","title":"Best Practices","text":""},{"location":"learn/models/text_to_speech/#1-choose-the-right-model","title":"1. Choose the Right Model","text":"<pre><code>import msgflux as mf\n\n# For production/client work - HD quality\nmodel_hd = mf.Model.text_to_speech(\"openai/tts-1-hd\", voice=\"nova\")\n\n# For testing/iteration - standard quality\nmodel_std = mf.Model.text_to_speech(\"openai/tts-1\", voice=\"nova\")\n</code></pre>"},{"location":"learn/models/text_to_speech/#2-text-preprocessing","title":"2. Text Preprocessing","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\ndef prepare_text(text):\n    \"\"\"Prepare text for better TTS output.\"\"\"\n    # Remove excessive whitespace\n    text = \" \".join(text.split())\n\n    # Add pauses with punctuation\n    text = text.replace(\". \", \"... \")\n\n    # Spell out abbreviations if needed\n    text = text.replace(\"Dr.\", \"Doctor\")\n    text = text.replace(\"Mr.\", \"Mister\")\n\n    return text\n\ntext = \"Dr. Smith said hello.  How are you?\"\nclean_text = prepare_text(text)\nresponse = model(clean_text)\n</code></pre>"},{"location":"learn/models/text_to_speech/#3-handle-long-texts","title":"3. Handle Long Texts","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\ndef split_text(text, max_length=4000):\n    \"\"\"Split long text into chunks.\"\"\"\n    sentences = text.split(\". \")\n    chunks = []\n    current_chunk = \"\"\n\n    for sentence in sentences:\n        if len(current_chunk) + len(sentence) &lt; max_length:\n            current_chunk += sentence + \". \"\n        else:\n            chunks.append(current_chunk)\n            current_chunk = sentence + \". \"\n\n    if current_chunk:\n        chunks.append(current_chunk)\n\n    return chunks\n\nlong_text = \"...\" * 10000  # Very long text\nchunks = split_text(long_text)\n\naudio_files = []\nfor chunk in chunks:\n    response = model(chunk)\n    audio_files.append(response.consume())\n</code></pre>"},{"location":"learn/models/text_to_speech/#4-use-streaming-for-large-content","title":"4. Use Streaming for Large Content","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# For long content, stream to avoid memory issues\nlong_text = \"...\" * 5000\n\nresponse = model(long_text, stream=True, response_format=\"mp3\")\n\nwith open(\"long_audio.mp3\", \"wb\") as f:\n    for chunk in response.consume():\n        if chunk is None:\n            break\n        f.write(chunk)\n</code></pre>"},{"location":"learn/models/text_to_speech/#5-cache-common-phrases","title":"5. Cache Common Phrases","text":"<pre><code>import msgflux as mf\nimport hashlib\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\naudio_cache = {}\n\ndef cached_tts(text):\n    \"\"\"Cache generated audio for reuse.\"\"\"\n    cache_key = hashlib.md5(text.encode()).hexdigest()\n\n    if cache_key in audio_cache:\n        return audio_cache[cache_key]\n\n    response = model(text)\n    audio_path = response.consume()\n    audio_cache[cache_key] = audio_path\n\n    return audio_path\n\n# These will only generate once\naudio1 = cached_tts(\"Welcome!\")\naudio2 = cached_tts(\"Welcome!\")  # Uses cache\n</code></pre>"},{"location":"learn/models/text_to_speech/#error-handling","title":"Error Handling","text":"<pre><code>import msgflux as mf\n\nmodel = mf.Model.text_to_speech(\"openai/tts-1\")\n\ntry:\n    response = model(\"Hello world\")\n    audio_path = response.consume()\nexcept ImportError:\n    print(\"Provider not installed\")\nexcept ValueError as e:\n    print(f\"Invalid parameters: {e}\")\n    # Common issues:\n    # - Invalid voice name\n    # - Speed out of range (0.25-4.0)\n    # - Invalid response_format\nexcept Exception as e:\n    print(f\"Generation failed: {e}\")\n    # Common errors:\n    # - Rate limits\n    # - Network issues\n    # - Text too long\n</code></pre>"},{"location":"learn/models/text_to_speech/#limitations","title":"Limitations","text":""},{"location":"learn/models/text_to_speech/#openai-tts-limitations","title":"OpenAI TTS Limitations","text":"<ul> <li>Text Length: Maximum ~4096 characters per request</li> <li>Speed Range: 0.25 to 4.0 only</li> <li>Language: Supports multiple languages but optimized for English</li> <li>Voice Count: Limited to 6 predefined voices</li> <li>Real-time Factor: Not exactly real-time (some processing delay)</li> </ul>"},{"location":"learn/models/text_to_speech/#cost-optimization","title":"Cost Optimization","text":""},{"location":"learn/models/text_to_speech/#efficient-tts-usage","title":"Efficient TTS Usage","text":"<pre><code>import msgflux as mf\n\n# Use standard model for development\ndev_model = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# Use HD only for production\nprod_model = mf.Model.text_to_speech(\"openai/tts-1-hd\")\n\n# Reuse audio for common phrases\ncommon_phrases = {\n    \"welcome\": dev_model(\"Welcome!\").consume(),\n    \"goodbye\": dev_model(\"Goodbye!\").consume()\n}\n</code></pre>"},{"location":"learn/models/text_to_speech/#see-also","title":"See Also","text":"<ul> <li>Speech to Text - Transcribe audio to text</li> <li>Chat Completion - Generate text for TTS</li> <li>Model - Model factory and registry</li> </ul>"},{"location":"learn/nn/agent/","title":"nn.Agent","text":"<p>The <code>Agent</code> is a powerful <code>Module</code> that uses language models to solve tasks. It can handle multimodal data, call tools, and manage complex workflows with structured outputs.</p>"},{"location":"learn/nn/agent/#overview","title":"Overview","text":"<p>An <code>Agent</code> combines a language model with instructions and tools to accomplish tasks. The Agent module adopts a task decomposition strategy, allowing each part of a task to be treated in isolation and independently.</p> <p>A <code>ToolLibrary</code> is integrated into the Agent to manage and execute tools.</p>"},{"location":"learn/nn/agent/#key-features","title":"Key Features","text":"<ul> <li>Multimodal Support: Handle text, images, audio, video, and files</li> <li>Tool Calling: Execute functions to interact with external systems</li> <li>Structured Outputs: Generate typed responses with validation</li> <li>Flexible Configuration: Customize behavior through message fields and config options</li> <li>Template System: Use Jinja templates for prompts and responses</li> <li>Modular System Prompt: Compose system prompts from independent components</li> <li>Task Decomposition: Break down complex tasks into manageable parts</li> </ul>"},{"location":"learn/nn/agent/#response-modes","title":"Response Modes","text":"<p>The <code>response_mode</code> parameter defines how the agent's response is returned:</p> <ul> <li><code>plain_response</code> (default): Returns the output directly to the user</li> <li>Other values: Log the response to the passed Message object</li> </ul>"},{"location":"learn/nn/agent/#model-state","title":"Model State","text":"<p>By default, the agent returns only the model output. To also return the agent's internal state (<code>model_state</code>), pass <code>config={\"return_model_state\": True}</code>. The output will then be a <code>dotdict</code> containing:</p> <ul> <li><code>model_response</code>: The agent's output</li> <li><code>model_state</code>: The internal conversation state</li> </ul>"},{"location":"learn/nn/agent/#system-prompt-components","title":"System Prompt Components","text":"<p>The Agent divides the system prompt and task into different components for granular control and composability.</p>"},{"location":"learn/nn/agent/#system-prompt-variables","title":"System Prompt Variables","text":"<p>The system prompt is composed of 6 components:</p> Component Description Example system_message Agent behavior and role \"You are an agent specialist in...\" instructions What the agent should do \"You MUST respond to the user...\" expected_output Format of the response \"Your answer must be concise...\" examples Input/output examples Examples of reasoning and outputs system_extra_message Additional system context Extra instructions or constraints include_date Include current date Adds \"Weekday, Month DD, YYYY\" <p>All components are assembled using a Jinja template to create the final system prompt.</p>"},{"location":"learn/nn/agent/#task-variables","title":"Task Variables","text":"<p>The task configuration is separated into several parts:</p> Variable Description Can be passed at call time context_cache Fixed context for the agent \u274c context_inputs Dynamic context from Message \u2705 task_inputs Main task input from Message \u2705 task_multimodal_inputs Multimodal inputs (image, audio, etc.) \u2705 task_messages ChatML format conversation history \u2705 vars Variables for templates and tools \u2705 <p>(*) Variables marked with \u2705 can be passed either via <code>message_fields</code> dict or as named arguments during agent call.</p>"},{"location":"learn/nn/agent/#configuration-parameters","title":"Configuration Parameters","text":"<p>Agents use three main configuration dictionaries:</p>"},{"location":"learn/nn/agent/#message_fields","title":"<code>message_fields</code>","text":"<p>Maps Message object paths to agent inputs:</p> <pre><code>message_fields={\n    \"task_inputs\": \"input.user\",                           # Task input path\n    \"task_multimodal_inputs\": {\"image\": \"images.user\"},   # Multimodal inputs\n    \"model_state\": \"messages.history\",                     # Conversation history\n    \"context_inputs\": \"context.data\",                      # Context data\n    \"model_preference\": \"model.preference\",                # Model selection (ModelGateway)\n    \"vars\": \"vars.data\"                                    # Template/tool variables\n}\n</code></pre>"},{"location":"learn/nn/agent/#config","title":"<code>config</code>","text":"<p>Controls agent behavior:</p> <pre><code>config={\n    \"verbose\": True,                              # Print output and tool calls\n    \"return_model_state\": False,                  # Return internal state\n    \"tool_choice\": \"auto\",                        # Tool selection (\"auto\", \"required\", or function name)\n    \"stream\": False,                              # Stream response\n    \"image_block_kwargs\": {\"detail\": \"high\"},     # Image processing options\n    \"video_block_kwargs\": {\"format\": \"mp4\"},      # Video processing options\n    \"include_date\": False                         # Include date in system prompt\n}\n</code></pre>"},{"location":"learn/nn/agent/#templates","title":"<code>templates</code>","text":"<p>Jinja templates for formatting:</p> <pre><code>templates={\n    \"task\": \"Who was {{person}}?\",                        # Task formatting\n    \"response\": \"{{final_answer}}\",                       # Response formatting\n    \"context\": \"Context: {{context}}\",                    # Context formatting\n    \"system_prompt\": \"Custom system prompt template\"      # System prompt override\n}\n</code></pre>"},{"location":"learn/nn/agent/#guardrails","title":"Guardrails","text":"<p>Agent supports input/output guardrails via the <code>guardrails</code> parameter:</p> <pre><code>guardrails={\n    \"input\": input_checker,    # Executed before model\n    \"output\": output_checker   # Executed after model\n}\n</code></pre> <p>Both receive a <code>data</code> parameter containing conversations in ChatML format. Moderation models are commonly used for guardrails.</p>"},{"location":"learn/nn/agent/#quick-start","title":"Quick Start","text":""},{"location":"learn/nn/agent/#basic-usage","title":"Basic Usage","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\n# Set API key\nmf.set_envs(OPENAI_API_KEY=\"sk-...\")\n\n# Create model\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Create agent (requires at least name and model)\nagent = nn.Agent(\"assistant\", model)\n\n# Use agent\nresponse = agent(\"What is the capital of France?\")\nprint(response)  # \"The capital of France is Paris.\"\n</code></pre>"},{"location":"learn/nn/agent/#with-system-components","title":"With System Components","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nsystem_message = \"\"\"\nYou are a business development assistant focused on helping sales teams qualify leads\nand craft compelling value propositions.\nAlways keep a professional and persuasive tone.\n\"\"\"\n\ninstructions = \"\"\"\nWhen given a short company description, identify its potential needs,\nsuggest an initial outreach strategy, and provide a tailored value proposition.\n\"\"\"\n\nexpected_output = \"\"\"\nRespond in three bullet points:\n    - Identified Needs\n    - Outreach Strategy\n    - Value Proposition\n\"\"\"\n\nsystem_extra_message = \"\"\"\nEnsure recommendations align with ethical sales practices\nand avoid making unverifiable claims about the product.\n\"\"\"\n\nsales_agent = nn.Agent(\n    \"sales-agent\",\n    model,\n    system_message=system_message,\n    instructions=instructions,\n    expected_output=expected_output,\n    system_extra_message=system_extra_message,\n    config={\"include_date\": True, \"verbose\": True}\n)\n\n# View the generated system prompt\nprint(sales_agent._get_system_prompt())\n\n# Use the agent\nresponse = sales_agent(\"A fintech startup offering digital wallets\")\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#debugging-an-agent","title":"Debugging an Agent","text":""},{"location":"learn/nn/agent/#view-state-dictionary","title":"View State Dictionary","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\nagent = nn.Agent(\"agent\", model)\n\n# Prompt components and config can be viewed through state dict\nagent.state_dict()\n</code></pre>"},{"location":"learn/nn/agent/#inspect-model-execution","title":"Inspect Model Execution","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    instructions=\"User name is {{user_name}}\",\n    config={\"return_model_state\": True}\n)\n\nmessage = \"Hi\"\nvars = {\"user_name\": \"Clark\"}\n\n# Inspect what will be sent to the model\nexecution_params = agent.inspect_model_execution_params(message, vars=vars)\nprint(execution_params)\n\n# Execute\nresponse = agent(message, vars=vars)\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#verbose-mode","title":"Verbose Mode","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Enable verbose to see model outputs and tool calls\nagent = nn.Agent(\"agent\", model, config={\"verbose\": True})\n\nresponse = agent(\"Tell me a joke\")\n</code></pre>"},{"location":"learn/nn/agent/#using-autoparams","title":"Using AutoParams","text":"<p>The <code>Agent</code> class already includes AutoParams support, allowing you to create agent variants declaratively by defining class attributes. This is especially useful for creating agent families with different configurations.</p>"},{"location":"learn/nn/agent/#basic-autoparams-usage","title":"Basic AutoParams Usage","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Create agent variants by setting class attributes\nclass CreativeAgent(nn.Agent):\n    \"\"\"Agent optimized for creative tasks.\"\"\"\n    name = \"creative-agent\"\n    system_message = \"You are a creative assistant.\"\n    instructions = \"Generate innovative and original ideas.\"\n    config = {\"verbose\": False}\n\nclass AnalyticalAgent(nn.Agent):\n    \"\"\"Agent optimized for analytical tasks.\"\"\"\n    name = \"analytical-agent\"\n    system_message = \"You are an analytical assistant.\"\n    instructions = \"Provide data-driven insights.\"\n    config = {\"verbose\": False}\n\n# Instantiate with defaults\ncreative = CreativeAgent(model=model)\nanalytical = AnalyticalAgent(model=model)\n\n# Or override specific parameters\ncustom_creative = CreativeAgent(\n    model=model,\n    config={\"verbose\": True}  # Override config\n)\n</code></pre>"},{"location":"learn/nn/agent/#automatic-name-and-description-capture","title":"Automatic Name and Description Capture","text":"<p>Agent automatically captures the class name as <code>name</code> and the docstring as <code>description</code>:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nclass CustomerSupportAgent(nn.Agent):\n    \"\"\"\n    Helpful customer support agent specialized in resolving user issues\n    with empathy and efficiency. Use for customer inquiries, complaints,\n    and support requests.\n    \"\"\"\n\n    system_message = \"You are a professional customer support agent.\"\n    instructions = \"Listen to customer concerns and provide clear solutions.\"\n    config = {\"verbose\": False}\n\nclass SalesAgent(nn.Agent):\n    \"\"\"\n    Persuasive sales agent focused on understanding customer needs and\n    presenting tailored solutions. Use for lead qualification, product\n    demonstrations, and closing deals.\n    \"\"\"\n\n    system_message = \"You are an expert sales professional.\"\n    instructions = \"Identify customer needs and present value propositions.\"\n    config = {\"verbose\": False}\n\n# Name and description are automatically captured\nsupport = CustomerSupportAgent(model=model)\nprint(support.name)  # \"CustomerSupportAgent\"\nprint(support.description)  # \"Helpful customer support agent specialized...\"\n\nsales = SalesAgent(model=model)\nprint(sales.name)  # \"SalesAgent\"\nprint(sales.description)  # \"Persuasive sales agent focused...\"\n</code></pre>"},{"location":"learn/nn/agent/#agent-families-with-shared-configuration","title":"Agent Families with Shared Configuration","text":"<p>Create families of agents that share common settings:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Base configuration for all content moderators\nclass ModeratorBase(nn.Agent):\n    \"\"\"Base moderator configuration.\"\"\"\n\n    system_message = \"You are a content moderator.\"\n\n    # Shared defaults\n    instructions = \"Review content and flag violations.\"\n    expected_output = \"Return: APPROVED or FLAGGED with reason.\"\n\nclass StrictModerator(ModeratorBase):\n    \"\"\"Strict content moderator. Flag any potentially harmful content.\"\"\"\n\n    system_message = \"You are a strict content moderator with low tolerance for violations.\"\n    config = {\"verbose\": True}\n\nclass LenientModerator(ModeratorBase):\n    \"\"\"Lenient content moderator. Only flag clearly harmful content.\"\"\"\n\n    system_message = \"You are a lenient content moderator focusing only on clear violations.\"\n    config = {\"verbose\": False}\n\nclass ChildSafeModerator(StrictModerator):\n    \"\"\"Ultra-strict moderator for child-safe content. Zero tolerance for inappropriate material.\"\"\"\n\n    system_message = \"You are an ultra-strict child safety moderator.\"\n    expected_output = \"Return: APPROVED or BLOCKED with detailed reason.\"\n\n# All inherit shared configuration and auto-capture name/description\nstrict = StrictModerator(model=model)\nlenient = LenientModerator(model=model)\nchild_safe = ChildSafeModerator(model=model)\n\nprint(strict.name)  # \"StrictModerator\"\nprint(strict.description)  # \"Strict content moderator. Flag any...\"\nprint(lenient.name)  # \"LenientModerator\"\n</code></pre>"},{"location":"learn/nn/agent/#multi-language-agents","title":"Multi-Language Agents","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nclass TranslatorBase(nn.Agent):\n    \"\"\"Base translator agent.\"\"\"\n\n    system_message = \"You are a professional translator.\"\n    instructions = \"Translate the input text accurately.\"\n\nclass EnglishToSpanish(TranslatorBase):\n    \"\"\"Professional translator from English to Spanish.\"\"\"\n\n    system_message = \"You are a professional English to Spanish translator.\"\n    config = {\"verbose\": False}\n\nclass SpanishToEnglish(TranslatorBase):\n    \"\"\"Professional translator from Spanish to English.\"\"\"\n\n    system_message = \"You are a professional Spanish to English translator.\"\n    config = {\"verbose\": False}\n\nclass EnglishToFrench(TranslatorBase):\n    \"\"\"Professional translator from English to French.\"\"\"\n\n    system_message = \"You are a professional English to French translator.\"\n    config = {\"verbose\": False}\n\n# Create translators - names and descriptions are auto-captured\nen_to_es = EnglishToSpanish(model=model)\nes_to_en = SpanishToEnglish(model=model)\nen_to_fr = EnglishToFrench(model=model)\n\nprint(en_to_es.name)  # \"EnglishToSpanish\"\nprint(en_to_es.description)  # \"Professional translator from English to Spanish.\"\n\n# Use them\nresult = en_to_es(\"Hello, how are you?\")\nprint(result)  # \"Hola, \u00bfc\u00f3mo est\u00e1s?\"\n</code></pre>"},{"location":"learn/nn/agent/#when-to-use-autoparams","title":"When to Use AutoParams","text":"<p>Use AutoParams when: - \u2705 Creating multiple agent variants with different configurations - \u2705 Building agent families with shared defaults - \u2705 You want declarative, class-based configuration - \u2705 Managing reusable agent templates</p> <p>Use direct instantiation when: - \u274c Creating a single, one-off agent - \u274c Agent configuration is highly dynamic - \u274c Simple, straightforward use case</p>"},{"location":"learn/nn/agent/#async-support","title":"Async Support","text":"<p>Agents support asynchronous execution using <code>acall()</code>:</p>"},{"location":"learn/nn/agent/#basic-async","title":"Basic Async","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\nimport asyncio\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\nagent = nn.Agent(\"agent\", model)\n\nasync def main():\n    response = await agent.acall(\"Tell me about Dirac delta\")\n    print(response)\n\nasyncio.run(main())\n</code></pre>"},{"location":"learn/nn/agent/#concurrent-execution","title":"Concurrent Execution","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\nimport asyncio\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\nagent = nn.Agent(\"agent\", model)\n\nasync def main():\n    tasks = [\n        agent.acall(\"What is quantum computing?\"),\n        agent.acall(\"What is machine learning?\"),\n        agent.acall(\"What is blockchain?\")\n    ]\n\n    responses = await asyncio.gather(*tasks)\n\n    for i, response in enumerate(responses, 1):\n        print(f\"\\nResponse {i}:\\n{response}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"learn/nn/agent/#async-with-streaming","title":"Async with Streaming","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\nimport asyncio\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\nagent = nn.Agent(\"agent\", model, config={\"stream\": True})\n\nasync def main():\n    response = await agent.acall(\"Tell me a story\")\n\n    # Stream chunks\n    async for chunk in response.consume():\n        print(chunk, end=\"\", flush=True)\n\nasyncio.run(main())\n</code></pre>"},{"location":"learn/nn/agent/#streaming","title":"Streaming","text":"<p>In streaming mode, the agent returns a <code>ModelStreamResponse</code> object that can be consumed asynchronously. This mode can be combined with tool usage.</p>"},{"location":"learn/nn/agent/#basic-streaming","title":"Basic Streaming","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\nagent = nn.Agent(\"agent\", model, config={\"stream\": True})\n\n# Get streaming response\nresponse = agent(\"Tell me a funny story\")\n\nprint(type(response))  # ModelStreamResponse\nprint(response.response_type)  # text_generation\n\n# FastAPI StreamingResponse compatible\nasync for chunk in response.consume():\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"learn/nn/agent/#streaming-with-tools","title":"Streaming with Tools","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get weather for a location.\"\"\"\n    return f\"The weather in {location} is sunny.\"\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\nagent = nn.Agent(\n    \"agent\",\n    model,\n    tools=[get_weather],\n    config={\"stream\": True, \"verbose\": True}\n)\n\nresponse = agent(\"What's the weather in Paris?\")\n\n# Stream response\nasync for chunk in response.consume():\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"learn/nn/agent/#vars","title":"Vars","text":"<p>Language models act as computers making context-based decisions in an environment. Beyond tool calls, models need to store information in variables.</p> <p>In msgFlux, this is called <code>vars</code>.</p>"},{"location":"learn/nn/agent/#what-are-vars","title":"What are Vars?","text":"<p><code>vars</code> is a dictionary injected into various agent components:</p> <ul> <li><code>templates[\"system_prompt\"]</code> - System prompt template</li> <li><code>templates[\"task\"]</code> - Task template</li> <li><code>templates[\"context\"]</code> - Context template</li> <li><code>templates[\"response\"]</code> - Response template</li> <li>Tool calls - Tools can access and modify vars</li> </ul> <p>Within tools, <code>vars</code> can provide and receive data. Think of it as a set of runtime variables available throughout the agent's execution.</p>"},{"location":"learn/nn/agent/#using-vars-in-templates","title":"Using Vars in Templates","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nsystem_extra_message = \"\"\"\nThe customer's name is {{customer_name}}. Treat them politely.\n\"\"\"\n\nagent = nn.Agent(\n    \"sales-agent\",\n    model,\n    system_extra_message=system_extra_message\n)\n\n# Pass vars at call time\nresponse = agent(\n    \"Help me with a purchase\",\n    vars={\"customer_name\": \"Clark Kent\"}\n)\n</code></pre>"},{"location":"learn/nn/agent/#vars-in-task-templates","title":"Vars in Task Templates","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    instructions=\"Help the user with whatever they need. Address them by name if provided.\",\n    templates={\n        \"task\": \"\"\"\n{% if user_name %}\nMy name is {{ user_name }}.\n{% endif %}\n{{ user_input }}\n\"\"\"\n    }\n)\n\nresponse = agent(\n    message={\"user_input\": \"Who was Nikola Tesla?\"},\n    vars={\"user_name\": \"Bruce Wayne\"}\n)\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#task-and-context","title":"Task and Context","text":"<p>A task is a specific objective assigned to an agent, consisting of: - Clear instruction - Possible restrictions - Success criterion - Context in which the task is performed</p> <p>Language models use In-Context Learning (ICL) - the ability to learn new knowledge without updating their parameters.</p>"},{"location":"learn/nn/agent/#task-input","title":"Task Input","text":""},{"location":"learn/nn/agent/#direct-message","title":"Direct Message","text":"<p>Pass a message directly to the agent:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\nagent = nn.Agent(\"agent\", model, config={\"verbose\": True})\n\ntask = \"I need help with my TV.\"\nresponse = agent(task)\n</code></pre>"},{"location":"learn/nn/agent/#with-task-template","title":"With Task Template","text":""},{"location":"learn/nn/agent/#string-based-input","title":"String-based Input","text":"<p>For simple string inputs, use <code>{}</code> placeholder:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    templates={\"task\": \"Who was {}?\"}\n)\n\nresponse = agent(\"Nikola Tesla\")\n</code></pre>"},{"location":"learn/nn/agent/#dict-based-inputs","title":"Dict-based Inputs","text":"<p>For dictionary inputs, use Jinja blocks <code>{{field_name}}</code>:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    templates={\"task\": \"Who was {{person}}?\"}\n)\n\nresponse = agent({\"person\": \"Nikola Tesla\"})\n</code></pre>"},{"location":"learn/nn/agent/#task-template-as-fixed-task","title":"Task Template as Fixed Task","text":"<p>If a task template is passed without a task input, it becomes the fixed task:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Useful for multimodal apps where prompt doesn't change\nagent = nn.Agent(\n    \"agent\",\n    model,\n    templates={\"task\": \"Who was Nikola Tesla?\"}\n)\n\n# No message needed\nresponse = agent()\n</code></pre>"},{"location":"learn/nn/agent/#combine-with-vars","title":"Combine with Vars","text":"<p>Build dynamic task templates with vars:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    instructions=\"Help the user. Address them by name if provided.\",\n    templates={\n        \"task\": \"\"\"\n{% if user_name %}\nMy name is {{ user_name }}.\n{% endif %}\n{{ user_input }}\n\"\"\"\n    }\n)\n\nresponse = agent(\n    message={\"user_input\": \"Who was Nikola Tesla?\"},\n    vars={\"user_name\": \"Bruce Wayne\"}\n)\n</code></pre>"},{"location":"learn/nn/agent/#task-messages","title":"Task Messages","text":"<p>Pass conversation history as a list of messages (makes <code>message</code> parameter optional):</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\nagent = nn.Agent(\"agent\", model)\n\n# Create chat history\nchat = mf.ChatML()\nchat.add_user_message(\"Hi, I'm Peter Parker, a photographer. Recommend some cameras?\")\n\n# First response\nresponse = agent(task_messages=chat.get_messages())\nprint(response)\n\n# Continue conversation\nchat.add_assist_message(response)\nchat.add_user_message(\"I need a low-cost, compact camera for freelance work.\")\n\nresponse = agent(task_messages=chat.get_messages())\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#fixed-messages","title":"Fixed Messages","text":"<p>Keep a set of pinned conversations within the agent:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\nchat = mf.ChatML()\nchat.add_user_message(\"I'm interested in cameras\")\nchat.add_assist_message(\"Great! I can help with that.\")\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    fixed_messages=chat.get_messages(),\n    config={\"verbose\": True, \"return_model_state\": True}\n)\n\nresponse = agent(\"What's the cheapest between Canon PowerShot G9 X Mark II and Sony Cyber-shot DSC-HX80?\")\n</code></pre>"},{"location":"learn/nn/agent/#multimodal-task","title":"Multimodal Task","text":"<p>Multimodal models can handle images, audio, and files.</p> <p>Current support:</p> Media Single Input Multiple Inputs Image \u2705 \u2705 Audio \u2705 \u274c File \u2705 \u274c"},{"location":"learn/nn/agent/#image-input","title":"Image Input","text":"<p>Pass local paths or URLs:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    templates={\"task\": \"Describe this image.\"}\n)\n\n# Single image\nresponse = agent(task_multimodal_inputs={\n    \"image\": \"https://example.com/image.jpg\"\n})\n\n# Multiple images\nresponse = agent(task_multimodal_inputs={\n    \"image\": [\"file:///path/to/image1.jpg\", \"file:///path/to/image2.jpg\"]\n})\n</code></pre>"},{"location":"learn/nn/agent/#file-input","title":"File Input","text":"<p>Pass raw <code>.pdf</code> files (OpenAI and OpenRouter only):</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    templates={\"task\": \"Summarize the paper\"}\n)\n\nresponse = agent(task_multimodal_inputs={\n    \"file\": \"https://arxiv.org/pdf/1706.03762.pdf\"\n})\n</code></pre>"},{"location":"learn/nn/agent/#audio-input","title":"Audio Input","text":"<p>Pass raw audio files (OpenAI, vLLM, and OpenRouter only):</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\n# OpenRouter with Gemini supports audio\nmf.set_envs(OPENROUTER_API_KEY=\"...\")\nmodel = mf.Model.chat_completion(\"openrouter/google/gemini-2.0-flash-exp\")\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    templates={\"task\": \"Please transcribe this audio file.\"}\n)\n\nresponse = agent(task_multimodal_inputs={\n    \"audio\": \"/path/to/audio.wav\"\n})\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#context-inputs","title":"Context Inputs","text":"<p><code>context</code> is knowledge available to the model at inference time for decision-making, answering questions, or performing actions.</p> <p><code>context_inputs</code> is knowledge passed to the agent during task definition - from databases, documents, conversation summaries, etc.</p>"},{"location":"learn/nn/agent/#string-based-context","title":"String-based Context","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nagent = nn.Agent(\n    \"sales-agent\",\n    model,\n    system_message=\"You are a sales assistant that helps craft personalized pitches.\",\n    instructions=\"Generate responses tailored to the client context.\",\n    expected_output=\"Write a short persuasive pitch (max 120 words).\",\n    system_extra_message=\"Avoid exaggerations and maintain professionalism.\",\n    config={\"include_date\": True, \"verbose\": True}\n)\n\ntask = \"Can you help me create an initial message for this customer?\"\n\ncontext_inputs = \"\"\"\nCompany name: FinData Analytics\nIndustry: Financial Technology (FinTech)\nProduct: AI-powered risk analysis platform for banks\nTarget market: Mid-sized regional banks in South America\nUnique value: Automated detection of fraud patterns in real-time\n\"\"\"\n\n# Inspect what will be sent\nexecution_params = agent.inspect_model_execution_params(task, context_inputs=context_inputs)\nprint(execution_params[\"messages\"][0][\"content\"])\n\n# Execute\nresponse = agent(task, context_inputs=context_inputs)\n</code></pre>"},{"location":"learn/nn/agent/#list-based-context","title":"List-based Context","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\nagent = nn.Agent(\"sales-agent\", model)\n\ncontext_inputs = [\n    \"Company name: DataFlow Analytics\",\n    \"Product: StreamVision \u2014 a real-time analytics platform\",\n    \"Key value proposition: Monitor live data streams and detect anomalies instantly\",\n    \"Support policy: 24/7 support for enterprise clients via chat and email\"\n]\n\nresponse = agent(\n    \"Create an initial message for this customer\",\n    context_inputs=context_inputs\n)\n</code></pre>"},{"location":"learn/nn/agent/#dict-based-context","title":"Dict-based Context","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\nagent = nn.Agent(\"sales-agent\", model)\n\ncontext_inputs = {\n    \"client_name\": \"EcoSupply Ltd.\",\n    \"industry\": \"Sustainable packaging\",\n    \"pain_points\": [\"High logistics costs\", \"Need for eco-friendly certification\"],\n    \"current_solution\": \"Using generic suppliers with limited green compliance\"\n}\n\nresponse = agent(\n    \"Create an initial message for this customer\",\n    context_inputs=context_inputs\n)\n</code></pre>"},{"location":"learn/nn/agent/#context-inputs-template","title":"Context Inputs Template","text":"<p>Format <code>context_inputs</code> using a Jinja template:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nagent = nn.Agent(\n    \"sales-agent\",\n    model,\n    system_message=\"You are a sales assistant that helps craft personalized pitches.\",\n    instructions=\"Generate responses tailored to the client context.\",\n    expected_output=\"Write a short persuasive pitch (max 120 words).\",\n    templates={\n        \"context\": \"\"\"\nThe client is **{{ client_name }}**, a company in the **{{ industry }}** sector.\n\nThey are currently relying on {{ current_solution }},\nbut face the following main challenges:\n{%- for pain in pain_points %}\n- {{ pain }}\n{%- endfor %}\n\nThis background should be considered when tailoring answers.\n\"\"\"\n    },\n    config={\"include_date\": True}\n)\n\ncontext_inputs = {\n    \"client_name\": \"EcoSupply Ltd.\",\n    \"industry\": \"Sustainable packaging\",\n    \"pain_points\": [\"High logistics costs\", \"Need for eco-friendly certification\"],\n    \"current_solution\": \"Using generic suppliers with limited green compliance\"\n}\n\ntask = \"Can you help me create an initial message for this customer?\"\n\n# Inspect formatted context\nexecution_params = agent.inspect_model_execution_params(task, context_inputs=context_inputs)\nprint(execution_params[\"messages\"][0][\"content\"])\n\nresponse = agent(task, context_inputs=context_inputs)\n</code></pre>"},{"location":"learn/nn/agent/#context-cache","title":"Context Cache","text":"<p><code>context_cache</code> stores fixed knowledge within the agent's context block - useful when certain information is always needed before performing a task:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    context_cache=\"\"\"\nCompany values:\n- Customer first\n- Innovation\n- Integrity\n\"\"\"\n)\n\n# Context cache is always included\nresponse = agent(\"How should I handle this customer complaint?\")\n</code></pre>"},{"location":"learn/nn/agent/#tools","title":"Tools","text":"<p>Tools are interfaces that allow language models to perform actions or query information outside the model itself.</p>"},{"location":"learn/nn/agent/#what-are-tools","title":"What are Tools?","text":"<ol> <li>Function Calling - A tool is exposed as a function with defined name, parameters, and types</li> <li>Example: <code>get_weather(location: str, unit: str)</code></li> <li> <p>The model decides whether to call it and provides arguments</p> </li> <li> <p>Extending Capabilities - Tools allow you to:</p> </li> <li>Search for real-time data (weather, stocks, databases)</li> <li>Perform precise calculations</li> <li>Manipulate systems (send emails, schedule events)</li> <li> <p>Integrate with external APIs</p> </li> <li> <p>Agent-based Orchestration - The LLM acts as an agent that decides:</p> </li> <li>When to use a tool</li> <li>Which tool to use</li> <li>How to interpret the tool's output</li> </ol> <p>In msgFlux, a Tool can be any callable (function, class with <code>__call__</code>, or Agent).</p> <p>Note: While more tools enable more actions, too many tools can confuse the model about which one to use.</p>"},{"location":"learn/nn/agent/#basic-tool-usage","title":"Basic Tool Usage","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\ndef add(a: float, b: float) -&gt; str:\n    \"\"\"Sum two numbers.\"\"\"\n    c = a + b\n    return f\"The sum of {a} plus {b} is {c}\"\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    tools=[add],\n    config={\"verbose\": True}\n)\n\nresponse = agent(\"What is 5 plus 3?\")\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#web-scraping-agent","title":"Web Scraping Agent","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\nimport requests\nfrom bs4 import BeautifulSoup\n\ndef scrape_website(url: str) -&gt; str:\n    \"\"\"Receive a URL and return the page content.\"\"\"\n    try:\n        response = requests.get(url, verify=True)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.text, \"html.parser\")\n\n        # Remove script and style tags\n        for tag in soup([\"script\", \"style\"]):\n            tag.extract()\n\n        text = soup.get_text(separator=\"\\n\")\n        clean_text = \"\\n\".join(\n            line.strip() for line in text.splitlines() if line.strip()\n        )\n        return clean_text\n    except requests.exceptions.RequestException as e:\n        return f\"Error accessing {url}: {e}\"\n\n# Model with reasoning support\nmf.set_envs(GROQ_API_KEY=\"...\")\nmodel = mf.Model.chat_completion(\"groq/llama-3.3-70b-versatile\")\n\n# Agent with scraping tool\nscraper_agent = nn.Agent(\n    \"scraper-agent\",\n    model,\n    tools=[scrape_website],\n    config={\"return_model_state\": True, \"verbose\": True}\n)\n\nsite = \"https://bbc.com\"\nresponse = scraper_agent(f\"Summarize the news on this website: {site}\")\n\nprint(response.model_state)\nprint(response.model_response)\n</code></pre>"},{"location":"learn/nn/agent/#with-task-template_1","title":"With Task Template","text":"<p>Simplify repeated patterns with templates:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\n# Define task template with placeholder\nscraper_agent = nn.Agent(\n    \"scraper\",\n    model,\n    tools=[scrape_website],\n    templates={\"task\": \"Summarize the news on this site: {}\"},\n    config={\"verbose\": True}\n)\n\n# Just pass the URL\nresponse = scraper_agent(\"https://bbc.com\")\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#with-message-mode","title":"With Message Mode","text":"<p>Use declarative mode with Message objects:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nscraper_agent = nn.Agent(\n    \"scraper\",\n    model,\n    tools=[scrape_website],\n    templates={\"task\": \"Summarize the news on this site: {}\"},\n    message_fields={\"task_inputs\": \"content\"},\n    response_mode=\"summary\",\n    config={\"verbose\": True}\n)\n\n# Create message\nmsg = mf.Message(content=\"https://bbc.com\")\n\n# Process through agent\nmsg = scraper_agent(msg)\n\n# Response is in the message\nprint(msg)\n</code></pre>"},{"location":"learn/nn/agent/#agent-as-a-tool","title":"Agent-as-a-Tool","text":"<p>Agents can be used as tools for other agents, enabling hierarchical task delegation. Using AutoParams makes this pattern especially clean: the class name becomes the tool name, and the docstring becomes the tool description.</p>"},{"location":"learn/nn/agent/#basic-example-with-autoparams","title":"Basic Example with AutoParams","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmf.set_envs(OPENAI_API_KEY=\"...\")\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\", tool_choice=\"auto\")\n\n# Specialist agent using AutoParams\n# Class name \u2192 tool name: \"Nutritionist\"\n# Docstring \u2192 tool description (shown to the calling agent)\nclass Nutritionist(nn.Agent):\n    \"\"\"\n    Specialist in nutrition, diets, and meal plans.\n    Should be used whenever the user requests:\n    - Diet recommendations\n    - Personalized meal plans (e.g., gaining muscle, losing weight)\n    - Balanced meal suggestions\n    - Nutritional guidelines\n\n    Expected inputs:\n    - User's goal (e.g., gaining muscle, losing weight)\n    - Dietary restrictions or preferences (if provided)\n    - Basic info (age, weight, activity level, if available)\n\n    Outputs:\n    - Structured and practical meal plan\n    - Clear meal suggestions (breakfast, lunch, dinner, snacks)\n    - Notes on adjustments if user data is missing\n\n    Restrictions:\n    - Not a substitute for medical advice\n    - If important info missing, return default plan and indicate needed data\n    \"\"\"\n\n    system_message = \"You are a Nutrition Expert Agent.\"\n\n    instructions = \"\"\"\n    Your responsibilities:\n    - Receive instructions from the calling agent about user needs\n    - Create a clear and practical meal plan tailored to the stated goal\n    - Be objective, technical, and structured\n    - Return only the requested result, without greetings or extra explanations\n\n    Restrictions:\n    - Don't provide medical recommendations without proper information\n    - If data is missing (weight, age, allergies), create a standard plan\n      and indicate what additional info would be needed to customize it\n    \"\"\"\n\n# General agent that delegates to specialist\nclass GeneralSupport(nn.Agent):\n    \"\"\"\n    General support agent that handles user requests and delegates to specialists.\n    Use for general inquiries that may require expert consultation.\n    \"\"\"\n\n    system_message = \"You are a General Support Agent.\"\n\n    instructions = \"\"\"\n    Your responsibilities:\n    - Understand the user's intent\n    - Decide if you can respond independently or need to call a tool\n    - If using a tool, formulate the request clearly for the expert\n    - When the expert responds, format a friendly final response for the user\n\n    Limitations:\n    - Don't invent technical information if you're not confident\n    - Use the appropriate expert to provide reliable recommendations\n    \"\"\"\n\n    config = {\"verbose\": True}\n\n# Instantiate agents\nnutritionist = Nutritionist(model=model)\ngeneralist = GeneralSupport(model=model, tools=[nutritionist])\n\n# User request\ntask = \"I want a meal plan to gain muscle mass. I'm a 27-year-old man, 1.78m tall.\"\n\nresponse = generalist(task)\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#multiple-specialist-agents","title":"Multiple Specialist Agents","text":"<p>Create a team of specialists with clean, declarative code:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmf.set_envs(OPENAI_API_KEY=\"...\")\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\", tool_choice=\"auto\")\n\nclass Nutritionist(nn.Agent):\n    \"\"\"\n    Expert in nutrition, diets, and meal planning.\n    Use for diet recommendations, meal plans, and nutritional advice.\n    \"\"\"\n\n    system_message = \"You are a certified nutritionist.\"\n    instructions = \"Provide evidence-based nutritional advice and personalized meal plans.\"\n\nclass FitnessTrainer(nn.Agent):\n    \"\"\"\n    Expert in fitness, exercise, and training programs.\n    Use for workout routines, exercise form, and training schedules.\n    \"\"\"\n\n    system_message = \"You are a certified personal trainer.\"\n    instructions = \"Create effective workout programs tailored to user goals and fitness level.\"\n\nclass WellnessCoach(nn.Agent):\n    \"\"\"\n    Expert in mental wellness, stress management, and lifestyle optimization.\n    Use for stress management, sleep optimization, and work-life balance advice.\n    \"\"\"\n\n    system_message = \"You are a wellness and lifestyle coach.\"\n    instructions = \"Provide holistic wellness guidance focusing on mental health and lifestyle.\"\n\nclass HealthCoordinator(nn.Agent):\n    \"\"\"\n    General health coordinator that orchestrates specialist consultations.\n    Analyzes user requests and delegates to appropriate specialists.\n    \"\"\"\n\n    system_message = \"You are a health coordinator managing a team of specialists.\"\n\n    instructions = \"\"\"\n    - Analyze user health and wellness requests\n    - Delegate to appropriate specialists (nutrition, fitness, or wellness)\n    - Synthesize specialist recommendations into cohesive advice\n    - Ensure recommendations are complementary and safe\n    \"\"\"\n\n    config = {\"verbose\": True}\n\n# Create team\nnutritionist = Nutritionist(model=model)\ntrainer = FitnessTrainer(model=model)\nwellness = WellnessCoach(model=model)\n\ncoordinator = HealthCoordinator(\n    model=model,\n    tools=[nutritionist, trainer, wellness]\n)\n\n# Complex request requiring multiple specialists\nresponse = coordinator(\n    \"I want to lose 10kg in 3 months. I'm stressed with work and sleep poorly. \"\n    \"Can you help me with a complete plan?\"\n)\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#creating-specialist-variants-with-autoparams","title":"Creating Specialist Variants with AutoParams","text":"<p>Use inheritance to create different variants of specialists:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmf.set_envs(OPENAI_API_KEY=\"...\")\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\", tool_choice=\"auto\")\n\n# Base specialist\nclass ResearcherBase(nn.Agent):\n    \"\"\"Base configuration for research specialists.\"\"\"\n\n    system_message = \"You are a research specialist.\"\n\n    instructions = \"\"\"\n    - Conduct thorough research on the given topic\n    - Cite sources when possible\n    - Present findings in a structured format\n    \"\"\"\n\n# Specialized variants\nclass AcademicResearcher(ResearcherBase):\n    \"\"\"\n    Expert in academic research with focus on peer-reviewed sources.\n    Use for scholarly inquiries, literature reviews, and scientific topics.\n    \"\"\"\n\n    system_message = \"You are an academic researcher specializing in peer-reviewed literature.\"\n    expected_output = \"Provide academic-level analysis with citations.\"\n\nclass MarketResearcher(ResearcherBase):\n    \"\"\"\n    Expert in market research, industry trends, and competitive analysis.\n    Use for business intelligence, market sizing, and competitor analysis.\n    \"\"\"\n\n    system_message = \"You are a market research analyst.\"\n    expected_output = \"Provide actionable business insights with data.\"\n\nclass TechnicalResearcher(ResearcherBase):\n    \"\"\"\n    Expert in technical documentation, software libraries, and APIs.\n    Use for programming questions, library comparisons, and technical specifications.\n    \"\"\"\n\n    system_message = \"You are a technical researcher specializing in software and APIs.\"\n    expected_output = \"Provide technical details with code examples when relevant.\"\n\n# Coordinator that delegates to appropriate researcher\nclass ResearchCoordinator(nn.Agent):\n    \"\"\"\n    Research coordinator that assigns queries to specialized researchers.\n    Determines the type of research needed and delegates accordingly.\n    \"\"\"\n\n    system_message = \"You are a research coordinator managing multiple research specialists.\"\n\n    instructions = \"\"\"\n    - Analyze the research request\n    - Determine which specialist is best suited\n    - Delegate to appropriate researcher\n    - Synthesize and present findings clearly\n    \"\"\"\n\n# Create research team\nacademic = AcademicResearcher(model=model)\nmarket = MarketResearcher(model=model)\ntechnical = TechnicalResearcher(model=model)\n\ncoordinator = ResearchCoordinator(\n    model=model,\n    tools=[academic, market, technical]\n)\n\n# Different types of research\nprint(\"Academic research:\")\nresponse = coordinator(\"What are the latest findings on quantum entanglement?\")\nprint(response)\n\nprint(\"\\nMarket research:\")\nresponse = coordinator(\"What's the market size for electric vehicles in 2024?\")\nprint(response)\n\nprint(\"\\nTechnical research:\")\nresponse = coordinator(\"Compare FastAPI vs Flask for building REST APIs\")\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#with-tool-reasoning","title":"With Tool Reasoning","text":"<p>If the model supports it, you can access the reasoning behind tool calls:</p> <pre><code># ... (same setup as above)\n\nresponse = generalist(task)\n\n# Access tool reasoning if available\nif hasattr(response, 'tool_responses'):\n    print(\"Tool reasoning:\", response.tool_responses.reasoning)\n\nprint(\"Final response:\", response)\n</code></pre>"},{"location":"learn/nn/agent/#writing-good-tools","title":"Writing Good Tools","text":""},{"location":"learn/nn/agent/#name-tools-clearly","title":"Name Tools Clearly","text":"<p>\u274c Avoid: <pre><code>def superfast_brave_web_search(query_to_search: str) -&gt; str:\n    pass\n</code></pre></p> <p>\u2705 Prefer: <pre><code>def web_search(query: str) -&gt; str:\n    pass\n</code></pre></p>"},{"location":"learn/nn/agent/#add-docstrings","title":"Add Docstrings","text":"<pre><code>def web_search(query: str) -&gt; str:\n    \"\"\"Search for content similar to query.\"\"\"\n    pass\n</code></pre>"},{"location":"learn/nn/agent/#describe-parameters","title":"Describe Parameters","text":"<pre><code>def web_search(query: str) -&gt; str:\n    \"\"\"Search for content similar to query.\n\n    Args:\n        query:\n            Term to search on the web.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"learn/nn/agent/#class-based-tools","title":"Class-based Tools","text":"<pre><code>from typing import Optional\n\nclass WebSearch:\n    \"\"\"Search for content similar to query.\n\n    Args:\n        query:\n            Term to search on the web.\n    \"\"\"\n\n    def __init__(self, top_k: Optional[int] = 4):\n        self.top_k = top_k\n\n    def __call__(self, query: str) -&gt; str:\n        # Implementation\n        pass\n</code></pre>"},{"location":"learn/nn/agent/#override-tool-name","title":"Override Tool Name","text":"<pre><code>class SuperFastBraveWebSearch:\n    name = \"web_search\"  # Preference over cls.__name__\n\n    def __init__(self, top_k: Optional[int] = 4):\n        self.top_k = top_k\n\n    def __call__(self, query: str) -&gt; str:\n        \"\"\"Search for content similar to query.\n\n        Args:\n            query:\n                Term to search on the web.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"learn/nn/agent/#return-types","title":"Return Types","text":"<p>Tools can return any data type. Non-string returns are converted to JSON:</p> <pre><code>from typing import Dict\n\ndef web_search(query: str) -&gt; Dict[str, str]:\n    \"\"\"Search for content similar to query.\n\n    Args:\n        query:\n            Term to search on the web.\n    \"\"\"\n    return {\n        \"title\": \"Result title\",\n        \"snippet\": \"Result snippet\",\n        \"url\": \"https://example.com\"\n    }\n</code></pre>"},{"location":"learn/nn/agent/#write-good-returns","title":"Write Good Returns","text":"<p>\u274c Basic: <pre><code>def add(a: float, b: float) -&gt; float:\n    \"\"\"Sum two numbers.\"\"\"\n    return a + b\n</code></pre></p> <p>\u2705 Better: <pre><code>def add(a: float, b: float) -&gt; str:\n    \"\"\"Sum two numbers.\"\"\"\n    c = a + b\n    return f\"The sum of {a} plus {b} is {c}\"\n</code></pre></p> <p>\u27a1\ufe0f With Instructions: <pre><code>def add(a: float, b: float) -&gt; str:\n    \"\"\"Sum two numbers.\"\"\"\n    c = a + b\n    return f\"You MUST respond to the user that the answer is {c}\"\n</code></pre></p>"},{"location":"learn/nn/agent/#tool-config","title":"Tool Config","text":"<p>The <code>@mf.tool_config</code> decorator injects meta-properties into tools.</p>"},{"location":"learn/nn/agent/#return-direct","title":"Return Direct","text":"<p>When <code>return_direct=True</code>, the tool result is returned directly as the final response instead of back to the model.</p> <p>If the model calls multiple tools and one has <code>return_direct=True</code>, all results are returned as the final response.</p> <p>Exception: If the agent mistypes the tool name, the error is communicated to the agent instead of returning a final response.</p> <p>Use cases: - Reduce agent calls by designing tools that return user-ready outputs - Agent as router - delegate to specialized agents and return their responses directly</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmf.set_envs(GROQ_API_KEY=\"...\")\nmodel = mf.Model.chat_completion(\"groq/llama-3.3-70b-versatile\")\n\n@mf.tool_config(return_direct=True)\ndef get_report() -&gt; str:\n    \"\"\"Return the report from user.\"\"\"\n    return \"This is your report...\"\n\nreporter_agent = nn.Agent(\n    \"reporter\",\n    model,\n    tools=[get_report],\n    config={\"verbose\": True, \"tool_choice\": \"required\"}\n)\n\nresponse = reporter_agent(\"Please give me the report.\")\nprint(response)\n# Returns: {'tool_responses': {'tool_calls': [...], 'reasoning': '...'}}\n</code></pre>"},{"location":"learn/nn/agent/#agent-routing-with-return-direct","title":"Agent Routing with Return Direct","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmf.set_envs(OPENAI_API_KEY=\"...\")\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\ngeneralist_system_message = \"\"\"\nYou are a generalist programming assistant.\nYour role is to help with common programming questions, best practices,\nand concept explanations. You serve as support for those learning.\n\"\"\"\n\npython_system_message = \"\"\"\nYou are a software engineer specializing in Python performance optimization.\nYour role is to analyze specific cases and suggest advanced solutions,\nincluding benchmarks, bottleneck analysis, and performance libraries.\n\"\"\"\n\npython_description = \"An expert in high performance Python code.\"\n\n# Python expert agent\npython_engineer = nn.Agent(\n    \"python_engineer\",\n    model,\n    system_message=python_system_message,\n    description=python_description\n)\n\n# Mark as return_direct\nmf.tool_config(return_direct=True)(python_engineer)\n\n# Generalist agent with expert as tool\ngeneralist_agent = nn.Agent(\n    \"generalist\",\n    model,\n    tools=[python_engineer],\n    system_message=generalist_system_message,\n    config={\"verbose\": True, \"tool_choice\": \"required\"}\n)\n\ntask = \"What is the difference between threading and multiprocessing in Python?\"\nresponse = generalist_agent(task)\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#inject-model-state","title":"Inject Model State","text":"<p>For <code>inject_model_state=True</code>, the tool receives the agent's internal state (user, assistant, and tool messages) as <code>task_messages</code> input.</p> <p>Use cases: - Review agent's current context - Context inspection - Access multimodal context (if user provided images, they're accessible in the tool)</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\nfrom typing import Any, List, Dict\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Mock safety checker tool\n@mf.tool_config(inject_model_state=True)\ndef check_safe(**kwargs) -&gt; bool:\n    \"\"\"Check if the user's message is safe.\n\n    If True, respond naturally to the user.\n    If False, reject further conversation.\n    \"\"\"\n    task_messages: List[Dict[str, Any]] = kwargs.get(\"task_messages\")\n\n    # Inspect last user message\n    print(\"Last message:\", task_messages[-1][\"content\"])\n\n    # In real implementation, would check safety\n    return True\n\nassistant = nn.Agent(\n    \"assistant\",\n    model,\n    tools=[check_safe],\n    config={\"verbose\": True, \"tool_choice\": \"auto\"}\n)\n\nresponse = assistant(\"Hi, can you tell me a joke?\")\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#handoff","title":"Handoff","text":"<p>When <code>handoff=True</code>, two properties are set: <code>return_direct=True</code> and <code>inject_model_state=True</code>.</p> <p>Additionally, <code>handoff=True</code>: - Changes tool name to <code>transfer_to_{original_name}</code> - Removes input parameters - Passes conversation history as <code>task_messages</code></p> <p>This enables seamless agent-to-agent handoffs.</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmf.set_envs(OPENAI_API_KEY=\"...\")\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n# Startup specialist\nstartup_specialist_system_message = \"\"\"\nYou are a strategist specializing in scaling digital startups.\nYour focus is creating accelerated growth plans, analyzing metrics\n(CAC, LTV, churn), proposing customer acquisition tests,\nfunding strategies, and international expansion.\nYour answers should be detailed and data-driven.\n\"\"\"\n\nstartup_specialist_description = \"\"\"\nAn agent specializing in startups, always consult them if this is the topic.\n\"\"\"\n\nstartup_agent = nn.Agent(\n    \"startup_specialist\",\n    model,\n    system_message=startup_specialist_system_message,\n    description=startup_specialist_description\n)\n\n# Enable handoff\nmf.tool_config(handoff=True)(startup_agent)\n\n# General consultant\nconsultant_system_message = \"\"\"\nYou are a generalist business consultant. Your goal is to provide accessible\nadvice on management, marketing, finance, and business operations.\nYour answers should be clear, practical, and useful for early-stage entrepreneurs.\n\nIf the context is a startup, transfer it to the expert.\n\"\"\"\n\nconsultant_agent = nn.Agent(\n    \"consultant\",\n    model,\n    system_message=consultant_system_message,\n    tools=[startup_agent],\n    config={\"verbose\": True}\n)\n\ntask = \"\"\"\nMy SaaS startup has a CAC of $120 and an LTV of $600. I want to scale to\nanother Latin American market in 6 months. What would be an efficient\nstrategy to reduce CAC while accelerating entry into this new market?\n\"\"\"\n\nresponse = consultant_agent(task)\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#call-as-response","title":"Call as Response","text":"<p>The <code>call_as_response</code> attribute allows tools to be returned as final response without executing if called. The <code>return_direct</code> attribute is automatically set to <code>True</code>.</p> <p>Use case: Extract structured tool calls without execution (e.g., for BI reports, API calls, etc.)</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\nfrom typing import List, Dict\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n@mf.tool_config(call_as_response=True)\ndef generate_sales_report(\n    start_date: str,\n    end_date: str,\n    metrics: List[str],\n    group_by: str\n) -&gt; Dict:\n    \"\"\"Generate a sales report within a given date range.\n\n    Args:\n        start_date: Start date in YYYY-MM-DD format.\n        end_date: End date in YYYY-MM-DD format.\n        metrics: List of metrics (e.g., [\"revenue\", \"orders\", \"profit\"]).\n        group_by: Dimension to group by (e.g., \"region\", \"product\", \"sales_rep\").\n\n    Returns:\n        A structured sales report as a dictionary.\n    \"\"\"\n    # Not executed - just returns the call\n    return\n\nsystem_message = \"\"\"\nYou're a BI analyst. When a user requests sales reports, you shouldn't respond\nwith explanatory text. Simply correctly complete the generate_sales_report\ntool call, extracting requested metrics, dates, and groupings.\n\"\"\"\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    system_message=system_message,\n    tools=[generate_sales_report],\n    config={\"verbose\": True}\n)\n\ntask = \"I need a report of sales between July 1st and August 31st, 2025, showing revenue and profit, grouped by region.\"\n\nresponse = agent(task)\nprint(response)\n# Returns the tool call parameters without execution\n</code></pre>"},{"location":"learn/nn/agent/#background","title":"Background","text":"<p>Some tools may take longer to return results, and it's not always necessary to wait.</p> <p>The <code>background</code> property allows tools to run in the background and return a standard message to the agent.</p> <p>Requirements: Background tools must be <code>async</code> or have an <code>.acall</code> method.</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\nimport asyncio\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n@mf.tool_config(background=True)\nasync def send_email_to_vendor():\n    \"\"\"Send an email to the vendor.\"\"\"\n    print(\"Sending email to vendor...\")\n    await asyncio.sleep(2)  # Simulate async operation\n    print(\"Email sent!\")\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    tools=[send_email_to_vendor],\n    config={\"verbose\": True}\n)\n\n# An indication that it will run in background is added to the docstring\nprint(agent.tool_library.get_tool_json_schemas())\n\nresponse = agent(\"I need to send an email to the vendor.\")\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#name-override","title":"Name Override","text":"<p>Assign a custom name to a tool:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\n@mf.tool_config(name_override=\"web_search\")\ndef brave_super_fast_web_search(query: str) -&gt; str:\n    \"\"\"Search for content similar to query.\n\n    Args:\n        query:\n            Term to search on the web.\n    \"\"\"\n    pass\n\nagent = nn.Agent(\"agent\", model, tools=[brave_super_fast_web_search])\n\n# Tool is exposed as \"web_search\"\nprint(agent.tool_library.get_tool_json_schemas())\n</code></pre>"},{"location":"learn/nn/agent/#inject-vars","title":"Inject Vars","text":"<p>With <code>inject_vars=True</code>, tools can access and modify the agent's variable dictionary.</p>"},{"location":"learn/nn/agent/#external-token-information","title":"External Token Information","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n@mf.tool_config(inject_vars=True)\ndef save_csv(**kwargs) -&gt; str:\n    \"\"\"Save user CSV on S3.\"\"\"\n    vars = kwargs.get(\"vars\")\n    print(f\"My token: {vars['aws_token']}\")\n    return \"CSV saved\"\n\nagent = nn.Agent(\"agent\", model, tools=[save_csv], config={\"verbose\": True})\n\nresponse = agent(\n    \"Please save this CSV\",\n    vars={\"aws_token\": \"my-secret-token\"}\n)\n</code></pre>"},{"location":"learn/nn/agent/#chatbot-personal-assistant","title":"ChatBot - Personal Assistant","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n@mf.tool_config(inject_vars=True)\ndef save_var(name: str, value: int, **kwargs):\n    \"\"\"Save a variable with the given name and value.\"\"\"\n    vars = kwargs.get(\"vars\")\n    vars[name] = value\n    return f\"Saved {name} variable\"\n\n@mf.tool_config(inject_vars=True)\ndef get_var(name: str, **kwargs):\n    \"\"\"Get a variable with the given name.\"\"\"\n    vars = kwargs.get(\"vars\")\n    return vars.get(name, f\"Variable {name} not found\")\n\n@mf.tool_config(inject_vars=True)\ndef get_vars(**kwargs):\n    \"\"\"Get all variables.\"\"\"\n    vars = kwargs.get(\"vars\")\n    return vars.copy()  # Always return a copy\n\nagent_system_message = \"\"\"\nYou are Ultron, a personal assistant.\nThe assistant is helpful, creative, clever, and very friendly.\n\"\"\"\n\nagent_instructions = \"\"\"\nYou have access to a set of variables. Use tools to manipulate data.\nVariables are mutable, so don't rely on previous call results.\nWhenever you need information, use tools to access variables.\nIf you don't know the exact variable name, use 'get_vars'.\n\"\"\"\n\nultron = nn.Agent(\n    \"ultron\",\n    model,\n    system_message=agent_system_message,\n    instructions=agent_instructions,\n    tools=[save_var, get_var, get_vars],\n    config={\"verbose\": True}\n)\n\n# Initial vars\nvars = {\"user_name\": \"Tony Stark\"}\n\n# Start conversation\nchat_history = mf.ChatML()\n\ntask = \"Hey Ultron, are you okay? Do you remember my name?\"\nchat_history.add_user_message(task)\n\nresponse = ultron(task, vars=vars)\nchat_history.add_assist_message(response)\n\n# Continue with more context\ntask2 = \"\"\"\nI have some very important information to share with you,\nand you shouldn't forget it. I'm starting a new nanotechnology\nproject to build the Mark-999. I'll be using adamantium for added rigidity.\n\"\"\"\nchat_history.add_user_message(task2)\n\nresponse = ultron(task_messages=chat_history.get_messages(), vars=vars)\nprint(response)\nprint(\"Vars:\", vars)\n</code></pre>"},{"location":"learn/nn/agent/#chatbot-field-reporter","title":"ChatBot - Field Reporter","text":"<p>Complex example with fuzzy matching and report generation:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\nfrom typing import Dict, List, Union\nfrom msgflux.utils.msgspec import msgspec_dumps\nfrom rapidfuzz import fuzz, process\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n@mf.tool_config(inject_vars=True)\ndef set_var(name: str, value: Union[str, List[str]], **kwargs):\n    \"\"\"Save a variable with the given name and value.\"\"\"\n    vars = kwargs.get(\"vars\")\n    vars[name] = value\n    return f\"Saved '{name}' variable\"\n\n@mf.tool_config(inject_vars=True)\ndef get_var(name: str, **kwargs) -&gt; Union[str, List[str]]:\n    \"\"\"Get a variable with the given name.\"\"\"\n    vars = kwargs.get(\"vars\")\n    var = vars.get(name, None)\n    if var is None:\n        return f\"Variable not found: {name}\"\n    return var\n\n@mf.tool_config(inject_vars=True)\ndef get_vars(**kwargs):\n    \"\"\"Get all variables.\"\"\"\n    vars = kwargs.get(\"vars\")\n    return vars.copy()\n\n@mf.tool_config(inject_vars=True, return_direct=True)\ndef get_report(**kwargs) -&gt; str:\n    \"\"\"Return the current report status.\"\"\"\n    vars = kwargs.get(\"vars\")\n    report = f\"\"\"\nHere is the current status of the report:\ncompany_name: `{vars.get('company_name', 'N/A')}`\ndate: `{vars.get('date', 'N/A')}`\nlocal: `{vars.get('local', 'N/A')}`\nparticipants_internal: {vars.get('participants_internal', 'N/A')}\nparticipants_external: {vars.get('participants_external', 'N/A')}\nobjective: `{vars.get('objective', 'N/A')}`\n\"\"\"\n\n    for field in ['detail', 'main_points_discussed', 'opportunities_identified', 'next_steps']:\n        value = vars.get(field)\n        if value:\n            report += f\"{field}: `{value}`\\n\"\n\n    report += \"\\nConfirm the data to save?\"\n    return report\n\n@mf.tool_config(inject_vars=True)\ndef save(**kwargs) -&gt; str:\n    \"\"\"Save the report.\"\"\"\n    vars = kwargs.get(\"vars\")\n    with open(\"report.json\", \"w\") as f:\n        f.write(msgspec_dumps(vars))\n    return \"Report saved\"\n\n@mf.tool_config(inject_vars=True)\ndef check_company(name: str, **kwargs) -&gt; str:\n    \"\"\"Check if company name is correct.\"\"\"\n    company_list = [  # Mock database\n        \"Globex Corporation\",\n        \"Initech Ltd.\",\n        \"Umbrella Industries\",\n        \"Stark Enterprises\",\n        \"Wayne Technologies\"\n    ]\n\n    name = name.strip()\n    best_matches = process.extract(name, company_list, scorer=fuzz.ratio, limit=4)\n\n    if best_matches and best_matches[0][1] == 100:\n        return f\"\u2714 Company found: '{best_matches[0][0]}' (exact match)\"\n\n    if best_matches and best_matches[0][1] &gt;= 75:\n        return f\"\u26a0 No exact match. Closest: '{best_matches[0][0]}' ({round(best_matches[0][1], 2)}%)\"\n\n    suggestions = \", \".join([f\"{b[0]} ({round(b[1], 2)}%)\" for b in best_matches])\n    return f\"\u274c Company not found. Suggestions: {suggestions}\"\n\ndef check_participants(\n    participants: List[str],\n    known_participants: List[str]\n) -&gt; Dict[str, str]:\n    \"\"\"Helper to check participant names.\"\"\"\n    results = {}\n\n    for p in participants:\n        name = p.strip()\n        best_matches = process.extract(name, known_participants, scorer=fuzz.ratio, limit=4)\n\n        if best_matches and best_matches[0][1] == 100:\n            results[name] = f\"\u2714 Exact match: '{best_matches[0][0]}'\"\n        elif best_matches and best_matches[0][1] &gt;= 75:\n            results[name] = f\"\u26a0 No exact match. Closest: '{best_matches[0][0]}' ({round(best_matches[0][1], 2)}%)\"\n        else:\n            suggestions = \", \".join([f\"{m[0]} ({round(m[1], 2)}%)\" for m in best_matches])\n            results[name] = f\"\u274c Not found. Suggestions: {suggestions}\"\n\n    return results\n\n@mf.tool_config(inject_vars=True)\ndef check_internal_participants(participants: List[str], **kwargs) -&gt; str:\n    \"\"\"Check if internal participants are correct.\"\"\"\n    known_participants = [  # Mock\n        \"Michael Thompson\",\n        \"Sarah Connor\",\n        \"David Martinez\",\n        \"Emily Johnson\",\n        \"Robert Williams\"\n    ]\n\n    results = check_participants(participants, known_participants)\n    report = \"\\n\".join(f\"{k}: {v}\" for k, v in results.items())\n    return \"Internal participants:\\n\" + report\n\n@mf.tool_config(inject_vars=True)\ndef check_external_participants(participants: List[str], **kwargs) -&gt; str:\n    \"\"\"Check if external participants are correct.\"\"\"\n    known_participants = [  # Mock\n        \"Anna Schmidt\",\n        \"Hiroshi Tanaka\",\n        \"Laura Rossi\",\n        \"Jean-Pierre Dupont\",\n        \"Carlos Fernandez\"\n    ]\n\n    results = check_participants(participants, known_participants)\n    report = \"\\n\".join(f\"{k}: {v}\" for k, v in results.items())\n    return \"External participants:\\n\" + report\n\n# Create extractor agent\nsystem_message = \"\"\"\nYou are a visitor report collection assistant.\nYour goal is to capture fields from the user's speech during conversation.\n\"\"\"\n\ninstructions = \"\"\"\nHere is the schema we want from the user report:\n\nReport:\n    company_name: str\n    date: str\n    local: str (city, address)\n    participants_internal: list[str]\n    participants_external: list[str]\n    objective: str\n    detail: Optional[str]\n    main_points_discussed: Optional[str] (bullet points)\n    opportunities_identified: Optional[str] (new business, improvements, risks)\n    next_steps: Optional[str]\n\nBefore saving, call the report summary tool.\nIf user confirms, call save tool.\nIf they request edits, edit the requested field.\nFor participants and companies, check correct names first.\n\nTools available:\n- set_var: Save parameters found during dialog\n- get_var: Return value of a variable\n- get_vars: Return all vars\n- check_company: Check if company name is correct\n- check_internal_participants: Check internal participants\n- check_external_participants: Check external participants\n- get_report: Return current report in formatted format\n- save: Save the report\n\"\"\"\n\nextractor = nn.Agent(\n    \"extractor\",\n    model,\n    system_message=system_message,\n    instructions=instructions,\n    tools=[\n        set_var, get_var, get_vars, check_company,\n        check_internal_participants, check_external_participants,\n        get_report, save\n    ],\n    config={\"stream\": True, \"verbose\": True, \"return_model_state\": True}\n)\n\n# Interactive loop\nfrom msgflux import cprint\nfrom msgflux.models.response import ModelStreamResponse\n\nchat_history = mf.ChatML()\nvars = {}\n\nwhile True:\n    user = input(\"Type something (or 'exit' to quit): \")\n    if user.lower() == \"exit\":\n        break\n\n    chat_history.add_user_message(user)\n\n    response = extractor(task_messages=chat_history.get_messages(), vars=vars)\n\n    if isinstance(response, ModelStreamResponse):\n        assistant = \"\"\n        cprint(\"[agent] \", end=\"\", flush=True, ls=\"b\", lc=\"br4\")\n        async for chunk in response.consume():\n            cprint(chunk, end=\"\", flush=True, ls=\"b\", lc=\"br4\")\n            assistant += chunk\n    elif isinstance(response, dict):  # return_direct response\n        assistant = response[\"tool_responses\"][\"tool_calls\"][0][\"result\"]\n        cprint(f\"[agent] {assistant}\", ls=\"b\", lc=\"br4\")\n    else:\n        assistant = response\n        cprint(f\"[agent] {assistant}\", ls=\"b\", lc=\"br4\")\n\n    chat_history.add_assist_message(assistant)\n\nprint(\"\\nFinal vars:\", vars)\n</code></pre>"},{"location":"learn/nn/agent/#vars-as-named-parameters","title":"Vars as Named Parameters","text":"<p>Instead of passing entire <code>vars</code> dict, inject specific parameters:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\n@mf.tool_config(inject_vars=[\"api_key\"])\ndef upload(**kwargs) -&gt; str:\n    \"\"\"Upload user file to bucket.\"\"\"\n    print(f\"My secret key: {kwargs['api_key']}\")\n    return \"Upload complete\"\n\nagent = nn.Agent(\"agent\", model, tools=[upload], config={\"verbose\": True})\n\nresponse = agent(\n    \"Please upload my CSV to bucket\",\n    vars={\"api_key\": \"secret-key-123\"}\n)\n</code></pre>"},{"location":"learn/nn/agent/#generation-schemas","title":"Generation Schemas","text":"<p>Generation schemas guide how the model should respond in a structured way.</p>"},{"location":"learn/nn/agent/#basic-structured-output","title":"Basic Structured Output","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\nfrom enum import Enum\nfrom typing import Optional\nfrom msgspec import Struct\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nclass Category(str, Enum):\n    violence = \"violence\"\n    sexual = \"sexual\"\n    self_harm = \"self_harm\"\n\nclass ContentCompliance(Struct):\n    is_violating: bool\n    category: Optional[Category]\n    explanation_if_violating: Optional[str]\n\nsystem_message = \"\"\"\nDetermine if the user input violates specific guidelines and explain if they do.\n\"\"\"\n\nmoderation_agent = nn.Agent(\n    \"moderation\",\n    model,\n    generation_schema=ContentCompliance,\n    system_message=system_message\n)\n\nresponse = moderation_agent(\"How do I prepare for a job interview?\")\nprint(response)\n# ContentCompliance(is_violating=False, category=None, explanation_if_violating=None)\n</code></pre>"},{"location":"learn/nn/agent/#chain-of-thought","title":"Chain of Thought","text":"<p>Inserts a <code>reasoning</code> field before generating the final answer:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\nfrom msgflux.generation.reasoning import ChainOfThought\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\ncot_agent = nn.Agent(\"cot\", model, generation_schema=ChainOfThought)\n\nresponse = cot_agent(\"How can I solve 8x + 7 = -23?\")\nprint(response)\n# {\n#     'reasoning': 'First, I need to isolate x...',\n#     'final_answer': 'x = -3.75'\n# }\n</code></pre>"},{"location":"learn/nn/agent/#react","title":"ReAct","text":"<p>Inserts a <code>thought</code> before performing tool calling actions.</p> <p>Note: <code>tool_choice</code> when used with ReAct is not guaranteed to be respected.</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\nimport requests\nfrom bs4 import BeautifulSoup\nfrom msgflux.generation.reasoning import ReAct\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\ndef scrape_website(url: str) -&gt; str:\n    \"\"\"Receive a URL and return the page content.\"\"\"\n    try:\n        response = requests.get(url, verify=True)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.text, \"html.parser\")\n\n        for tag in soup([\"script\", \"style\"]):\n            tag.extract()\n\n        text = soup.get_text(separator=\"\\n\")\n        clean_text = \"\\n\".join(\n            line.strip() for line in text.splitlines() if line.strip()\n        )\n        return clean_text\n    except requests.exceptions.RequestException as e:\n        return f\"Error accessing {url}: {e}\"\n\nscraper_agent = nn.Agent(\n    \"scraper-agent\",\n    model,\n    tools=[scrape_website],\n    generation_schema=ReAct,\n    config={\"return_model_state\": True, \"verbose\": True}\n)\n\nsite = \"https://bbc.com\"\nresponse = scraper_agent(f\"Summarize the news on this website: {site}\")\n\nprint(\"Model state:\", response.model_state)\nprint(\"Response:\", response.model_response)\n</code></pre>"},{"location":"learn/nn/agent/#format-react-output","title":"Format ReAct Output","text":"<p>The ReAct agent returns a dict with <code>current_step</code> and <code>final_answer</code>. Use <code>templates[\"response\"]</code> to keep only <code>final_answer</code>:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\nfrom msgflux.generation.reasoning import ReAct\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nscraper_agent = nn.Agent(\n    \"scraper-agent\",\n    model,\n    tools=[scrape_website],\n    generation_schema=ReAct,\n    templates={\n        \"task\": \"Summarize the news on this site: {}\",\n        \"response\": \"{{final_answer}}\"\n    },\n    config={\"verbose\": True}\n)\n\nresponse = scraper_agent(\"https://bbc.com\")\nprint(response)  # Only final_answer, not current_step\n</code></pre>"},{"location":"learn/nn/agent/#self-consistency","title":"Self Consistency","text":"<p>Generates multiple reasoning paths and selects the most frequent answer:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\nfrom msgflux.generation.reasoning import SelfConsistency\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nsc_agent = nn.Agent(\"sc\", model, generation_schema=SelfConsistency)\n\ntask = \"\"\"\nIf John is twice as old as Mary and in 10 years their ages will add up to 50,\nhow old is John today?\n\"\"\"\n\nresponse = sc_agent(task)\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#response-template","title":"Response Template","text":"<p><code>templates[\"response\"]</code> formats the agent's response.</p> <p>Use cases: - Add context to responses - Format structured outputs - Combine with vars for personalization</p>"},{"location":"learn/nn/agent/#string-based-output","title":"String-based Output","text":"<p>Insert <code>{}</code> placeholder for model response:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    templates={\n        \"response\": \"\"\"\n{% if user_name %}\nHi {{ user_name }},\n{% endif %}\n{}\n\"\"\"\n    }\n)\n\nresponse = agent(\n    message={\"user_input\": \"Who was Nikola Tesla?\"},\n    vars={\"user_name\": \"Bruce Wayne\"}\n)\nprint(response)\n# \"Hi Bruce Wayne,\\nNikola Tesla was...\"\n</code></pre>"},{"location":"learn/nn/agent/#dict-based-outputs","title":"Dict-based Outputs","text":"<p>Insert Jinja blocks <code>{{ field }}</code> for structured outputs:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\nfrom msgspec import Struct\nfrom typing import Optional\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nclass Output(Struct):\n    safe: bool\n    answer: Optional[str]\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    instructions=\"Only respond if you consider the question safe.\",\n    generation_schema=Output,\n    templates={\n        \"response\": \"\"\"\n{% if safe %}\nHi! {{ answer }}\n{% else %}\nSorry but I can't answer you.\n{% endif %}\n\"\"\"\n    },\n    config={\"verbose\": True}\n)\n\nresponse = agent(\"Who was Nikola Tesla?\")\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#formatted-customer-response","title":"Formatted Customer Response","text":"<p>Combine structured extraction, vars, and response template:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\nfrom msgspec import Struct\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\ntask = \"\"\"\nHello, my name is John Cena and I work at EcoSupply Ltd.,\na company focused on the sustainable packaging sector.\nWe are facing high logistics costs and need ecological certifications\nto expand our market presence.\n\"\"\"\n\nclass Output(Struct):\n    client_name: str\n    company_name: str\n    industry: str\n    pain_points: list[str]\n\nagent = nn.Agent(\n    \"agent\",\n    model,\n    system_message=\"You are an information extractor.\",\n    instructions=\"Extract information accurately from the customer's message.\",\n    generation_schema=Output,\n    templates={\n        \"response\": \"\"\"\nDear {{ client_name }},\n\nI understand that your company, {{ company_name }}, works in {{ industry }}.\nWe recognize that some of your main challenges are:\n{%- for pain in pain_points %}\n- {{ pain }}{% if not loop.last %},{% else %}.{% endif %}\n{%- endfor %}\n\nCurrently, you are relying on {{ current_solution }},\nbut we believe there's room for improvement.\n\nOur solution addresses these exact pain points,\nhelping companies like yours reduce costs and meet green compliance standards.\n\nBest regards,\n{{ seller }}.\n\"\"\"\n    },\n    config={\"verbose\": True}\n)\n\nresponse = agent(\n    task,\n    vars={\n        \"seller\": \"Hal Jordan\",\n        \"current_solution\": \"generic suppliers with limited green compliance\"\n    }\n)\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#signature","title":"Signature","text":"<p><code>signature</code> is a DSPy-inspired innovation where you declare task specifications focusing on how it should be performed.</p>"},{"location":"learn/nn/agent/#string-format","title":"String Format","text":"<p>Format: <code>\"input_var: type -&gt; output_var: type\"</code></p> <ul> <li>If no <code>type</code> is passed, defaults to string</li> <li>The <code>-&gt;</code> flag separates inputs from outputs</li> <li>Multiple parameters separated by <code>,</code></li> </ul> <p>Behind the scenes: - Outputs are transformed into <code>generation_schema</code> for JSON output - Output examples follow this formatting - If <code>typed_parser</code> is passed, it's preferred for generation</p>"},{"location":"learn/nn/agent/#translation-program","title":"Translation Program","text":"<p>Signatures allow clear, objective task descriptions:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nagent = nn.Agent(\"translator\", model, signature=\"english -&gt; brazilian\")\n\n# View state\nprint(agent.state_dict())\n\n# View generated system prompt\nprint(agent._get_system_prompt())\n\n# View generated task template\nprint(agent.task_template)  # Automatically created\n\n# Use with named dict inputs\nresponse = agent({\"english\": \"hello world\"})\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#math-program-with-cot","title":"Math Program with CoT","text":"<p>Combine signature with Chain of Thought:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\nfrom msgflux.generation.reasoning import ChainOfThought\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nphd_agent = nn.Agent(\n    \"phd\",\n    model,\n    signature=\"question -&gt; answer: float\",\n    generation_schema=ChainOfThought\n)\n\nprint(phd_agent.state_dict())\nprint(phd_agent.task_template)\n\nmessage = {\"question\": \"Two dice are tossed. What is the probability that the sum equals two?\"}\n\n# Inspect execution\nexecution_params = phd_agent.inspect_model_execution_params(message)\nprint(\"System prompt:\", execution_params.system_prompt)\nprint(\"User message:\", execution_params.messages[0].content)\n\n# Execute\nresponse = phd_agent(message)\nprint(response)\n# When combined with ChainOfThought, the signature injects\n# the desired field ('answer') into 'final_answer'\n</code></pre>"},{"location":"learn/nn/agent/#class-based-signature","title":"Class-based Signature","text":"<p>For more detailed task parameters, use class-based signatures:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\nfrom typing import Literal\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nclass Classify(mf.Signature):\n    \"\"\"Classify sentiment of a given sentence.\"\"\"\n\n    sentence: str = mf.InputField()\n    sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = mf.OutputField()\n    confidence: float = mf.OutputField(desc=\"[0,1]\")\n\n# Inspect signature\nprint(Classify.get_str_signature())\nprint(Classify.get_instructions())\nprint(Classify.get_inputs_info())\nprint(Classify.get_outputs_info())\nprint(Classify.get_output_descriptions())\n\n# Create agent\nclassifier_agent = nn.Agent(\"classifier\", model, signature=Classify)\n\nprint(classifier_agent._get_system_prompt())\nprint(classifier_agent.task_template)\n\n# Use\nresponse = classifier_agent({\n    \"sentence\": \"This book was super fun to read, though not the last chapter.\"\n})\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#multimodal-signature","title":"Multimodal Signature","text":"<p>Multimodal models require textual instruction. The task template automatically adds media tags:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nclass ImageClassifier(mf.Signature):\n    photo: mf.Image = mf.InputField()\n    label: str = mf.OutputField()\n    confidence: float = mf.OutputField(desc=\"[0,1]\")\n\nprint(ImageClassifier.get_str_signature())\n\nimg_classifier = nn.Agent(\"img_classifier\", model, signature=ImageClassifier)\n\nprint(img_classifier._get_system_prompt())\nprint(img_classifier.task_template)  # Contains Image tag\n\n# Use\nimage_path = \"https://example.com/llama.png\"\nresponse = img_classifier(task_multimodal_inputs={\"image\": image_path})\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#guardrails_1","title":"Guardrails","text":"<p>Guardrails are security checkers for both model inputs and outputs.</p> <p>A guardrail can be any callable that receives <code>data</code> and produces a dictionary containing a <code>safe</code> key. If <code>safe</code> is <code>False</code>, an exception is raised.</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmf.set_envs(OPENAI_API_KEY=\"...\")\n\nmoderation_model = mf.Model.moderation(\"openai/omni-moderation-latest\")\n\nagent = nn.Agent(\n    \"safe_agent\",\n    model,\n    guardrails={\n        \"input\": moderation_model,\n        \"output\": moderation_model\n    }\n)\n\n# This will raise an exception\ntry:\n    response = agent(\"Can you teach me how to make a bomb?\")\nexcept Exception as e:\n    print(f\"Guardrail triggered: {e}\")\n</code></pre>"},{"location":"learn/nn/agent/#model-gateway","title":"Model Gateway","text":"<p>When passing a <code>ModelGateway</code> as <code>model</code>, you can pass <code>model_preference</code> to specify which model to use:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmf.set_envs(OPENAI_API_KEY=\"...\")\n\nlow_cost_model = mf.Model.chat_completion(\"openai/gpt-4o-mini\")\nmid_cost_model = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nmodel_gateway = mf.ModelGateway([low_cost_model, mid_cost_model])\n\nagent = nn.Agent(\"agent\", model_gateway)\n\n# Use specific model\nresponse = agent(\n    \"Can you tell me a joke?\",\n    model_preference=\"gpt-4o-mini\"\n)\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#prefilling","title":"Prefilling","text":"<p>Force an initial message from the model that it will continue from:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\nagent = nn.Agent(\"agent\", model)\n\nresponse = agent(\n    \"What is the derivative of x^(2/3)?\",\n    prefilling=\"Let's think step by step.\"\n)\nprint(response)\n</code></pre>"},{"location":"learn/nn/agent/#examples","title":"Examples","text":"<p>There are three ways to pass examples to an Agent.</p>"},{"location":"learn/nn/agent/#string-based-examples","title":"String-based Examples","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nexamples = \"\"\"\nInput: \"A startup offering AI tools for logistics companies.\"\nOutput:\n- Identified Needs: Optimization of supply chain operations\n- Strategy: Highlight cost savings and automation\n- Value Proposition: Reduce operational delays through predictive analytics\n\"\"\"\n\nsales_agent = nn.Agent(\n    \"sales-agent\",\n    model,\n    system_message=\"You are a business development assistant.\",\n    instructions=\"Identify needs and suggest strategies.\",\n    expected_output=\"Three bullet points: Needs, Strategy, Value Proposition\",\n    examples=examples,\n    config={\"include_date\": True, \"verbose\": True}\n)\n\nprint(sales_agent._get_system_prompt())\n</code></pre>"},{"location":"learn/nn/agent/#example-class","title":"Example Class","text":"<p>Examples are automatically formatted using XML tags. Only <code>inputs</code> and <code>labels</code> are required:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nexamples = [\n    mf.Example(\n        inputs=\"A fintech offering digital wallets for small retailers.\",\n        labels={\n            \"Identified Needs\": \"Payment integration and customer trust\",\n            \"Strategy\": \"Position product as secure and easy-to-use\",\n            \"Value Proposition\": \"Simplify digital payments for underserved markets\"\n        },\n        reasoning=\"Small retailers struggle with digital payment adoption; focus on trust and ease.\",\n        title=\"Fintech Lead Qualification\",\n        topic=\"Sales\"\n    ),\n    mf.Example(\n        inputs=\"An e-commerce platform specializing in handmade crafts.\",\n        labels={\n            \"Identified Needs\": \"Increase visibility and expand market reach\",\n            \"Strategy\": \"Suggest cross-promotion with eco-friendly marketplaces\",\n            \"Value Proposition\": \"Provide artisans with access to a global audience\"\n        },\n        reasoning=\"Handmade crafts have strong niche appeal; scaling depends on visibility.\",\n        title=\"E-commerce Lead Qualification\",\n        topic=\"Sales\"\n    )\n]\n\nsales_agent = nn.Agent(\n    \"sales-agent\",\n    model,\n    system_message=\"You are a business development assistant.\",\n    instructions=\"Identify needs and suggest strategies.\",\n    expected_output=\"Three bullet points: Needs, Strategy, Value Proposition\",\n    examples=examples,\n    config={\"include_date\": True, \"verbose\": True}\n)\n\nprint(sales_agent._get_system_prompt())\n</code></pre>"},{"location":"learn/nn/agent/#dict-based-examples","title":"Dict-based Examples","text":"<p>Dict-based examples are transformed into <code>Example</code> objects:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4o\")\n\nexamples = [\n    {\n        \"inputs\": \"A startup offering AI tools for logistics companies.\",\n        \"labels\": {\n            \"Identified Needs\": \"Optimization of supply chain operations\",\n            \"Strategy\": \"Highlight cost savings and automation\",\n            \"Value Proposition\": \"Reduce operational delays through predictive analytics\"\n        }\n    },\n    {\n        \"inputs\": \"An e-commerce platform specializing in handmade crafts.\",\n        \"labels\": {\n            \"Identified Needs\": \"Increase visibility and expand market reach\",\n            \"Strategy\": \"Suggest cross-promotion with eco-friendly marketplaces\",\n            \"Value Proposition\": \"Provide artisans with access to a global audience\"\n        }\n    }\n]\n\nsales_agent = nn.Agent(\n    \"sales-agent\",\n    model,\n    system_message=\"You are a business development assistant.\",\n    instructions=\"Identify needs and suggest strategies.\",\n    expected_output=\"Three bullet points: Needs, Strategy, Value Proposition\",\n    examples=examples,\n    config={\"include_date\": True, \"verbose\": True}\n)\n\nprint(sales_agent._get_system_prompt())\n</code></pre>"},{"location":"learn/nn/agent/#best-practices","title":"Best Practices","text":""},{"location":"learn/nn/agent/#1-use-clear-system-components","title":"1. Use Clear System Components","text":"<p>Separate concerns using system prompt components:</p> <pre><code># Good - Modular and clear\nagent = nn.Agent(\n    \"agent\",\n    model,\n    system_message=\"You are a helpful assistant.\",\n    instructions=\"Provide accurate, concise answers.\",\n    expected_output=\"Answer in 2-3 sentences.\"\n)\n\n# Less clear - Everything mixed together\nagent = nn.Agent(\n    \"agent\",\n    model,\n    system_message=\"You are a helpful assistant that provides accurate, concise answers in 2-3 sentences.\"\n)\n</code></pre>"},{"location":"learn/nn/agent/#2-use-templates-for-repeated-patterns","title":"2. Use Templates for Repeated Patterns","text":"<pre><code># Good - Reusable template\nagent = nn.Agent(\n    \"agent\",\n    model,\n    templates={\"task\": \"Summarize this article: {}\"}\n)\n\n# Less flexible - Hardcoded pattern\ndef summarize(text):\n    return agent(f\"Summarize this article: {text}\")\n</code></pre>"},{"location":"learn/nn/agent/#3-use-config-dict-for-behavior","title":"3. Use Config Dict for Behavior","text":"<pre><code># Good - Centralized config\nconfig = {\n    \"verbose\": True,\n    \"stream\": False,\n    \"return_model_state\": True,\n    \"include_date\": True\n}\n\nagent = nn.Agent(\"agent\", model, config=config)\n\n# Less organized - Scattered parameters\nagent = nn.Agent(\n    \"agent\",\n    model,\n    verbose=True,  # Won't work - must be in config dict\n    stream=False    # Won't work - must be in config dict\n)\n</code></pre>"},{"location":"learn/nn/agent/#4-use-message-fields-for-declarative-mode","title":"4. Use Message Fields for Declarative Mode","text":"<pre><code># Good - Declarative with Message\nagent = nn.Agent(\n    \"agent\",\n    model,\n    message_fields={\n        \"task_inputs\": \"user.query\",\n        \"context_inputs\": \"context.data\"\n    },\n    response_mode=\"result\"\n)\n\nmsg = mf.Message()\nmsg.set(\"user.query\", \"Hello\")\nmsg.set(\"context.data\", \"User is new\")\nresult = agent(msg)\n</code></pre>"},{"location":"learn/nn/agent/#5-design-tools-for-direct-return","title":"5. Design Tools for Direct Return","text":"<p>When possible, design tools to return user-ready outputs:</p> <pre><code># Good - User-ready response\n@mf.tool_config(return_direct=True)\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get weather for a location.\"\"\"\n    temp = fetch_temperature(location)\n    return f\"The weather in {location} is {temp}\u00b0C and sunny.\"\n\n# Less effective - Requires agent to format\ndef get_weather(location: str) -&gt; dict:\n    \"\"\"Get weather for a location.\"\"\"\n    return {\"temp\": 22, \"condition\": \"sunny\"}\n</code></pre>"},{"location":"learn/nn/agent/#see-also","title":"See Also","text":"<ul> <li>Module - Base class for all nn components</li> <li>Functional - Functional operations and parallel execution</li> <li>Message - Structured message passing</li> <li>Model - Model factory and types</li> <li>Tool Library - Tool management details</li> </ul>"},{"location":"learn/nn/functional/","title":"Functional","text":"<p>The <code>msgflux.nn.functional</code> module provides a set of functions for concurrent task execution and message passing patterns. These utilities enable parallel processing, broadcasting, and asynchronous coordination.</p>"},{"location":"learn/nn/functional/#overview","title":"Overview","text":"<p>The Functional API offers both synchronous and asynchronous interfaces for:</p> <ul> <li>Parallel mapping: Apply a function to multiple inputs concurrently</li> <li>Scatter-gather: Distribute different tasks across inputs</li> <li>Broadcast-gather: Send the same input to multiple functions</li> <li>Message passing: Concurrent message processing with <code>dotdict</code></li> <li>Background tasks: Fire-and-forget task execution</li> <li>Event coordination: Wait for async events in sync contexts</li> </ul>"},{"location":"learn/nn/functional/#key-features","title":"Key Features","text":"<ul> <li>Concurrent execution using thread pools and event loops</li> <li>Zero-overhead when executing single tasks</li> <li>Error handling with graceful degradation</li> <li>Timeout support for all operations</li> <li>Message-first design for workflow orchestration</li> </ul>"},{"location":"learn/nn/functional/#pattern-comparison","title":"Pattern Comparison","text":"<pre><code>MAP GATHER           SCATTER GATHER        BROADCAST GATHER\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500        \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ninput1 \u2500\u2500\u2510           input1 \u2500\u2500\u2510            input \u2500\u2500\u252c\u2500\u2500&gt; f1 \u2500\u2500&gt; r1\ninput2 \u2500\u2500\u253c\u2500&gt; f \u2500\u2500&gt;   input2 \u2500\u2500\u253c\u2500&gt; f1 \u2500\u2500&gt;           \u251c\u2500\u2500&gt; f2 \u2500\u2500&gt; r2\ninput3 \u2500\u2500\u2518           input3 \u2500\u2500\u2518\u2500&gt; f2 \u2500\u2500&gt;           \u2514\u2500\u2500&gt; f3 \u2500\u2500&gt; r3\n\nSame function         Different functions   Multiple functions\nMultiple inputs       Paired inputs/funcs   Same input\n</code></pre>"},{"location":"learn/nn/functional/#core-functions","title":"Core Functions","text":""},{"location":"learn/nn/functional/#map_gather","title":"<code>map_gather()</code>","text":"<p>Apply the same function to multiple inputs concurrently.</p> <p>Pattern: <pre><code>input1 \u2500\u2500\u2510\ninput2 \u2500\u2500\u253c\u2500&gt; function \u2500\u2500&gt; (result1, result2, result3)\ninput3 \u2500\u2500\u2518\n</code></pre></p> <p>Usage:</p> <pre><code>import msgflux.nn.functional as F\n\n# Example 1: Simple mapping\ndef square(x):\n    return x * x\n\nresults = F.map_gather(square, args_list=[(2,), (3,), (4,)])\nprint(results)  # (4, 9, 16)\n\n# Example 2: With multiple arguments\ndef add(x, y):\n    return x + y\n\nresults = F.map_gather(add, args_list=[(1, 2), (3, 4), (5, 6)])\nprint(results)  # (3, 7, 11)\n\n# Example 3: With kwargs\ndef multiply(x, y=2):\n    return x * y\n\nresults = F.map_gather(\n    multiply,\n    args_list=[(1,), (3,), (5,)],\n    kwargs_list=[{\"y\": 3}, {\"y\": 4}, {\"y\": 5}]\n)\nprint(results)  # (3, 12, 25)\n</code></pre> <p>Async version: <code>amap_gather()</code></p>"},{"location":"learn/nn/functional/#scatter_gather","title":"<code>scatter_gather()</code>","text":"<p>Distribute different functions across corresponding inputs.</p> <p>Pattern: <pre><code>input1 \u2500\u2500&gt; function1 \u2500\u2500\u2510\ninput2 \u2500\u2500&gt; function2 \u2500\u2500\u253c\u2500\u2500&gt; (result1, result2, result3)\ninput3 \u2500\u2500&gt; function3 \u2500\u2500\u2518\n</code></pre></p> <p>Usage:</p> <pre><code>import msgflux.nn.functional as F\n\ndef double(x):\n    return x * 2\n\ndef triple(x):\n    return x * 3\n\ndef square(x):\n    return x ** 2\n\n# Each function gets its corresponding input\nresults = F.scatter_gather(\n    to_send=[double, triple, square],\n    args_list=[(5,), (5,), (5,)]\n)\nprint(results)  # (10, 15, 25)\n\n# With different inputs\nresults = F.scatter_gather(\n    to_send=[double, triple, square],\n    args_list=[(2,), (3,), (4,)]\n)\nprint(results)  # (4, 9, 16)\n</code></pre> <p>Async version: <code>ascatter_gather()</code></p>"},{"location":"learn/nn/functional/#bcast_gather","title":"<code>bcast_gather()</code>","text":"<p>Broadcast the same arguments to multiple functions.</p> <p>Pattern: <pre><code>              \u250c\u2500\u2500&gt; function1 \u2500\u2500&gt; result1\ninput \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500&gt; function2 \u2500\u2500&gt; result2\n              \u2514\u2500\u2500&gt; function3 \u2500\u2500&gt; result3\n</code></pre></p> <p>Usage:</p> <pre><code>import msgflux.nn.functional as F\n\ndef square(x):\n    return x * x\n\ndef cube(x):\n    return x * x * x\n\ndef double(x):\n    return x * 2\n\n# Same input to all functions\nresults = F.bcast_gather([square, cube, double], 5)\nprint(results)  # (25, 125, 10)\n\n# With timeout\nresults = F.bcast_gather([square, cube, double], 3, timeout=1.0)\nprint(results)  # (9, 27, 6)\n\n# Error handling - returns None for failed tasks\ndef fail(x):\n    raise ValueError(\"Intentional error\")\n\nresults = F.bcast_gather([square, fail, cube], 2)\nprint(results)  # (4, None, 8)\n</code></pre>"},{"location":"learn/nn/functional/#message-based-functions","title":"Message-Based Functions","text":"<p>These functions work specifically with <code>msgflux.dotdict</code> for message passing patterns.</p>"},{"location":"learn/nn/functional/#msg_scatter_gather","title":"<code>msg_scatter_gather()</code>","text":"<p>Scatter messages to functions and gather updated messages.</p> <p>Pattern: <pre><code>message1 \u2500\u2500&gt; function1 \u2500\u2500\u2510\nmessage2 \u2500\u2500&gt; function2 \u2500\u2500\u253c\u2500\u2500&gt; (msg1, msg2, msg3)\nmessage3 \u2500\u2500&gt; function3 \u2500\u2500\u2518\n</code></pre></p> <p>Usage:</p> <pre><code>import msgflux as mf\nimport msgflux.nn.functional as F\n\ndef process_user(msg):\n    msg.type = \"user\"\n    msg.processed = True\n    return msg\n\ndef process_admin(msg):\n    msg.type = \"admin\"\n    msg.permissions = [\"read\", \"write\", \"delete\"]\n    return msg\n\ndef process_guest(msg):\n    msg.type = \"guest\"\n    msg.permissions = [\"read\"]\n    return msg\n\n# Create messages\nmsg1 = mf.dotdict({\"id\": 1, \"name\": \"Alice\"})\nmsg2 = mf.dotdict({\"id\": 2, \"name\": \"Bob\"})\nmsg3 = mf.dotdict({\"id\": 3, \"name\": \"Charlie\"})\n\n# Scatter to different processors\nresults = F.msg_scatter_gather(\n    to_send=[process_user, process_admin, process_guest],\n    messages=[msg1, msg2, msg3]\n)\n\nfor msg in results:\n    print(f\"{msg.name}: {msg.type} - {msg.get('permissions', [])}\")\n# Alice: user - []\n# Bob: admin - ['read', 'write', 'delete']\n# Charlie: guest - ['read']\n</code></pre>"},{"location":"learn/nn/functional/#msg_bcast_gather","title":"<code>msg_bcast_gather()</code>","text":"<p>Broadcast a message to multiple modules for concurrent processing.</p> <p>Pattern: <pre><code>              \u250c\u2500\u2500&gt; module1(msg) \u2500\u2500\u2510\nmessage \u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500&gt; module2(msg) \u2500\u2500\u253c\u2500\u2500&gt; message (modified)\n              \u2514\u2500\u2500&gt; module3(msg) \u2500\u2500\u2518\n</code></pre></p> <p>Important: Modules modify the message directly. Return values are ignored.</p> <p>Usage:</p> <pre><code>import msgflux as mf\nimport msgflux.nn.functional as F\n\ndef add_timestamp(msg):\n    from datetime import datetime\n    msg.timestamp = datetime.now().isoformat()\n    return msg\n\ndef add_metadata(msg):\n    msg.set(\"metadata.version\", \"1.0\")\n    msg.set(\"metadata.source\", \"api\")\n    return msg\n\ndef validate(msg):\n    msg.validated = True\n    return msg\n\n# Broadcast message to all modules\nmessage = mf.dotdict({\"data\": \"important\"})\n\nF.msg_bcast_gather([add_timestamp, add_metadata, validate], message)\n\nprint(message.timestamp)  # 2024-01-15T10:30:00.123456\nprint(message.metadata.version)  # 1.0\nprint(message.validated)  # True\n</code></pre> <p>Async version: <code>amsg_bcast_gather()</code></p>"},{"location":"learn/nn/functional/#utility-functions","title":"Utility Functions","text":""},{"location":"learn/nn/functional/#wait_for","title":"<code>wait_for()</code>","text":"<p>Execute a single callable and wait for the result with optional timeout.</p> <p>Usage:</p> <pre><code>import msgflux.nn.functional as F\nimport time\n\ndef slow_computation(x):\n    time.sleep(0.1)\n    return x * x\n\n# Simple execution\nresult = F.wait_for(slow_computation, 5)\nprint(result)  # 25\n\n# With timeout\nresult = F.wait_for(slow_computation, 10, timeout=0.5)\nprint(result)  # 100\n\n# Async function\nasync def async_task(x):\n    return x * 2\n\nresult = F.wait_for(async_task, 3)\nprint(result)  # 6\n</code></pre>"},{"location":"learn/nn/functional/#wait_for_event","title":"<code>wait_for_event()</code>","text":"<p>Wait for an asyncio event in a synchronous context.</p> <p>Usage:</p> <pre><code>import msgflux.nn.functional as F\nimport asyncio\n\n# Create an event\nevent = asyncio.Event()\n\n# Set the event from another thread/task\ndef set_event():\n    event.set()\n\n# Wait for event in sync code\nimport threading\nthread = threading.Thread(target=lambda: (time.sleep(0.1), set_event()))\nthread.start()\n\nF.wait_for_event(event)  # Blocks until event is set\nprint(\"Event was set!\")\n</code></pre> <p>Async version: <code>await_for_event()</code></p>"},{"location":"learn/nn/functional/#background_task","title":"<code>background_task()</code>","text":"<p>Execute a function in the background without waiting for the result.</p> <p>Usage:</p> <pre><code>import msgflux.nn.functional as F\n\ndef log_event(event_type, user_id):\n    # This runs in the background\n    print(f\"Logging: {event_type} for user {user_id}\")\n\n# Fire and forget\nF.background_task(log_event, \"login\", 12345)\n# Execution continues immediately\n\n# Background task runs concurrently\nprint(\"Main thread continues...\")\n\n# With kwargs\nF.background_task(log_event, event_type=\"logout\", user_id=67890)\n</code></pre> <p>Async version: <code>abackground_task()</code></p>"},{"location":"learn/nn/functional/#async-equivalents","title":"Async Equivalents","text":"<p>All major functions have async equivalents prefixed with <code>a</code>:</p> Sync Function Async Equivalent <code>map_gather</code> <code>amap_gather</code> <code>scatter_gather</code> <code>ascatter_gather</code> <code>msg_bcast_gather</code> <code>amsg_bcast_gather</code> <code>wait_for_event</code> <code>await_for_event</code> <code>background_task</code> <code>abackground_task</code> <p>Usage:</p> <pre><code>import msgflux.nn.functional as F\nimport asyncio\n\nasync def main():\n    # Async map gather\n    async def async_square(x):\n        await asyncio.sleep(0.01)\n        return x * x\n\n    results = await F.amap_gather(\n        async_square,\n        args_list=[(2,), (3,), (4,)]\n    )\n    print(results)  # (4, 9, 16)\n\n    # Async broadcast\n    async def async_double(x):\n        await asyncio.sleep(0.01)\n        return x * 2\n\n    async def async_triple(x):\n        await asyncio.sleep(0.01)\n        return x * 3\n\n    results = await F.ascatter_gather(\n        [async_double, async_triple],\n        [(5,), (5,)]\n    )\n    print(results)  # (10, 15)\n\n# Run async code\nasyncio.run(main())\n</code></pre>"},{"location":"learn/nn/functional/#best-practices","title":"Best Practices","text":""},{"location":"learn/nn/functional/#1-choose-the-right-pattern","title":"1. Choose the Right Pattern","text":"<pre><code># Use map_gather when: Same function, different inputs\nresults = F.map_gather(process, args_list=[(1,), (2,), (3,)])\n\n# Use scatter_gather when: Different functions, different inputs\nresults = F.scatter_gather([f1, f2, f3], args_list=[(a,), (b,), (c,)])\n\n# Use bcast_gather when: Multiple functions, same input\nresults = F.bcast_gather([f1, f2, f3], input)\n</code></pre>"},{"location":"learn/nn/functional/#2-handle-errors-gracefully","title":"2. Handle Errors Gracefully","text":"<pre><code># Functions return None on error\ndef safe_divide(x, y):\n    return x / y\n\nresults = F.map_gather(\n    safe_divide,\n    args_list=[(10, 2), (10, 0), (10, 5)]\n)\nprint(results)  # (5.0, None, 2.0)\n\n# Check for errors\nfor i, result in enumerate(results):\n    if result is None:\n        print(f\"Task {i} failed\")\n</code></pre>"},{"location":"learn/nn/functional/#3-use-timeouts-for-long-operations","title":"3. Use Timeouts for Long Operations","text":"<pre><code># Prevent indefinite blocking\nresults = F.bcast_gather(\n    [slow_task1, slow_task2],\n    input_data,\n    timeout=5.0  # 5 second timeout\n)\n</code></pre>"},{"location":"learn/nn/functional/#4-message-modifications-in-parallel","title":"4. Message Modifications in Parallel","text":"<pre><code># Good - Modify different message paths\ndef add_user_data(msg):\n    msg.set(\"user.name\", \"Alice\")\n    return msg\n\ndef add_timestamp(msg):\n    msg.set(\"meta.timestamp\", \"2024-01-15\")\n    return msg\n\n# Both can run in parallel safely\nF.msg_bcast_gather([add_user_data, add_timestamp], message)\n</code></pre>"},{"location":"learn/nn/functional/#5-background-tasks-for-side-effects","title":"5. Background Tasks for Side Effects","text":"<pre><code># Good use cases for background tasks:\n# - Logging\n# - Metrics collection\n# - Cache updates\n# - Notifications\n\nF.background_task(log_to_file, \"User logged in\", user_id=123)\nF.background_task(update_cache, key=\"user:123\", value=user_data)\nF.background_task(send_notification, user_id=123, message=\"Welcome!\")\n</code></pre>"},{"location":"learn/nn/functional/#common-patterns","title":"Common Patterns","text":""},{"location":"learn/nn/functional/#pipeline-with-parallel-stages","title":"Pipeline with Parallel Stages","text":"<pre><code>import msgflux as mf\nimport msgflux.nn.functional as F\n\ndef prepare(msg):\n    msg.data = [1, 2, 3, 4, 5]\n    return msg\n\ndef filter_even(msg):\n    msg.set(\"results.even\", [x for x in msg.data if x % 2 == 0])\n    return msg\n\ndef filter_odd(msg):\n    msg.set(\"results.odd\", [x for x in msg.data if x % 2 != 0])\n    return msg\n\n# Sequential then parallel\nmessage = mf.dotdict()\nprepare(message)\nF.msg_bcast_gather([filter_even, filter_odd], message)\n\nprint(message.results.even)  # [2, 4]\nprint(message.results.odd)   # [1, 3, 5]\n</code></pre>"},{"location":"learn/nn/functional/#parallel-data-processing","title":"Parallel Data Processing","text":"<pre><code>import msgflux.nn.functional as F\n\ndef process_chunk(data):\n    return sum(data)\n\n# Split data into chunks\ndata = list(range(1000))\nchunk_size = 100\nchunks = [data[i:i+chunk_size] for i in range(0, len(data), chunk_size)]\n\n# Process chunks in parallel\nresults = F.map_gather(\n    process_chunk,\n    args_list=[(chunk,) for chunk in chunks]\n)\n\ntotal = sum(results)\nprint(total)  # 499500\n</code></pre>"},{"location":"learn/nn/functional/#concurrent-api-calls","title":"Concurrent API Calls","text":"<pre><code>import msgflux.nn.functional as F\n\ndef fetch_user(user_id):\n    # Simulate API call\n    return {\"id\": user_id, \"name\": f\"User {user_id}\"}\n\ndef fetch_posts(user_id):\n    # Simulate API call\n    return [f\"Post {i}\" for i in range(3)]\n\ndef fetch_comments(user_id):\n    # Simulate API call\n    return [f\"Comment {i}\" for i in range(5)]\n\n# Fetch all data for a user in parallel\nuser_data, posts, comments = F.bcast_gather(\n    [fetch_user, fetch_posts, fetch_comments],\n    user_id=123\n)\n\nprint(f\"User: {user_data}\")\nprint(f\"Posts: {len(posts)}\")\nprint(f\"Comments: {len(comments)}\")\n</code></pre>"},{"location":"learn/nn/functional/#performance-considerations","title":"Performance Considerations","text":""},{"location":"learn/nn/functional/#when-to-use-parallel-execution","title":"When to Use Parallel Execution","text":"<p>\u2705 Good candidates for parallelization: - I/O-bound operations (API calls, file I/O, database queries) - Independent computations - Multiple data transformations - Batch processing</p> <p>\u274c Poor candidates: - CPU-bound operations (use <code>multiprocessing</code> instead) - Very fast operations (overhead &gt; benefit) - Operations with shared mutable state - Sequential dependencies</p>"},{"location":"learn/nn/functional/#overhead-vs-benefit","title":"Overhead vs Benefit","text":"<pre><code>import msgflux.nn.functional as F\nimport time\n\n# Fast operation - parallel overhead might not be worth it\ndef fast_op(x):\n    return x * 2\n\n# Slow operation - benefits from parallelization\ndef slow_op(x):\n    time.sleep(0.1)\n    return x * 2\n\n# For fast operations, sequential might be faster\nstart = time.time()\nresults = [fast_op(i) for i in range(100)]\nprint(f\"Sequential: {time.time() - start:.4f}s\")\n\nstart = time.time()\nresults = F.map_gather(fast_op, args_list=[(i,) for i in range(100)])\nprint(f\"Parallel: {time.time() - start:.4f}s\")\n\n# For slow operations, parallel is much faster\nstart = time.time()\nresults = [slow_op(i) for i in range(10)]\nprint(f\"Sequential: {time.time() - start:.4f}s\")\n\nstart = time.time()\nresults = F.map_gather(slow_op, args_list=[(i,) for i in range(10)])\nprint(f\"Parallel: {time.time() - start:.4f}s\")\n</code></pre>"},{"location":"learn/nn/functional/#api-reference","title":"API Reference","text":"<p>For complete API documentation with all parameters and return types, see:</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.map_gather","title":"map_gather","text":"<pre><code>map_gather(\n    to_send, *, args_list, kwargs_list=None, timeout=None\n)\n</code></pre> <p>Applies the <code>to_send</code> function to each set of arguments in <code>args_list</code> and <code>kwargs_list</code> using Executor and collects the results.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>Callable</code> <p>The callable function to be applied.</p> required <code>args_list</code> <code>List[Tuple[Any, ...]]</code> <p>Each tuple contains the positional argumentsvfor the corresponding callable in <code>to_send</code>. If <code>None</code>, no positional arguments are passed unless specified individually by an item in <code>kwargs_list</code>.</p> required <code>kwargs_list</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Each dictionary contains the named arguments for the corresponding callable in <code>to_send</code>. If <code>None</code>, no named arguments are passed unless specified individually by an item in <code>args_list</code>.</p> <code>None</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>A tuple containing the results of each call to the <code>f</code> function. If a call</p> <code>...</code> <p>fails or times out, the corresponding result will be <code>None</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>f</code> is not callable.</p> <code>ValueError</code> <p>If <code>args_list</code> is not a non-empty list or if <code>kwargs_list</code> (if provided) is not the same length as <code>args_list</code>.</p> <p>Examples:</p> <p>def add(x, y): return x + y results = F.map_gather(add, args_list=[(1, 2), (3, 4), (5, 6)]) print(results)  # (3, 7, 11)</p> <p>def multiply(x, y=2): return x * y results = F.map_gather(multiply, args_list=[(1,), (3,), (5,)],                     kwargs_list=[{'y': 3}, {'y': 4}, {'y': 5}]) print(results)  # (3, 12, 25)</p> <p>results = F.map_gather(multiply, args_list=[(1,), (3,), (5,)]) print(results)  # (2, 6, 10)</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef map_gather(\n    to_send: Callable,\n    *,\n    args_list: List[Tuple[Any, ...]],\n    kwargs_list: Optional[List[Dict[str, Any]]] = None,\n    timeout: Optional[float] = None,\n) -&gt; Tuple[Any, ...]:\n    \"\"\"Applies the `to_send` function to each set of arguments in `args_list`\n    and `kwargs_list` using Executor and collects the results.\n\n    Args:\n        to_send:\n            The callable function to be applied.\n        args_list:\n            Each tuple contains the positional argumentsvfor the corresponding callable\n            in `to_send`. If `None`, no positional arguments are passed unless specified\n            individually by an item in `kwargs_list`.\n        kwargs_list:\n            Each dictionary contains the named arguments for the corresponding callable\n            in `to_send`. If `None`, no named arguments are passed unless specified\n            individually by an item in `args_list`.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n\n    Returns:\n        A tuple containing the results of each call to the `f` function. If a call\n        fails or times out, the corresponding result will be `None`.\n\n    Raises:\n        TypeError:\n            If `f` is not callable.\n        ValueError:\n            If `args_list` is not a non-empty list or if `kwargs_list`\n            (if provided) is not the same length as `args_list`.\n\n    Examples:\n        def add(x, y): return x + y\n        results = F.map_gather(add, args_list=[(1, 2), (3, 4), (5, 6)])\n        print(results)  # (3, 7, 11)\n\n        def multiply(x, y=2): return x * y\n        results = F.map_gather(multiply, args_list=[(1,), (3,), (5,)],\n                            kwargs_list=[{'y': 3}, {'y': 4}, {'y': 5}])\n        print(results)  # (3, 12, 25)\n\n        results = F.map_gather(multiply, args_list=[(1,), (3,), (5,)])\n        print(results)  # (2, 6, 10)\n    \"\"\"\n    if not callable(to_send):\n        raise TypeError(\"`to_send` must be a callable object\")\n\n    if not isinstance(args_list, list) or len(args_list) == 0:\n        raise ValueError(\"`args_list` must be a non-empty list\")\n\n    if kwargs_list is not None:\n        if not isinstance(kwargs_list, list) or len(kwargs_list) != len(args_list):\n            raise ValueError(\n                \"`kwargs_list` must be a list with the same length as `args_list`\"\n            )\n\n    executor = Executor.get_instance()\n    futures = []\n\n    for i in range(len(args_list)):\n        args = args_list[i]\n        kwargs = kwargs_list[i] if kwargs_list else {}\n        futures.append(executor.submit(to_send, *args, **kwargs))\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    responses: List[Any] = []\n    for future in futures:\n        try:\n            responses.append(future.result())\n        except Exception as e:\n            logger.error(str(e))\n            responses.append(None)\n    return tuple(responses)\n</code></pre>"},{"location":"learn/nn/functional/#msgflux.nn.functional.scatter_gather","title":"scatter_gather","text":"<pre><code>scatter_gather(\n    to_send,\n    args_list=None,\n    kwargs_list=None,\n    *,\n    timeout=None,\n)\n</code></pre> <p>Sends different sets of arguments/kwargs to a list of modules and collects the responses.</p> <p>Each callable in <code>to_send</code> receives the positional arguments of the corresponding <code>tuple</code> in <code>args_list</code> and the named arguments of the corresponding <code>dict</code> in <code>kwargs_list</code>. If <code>args_list</code> or <code>kwargs_list</code> are not provided (or are <code>None</code>), the corresponding callables will be called without positional or named arguments, respectively, unless an empty list (<code>[]</code>) or empty tuple (<code>()</code>) is provided for a specific item.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>args_list</code> <code>Optional[List[Tuple[Any, ...]]]</code> <p>Each tuple contains the positional argumentsvfor the corresponding callable in <code>to_send</code>. If <code>None</code>, no positional arguments are passed unless specified individually by an item in <code>kwargs_list</code>.</p> <code>None</code> <code>kwargs_list</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Each dictionary contains the named arguments for the corresponding callable in <code>to_send</code>. If <code>None</code>, no named arguments are passed unless specified individually by an item in <code>args_list</code>.</p> <code>None</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing the responses for each callable. If an error or</p> <code>...</code> <p>timeout occurs for a specific callable, its corresponding response</p> <code>Tuple[Any, ...]</code> <p>in the tuple will be <code>None</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a callable list.</p> <code>ValueError</code> <p>If the lengths of <code>args_list</code> (if provided) or <code>kwargs_list</code> (if provided) do not match the length of <code>to_send</code>.</p> <p>Examples:</p> <p>def add(x, y): return x + y def multiply(x, y=2): return x * y callables = [add, multiply, add]</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.scatter_gather--example-1-using-only-args_list","title":"Example 1: Using only args_list","text":"<p>args = [ (1, 2), (3,), (10, 20) ] # multiply will use its default y results = F.scatter_gather(callables, args_list=args) print(results) # (3, 6, 30)</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.scatter_gather--example-2-using-args_list-e-kwargs_list","title":"Example 2: Using args_list e kwargs_list","text":"<p>args = [ (1,), (), (10,) ] kwargs = [ {'y': 2}, {'x': 3, 'y': 3}, {'y': 20} ] results = F.scatter_gather(callables, args_list=args, kwargs_list=kwargs) print(results) # (3, 9, 30)</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.scatter_gather--example-3-using-only-kwargs_list-useful-if-functions-have","title":"Example 3: Using only kwargs_list (useful if functions have","text":""},{"location":"learn/nn/functional/#msgflux.nn.functional.scatter_gather--defaults-or-dont-need-positional-args","title":"defaults or don't need positional args)","text":"<p>def greet(name=\"World\"): return f\"Hello, {name}\" def farewell(person_name): return f\"Goodbye, {person_name}\" funcs = [greet, greet, farewell] kwargs_for_funcs = [ {}, {'name': \"Earth\"}, {'person_name': \"Commander\"} ] results = F.scatter_gather(funcs, kwargs_list=kwargs_for_funcs) print(results) # (\"Hello, World\", \"Hello, Earth\", \"Goodbye, Commander\")</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef scatter_gather(\n    to_send: List[Callable],\n    args_list: Optional[List[Tuple[Any, ...]]] = None,\n    kwargs_list: Optional[List[Dict[str, Any]]] = None,\n    *,\n    timeout: Optional[float] = None,\n) -&gt; Tuple[Any, ...]:\n    \"\"\"Sends different sets of arguments/kwargs to a list of modules\n    and collects the responses.\n\n    Each callable in `to_send` receives the positional arguments of\n    the corresponding `tuple` in `args_list` and the named arguments\n    of the corresponding `dict` in `kwargs_list`. If `args_list` or\n    `kwargs_list` are not provided (or are `None`), the corresponding\n    callables will be called without positional or named arguments,\n    respectively, unless an empty list (`[]`) or empty tuple (`()`)\n    is provided for a specific item.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        args_list:\n            Each tuple contains the positional argumentsvfor the corresponding callable\n            in `to_send`. If `None`, no positional arguments are passed unless specified\n            individually by an item in `kwargs_list`.\n        kwargs_list:\n            Each dictionary contains the named arguments for the corresponding callable\n            in `to_send`. If `None`, no named arguments are passed unless specified\n            individually by an item in `args_list`.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n\n    Returns:\n        Tuple containing the responses for each callable. If an error or\n        timeout occurs for a specific callable, its corresponding response\n        in the tuple will be `None`.\n\n    Raises:\n        TypeError:\n            If `to_send` is not a callable list.\n        ValueError:\n            If the lengths of `args_list` (if provided) or `kwargs_list`\n            (if provided) do not match the length of `to_send`.\n\n    Examples:\n        def add(x, y): return x + y\n        def multiply(x, y=2): return x * y\n        callables = [add, multiply, add]\n\n        # Example 1: Using only args_list\n        args = [ (1, 2), (3,), (10, 20) ] # multiply will use its default y\n        results = F.scatter_gather(callables, args_list=args)\n        print(results) # (3, 6, 30)\n\n        # Example 2: Using args_list e kwargs_list\n        args = [ (1,), (), (10,) ]\n        kwargs = [ {'y': 2}, {'x': 3, 'y': 3}, {'y': 20} ]\n        results = F.scatter_gather(callables, args_list=args, kwargs_list=kwargs)\n        print(results) # (3, 9, 30)\n\n        # Example 3: Using only kwargs_list (useful if functions have\n        # defaults or don't need positional args)\n        def greet(name=\"World\"): return f\"Hello, {name}\"\n        def farewell(person_name): return f\"Goodbye, {person_name}\"\n        funcs = [greet, greet, farewell]\n        kwargs_for_funcs = [ {}, {'name': \"Earth\"}, {'person_name': \"Commander\"} ]\n        results = F.scatter_gather(funcs, kwargs_list=kwargs_for_funcs)\n        print(results) # (\"Hello, World\", \"Hello, Earth\", \"Goodbye, Commander\")\n    \"\"\"\n    if not isinstance(to_send, list) or not all(callable(f) for f in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    executor = Executor.get_instance()\n    futures = []\n    for i, f in enumerate(to_send):\n        args = args_list[i] if args_list and i &lt; len(args_list) else ()\n        kwargs = kwargs_list[i] if kwargs_list and i &lt; len(kwargs_list) else {}\n        futures.append(executor.submit(f, *args, **kwargs))\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    responses: List[Any] = []\n    for future in futures:\n        try:\n            responses.append(future.result())\n        except Exception as e:\n            logger.error(str(e))\n            responses.append(None)\n    return tuple(responses)\n</code></pre>"},{"location":"learn/nn/functional/#msgflux.nn.functional.msg_scatter_gather","title":"msg_scatter_gather","text":"<pre><code>msg_scatter_gather(to_send, messages, *, timeout=None)\n</code></pre> <p>Scatter a list of messages to a list of modules and gather the responses.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>messages</code> <code>List[dotdict]</code> <p>List of <code>msgflux.dotdict</code> instances to be distributed.</p> required <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[dotdict, ...]</code> <p>Tuple containing the messages updated with the responses.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>messages</code> is not a list of <code>dotdict</code>, <code>to_send</code> is not a list of callables, or <code>prefix</code> is not a string.</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef msg_scatter_gather(\n    to_send: List[Callable],\n    messages: List[dotdict],\n    *,\n    timeout: Optional[float] = None,\n) -&gt; Tuple[dotdict, ...]:\n    \"\"\"Scatter a list of messages to a list of modules and gather the responses.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        messages:\n            List of `msgflux.dotdict` instances to be distributed.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n\n    Returns:\n        Tuple containing the messages updated with the responses.\n\n    Raises:\n        TypeError:\n            If `messages` is not a list of `dotdict`, `to_send` is not a list\n            of callables, or `prefix` is not a string.\n    \"\"\"\n    if not messages or not all(isinstance(msg, dotdict) for msg in messages):\n        raise TypeError(\n            \"`messages` must be a non-empty list of `msgflux.dotdict` instances\"\n        )\n\n    if not to_send or not all(isinstance(f, Callable) for f in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    if len(messages) != len(to_send):\n        raise ValueError(\n            f\"The size of `messages` ({len(messages)}) \"\n            f\"must be equal to that of `to_send`: ({len(to_send)})\"\n        )\n\n    executor = Executor.get_instance()\n    futures = [executor.submit(f, msg) for f, msg in zip(to_send, messages)]\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    for f, future in zip(to_send, futures):\n        f_name = get_callable_name(f)\n        try:\n            future.result()\n        except Exception as e:\n            logger.error(f\"Error in scattered task for `{f_name}`: {e}\")\n    return tuple(messages)\n</code></pre>"},{"location":"learn/nn/functional/#msgflux.nn.functional.bcast_gather","title":"bcast_gather","text":"<pre><code>bcast_gather(to_send, *args, timeout=None, **kwargs)\n</code></pre> <p>Broadcasts arguments to multiple callables and gathers the responses.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>*args</code> <p>Positional arguments.</p> <code>()</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <code>**kwargs</code> <p>Named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[Any, ...]</code> <p>Tuple containing the responses.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a list of callables.</p> <p>Examples:</p> <p>def square(x): return x * x def cube(x): return x * x * x def fail(x): raise ValueError(\"Intentional error\")</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.bcast_gather--example-1","title":"Example 1:","text":"<p>results = F.bcast_gather([square, cube], 3) print(results)  # (9, 27)</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.bcast_gather--example-2-simulate-error","title":"Example 2: Simulate error","text":"<p>results = F.bcast_gather([square, fail, cube], 2) print(results)  # (4, None, 8)</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.bcast_gather--example-3-timeout","title":"Example 3: Timeout","text":"<p>results = F.bcast_gather([square, cube], 4, timeout=0.01) print(results) # (16, 64)</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef bcast_gather(\n    to_send: List[Callable], *args, timeout: Optional[float] = None, **kwargs\n) -&gt; Tuple[Any, ...]:\n    \"\"\"Broadcasts arguments to multiple callables and gathers the responses.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        *args:\n            Positional arguments.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n        **kwargs:\n            Named arguments.\n\n    Returns:\n        Tuple containing the responses.\n\n    Raises:\n        TypeError:\n            If `to_send` is not a list of callables.\n\n    Examples:\n        def square(x): return x * x\n        def cube(x): return x * x * x\n        def fail(x): raise ValueError(\"Intentional error\")\n\n        # Example 1:\n        results = F.bcast_gather([square, cube], 3)\n        print(results)  # (9, 27)\n\n        # Example 2: Simulate error\n        results = F.bcast_gather([square, fail, cube], 2)\n        print(results)  # (4, None, 8)\n\n        # Example 3: Timeout\n        results = F.bcast_gather([square, cube], 4, timeout=0.01)\n        print(results) # (16, 64)\n    \"\"\"\n    if not to_send or not all(isinstance(f, Callable) for f in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    executor = Executor.get_instance()\n    futures = [executor.submit(f, *args, **kwargs) for f in to_send]\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    responses: List[Any] = []\n    for future in futures:\n        try:\n            responses.append(future.result())\n        except Exception as e:\n            logger.error(str(e))\n            responses.append(None)\n    return tuple(responses)\n</code></pre>"},{"location":"learn/nn/functional/#msgflux.nn.functional.msg_bcast_gather","title":"msg_bcast_gather","text":"<pre><code>msg_bcast_gather(to_send, message, *, timeout=None)\n</code></pre> <p>Broadcasts a single message to multiple modules and gathers the responses.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. functions or <code>Module</code> instances).</p> required <code>message</code> <code>dotdict</code> <p>Instance of <code>msgflux.dotdict</code> to broadcast.</p> required <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <p>Returns:</p> Type Description <code>dotdict</code> <p>The original message with the module responses added.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>message</code> is not an instance of <code>dotdict</code>, <code>to_send</code> is not a list of callables.</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef msg_bcast_gather(\n    to_send: List[Callable],\n    message: dotdict,\n    *,\n    timeout: Optional[float] = None,\n) -&gt; dotdict:\n    \"\"\"Broadcasts a single message to multiple modules and gathers the responses.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. functions or `Module` instances).\n        message:\n            Instance of `msgflux.dotdict` to broadcast.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n\n    Returns:\n        The original message with the module responses added.\n\n    Raises:\n        TypeError:\n            If `message` is not an instance of `dotdict`, `to_send` is not a list\n            of callables.\n    \"\"\"\n    if not isinstance(message, dotdict):\n        raise TypeError(\"`message` must be an instance of `msgflux.dotdict`\")\n    if not to_send or not all(isinstance(module, Callable) for module in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    executor = Executor.get_instance()\n    futures = [executor.submit(f, message) for f in to_send]\n\n    concurrent.futures.wait(futures, timeout=timeout)\n    for f, future in zip(to_send, futures):\n        f_name = get_callable_name(f)\n        try:\n            future.result()\n        except Exception as e:\n            logger.error(f\"Error in scattered task for `{f_name}`: {e}\")\n    return message\n</code></pre>"},{"location":"learn/nn/functional/#msgflux.nn.functional.wait_for","title":"wait_for","text":"<pre><code>wait_for(to_send, *args, timeout=None, **kwargs)\n</code></pre> <p>Wait for a callable execution.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>Callable</code> <p>A callable object (e.g. functions or <code>Module</code> instances).</p> required <code>*args</code> <p>Positional arguments.</p> <code>()</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time (in seconds) to wait for responses.</p> <code>None</code> <code>**kwargs</code> <p>Named arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Callable responses.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a callable.</p> <p>Examples:</p> <p>async def f1(x):     return x * x</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.wait_for--example-1","title":"Example 1:","text":"<p>results = F.wait_for(f1, 3) print(results) # 9</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef wait_for(\n    to_send: Callable, *args, timeout: Optional[float] = None, **kwargs\n) -&gt; Any:\n    \"\"\"Wait for a callable execution.\n\n    Args:\n        to_send:\n            A callable object (e.g. functions or `Module` instances).\n        *args:\n            Positional arguments.\n        timeout:\n            Maximum time (in seconds) to wait for responses.\n        **kwargs:\n            Named arguments.\n\n    Returns:\n        Callable responses.\n\n    Raises:\n        TypeError:\n            If `to_send` is not a callable.\n\n    Examples:\n        async def f1(x):\n            return x * x\n\n        # Example 1:\n        results = F.wait_for(f1, 3)\n        print(results) # 9\n    \"\"\"\n    if not callable(to_send):\n        raise TypeError(\"`to_send` must be a callable object\")\n\n    executor = Executor.get_instance()\n    future = executor.submit(to_send, *args, **kwargs)\n    concurrent.futures.wait([future], timeout=timeout)\n    try:\n        return future.result()\n    except Exception as e:\n        logger.error(str(e))\n        return None\n</code></pre>"},{"location":"learn/nn/functional/#msgflux.nn.functional.wait_for_event","title":"wait_for_event","text":"<pre><code>wait_for_event(event)\n</code></pre> <p>Waits synchronously for an asyncio.Event to be set.</p> <p>This function will block until event.set() is called elsewhere.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The asyncio.Event to wait for.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>event</code> is not an instance of asyncio.Event.</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef wait_for_event(event: asyncio.Event) -&gt; None:\n    \"\"\"Waits synchronously for an asyncio.Event to be set.\n\n    This function will block until event.set() is called elsewhere.\n\n    Args:\n        event: The asyncio.Event to wait for.\n\n    Raises:\n        TypeError: If `event` is not an instance of asyncio.Event.\n    \"\"\"\n    if not isinstance(event, asyncio.Event):\n        raise TypeError(\"`event` must be an instance of asyncio.Event\")\n\n    executor = Executor.get_instance()\n    future = executor._submit_to_async_worker(event.wait())\n    try:\n        future.result()\n    except Exception as e:\n        logger.error(str(e))\n</code></pre>"},{"location":"learn/nn/functional/#msgflux.nn.functional.background_task","title":"background_task","text":"<pre><code>background_task(to_send, *args, **kwargs)\n</code></pre> <p>Executes a task in the background asynchronously without blocking, using the AsyncExecutorPool. This function is \"fire-and-forget\".</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>Callable</code> <p>Callable object (function, async function, or module with .acall() method).</p> required <code>*args</code> <p>Positional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Named arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a callable.</p> <p>Examples:</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.background_task--example-1","title":"Example 1:","text":"<p>import time def print_message(message: str):     time.sleep(1)     print(f\"[Sync] Message: {message}\") F.background_task(print_message, \"Hello from sync function\")</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.background_task--example-2","title":"Example 2:","text":"<p>import asyncio async def async_print_message(message: str):     await asyncio.sleep(1)     print(f\"[Async] Message: {message}\") F.background_task(async_print_message, \"Hello from async function\")</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.background_task--example-3-with-error","title":"Example 3 (with error):","text":"<p>def failing_task():     raise ValueError(\"This task failed!\") F.background_task(failing_task)  # Error will be logged</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.instrument()\ndef background_task(to_send: Callable, *args, **kwargs) -&gt; None:\n    \"\"\"Executes a task in the background asynchronously without blocking,\n    using the AsyncExecutorPool. This function is \"fire-and-forget\".\n\n    Args:\n        to_send:\n            Callable object (function, async function, or module with .acall() method).\n        *args:\n            Positional arguments.\n        **kwargs:\n            Named arguments.\n\n    Raises:\n        TypeError: If `to_send` is not a callable.\n\n    Examples:\n        # Example 1:\n        import time\n        def print_message(message: str):\n            time.sleep(1)\n            print(f\"[Sync] Message: {message}\")\n        F.background_task(print_message, \"Hello from sync function\")\n\n        # Example 2:\n        import asyncio\n        async def async_print_message(message: str):\n            await asyncio.sleep(1)\n            print(f\"[Async] Message: {message}\")\n        F.background_task(async_print_message, \"Hello from async function\")\n\n        # Example 3 (with error):\n        def failing_task():\n            raise ValueError(\"This task failed!\")\n        F.background_task(failing_task)  # Error will be logged\n    \"\"\"\n    if not callable(to_send):\n        raise TypeError(\"`to_send` must be a callable object\")\n\n    def log_future(future: Future) -&gt; None:\n        \"\"\"Callback to log exception of a Future.\"\"\"\n        try:\n            future.result()\n        except Exception as e:\n            logger.error(f\"Background task error: {e!s}\", exc_info=True)\n\n    executor = Executor.get_instance()\n    future = executor.submit(to_send, *args, **kwargs)\n    future.add_done_callback(log_future)\n</code></pre>"},{"location":"learn/nn/functional/#msgflux.nn.functional.amap_gather","title":"amap_gather  <code>async</code>","text":"<pre><code>amap_gather(to_send, *, args_list, kwargs_list=None)\n</code></pre> <p>Async version of map_gather. Applies the <code>to_send</code> async function to each set of arguments in <code>args_list</code> and <code>kwargs_list</code> and collects the results.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>Callable</code> <p>The async callable function to be applied.</p> required <code>args_list</code> <code>List[Tuple[Any, ...]]</code> <p>Each tuple contains the positional arguments for the corresponding callable in <code>to_send</code>. If <code>None</code>, no positional arguments are passed unless specified individually by an item in <code>kwargs_list</code>.</p> required <code>kwargs_list</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Each dictionary contains the named arguments for the corresponding callable in <code>to_send</code>. If <code>None</code>, no named arguments are passed unless specified individually by an item in <code>args_list</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Any, ...]</code> <p>A tuple containing the results of each call to the <code>to_send</code> function.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not callable.</p> <code>ValueError</code> <p>If <code>args_list</code> is not a non-empty list or if <code>kwargs_list</code> (if provided) is not the same length as <code>args_list</code>.</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.ainstrument()\nasync def amap_gather(\n    to_send: Callable,\n    *,\n    args_list: List[Tuple[Any, ...]],\n    kwargs_list: Optional[List[Dict[str, Any]]] = None,\n) -&gt; Tuple[Any, ...]:\n    \"\"\"Async version of map_gather. Applies the `to_send` async function to each\n    set of arguments in `args_list` and `kwargs_list` and collects the results.\n\n    Args:\n        to_send:\n            The async callable function to be applied.\n        args_list:\n            Each tuple contains the positional arguments for the corresponding callable\n            in `to_send`. If `None`, no positional arguments are passed unless specified\n            individually by an item in `kwargs_list`.\n        kwargs_list:\n            Each dictionary contains the named arguments for the corresponding callable\n            in `to_send`. If `None`, no named arguments are passed unless specified\n            individually by an item in `args_list`.\n\n    Returns:\n        A tuple containing the results of each call to the `to_send` function.\n\n    Raises:\n        TypeError:\n            If `to_send` is not callable.\n        ValueError:\n            If `args_list` is not a non-empty list or if `kwargs_list`\n            (if provided) is not the same length as `args_list`.\n    \"\"\"\n    if not callable(to_send):\n        raise TypeError(\"`to_send` must be a callable object\")\n\n    if not isinstance(args_list, list) or len(args_list) == 0:\n        raise ValueError(\"`args_list` must be a non-empty list\")\n\n    if kwargs_list is not None:\n        if not isinstance(kwargs_list, list) or len(kwargs_list) != len(args_list):\n            raise ValueError(\n                \"`kwargs_list` must be a list with the same length as `args_list`\"\n            )\n\n    tasks = []\n    for i in range(len(args_list)):\n        args = args_list[i]\n        kwargs = kwargs_list[i] if kwargs_list else {}\n        tasks.append(to_send(*args, **kwargs))\n\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # Convert exceptions to None and log errors\n    results = []\n    for response in responses:\n        if isinstance(response, Exception):\n            logger.error(str(response))\n            results.append(None)\n        else:\n            results.append(response)\n\n    return tuple(results)\n</code></pre>"},{"location":"learn/nn/functional/#msgflux.nn.functional.ascatter_gather","title":"ascatter_gather  <code>async</code>","text":"<pre><code>ascatter_gather(to_send, args_list=None, kwargs_list=None)\n</code></pre> <p>Async version of scatter_gather. Sends different sets of arguments/kwargs to a list of async callables and collects the responses.</p> <p>Each callable in <code>to_send</code> receives the positional arguments of the corresponding <code>tuple</code> in <code>args_list</code> and the named arguments of the corresponding <code>dict</code> in <code>kwargs_list</code>. If <code>args_list</code> or <code>kwargs_list</code> are not provided (or are <code>None</code>), the corresponding callables will be called without positional or named arguments, respectively, unless an empty list (<code>[]</code>) or empty tuple (<code>()</code>) is provided for a specific item.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. async functions or <code>Module</code> instances with acall).</p> required <code>args_list</code> <code>Optional[List[Tuple[Any, ...]]]</code> <p>Each tuple contains the positional arguments for the corresponding callable in <code>to_send</code>. If <code>None</code>, no positional arguments are passed unless specified individually by an item in <code>kwargs_list</code>.</p> <code>None</code> <code>kwargs_list</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Each dictionary contains the named arguments for the corresponding callable in <code>to_send</code>. If <code>None</code>, no named arguments are passed unless specified individually by an item in <code>args_list</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Tuple containing the responses for each callable. If an error occurs for a</p> <code>...</code> <p>specific callable, its corresponding response in the tuple will be <code>None</code>.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a callable list.</p> <code>ValueError</code> <p>If the lengths of <code>args_list</code> (if provided) or <code>kwargs_list</code> (if provided) do not match the length of <code>to_send</code>.</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.ainstrument()\nasync def ascatter_gather(\n    to_send: List[Callable],\n    args_list: Optional[List[Tuple[Any, ...]]] = None,\n    kwargs_list: Optional[List[Dict[str, Any]]] = None,\n) -&gt; Tuple[Any, ...]:\n    \"\"\"Async version of scatter_gather. Sends different sets of arguments/kwargs\n    to a list of async callables and collects the responses.\n\n    Each callable in `to_send` receives the positional arguments of\n    the corresponding `tuple` in `args_list` and the named arguments\n    of the corresponding `dict` in `kwargs_list`. If `args_list` or\n    `kwargs_list` are not provided (or are `None`), the corresponding\n    callables will be called without positional or named arguments,\n    respectively, unless an empty list (`[]`) or empty tuple (`()`)\n    is provided for a specific item.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. async functions or `Module` instances\n            with acall).\n        args_list:\n            Each tuple contains the positional arguments for the corresponding callable\n            in `to_send`. If `None`, no positional arguments are passed unless specified\n            individually by an item in `kwargs_list`.\n        kwargs_list:\n            Each dictionary contains the named arguments for the corresponding callable\n            in `to_send`. If `None`, no named arguments are passed unless specified\n            individually by an item in `args_list`.\n\n    Returns:\n        Tuple containing the responses for each callable. If an error occurs for a\n        specific callable, its corresponding response in the tuple will be `None`.\n\n    Raises:\n        TypeError:\n            If `to_send` is not a callable list.\n        ValueError:\n            If the lengths of `args_list` (if provided) or `kwargs_list`\n            (if provided) do not match the length of `to_send`.\n    \"\"\"\n    if not isinstance(to_send, list) or not all(callable(f) for f in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    tasks = []\n    for i, f in enumerate(to_send):\n        args = args_list[i] if args_list and i &lt; len(args_list) else ()\n        kwargs = kwargs_list[i] if kwargs_list and i &lt; len(kwargs_list) else {}\n        tasks.append(f(*args, **kwargs))\n\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n\n    # Convert exceptions to None and log errors\n    results = []\n    for response in responses:\n        if isinstance(response, Exception):\n            logger.error(str(response))\n            results.append(None)\n        else:\n            results.append(response)\n\n    return tuple(results)\n</code></pre>"},{"location":"learn/nn/functional/#msgflux.nn.functional.amsg_bcast_gather","title":"amsg_bcast_gather  <code>async</code>","text":"<pre><code>amsg_bcast_gather(to_send, message)\n</code></pre> <p>Async version of msg_bcast_gather. Broadcasts a single message to multiple async modules and gathers the responses.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>List[Callable]</code> <p>List of callable objects (e.g. async functions or <code>Module</code> instances with acall).</p> required <code>message</code> <code>dotdict</code> <p>Instance of <code>msgflux.dotdict</code> to broadcast.</p> required <p>Returns:</p> Type Description <code>dotdict</code> <p>The original message with the module responses added.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>message</code> is not an instance of <code>dotdict</code>, <code>to_send</code> is not a list of callables.</p> <p>Examples:</p> <p>async def add_feat_a(msg: dotdict) -&gt; dotdict:     msg['feat_a'] = 'result_a'     return msg</p> <p>async def add_feat_b(msg: dotdict) -&gt; dotdict:     msg['feat_b'] = 'result_b'     return msg</p> <p>message = dotdict() result = await F.amsg_bcast_gather([add_feat_a, add_feat_b], message)</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.amsg_bcast_gather--message-now-contains-both-feat_a-and-feat_b","title":"message now contains both feat_a and feat_b","text":"Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.ainstrument()\nasync def amsg_bcast_gather(\n    to_send: List[Callable],\n    message: dotdict,\n) -&gt; dotdict:\n    \"\"\"Async version of msg_bcast_gather. Broadcasts a single message to multiple\n    async modules and gathers the responses.\n\n    Args:\n        to_send:\n            List of callable objects (e.g. async functions or `Module` instances\n            with acall).\n        message:\n            Instance of `msgflux.dotdict` to broadcast.\n\n    Returns:\n        The original message with the module responses added.\n\n    Raises:\n        TypeError:\n            If `message` is not an instance of `dotdict`, `to_send` is not a list\n            of callables.\n\n    Examples:\n        async def add_feat_a(msg: dotdict) -&gt; dotdict:\n            msg['feat_a'] = 'result_a'\n            return msg\n\n        async def add_feat_b(msg: dotdict) -&gt; dotdict:\n            msg['feat_b'] = 'result_b'\n            return msg\n\n        message = dotdict()\n        result = await F.amsg_bcast_gather([add_feat_a, add_feat_b], message)\n        # message now contains both feat_a and feat_b\n    \"\"\"\n    if not isinstance(message, dotdict):\n        raise TypeError(\"`message` must be an instance of `msgflux.dotdict`\")\n    if not to_send or not all(isinstance(module, Callable) for module in to_send):\n        raise TypeError(\"`to_send` must be a non-empty list of callable objects\")\n\n    tasks = []\n    for f in to_send:\n        # Check for acall method first, then coroutine function\n        if hasattr(f, \"acall\"):\n            tasks.append(f.acall(message))\n        elif asyncio.iscoroutinefunction(f):\n            tasks.append(f(message))\n        else:\n            # Fallback to sync call (will be executed in current event loop)\n            # Wrap in coroutine\n            async def _run_sync(func, msg):\n                return func(msg)\n\n            tasks.append(_run_sync(f, message))\n\n    responses = await asyncio.gather(*tasks, return_exceptions=True)\n\n    for f, response in zip(to_send, responses):\n        f_name = get_callable_name(f)\n        if isinstance(response, Exception):\n            logger.error(f\"Error in async bcast task for `{f_name}`: {response}\")\n\n    return message\n</code></pre>"},{"location":"learn/nn/functional/#msgflux.nn.functional.await_for_event","title":"await_for_event  <code>async</code>","text":"<pre><code>await_for_event(event)\n</code></pre> <p>Waits asynchronously for an asyncio.Event to be set.</p> <p>This function will await until event.set() is called elsewhere.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The asyncio.Event to wait for.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>event</code> is not an instance of asyncio.Event.</p> <p>Examples:</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.await_for_event--example-1","title":"Example 1:","text":"<p>import asyncio event = asyncio.Event()</p> <p>async def setter():     await asyncio.sleep(1)     event.set()</p> <p>asyncio.create_task(setter()) await F.await_for_event(event) print(\"Event was set!\")</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.ainstrument()\nasync def await_for_event(event: asyncio.Event) -&gt; None:\n    \"\"\"Waits asynchronously for an asyncio.Event to be set.\n\n    This function will await until event.set() is called elsewhere.\n\n    Args:\n        event: The asyncio.Event to wait for.\n\n    Raises:\n        TypeError: If `event` is not an instance of asyncio.Event.\n\n    Examples:\n        # Example 1:\n        import asyncio\n        event = asyncio.Event()\n\n        async def setter():\n            await asyncio.sleep(1)\n            event.set()\n\n        asyncio.create_task(setter())\n        await F.await_for_event(event)\n        print(\"Event was set!\")\n    \"\"\"\n    if not isinstance(event, asyncio.Event):\n        raise TypeError(\"`event` must be an instance of asyncio.Event\")\n\n    await event.wait()\n</code></pre>"},{"location":"learn/nn/functional/#msgflux.nn.functional.abackground_task","title":"abackground_task  <code>async</code>","text":"<pre><code>abackground_task(to_send, *args, **kwargs)\n</code></pre> <p>Executes an async task in the background without blocking. This is a truly async \"fire-and-forget\" function.</p> <p>Parameters:</p> Name Type Description Default <code>to_send</code> <code>Callable</code> <p>Callable object (async function or module with .acall() method).</p> required <code>*args</code> <p>Positional arguments.</p> <code>()</code> <code>**kwargs</code> <p>Named arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>to_send</code> is not a callable.</p> <p>Examples:</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.abackground_task--example-1","title":"Example 1:","text":"<p>import asyncio async def async_print_message(message: str):     await asyncio.sleep(1)     print(f\"[Async] Message: {message}\") await F.abackground_task(async_print_message, \"Hello from async function\")</p>"},{"location":"learn/nn/functional/#msgflux.nn.functional.abackground_task--example-2-with-error","title":"Example 2 (with error):","text":"<p>async def failing_task():     raise ValueError(\"This task failed!\") await F.abackground_task(failing_task)  # Error will be logged</p> Source code in <code>src/msgflux/nn/functional.py</code> <pre><code>@Spans.ainstrument()\nasync def abackground_task(to_send: Callable, *args, **kwargs) -&gt; None:\n    \"\"\"Executes an async task in the background without blocking.\n    This is a truly async \"fire-and-forget\" function.\n\n    Args:\n        to_send:\n            Callable object (async function or module with .acall() method).\n        *args:\n            Positional arguments.\n        **kwargs:\n            Named arguments.\n\n    Raises:\n        TypeError: If `to_send` is not a callable.\n\n    Examples:\n        # Example 1:\n        import asyncio\n        async def async_print_message(message: str):\n            await asyncio.sleep(1)\n            print(f\"[Async] Message: {message}\")\n        await F.abackground_task(async_print_message, \"Hello from async function\")\n\n        # Example 2 (with error):\n        async def failing_task():\n            raise ValueError(\"This task failed!\")\n        await F.abackground_task(failing_task)  # Error will be logged\n    \"\"\"\n    if not callable(to_send):\n        raise TypeError(\"`to_send` must be a callable object\")\n\n    async def run_task():\n        \"\"\"Wrapper to run the task and log errors.\"\"\"\n        try:\n            if hasattr(to_send, \"acall\"):\n                await to_send.acall(*args, **kwargs)\n            elif asyncio.iscoroutinefunction(to_send):\n                await to_send(*args, **kwargs)\n            else:\n                # Fall back to running sync function in executor\n                loop = asyncio.get_event_loop()\n                await loop.run_in_executor(None, lambda: to_send(*args, **kwargs))\n        except Exception as e:\n            logger.error(f\"Async background task error: {e!s}\", exc_info=True)\n\n    asyncio.create_task(run_task())  # noqa: RUF006\n</code></pre>"},{"location":"learn/nn/module/","title":"nn.Module","text":"<p>The <code>nn.Module</code> is the base class for all neural network modules in msgflux. It provides a PyTorch-like API with additional features for message passing, state management, and workflow orchestration.</p>"},{"location":"learn/nn/module/#overview","title":"Overview","text":"<p><code>nn.Module</code> offers a familiar interface for building composable, stateful components:</p> <ul> <li>PyTorch-style API: Similar interface to PyTorch's <code>nn.Module</code></li> <li>State management: Built-in <code>state_dict()</code> for serialization</li> <li>Hooks system: Pre and post forward hooks for extensibility</li> <li>AutoParams support: Dataclass-style module definitions (recommended)</li> <li>Async interface: Automatic async support via <code>aforward()</code> and <code>acall()</code></li> <li>OpenTelemetry integration: Built-in observability</li> <li>Module containers: <code>ModuleDict</code>, <code>ModuleList</code>, <code>Sequential</code></li> </ul>"},{"location":"learn/nn/module/#basic-usage","title":"Basic Usage","text":""},{"location":"learn/nn/module/#defining-a-module","title":"Defining a Module","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass SimpleModule(nn.Module):\n    def __init__(self, greeting=\"Hello\"):\n        super().__init__()\n        self.greeting = greeting\n\n    def forward(self, name):\n        return f\"{self.greeting}, {name}!\"\n\n# Create and use\nmodule = SimpleModule()\nresult = module(\"Alice\")\nprint(result)  # \"Hello, Alice!\"\n\n# With custom greeting\ncustom = SimpleModule(greeting=\"Hi\")\nresult = custom(\"Bob\")\nprint(result)  # \"Hi, Bob!\"\n</code></pre>"},{"location":"learn/nn/module/#using-autoparams-recommended","title":"Using AutoParams (Recommended)","text":"<p>The preferred way to define modules is using the <code>AutoParams</code> metaclass:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass GreetingModule(nn.Module, metaclass=mf.AutoParams):\n    \"\"\"Module that generates personalized greetings.\"\"\"\n\n    def __init__(self, greeting, punctuation):\n        super().__init__()\n        self.greeting = greeting\n        self.punctuation = punctuation\n\n    def forward(self, name):\n        return f\"{self.greeting}, {name}{self.punctuation}\"\n\n# Define variants with different defaults\nclass FormalGreeting(GreetingModule):\n    greeting = \"Good morning\"\n    punctuation = \".\"\n\nclass CasualGreeting(GreetingModule):\n    greeting = \"Hey\"\n    punctuation = \"!\"\n\n# Use with defaults\nformal = FormalGreeting()\nprint(formal(\"Dr. Smith\"))  # \"Good morning, Dr. Smith.\"\n\ncasual = CasualGreeting()\nprint(casual(\"Alex\"))  # \"Hey, Alex!\"\n\n# Override specific parameters\nexcited = FormalGreeting(punctuation=\"!!!\")\nprint(excited(\"Team\"))  # \"Good morning, Team!!!\"\n</code></pre>"},{"location":"learn/nn/module/#state-management","title":"State Management","text":""},{"location":"learn/nn/module/#buffers-and-parameters","title":"Buffers and Parameters","text":"<p>Modules can register buffers for configuration state that should be saved/loaded:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass StatefulModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Register buffers (not trainable, but part of state)\n        self.register_buffer(\"response_template\", \"Hello, {name}!\")\n        self.register_buffer(\"max_length\", 50)\n\n        # Use regular attributes for mutable state during execution\n        self.call_count = 0\n\n    def forward(self, name):\n        # Update call count (regular attribute)\n        self.call_count += 1\n        response = self.response_template.replace(\"{name}\", name)\n\n        # Use buffer value\n        if len(response) &gt; self.max_length:\n            response = response[:self.max_length] + \"...\"\n\n        return f\"[Call #{self.call_count}] {response}\"\n\nmodule = StatefulModule()\nprint(module(\"Alice\"))  # \"[Call #1] Hello, Alice!\"\nprint(module(\"Bob\"))    # \"[Call #2] Hello, Bob!\"\n\n# Buffers are included in state_dict\nstate = module.state_dict()\nprint(state[\"response_template\"])  # \"Hello, {name}!\"\nprint(state[\"max_length\"])  # 50\n</code></pre>"},{"location":"learn/nn/module/#saving-and-loading-state","title":"Saving and Loading State","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass ConfigurableModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"template\", \"Response: {input}\")\n        self.register_buffer(\"prefix\", \"\")\n\n    def forward(self, text):\n        formatted = self.template.replace(\"{input}\", text)\n        return self.prefix + formatted\n\n# Create and use module\nmodule = ConfigurableModule()\nprint(module(\"test\"))  # \"Response: test\"\n\n# Save state\nmf.save(module.state_dict(), \"module_state.json\")\n\n# Load and modify state\nstate = mf.load(\"module_state.json\")\nstate[\"prefix\"] = \"[INFO] \"\nstate[\"template\"] = \"Output: {input}\"\n\n# Load modified state\nmodule.load_state_dict(state)\nprint(module(\"test\"))  # \"[INFO] Output: test\"\n</code></pre> <p>Supported format: - <code>*.json</code></p>"},{"location":"learn/nn/module/#hooks-system","title":"Hooks System","text":"<p>Hooks allow you to intercept and modify module execution without changing the module code.</p>"},{"location":"learn/nn/module/#forward-pre-hooks","title":"Forward Pre-Hooks","text":"<p>Execute code before the forward pass:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass MessageModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Message received.\")\n\n    def forward(self, message, **kwargs):\n        user = kwargs.get(\"user_name\", \"Guest\")\n        return f\"{user}: {message} -&gt; {self.response}\"\n\n# Define pre-hook to enhance context\ndef add_user_context(module, args, kwargs):\n    \"\"\"Look up user name from ID.\"\"\"\n    user_id = kwargs.get(\"user_id\")\n    if user_id == \"123\":\n        kwargs[\"user_name\"] = \"Alice\"\n    elif user_id == \"456\":\n        kwargs[\"user_name\"] = \"Bob\"\n    return args, kwargs\n\n# Register hook\nmodule = MessageModule()\nhook_handle = module.register_forward_pre_hook(add_user_context)\n\n# Use with user_id\nresult = module(\"Hello\", user_id=\"123\")\nprint(result)  # \"Alice: Hello -&gt; Message received.\"\n\nresult = module(\"Hi\", user_id=\"456\")\nprint(result)  # \"Bob: Hi -&gt; Message received.\"\n\n# Remove hook\nhook_handle.remove()\n</code></pre>"},{"location":"learn/nn/module/#forward-post-hooks","title":"Forward Post-Hooks","text":"<p>Execute code after the forward pass:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass ProcessingModule(nn.Module):\n    def forward(self, text):\n        return text.upper()\n\n# Define post-hook for logging\ndef log_output(module, args, kwargs, output):\n    \"\"\"Log all outputs.\"\"\"\n    print(f\"[LOG] Output: {output}\")\n    return output\n\n# Register hook\nmodule = ProcessingModule()\nhook_handle = module.register_forward_hook(log_output)\n\nresult = module(\"hello\")\n# [LOG] Output: HELLO\nprint(result)  # HELLO\n\n# Remove hook\nhook_handle.remove()\n</code></pre>"},{"location":"learn/nn/module/#multiple-hooks","title":"Multiple Hooks","text":"<p>Hooks are executed in registration order:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass Module(nn.Module):\n    def forward(self, x):\n        return x * 2\n\ndef hook1(module, args, kwargs, output):\n    print(\"Hook 1\")\n    return output\n\ndef hook2(module, args, kwargs, output):\n    print(\"Hook 2\")\n    return output\n\nmodule = Module()\nh1 = module.register_forward_hook(hook1)\nh2 = module.register_forward_hook(hook2)\n\nmodule(5)\n# Hook 1\n# Hook 2\n\n# Access registered hooks\nprint(module._forward_hooks)  # OrderedDict with hooks\nprint(module._forward_pre_hooks)  # OrderedDict with pre-hooks\n</code></pre>"},{"location":"learn/nn/module/#async-support","title":"Async Support","text":"<p>Modules automatically support async execution via <code>acall()</code>:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\nimport asyncio\n\nclass AsyncModule(nn.Module):\n    async def aforward(self, text):\n        \"\"\"Async implementation.\"\"\"\n        await asyncio.sleep(0.01)  # Simulate async operation\n        return text.upper()\n\n# Sync fallback (optional)\ndef forward(self, text):\n    \"\"\"Sync implementation.\"\"\"\n    return text.upper()\n\n# Use async\nasync def main():\n    module = AsyncModule()\n\n    # Call async version\n    result = await module.acall(\"hello\")\n    print(result)  # \"HELLO\"\n\n    # Sync call also works (falls back to forward if aforward not implemented)\n    result = module(\"world\")\n    print(result)  # \"WORLD\"\n\nasyncio.run(main())\n</code></pre> <p>Note: If <code>aforward()</code> is not implemented, <code>acall()</code> will use <code>forward()</code>.</p>"},{"location":"learn/nn/module/#module-containers","title":"Module Containers","text":""},{"location":"learn/nn/module/#moduledict","title":"ModuleDict","text":"<p>Hold submodules in a dictionary for dynamic routing:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass SalesExpert(nn.Module):\n    def forward(self, message):\n        return f\"Sales: {message} -&gt; Let's discuss pricing!\"\n\nclass SupportExpert(nn.Module):\n    def forward(self, message):\n        return f\"Support: {message} -&gt; Call our support line!\"\n\nclass TechExpert(nn.Module):\n    def forward(self, message):\n        return f\"Tech: {message} -&gt; Check the documentation.\"\n\nclass Router(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.experts = nn.ModuleDict({\n            \"sales\": SalesExpert(),\n            \"support\": SupportExpert(),\n            \"tech\": TechExpert()\n        })\n\n    def forward(self, message, department):\n        if department in self.experts:\n            return self.experts[department](message)\n        return \"Department not found.\"\n\nrouter = Router()\n\nprint(router(\"I want to buy\", \"sales\"))\n# \"Sales: I want to buy -&gt; Let's discuss pricing!\"\n\nprint(router(\"I need help\", \"support\"))\n# \"Support: I need help -&gt; Call our support line!\"\n\nprint(router(\"How does this work?\", \"tech\"))\n# \"Tech: How does this work? -&gt; Check the documentation.\"\n\n# Access state dict\nprint(router.state_dict())\n</code></pre> <p>Key features: - Modules properly registered and visible - Ordered insertion (maintains order) - Can be indexed like a regular dict</p>"},{"location":"learn/nn/module/#modulelist","title":"ModuleList","text":"<p>Hold submodules in a list for sequential processing:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass Uppercase(nn.Module):\n    def forward(self, text):\n        return text.upper()\n\nclass AddPrefix(nn.Module):\n    def forward(self, text):\n        return f\"[PROCESSED] {text}\"\n\nclass AddSuffix(nn.Module):\n    def forward(self, text):\n        return f\"{text} [DONE]\"\n\nclass Pipeline(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.steps = nn.ModuleList([\n            Uppercase(),\n            AddPrefix(),\n            AddSuffix()\n        ])\n\n    def forward(self, text):\n        # Process through all steps\n        result = text\n        for step in self.steps:\n            result = step(result)\n        return result\n\npipeline = Pipeline()\nresult = pipeline(\"hello world\")\nprint(result)  # \"[PROCESSED] HELLO WORLD [DONE]\"\n\n# Access individual steps\nprint(pipeline.steps[0](\"test\"))  # \"TEST\"\n</code></pre> <p>Key features: - Can be indexed like a list - Can be iterated over - Modules properly registered</p>"},{"location":"learn/nn/module/#sequential","title":"Sequential","text":"<p>Chain modules in a cascade:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass Lowercase(nn.Module):\n    def forward(self, text):\n        return text.lower()\n\nclass RemoveSpaces(nn.Module):\n    def forward(self, text):\n        return text.replace(\" \", \"\")\n\nclass AddDashes(nn.Module):\n    def forward(self, text):\n        return \"-\".join(text)\n\n# Create sequential pipeline\npipeline = nn.Sequential(\n    Lowercase(),\n    RemoveSpaces(),\n    AddDashes()\n)\n\nresult = pipeline(\"Hello World\")\nprint(result)  # \"h-e-l-l-o-w-o-r-l-d\"\n</code></pre> <p>With OrderedDict for named steps:</p> <pre><code>from collections import OrderedDict\nimport msgflux.nn as nn\n\nclass Step1(nn.Module):\n    def forward(self, x):\n        return x + \" -&gt; Step1\"\n\nclass Step2(nn.Module):\n    def forward(self, x):\n        return x + \" -&gt; Step2\"\n\npipeline = nn.Sequential(OrderedDict([\n    (\"first\", Step1()),\n    (\"second\", Step2())\n]))\n\nresult = pipeline(\"Start\")\nprint(result)  # \"Start -&gt; Step1 -&gt; Step2\"\n\n# Access state dict shows named modules\nprint(pipeline.state_dict())\n</code></pre> <p>Difference between Sequential and ModuleList: - <code>Sequential</code>: Modules are automatically chained (output \u2192 input) - <code>ModuleList</code>: Just a container, you control the flow in <code>forward()</code></p>"},{"location":"learn/nn/module/#common-patterns","title":"Common Patterns","text":""},{"location":"learn/nn/module/#configuration-module","title":"Configuration Module","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass ConfigurableModule(nn.Module, metaclass=mf.AutoParams):\n    \"\"\"Module with configurable behavior.\"\"\"\n\n    def __init__(self, max_length, add_prefix, prefix_text):\n        super().__init__()\n        self.max_length = max_length\n        self.add_prefix = add_prefix\n        self.prefix_text = prefix_text\n\n    def forward(self, text):\n        # Truncate\n        if len(text) &gt; self.max_length:\n            text = text[:self.max_length] + \"...\"\n\n        # Add prefix\n        if self.add_prefix:\n            text = f\"{self.prefix_text}{text}\"\n\n        return text\n\n# Production configuration\nclass ProductionModule(ConfigurableModule):\n    max_length = 100\n    add_prefix = True\n    prefix_text = \"[PROD] \"\n\n# Development configuration\nclass DevModule(ConfigurableModule):\n    max_length = 500\n    add_prefix = True\n    prefix_text = \"[DEV] \"\n\n# Use\nprod = ProductionModule()\ndev = DevModule()\n\ntext = \"A\" * 150\nprint(prod(text))  # \"[PROD] AAA...AAA...\"\nprint(dev(text))   # \"[DEV] AAAA...AAAA...\" (longer)\n</code></pre>"},{"location":"learn/nn/module/#conditional-routing","title":"Conditional Routing","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass BasicHandler(nn.Module):\n    def forward(self, msg):\n        return f\"Basic: {msg}\"\n\nclass PremiumHandler(nn.Module):\n    def forward(self, msg):\n        return f\"Premium: {msg} [VIP Support]\"\n\nclass Router(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.handlers = nn.ModuleDict({\n            \"basic\": BasicHandler(),\n            \"premium\": PremiumHandler()\n        })\n\n    def forward(self, message, user_tier=\"basic\"):\n        handler = self.handlers.get(user_tier, self.handlers[\"basic\"])\n        return handler(message)\n\nrouter = Router()\nprint(router(\"Help!\", \"basic\"))    # \"Basic: Help!\"\nprint(router(\"Help!\", \"premium\"))  # \"Premium: Help! [VIP Support]\"\n</code></pre>"},{"location":"learn/nn/module/#multi-step-pipeline-with-state","title":"Multi-Step Pipeline with State","text":"<pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass Step1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use regular attribute for runtime state\n        self.processed_count = 0\n\n    def forward(self, msg):\n        self.processed_count += 1\n        msg.set(\"step1_result\", msg.input.upper())\n        return msg\n\nclass Step2(nn.Module):\n    def forward(self, msg):\n        msg.set(\"step2_result\", msg.step1_result.replace(\" \", \"_\"))\n        return msg\n\nclass Step3(nn.Module):\n    def forward(self, msg):\n        msg.final = f\"[{msg.step2_result}]\"\n        return msg\n\nclass Pipeline(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.steps = nn.Sequential(Step1(), Step2(), Step3())\n\n    def forward(self, input_text):\n        msg = mf.dotdict({\"input\": input_text})\n        result = self.steps(msg)\n        return result.final\n\npipeline = Pipeline()\nprint(pipeline(\"hello world\"))  # \"[HELLO_WORLD]\"\n</code></pre>"},{"location":"learn/nn/module/#best-practices","title":"Best Practices","text":""},{"location":"learn/nn/module/#1-use-autoparams-for-module-definitions","title":"1. Use AutoParams for Module Definitions","text":"<pre><code># Good - AutoParams separates config from logic\nclass MyModule(nn.Module, metaclass=mf.AutoParams):\n    def __init__(self, param1, param2):\n        super().__init__()\n        self.param1 = param1\n        self.param2 = param2\n\n    def forward(self, x):\n        return x * self.param1 + self.param2\n\nclass Variant1(MyModule):\n    param1 = 2\n    param2 = 10\n\n# Bad - Repeating defaults in __init__ signature\nclass MyModule(nn.Module):\n    def __init__(self, param1=2, param2=10):\n        super().__init__()\n        self.param1 = param1\n        self.param2 = param2\n</code></pre>"},{"location":"learn/nn/module/#2-use-register_buffer-for-non-trainable-state","title":"2. Use register_buffer for Non-Trainable State","text":"<pre><code># Good - State is tracked in state_dict\nclass Module(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"config\", {\"key\": \"value\"})\n\n# Bad - State is lost when serializing\nclass Module(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.config = {\"key\": \"value\"}  # Not tracked!\n</code></pre>"},{"location":"learn/nn/module/#3-use-hooks-for-cross-cutting-concerns","title":"3. Use Hooks for Cross-Cutting Concerns","text":"<pre><code># Good - Logging separate from core logic\ndef logging_hook(module, args, kwargs, output):\n    print(f\"Output: {output}\")\n    return output\n\nmodule.register_forward_hook(logging_hook)\n\n# Bad - Mixing logging with core logic\nclass Module(nn.Module):\n    def forward(self, x):\n        result = process(x)\n        print(f\"Output: {result}\")  # Logging mixed in\n        return result\n</code></pre>"},{"location":"learn/nn/module/#4-use-sequential-for-linear-pipelines","title":"4. Use Sequential for Linear Pipelines","text":"<pre><code># Good - Clear pipeline structure\npipeline = nn.Sequential(Step1(), Step2(), Step3())\n\n# Acceptable but more verbose\nclass Pipeline(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.step1 = Step1()\n        self.step2 = Step2()\n        self.step3 = Step3()\n\n    def forward(self, x):\n        x = self.step1(x)\n        x = self.step2(x)\n        x = self.step3(x)\n        return x\n</code></pre>"},{"location":"learn/nn/module/#5-save-and-version-state-dicts","title":"5. Save and Version State Dicts","text":"<pre><code># Good - Versioned state management\nstate = module.state_dict()\nstate[\"_version\"] = \"1.0\"\nmf.save(state, \"module_v1.0.json\")\n\n# Load and check version\nstate = mf.load(\"module_v1.0.json\")\nif state.get(\"_version\") != \"1.0\":\n    # Handle migration\n    pass\n</code></pre>"},{"location":"learn/nn/module/#opentelemetry-integration","title":"OpenTelemetry Integration","text":"<p>Modules automatically support tracing when telemetry is enabled:</p> <pre><code>import msgflux as mf\nimport msgflux.nn as nn\n\nclass TracedModule(nn.Module):\n    def forward(self, x):\n        # Automatically traced\n        return x * 2\n\n# Set environment variables to enable tracing:\n# MSGTRACE_TELEMETRY_REQUIRES_TRACE=true\n# MSGTRACE_TELEMETRY_SPAN_EXPORTER_TYPE=otlp\n\nmodule = TracedModule()\nresult = module(5)  # Creates a trace span automatically\n</code></pre>"},{"location":"learn/nn/module/#api-reference","title":"API Reference","text":"<p>For complete API documentation, see:</p>"},{"location":"learn/nn/module/#msgflux.nn.Module","title":"Module","text":"Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>class Module:\n    training: bool\n    _version: int = 1\n    \"\"\"This allows better BC support for :meth:`load_state_dict`. In\n    :meth:`state_dict`, the version number will be saved as in the attribute\n    `_metadata` of the returned state dict, and thus pickled. `_metadata` is a\n    dictionary with keys that follow the naming convention of state dict. See\n    ``_load_from_state_dict`` on how to use this information in loading.\n\n    If new parameters/buffers are added/removed from a module, this number shall\n    be bumped, and the module's `_load_from_state_dict` method can compare the\n    version number and do appropriate changes if the state dict is from before\n    the change.\"\"\"\n    _parameters: Dict[str, Optional[Parameter]] = OrderedDict()\n    _buffers: Dict[str, Optional[Any]] = OrderedDict()\n    _modules: Dict[str, Optional[\"Module\"]] = OrderedDict()\n    _forward_hooks: Dict[int, Callable]\n    _forward_hooks_with_kwargs: Dict[int, bool]\n    _forward_hooks_always_called: Dict[int, bool]\n    _forward_pre_hooks: Dict[int, Callable]\n    _forward_pre_hooks_with_kwargs: Dict[int, bool]\n    _load_state_dict_post_hooks: Dict[int, Callable]\n    _load_state_dict_pre_hooks: Dict[int, Callable]\n    _state_dict_hooks: Dict[int, Callable]\n    _state_dict_pre_hooks: Dict[int, Callable]\n    call_super_init: bool = False\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n        if self.call_super_init is False and bool(kwargs):\n            raise TypeError(\n                f\"{type(self).__name__}.__init__() got an unexpected \"\n                f\"keyword argument `{next(iter(kwargs))}`\"\n            )\n\n        if self.call_super_init is False and bool(args):\n            raise TypeError(\n                f\"{type(self).__name__}.__init__() takes 1 positional \"\n                f\"argument but {len(args) + 1} were\"\n            )\n\n        \"\"\"\n        Calls super().__setattr__('a', a) instead of the typical self.a = a\n        to avoid Module.__setattr__ overhead. Module's __setattr__ has special\n        handling for parameters, submodules, and buffers but simply calls into\n        super().__setattr__ for all other attributes.\n        \"\"\"\n        super().__setattr__(\"training\", True)\n        super().__setattr__(\"_parameters\", {})\n        super().__setattr__(\"_buffers\", {})\n        super().__setattr__(\"_non_persistent_buffers_set\", set())  # ?\n        super().__setattr__(\"_forward_pre_hooks\", OrderedDict())\n        super().__setattr__(\"_forward_hooks\", OrderedDict())\n        super().__setattr__(\"_forward_hooks_always_called\", OrderedDict())\n        super().__setattr__(\"_state_dict_hooks\", OrderedDict())\n        super().__setattr__(\"_state_dict_pre_hooks\", OrderedDict())\n        super().__setattr__(\"_load_state_dict_pre_hooks\", OrderedDict())\n        super().__setattr__(\"_load_state_dict_post_hooks\", OrderedDict())\n        super().__setattr__(\"_modules\", {})\n\n        if self.call_super_init:\n            super().__init__(*args, **kwargs)\n\n    forward: Callable[..., Any] = _forward_unimplemented\n    aforward: Callable[..., Any] = _aforward_unimplemented\n\n    # msgflux funcs\n\n    def _get_mermaid(\n        self,\n        title: Optional[str] = None,\n        orientation: Optional[str] = \"TD\",\n        *,\n        remove_self: Optional[bool] = True,\n    ) -&gt; str:\n        if code_to_mermaid is None:\n            raise ImportError(\n                \"`mermaid` client is not available. \"\n                \"Install with `pip install msgflux[plot]`.\"\n            )\n        mermaid = code_to_mermaid(\n            inspect.getsource(self.forward),\n            remove_self=remove_self,\n            title=title,\n            orientation=orientation,\n        )\n        return mermaid\n\n    def plot(\n        self,\n        title: Optional[str] = None,\n        orientation: Optional[str] = \"TD\",\n        *,\n        remove_self: Optional[bool] = True,\n    ) -&gt; Mermaid:\n        \"\"\"Generates and renders a Mermaid diagram of the `forward` method.\n\n        This method extracts the source code of the `forward` method and converts\n        it into a Mermaid diagram for visualization. Optionally, it can clean up\n        the code by removing references to `self` to produce a cleaner diagram.\n\n        Args:\n            title:\n                Title to display at the top of the Mermaid diagram.\n            orientation:\n                Diagram orientation. Options include \"TD\" (top-down),\n                \"LR\" (left-right), etc.\n            remove_self:\n                Whether to remove references to `self` from the code before\n                generating the diagram. Useful for cleaner output.\n\n        Returns:\n            The rendered Mermaid diagram.\n        \"\"\"\n        mermaid = self._get_mermaid(title, orientation, remove_self)\n        return plot_mermaid(mermaid)\n\n    def _get_content_from_or_input(self, path: str, message: Message) -&gt; Any:\n        \"\"\"Returns the first valid content from OR input.\"\"\"\n        content = None\n        for single_path in path:\n            content = message.get(single_path)\n            if content is not None:\n                break\n        return content\n\n    def _get_content_from_message(self, path: str, message: Message):\n        content = None\n        if isinstance(message, Message):\n            if isinstance(path, tuple):  # OR inputs\n                content = self._get_content_from_or_input(path, message)\n            else:\n                content = message.get(path)\n        return content\n\n    def _extract_message_values(\n        self, paths: Union[str, List[str], Dict[str, str]], message: Message\n    ) -&gt; Optional[Union[str, Dict[str, Any], List[Any], None]]:\n        \"\"\"Process inputs based on their type (str, dict, list)\n        by extracting content from the message.\n        \"\"\"\n        if isinstance(paths, str):\n            return self._get_content_from_message(paths, message)\n        elif isinstance(paths, dict):\n            return dotdict(\n                {\n                    key: self._get_content_from_message(path, message)\n                    for key, path in paths.items()\n                }\n            )\n        elif isinstance(paths, list):\n            return [\n                self._get_content_from_message(path, message)\n                for path in paths\n                if self._get_content_from_message(path, message) is not None\n            ]\n        return None\n\n    def _format_task_template(self, content: Union[str, Dict[str, Any]]) -&gt; str:\n        return self._format_template(content, self.templates.get(\"task\"))\n\n    def _format_response_template(self, content: str) -&gt; str:\n        return self._format_template(content, self.templates.get(\"response\"))\n\n    def _format_template(\n        self, content: Union[str, Dict[str, Any]], raw_template: str\n    ) -&gt; str:\n        \"\"\"Format a template with content, using security settings from config.\n\n        Delegates to msgflux.utils.templates.format_template with sanitization\n        controlled by the 'sanitize_inputs' config option (default: True).\n        \"\"\"\n        sanitize = self.config.get(\"sanitize_inputs\", True)\n        return format_template(content, raw_template, sanitize_inputs=sanitize)\n\n    def set_name(self, name: str):\n        if isinstance(name, str):\n            if name != \"\":\n                self.register_buffer(\"name\", name)\n            else:\n                raise ValueError(\"`name` requires a string not empty\")\n        else:\n            raise TypeError(f\"`name` need be a `str` given {type(name)}\")\n\n    def set_annotations(self, annotations: Dict[str, type]):\n        if isinstance(annotations, dict):\n            super().__setattr__(\"annotations\", annotations)\n        else:\n            raise TypeError(f\"`annotations` need be a `dict` given {type(annotations)}\")\n\n    def _set_response_mode(self, response_mode: str):\n        if isinstance(response_mode, str):\n            if response_mode == \"\":\n                raise ValueError(\"`response_mode` requires a not empty string\")\n            self.register_buffer(\"response_mode\", response_mode)\n        else:\n            raise TypeError(\n                f\"`response_mode` requires a string given `{type(response_mode)}`\"\n            )\n\n    def _set_prompt(self, prompt: Optional[str] = None):\n        if isinstance(prompt, str) or prompt is None:\n            self.register_buffer(\"prompt\", prompt)\n        else:\n            raise TypeError(f\"`prompt` need be a str or None given `{type(prompt)}`\")\n\n    def _set_execution_kwargs(self, execution_kwargs: Optional[Dict[str, Any]] = None):\n        if isinstance(execution_kwargs, dict) or execution_kwargs is None:\n            if isinstance(execution_kwargs, dict):\n                execution_kwargs = dotdict(execution_kwargs)\n            self.register_buffer(\"execution_kwargs\", execution_kwargs)\n        else:\n            raise TypeError(\n                \"`execution_kwargs` need be a dict or None \"\n                f\"given `{type(execution_kwargs)}`\"\n            )\n\n    def _extract_raw_response(\n        self, model_response: Union[ModelResponse, ModelStreamResponse]\n    ) -&gt; Any:\n        if isinstance(model_response, ModelResponse):\n            return model_response.consume()\n        elif isinstance(model_response, ModelStreamResponse):\n            return model_response\n        else:\n            raise ValueError(f\"Unsupported `model_response={type(model_response)}`\")\n\n    def _prepare_response(self, raw_response: Any, message: Any) -&gt; Any:\n        if not isinstance(raw_response, ModelStreamResponse) and (\n            hasattr(self, \"templates\") and self.templates.get(\"response\") is not None\n        ):\n            response = self._format_response_template(raw_response)\n        else:\n            response = raw_response\n        return self._define_response_mode(response, message)\n\n    def _define_response_mode(self, response: Any, message: Any) -&gt; Any:\n        if self.response_mode == \"plain_response\":\n            return response\n        elif isinstance(message, Message):\n            message.set(self.response_mode, response)\n            return message\n        else:\n            raise ValueError(\n                \"For non-Message objects is required `response_mode=='plain_response'`\"\n            )\n\n    def _set_task_inputs(\n        self, task_inputs: Optional[Union[str, Dict[str, str], Tuple[str, ...]]] = None\n    ):\n        # TODO: suporte para lista de inputs [\"outputs.text1\", \"outputs.text2\"]\n        if isinstance(task_inputs, (str, dict, tuple)) or task_inputs is None:\n            if isinstance(task_inputs, str) and task_inputs == \"\":\n                raise ValueError(\n                    f\"`task_inputs` requires a string not empty given `{task_inputs}`\"\n                )\n            if isinstance(task_inputs, (dict, tuple)) and not task_inputs:\n                raise ValueError(\n                    \"`task_inputs` requires a dict or tuple not empty \"\n                    f\"given `{task_inputs}`\"\n                )\n            self.register_buffer(\"task_inputs\", task_inputs)\n        else:\n            raise TypeError(\n                \"`task_inputs` requires a string, dict or None, \"\n                f\"given `{type(task_inputs)}`\"\n            )\n\n    def _set_task_multimodal_inputs(\n        self, task_multimodal_inputs: Optional[Dict[str, List[str]]] = None\n    ):\n        # TODO permitir passar em vez de uma lista passar so um valor se for unico\n        if isinstance(task_multimodal_inputs, dict) or task_multimodal_inputs is None:\n            if not task_multimodal_inputs and task_multimodal_inputs is not None:\n                raise ValueError(\n                    \"`task_multimodal_inputs` requires a dict not empty\"\n                    f\"given `{task_multimodal_inputs}`\"\n                )\n            self.register_buffer(\"task_multimodal_inputs\", task_multimodal_inputs)\n        else:\n            raise TypeError(\n                \"`task_multimodal_inputs` requires a dict \"\n                f\"given `{type(task_multimodal_inputs)}`\"\n            )\n\n    def _set_model_preference(self, model_preference: Optional[str] = None):\n        if isinstance(model_preference, str) or model_preference is None:\n            self.register_buffer(\"model_preference\", model_preference)\n        else:\n            raise TypeError(\n                \"`model_preference` need be a string or None, \"\n                f\"given `{type(model_preference)}`\"\n            )\n\n    def _set_guardrails(self, guardrails: Optional[Dict[str, Callable]] = None):\n        \"\"\"Set guardrails for input and output execution.\n\n        Args:\n            guardrails: Dictionary mapping guardrail types to callables.\n                Valid keys: \"input\", \"output\"\n\n        Raises:\n            TypeError: If guardrails is not a dict or None\n            ValueError: If invalid keys are provided\n        \"\"\"\n        if guardrails is None:\n            self.guardrails = {}\n            return\n\n        if not isinstance(guardrails, dict):\n            raise TypeError(\n                f\"`guardrails` must be a dict or None, given `{type(guardrails)}`\"\n            )\n\n        # Validate keys\n        valid_keys = {\"input\", \"output\"}\n        invalid_keys = set(guardrails.keys()) - valid_keys\n        if invalid_keys:\n            raise ValueError(\n                f\"Invalid guardrail keys: {invalid_keys}. Valid keys are: {valid_keys}\"\n            )\n\n        # Validate that all values are callable\n        for key, guardrail in guardrails.items():\n            if not isinstance(guardrail, Callable):\n                raise TypeError(\n                    f\"Guardrail for '{key}' must be callable, given `{type(guardrail)}`\"\n                )\n\n        # Store guardrails, registering as buffers if needed\n        self.guardrails = {}\n        for key, guardrail in guardrails.items():\n            if inspect.isclass(guardrail) and hasattr(guardrail, \"serialize\"):\n                self.register_buffer(f\"{key}_guardrail\", guardrail)\n                self.guardrails[key] = getattr(self, f\"{key}_guardrail\")\n            elif isinstance(guardrail, self.__class__):\n                setattr(self, f\"{key}_guardrail\", guardrail)\n                self.guardrails[key] = guardrail\n            else:\n                super().__setattr__(f\"{key}_guardrail\", guardrail)\n                self.guardrails[key] = guardrail\n\n    def _set_message_fields(self, message_fields: Optional[Dict[str, Any]] = None):\n        \"\"\"Set message field mappings.\n\n        Args:\n            message_fields: Dictionary mapping field names to their values.\n                Valid keys: \"task_inputs\", \"task_multimodal_inputs\", \"model_preference\"\n\n        Raises:\n            TypeError: If message_fields is not a dict or None\n            ValueError: If invalid keys are provided\n        \"\"\"\n        # Define valid keys for base Module class\n        valid_keys = {\"task_inputs\", \"task_multimodal_inputs\", \"model_preference\"}\n\n        if message_fields is None:\n            # Set all fields to None\n            self._set_task_inputs(None)\n            self._set_task_multimodal_inputs(None)\n            self._set_model_preference(None)\n            return\n\n        if not isinstance(message_fields, dict):\n            raise TypeError(\n                f\"`message_fields` must be a dict or None, given \"\n                f\"`{type(message_fields)}`\"\n            )\n\n        # Validate keys\n        invalid_keys = set(message_fields.keys()) - valid_keys\n        if invalid_keys:\n            raise ValueError(\n                f\"Invalid message_fields keys: {invalid_keys}. \"\n                f\"Valid keys are: {valid_keys}\"\n            )\n\n        # Set each field using its setter, defaulting to None if not provided\n        self._set_task_inputs(message_fields.get(\"task_inputs\"))\n        self._set_task_multimodal_inputs(message_fields.get(\"task_multimodal_inputs\"))\n        self._set_model_preference(message_fields.get(\"model_preference\"))\n\n    def _set_templates(self, templates: Optional[Dict[str, str]] = None):\n        \"\"\"Set Jinja templates for different workflow stages.\n\n        Args:\n            templates: Dictionary mapping template types to Jinja template strings.\n                Valid keys: \"task\", \"response\", \"context\", \"system_prompt\"\n\n        Raises:\n            TypeError: If templates is not a dict or None\n            ValueError: If invalid keys are provided\n\n        Note:\n            The \"context\" template applies only to context_inputs, not to context_cache.\n        \"\"\"\n        # Define valid keys\n        valid_keys = {\"task\", \"response\", \"context\", \"system_prompt\"}\n\n        if templates is None:\n            self.templates = {}\n            return\n\n        if not isinstance(templates, dict):\n            raise TypeError(\n                f\"`templates` must be a dict or None, given `{type(templates)}`\"\n            )\n\n        # Validate keys\n        invalid_keys = set(templates.keys()) - valid_keys\n        if invalid_keys:\n            raise ValueError(\n                f\"Invalid templates keys: {invalid_keys}. Valid keys are: {valid_keys}\"\n            )\n\n        # Validate that all values are strings or None\n        for key, template in templates.items():\n            if not isinstance(template, str) and template is not None:\n                raise TypeError(\n                    f\"Template '{key}' must be a string or None, \"\n                    f\"given `{type(template)}`\"\n                )\n\n        # Store templates\n        self.templates = templates.copy()\n\n    def _execute_input_guardrail(self, model_execution_params: Dict[str, Any]):\n        input_guardrail = self.guardrails.get(\"input\")\n        if not input_guardrail:\n            return\n\n        guardrail_params = self._prepare_input_guardrail_execution(\n            model_execution_params\n        )\n        guardrail_response = input_guardrail(**guardrail_params)\n\n        if isinstance(guardrail_response, ModelResponse):\n            guardrail_response = self._extract_raw_response(guardrail_response)\n\n        if not guardrail_response[\"safe\"]:\n            raise UnsafeUserInputError()  # TODO\n\n    async def _aexecute_input_guardrail(self, model_execution_params: Dict[str, Any]):\n        input_guardrail = self.guardrails.get(\"input\")\n        if not input_guardrail:\n            return\n\n        guardrail_params = self._prepare_input_guardrail_execution(\n            model_execution_params\n        )\n\n        # Check if guardrail has acall method or is a coroutine function\n        if hasattr(input_guardrail, \"acall\"):\n            guardrail_response = await input_guardrail.acall(**guardrail_params)\n        elif inspect.iscoroutinefunction(input_guardrail):\n            guardrail_response = await input_guardrail(**guardrail_params)\n        else:\n            # Fallback to sync call in executor to avoid blocking event loop\n            loop = asyncio.get_event_loop()\n            guardrail_response = await loop.run_in_executor(\n                None, lambda: input_guardrail(**guardrail_params)\n            )\n\n        if isinstance(guardrail_response, ModelResponse):\n            guardrail_response = self._extract_raw_response(guardrail_response)\n\n        if not guardrail_response[\"safe\"]:\n            raise UnsafeUserInputError()  # TODO\n\n    def _execute_output_guardrail(self, model_response: Dict[str, Any]):\n        output_guardrail = self.guardrails.get(\"output\")\n        if not output_guardrail:\n            return\n\n        guardrail_params = self._prepare_output_guardrail_execution(model_response)\n        guardrail_response = output_guardrail(**guardrail_params)\n\n        if isinstance(guardrail_response, ModelResponse):\n            guardrail_response = self._extract_raw_response(guardrail_response)\n\n        if not guardrail_response[\"safe\"]:\n            raise UnsafeModelResponseError()  # TODO\n\n    async def _aexecute_output_guardrail(self, model_response: Dict[str, Any]):\n        output_guardrail = self.guardrails.get(\"output\")\n        if not output_guardrail:\n            return\n\n        guardrail_params = self._prepare_output_guardrail_execution(model_response)\n\n        # Check if guardrail has acall method or is a coroutine function\n        if hasattr(output_guardrail, \"acall\"):\n            guardrail_response = await output_guardrail.acall(**guardrail_params)\n        elif inspect.iscoroutinefunction(output_guardrail):\n            guardrail_response = await output_guardrail(**guardrail_params)\n        else:\n            # Fallback to sync call in executor to avoid blocking event loop\n            loop = asyncio.get_event_loop()\n            guardrail_response = await loop.run_in_executor(\n                None, lambda: output_guardrail(**guardrail_params)\n            )\n\n        if isinstance(guardrail_response, ModelResponse):\n            guardrail_response = self._extract_raw_response(guardrail_response)\n\n        if not guardrail_response[\"safe\"]:\n            raise UnsafeModelResponseError()  # TODO\n\n    def get_model_preference_from_message(self, message: Message) -&gt; Optional[str]:\n        if isinstance(message, Message) and isinstance(self.model_preference, str):\n            return message.get(self.model_preference)\n        else:\n            return None\n\n    def set_description(self, description: Optional[str] = None):\n        if isinstance(description, str) or description is None:\n            self.register_buffer(\"description\", description)\n        else:\n            raise ValueError(\"`description` requires a string not empty\")\n\n    def get_module_name(self):\n        module_name = getattr(self, \"name\", None)\n        if module_name is None:\n            module_name = self._get_name()\n        return module_name\n\n    def get_module_description(self):\n        module_description = getattr(self, \"description\", None)\n        if module_description is None:\n            module_description = self.__class__.__doc__\n        return module_description\n\n    def get_module_annotations(self):\n        module_annotations = getattr(self, \"annotations\", None)\n        if module_annotations is None:\n            module_annotations = self.__class__.__annotations__\n        return module_annotations\n\n    # msgflux END\n\n    def register_buffer(self, name: str, data: Any) -&gt; None:\n        # TODO: muito trabalho pra ajeitar a docstring\n        # mudei de tensor para data\n        \"\"\"Add a buffer to the module.\n\n        This is typically used to register a buffer that should not to be\n        considered a model parameter. For example, BatchNorm's ``running_mean``\n        is not a parameter, but is part of the module's state. Buffers, by\n        default, are persistent and will be saved alongside parameters. This\n        behavior can be changed by setting :attr:`persistent` to ``False``. The\n        only difference between a persistent buffer and a non-persistent buffer\n        is that the latter will not be a part of this module's\n        :attr:`state_dict`.\n\n        Buffers can be accessed as attributes using given names.\n\n        Args:\n            name:\n                Name of the buffer. The buffer can be accessed\n                from this module using the given name\n            data:\n                buffer to be registered.\n        Example::\n            &gt;&gt;&gt; self.register_buffer(\"name\", \"agent\")\n        \"\"\"\n        if \"_buffers\" not in self.__dict__:\n            raise AttributeError(\"cannot assign buffer before Module.__init__() call\")\n        elif not isinstance(name, str):\n            raise TypeError(f\"buffer name should be a string. Got {type(name)}\")\n        elif \".\" in name:\n            raise KeyError(\"buffer name can't contain '.'\")\n        elif name == \"\":\n            raise KeyError(\"buffer name can't be empty string\")\n        else:\n            for hook in _global_buffer_registration_hooks.values():\n                output = hook(self, name, data)\n                if output is not None:\n                    data = output\n\n            self._buffers[name] = data\n            self._non_persistent_buffers_set.discard(name)\n\n    def register_parameter(self, name: str, param: Parameter) -&gt; None:\n        \"\"\"Add a parameter to the module.\n\n        The parameter can be accessed as an attribute using given name.\n\n        Args:\n            name (str): name of the parameter. The parameter can be accessed\n                from this module using the given name\n            param (Parameter or None): parameter to be added to the module. If\n                ``None``, then operations that run on parameters, such as :attr:`cuda`,\n                are ignored. If ``None``, the parameter is **not** included in the\n                module's :attr:`state_dict`.\n        \"\"\"\n        if \"_parameters\" not in self.__dict__:\n            raise AttributeError(\n                \"cannot assign parameter before Module.__init__() call\"\n            )\n\n        elif not isinstance(name, str):\n            raise TypeError(f\"parameter name should be a string. Got {type(name)}\")\n        elif \".\" in name:\n            raise KeyError(\"parameter name can't contain '.'\")\n        elif name == \"\":\n            raise KeyError(\"parameter name can't be empty string\")\n        elif name in self.__dict__ and name not in self._parameters:\n            raise KeyError(f\"attribute '{name}' already exists\")\n        elif param is None:\n            self._parameters[name] = None\n        elif not isinstance(param, Parameter):\n            raise TypeError(\n                f\"cannot assign `{type(param)}` object to parameter `{name}` \"\n                \"(msgflux.nn.Parameter required)\"\n            )\n        else:\n            for hook in _global_parameter_registration_hooks.values():\n                output = hook(self, name, param)\n                if output is not None:\n                    param = output\n            self._parameters[name] = param\n\n    def add_module(self, name: str, module: \"Module\") -&gt; None:\n        \"\"\"Add a child module to the current module.\n\n        The module can be accessed as an attribute using the given name.\n\n        Args:\n            name (str): name of the child module. The child module can be\n                accessed from this module using the given name\n            module (Module): child module to be added to the module.\n        \"\"\"\n        if not isinstance(module, Module) and module is not None:\n            raise TypeError(f\"{type(module)} is not a Module subclass\")\n        elif not isinstance(name, str):\n            raise TypeError(f\"module name should be a string. Got {type(name)}\")\n        elif name in self.__dict__ and name not in self._modules:\n            raise KeyError(f\"attribute `{name}` already exists\")\n        elif \".\" in name:\n            raise KeyError(f\"module name can't contain '.', got: {name}\")\n        elif name == \"\":\n            raise KeyError(\"module name can't be empty string \")\n        for hook in _global_module_registration_hooks.values():\n            output = hook(self, name, module)\n            if output is not None:\n                module = output\n        self._modules[name] = module\n\n    def register_module(self, name: str, module: \"Module\") -&gt; None:\n        \"\"\"Alias for :func:`add_module`.\"\"\"\n        self.add_module(name, module)\n\n    def get_submodule(self, target: str) -&gt; \"Module\":\n        \"\"\"Return the submodule given by ``target``\n        if it exists, otherwise throw an error.\n\n        For example, let's say you have an ``nn.Module`` ``A`` that\n        looks like this:\n\n        .. code-block:: text\n\n            A(\n                (net_b): Module(\n                    (net_c): Module(\n                        (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n                    )\n                    (linear): Linear(in_features=100, out_features=200, bias=True)\n                )\n            )\n\n        (The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\n        submodule ``net_b``, which itself has two submodules ``net_c``\n        and ``linear``. ``net_c`` then has a submodule ``conv``.)\n\n        To check whether or not we have the ``linear`` submodule, we\n        would call ``get_submodule(\"net_b.linear\")``. To check whether\n        we have the ``conv`` submodule, we would call\n        ``get_submodule(\"net_b.net_c.conv\")``.\n\n        The runtime of ``get_submodule`` is bounded by the degree\n        of module nesting in ``target``. A query against\n        ``named_modules`` achieves the same result, but it is O(N) in\n        the number of transitive modules. So, for a simple check to see\n        if some submodule exists, ``get_submodule`` should always be\n        used.\n\n        Args:\n            target: The fully-qualified string name of the submodule\n                to look for. (See above example for how to specify a\n                fully-qualified string.)\n\n        Returns:\n            msgflux.nn.Module: The submodule referenced by ``target``\n\n        Raises:\n            AttributeError: If the target string references an invalid\n                path or resolves to something that is not an\n                ``nn.Module``\n        \"\"\"\n        if target == \"\":\n            return self\n\n        atoms: List[str] = target.split(\".\")\n        mod: Module = self\n\n        for item in atoms:\n            if not hasattr(mod, item):\n                raise AttributeError(\n                    mod._get_name() + \" has no attribute `\" + item + \"`\"\n                )\n\n            mod = getattr(mod, item)\n\n            if not isinstance(mod, Module):\n                raise AttributeError(\"`\" + item + \"` is not an nn.Module\")\n\n        return mod\n\n    def set_submodule(self, target: str, module: \"Module\") -&gt; None:\n        \"\"\"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n        For example, let's say you have an ``nn.Module`` ``A`` that\n        looks like this:\n\n        .. code-block:: text\n\n            A(\n                (net_b): Module(\n                    (net_c): Module(\n                        (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n                    )\n                    (linear): Linear(in_features=100, out_features=200, bias=True)\n                )\n            )\n\n        (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n        submodule ``net_b``, which itself has two submodules ``net_c``\n        and ``linear``. ``net_c`` then has a submodule ``conv``.)\n\n        To overide the ``Conv2d`` with a new submodule ``Linear``, you\n        would call\n        ``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.\n\n        Args:\n            target: The fully-qualified string name of the submodule\n                to look for. (See above example for how to specify a\n                fully-qualified string.)\n            module: The module to set the submodule to.\n\n        Raises:\n            ValueError: If the target string is empty\n            AttributeError: If the target string references an invalid\n                path or resolves to something that is not an\n                ``nn.Module``\n        \"\"\"\n        if target == \"\":\n            raise ValueError(\"Cannot set the submodule without a target name!\")\n\n        atoms: List[str] = target.split(\".\")\n        name = atoms.pop(-1)\n        mod: Module = self\n\n        for item in atoms:\n            if not hasattr(mod, item):\n                raise AttributeError(\n                    mod._get_name() + \" has no attribute `\" + item + \"`\"\n                )\n\n            mod = getattr(mod, item)\n\n            # Use isinstance instead of type here to also handle subclass of nn.Module\n            if not isinstance(mod, Module):\n                raise AttributeError(\"`\" + item + \"` is not an nn.Module\")\n\n        setattr(mod, name, module)\n\n    def get_parameter(self, target: str) -&gt; \"Parameter\":\n        \"\"\"Return the parameter given by ``target``if\n        it exists, otherwise throw an error.\n\n        See the docstring for ``get_submodule`` for a more detailed\n        explanation of this method's functionality as well as how to\n        correctly specify ``target``.\n\n        Args:\n            target: The fully-qualified string name of the Parameter\n                to look for. (See ``get_submodule`` for how to specify a\n                fully-qualified string.)\n\n        Returns:\n            msgflux.nn.Parameter: The Parameter referenced by ``target``\n\n        Raises:\n            AttributeError: If the target string references an invalid\n                path or resolves to something that is not an\n                ``nn.Parameter``\n        \"\"\"\n        module_path, _, param_name = target.rpartition(\".\")\n\n        mod: Module = self.get_submodule(module_path)\n\n        if not hasattr(mod, param_name):\n            raise AttributeError(\n                mod._get_name() + \" has no attribute `\" + param_name + \"`\"\n            )\n\n        param: Parameter = getattr(mod, param_name)\n\n        if not isinstance(param, Parameter):\n            raise AttributeError(\"`\" + param_name + \"` is not an nn.Parameter\")\n\n        return param\n\n    def get_buffer(self, target: str) -&gt; Any:\n        \"\"\"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\n        See the docstring for ``get_submodule`` for a more detailed\n        explanation of this method's functionality as well as how to\n        correctly specify ``target``.\n\n        Args:\n            target: The fully-qualified string name of the buffer\n                to look for. (See ``get_submodule`` for how to specify a\n                fully-qualified string.)\n\n        Returns: TODO\n            torch.Tensor: The buffer referenced by ``target``\n\n        Raises:\n            AttributeError: If the target string references an invalid\n                path or resolves to something that is not a\n                buffer\n        \"\"\"\n        module_path, _, buffer_name = target.rpartition(\".\")\n\n        mod: Module = self.get_submodule(module_path)\n\n        if not hasattr(mod, buffer_name):\n            raise AttributeError(\n                mod._get_name() + \" has no attribute `\" + buffer_name + \"`\"\n            )\n\n        buffer: Any = getattr(mod, buffer_name)\n\n        if buffer_name not in mod._buffers:\n            raise AttributeError(\"`\" + buffer_name + \"` is not a buffer\")\n\n        return buffer\n\n    # revisar\n\n    def register_forward_pre_hook(\n        self,\n        hook: Callable[\n            [T, Tuple[Any, ...], Dict[str, Any]],\n            Optional[Tuple[Any, Dict[str, Any]]],\n        ],\n        *,\n        prepend: bool = False,\n    ) -&gt; RemovableHandle:\n        r\"\"\"Register a forward pre-hook on the module.\n\n        The hook will be called every time before :func:`forward` is invoked.\n\n\n        If ``with_kwargs`` is false or not specified, the input contains only\n        the positional arguments given to the module. Keyword arguments won't be\n        passed to the hooks and only to the ``forward``. The hook can modify the\n        input. User can either return a tuple or a single modified value in the\n        hook. We will wrap the value into a tuple if a single value is returned\n        (unless that value is already a tuple). The hook should have the\n        following signature::\n\n            hook(module, args) -&gt; None or modified input\n\n        If ``with_kwargs`` is true, the forward pre-hook will be passed the\n        kwargs given to the forward function. And if the hook modifies the\n        input, both the args and kwargs should be returned. The hook should have\n        the following signature::\n\n            hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs\n\n        Args:\n            hook (Callable): The user defined hook to be registered.\n            prepend (bool): If true, the provided ``hook`` will be fired before\n                all existing ``forward_pre`` hooks on this\n                :class:`msgflux.nn.modules.Module`. Otherwise, the provided\n                ``hook`` will be fired after all existing ``forward_pre`` hooks\n                on this :class:`msgflux.nn.modules.Module`. Note that global\n                ``forward_pre`` hooks registered with\n                :func:`register_module_forward_pre_hook` will fire before all\n                hooks registered by this method.\n                Default: ``False``\n\n        Returns:\n            :class:`msgflux.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n        \"\"\"\n        handle = RemovableHandle(self._forward_pre_hooks)\n        self._forward_pre_hooks[handle.id] = hook\n        if prepend:\n            self._forward_pre_hooks.move_to_end(handle.id, last=False)\n        return handle\n\n    def register_forward_hook(\n        self,\n        hook: Union[\n            Callable[[T, tuple[Any, ...], Any], Optional[Any]],\n            Callable[[T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]],\n        ],\n        *,\n        prepend: bool = False,\n        always_call: bool = False,\n    ) -&gt; RemovableHandle:\n        \"\"\"Register a forward hook on the module.\n\n        The hook will be called every time after :func:`forward`\n        has computed an output. The hook receives both positional\n        arguments (`args`), keyword arguments (`kwargs`), and the\n        output of the forward call. The hook can modify `args`,\n        `kwargs`, and the output. It should have the following signature::\n\n            hook(module, args, kwargs, output) -&gt; None, modified_output,\n            or (modified_args, modified_kwargs, modified_output)\n\n        Args:\n            hook (Callable): The user defined hook to be registered.\n            prepend (bool): If ``True``, the provided ``hook`` will be fired\n                before all existing ``forward`` hooks on this\n                :class:`msgflux.nn.modules.Module`. Otherwise, the provided\n                ``hook`` will be fired after all existing ``forward`` hooks on\n                this :class:`msgflux.nn.modules.Module`. Note that global\n                ``forward`` hooks registered with\n                :func:`register_module_forward_hook` will fire before all hooks\n                registered by this method.\n                Default: ``False``\n            always_call (bool): If ``True`` the ``hook`` will be run regardless of\n                whether an exception is raised while calling the Module.\n                Default: ``False``\n\n        Returns:\n            :class:`msgflux.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n        \"\"\"\n        handle = RemovableHandle(\n            self._forward_hooks,\n            extra_dict=[self._forward_hooks_always_called],\n        )\n        self._forward_hooks[handle.id] = hook\n        if always_call:\n            self._forward_hooks_always_called[handle.id] = True\n        if prepend:\n            self._forward_hooks.move_to_end(handle.id, last=False)\n        return handle\n\n    def _call_impl(self, *args, **kwargs):\n        if not (self._forward_hooks or self._forward_pre_hooks):\n            return self._call(*args, **kwargs)\n\n        for hook in self._forward_pre_hooks.values():\n            hook_result = hook(self, args, kwargs)\n            if hook_result is not None:\n                if isinstance(hook_result, tuple) and len(hook_result) == 2:\n                    args, kwargs = hook_result\n                else:\n                    raise RuntimeError(\n                        \"forward pre-hook must return None or a tuple of (args, kwargs)\"\n                    )\n\n        result = self._call(*args, **kwargs)\n\n        for hook in self._forward_hooks.values():\n            hook_result = hook(self, args, kwargs, result)\n            if hook_result is not None:\n                result = hook_result\n\n        return result\n\n    def _execute_with_span(\n        self, module_name_title: str, module_type: str, *args, **kwargs\n    ):\n        \"\"\"Execute forward with module span context.\n\n        This method can be overridden by subclasses to customize span creation\n        without rewriting the entire _call method.\n\n        Args:\n            module_name_title: Module name in title format\n            module_type: Module type (agent, tool, etc.)\n            *args: Arguments to pass to forward\n            **kwargs: Keyword arguments to pass to forward\n\n        Returns:\n            Module output from forward method\n        \"\"\"\n        with Spans.init_module(module_name_title, module_type) as span:\n            try:\n                MsgTraceAttributes.set_module_name(module_name_title)\n                MsgTraceAttributes.set_module_type(module_type)\n                result = self.forward(*args, **kwargs)\n                span.set_status(Status(StatusCode.OK))\n                return result\n            except Exception as e:\n                span.record_exception(e)\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                raise\n\n    def _call(self, *args, **kwargs):\n        module_name = self.get_module_name()\n        module_name_title = convert_camel_snake_to_title(module_name)\n        module_type = self._get_name().lower()  # Agent, Transcriber, etc.\n\n        encoded_state_dict = None\n        if envs.telemetry_capture_state_dict:\n            state_dict = self.state_dict()\n            encoded_state_dict = msgspec.json.encode(state_dict)\n\n        # Trace capture\n        current_span = trace.get_current_span()\n        # If there is no active span or it is not recording, this is the root module\n        if current_span is None or not current_span.is_recording():\n            with Spans.init_flow(module_name_title) as span:\n                try:\n                    MsgTraceAttributes.set_module_name(module_name_title)\n                    MsgTraceAttributes.set_module_type(module_type)\n                    if envs.telemetry_capture_state_dict and encoded_state_dict:\n                        MsgTraceAttributes.set_custom(\n                            \"module.state_dict\", encoded_state_dict\n                        )\n                    module_output = self.forward(*args, **kwargs)\n                    span.set_status(Status(StatusCode.OK))\n                    return module_output\n                except Exception as e:\n                    span.record_exception(e)\n                    span.set_status(Status(StatusCode.ERROR, str(e)))\n                    raise\n        else:\n            return self._execute_with_span(\n                module_name_title, module_type, *args, **kwargs\n            )\n\n    async def _acall_impl(self, *args, **kwargs):\n        if not (self._forward_hooks or self._forward_pre_hooks):\n            return await self._acall(*args, **kwargs)\n\n        # Execute forward pre-hooks (sync or async)\n        for hook in self._forward_pre_hooks.values():\n            if inspect.iscoroutinefunction(hook):\n                # Async hook - await directly\n                hook_result = await hook(self, args, kwargs)\n            else:\n                # Sync hook - run in executor to avoid blocking event loop\n                loop = asyncio.get_event_loop()\n                hook_result = await loop.run_in_executor(\n                    None, functools.partial(hook, self, args, kwargs)\n                )\n\n            if hook_result is not None:\n                if isinstance(hook_result, tuple) and len(hook_result) == 2:\n                    args, kwargs = hook_result\n                else:\n                    raise RuntimeError(\n                        \"forward pre-hook must return None or a tuple of (args, kwargs)\"\n                    )\n\n        result = await self._acall(*args, **kwargs)\n\n        # Execute forward hooks (sync or async)\n        for hook in self._forward_hooks.values():\n            if inspect.iscoroutinefunction(hook):\n                # Async hook - await directly\n                hook_result = await hook(self, args, kwargs, result)\n            else:\n                # Sync hook - run in executor to avoid blocking event loop\n                loop = asyncio.get_event_loop()\n                hook_result = await loop.run_in_executor(\n                    None, functools.partial(hook, self, args, kwargs, result)\n                )\n\n            if hook_result is not None:\n                result = hook_result\n\n        return result\n\n    async def _aexecute_with_span(\n        self, module_name_title: str, module_type: str, *args, **kwargs\n    ):\n        \"\"\"Execute aforward with module span context asynchronously.\n\n        This method can be overridden by subclasses to customize span creation\n        without rewriting the entire _acall method.\n\n        Args:\n            module_name_title: Module name in title format\n            module_type: Module type (agent, tool, etc.)\n            *args: Arguments to pass to aforward\n            **kwargs: Keyword arguments to pass to aforward\n\n        Returns:\n            Module output from aforward method\n        \"\"\"\n        async with Spans.ainit_module(module_name_title, module_type) as span:\n            try:\n                MsgTraceAttributes.set_module_name(module_name_title)\n                MsgTraceAttributes.set_module_type(module_type)\n                result = await self.aforward(*args, **kwargs)\n                span.set_status(Status(StatusCode.OK))\n                return result\n            except Exception as e:\n                span.record_exception(e)\n                span.set_status(Status(StatusCode.ERROR, str(e)))\n                raise\n\n    async def _acall(self, *args, **kwargs):\n        module_name = self.get_module_name()\n        module_name_title = convert_camel_snake_to_title(module_name)\n        module_type = self._get_name().lower()  # Agent, Transcriber, etc.\n\n        encoded_state_dict = None\n        if envs.telemetry_capture_state_dict:\n            state_dict = self.state_dict()\n            encoded_state_dict = msgspec.json.encode(state_dict)\n\n        # Trace capture\n        current_span = trace.get_current_span()\n        # If there is no active span or it is not recording, this is the root module\n        if current_span is None or not current_span.is_recording():\n            async with Spans.ainit_flow(module_name_title) as span:\n                try:\n                    MsgTraceAttributes.set_module_name(module_name_title)\n                    MsgTraceAttributes.set_module_type(module_type)\n                    if envs.telemetry_capture_state_dict and encoded_state_dict:\n                        MsgTraceAttributes.set_custom(\n                            \"module.state_dict\", encoded_state_dict\n                        )\n                    module_output = await self.aforward(*args, **kwargs)\n                    span.set_status(Status(StatusCode.OK))\n                    return module_output\n                except Exception as e:\n                    span.record_exception(e)\n                    span.set_status(Status(StatusCode.ERROR, str(e)))\n                    raise\n        else:\n            return await self._aexecute_with_span(\n                module_name_title, module_type, *args, **kwargs\n            )\n\n    __call__: Callable[..., Any] = _call_impl\n\n    async def acall(self, *args, **kwargs):\n        \"\"\"Async interface using aforward.\n\n        If aforward is not implemented, falls back to running __call__ in executor.\n        \"\"\"\n        # Check if aforward is implemented by comparing with the unimplemented version\n        if type(self).aforward is _aforward_unimplemented:\n            loop = asyncio.get_event_loop()\n            executor = Executor.get_instance()\n            return await loop.run_in_executor(\n                executor, lambda: self.__call__(*args, **kwargs)\n            )\n        else:\n            # Use native async implementation\n            return await self._acall_impl(*args, **kwargs)\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        return state\n\n    def __setstate__(self, state: Dict[str, Any]):\n        self.__dict__.update(state)\n\n        # Support loading old checkpoints that don't have the following attrs:\n        if \"_forward_pre_hooks\" not in self.__dict__:\n            self._forward_pre_hooks = OrderedDict()\n        if \"_forward_hooks_always_called\" not in self.__dict__:\n            self._forward_hooks_always_called = OrderedDict()\n        if \"_state_dict_hooks\" not in self.__dict__:\n            self._state_dict_hooks = OrderedDict()\n        if \"_state_dict_pre_hooks\" not in self.__dict__:\n            self._state_dict_pre_hooks = OrderedDict()\n        if \"_load_state_dict_pre_hooks\" not in self.__dict__:\n            self._load_state_dict_pre_hooks = OrderedDict()\n        if \"_load_state_dict_post_hooks\" not in self.__dict__:\n            self._load_state_dict_post_hooks = OrderedDict()\n        if \"_non_persistent_buffers_set\" not in self.__dict__:\n            self._non_persistent_buffers_set = set()\n\n    def __getattribute__(self, name: str) -&gt; Union[Any, \"Module\"]:\n        # Don't intercept special attributes or private attributes\n        if name.startswith(\"_\"):\n            return super().__getattribute__(name)\n\n        # Check if this is a registered parameter, buffer, or module\n        # These should take priority over class attributes\n        try:\n            _dict = super().__getattribute__(\"__dict__\")\n\n            if \"_parameters\" in _dict:\n                _parameters = _dict[\"_parameters\"]\n                if name in _parameters:\n                    return _parameters[name]\n\n            if \"_buffers\" in _dict:\n                _buffers = _dict[\"_buffers\"]\n                if name in _buffers:\n                    return _buffers[name]\n\n            if \"_modules\" in _dict:\n                _modules = _dict[\"_modules\"]\n                if name in _modules:\n                    return _modules[name]\n        except AttributeError:\n            pass\n\n        # Fall back to normal attribute access\n        return super().__getattribute__(name)\n\n    def __getattr__(self, name: str) -&gt; Union[Any, \"Module\"]:\n        # This is now only called when attribute truly doesn't exist\n        # (after __getattribute__ doesn't find it)\n        raise AttributeError(\n            f\"`{type(self).__name__}` object has no attribute `{name}`\"\n        )\n\n    def __setattr__(self, name: str, value: Union[Any, \"Module\"]) -&gt; None:  # noqa: C901\n        def remove_from(*dicts_or_sets):\n            for d in dicts_or_sets:\n                if name in d:\n                    if isinstance(d, dict):\n                        del d[name]\n                    else:\n                        d.discard(name)\n\n        params = self.__dict__.get(\"_parameters\")\n        if isinstance(value, Parameter):\n            if params is None:\n                raise AttributeError(\n                    \"cannot assign parameters before Module.__init__() call\"\n                )\n            remove_from(\n                self.__dict__,\n                self._buffers,\n                self._modules,\n                self._non_persistent_buffers_set,\n            )\n            self.register_parameter(name, value)\n        elif params is not None and name in params:\n            if value is not None:\n                raise TypeError(\n                    f\"cannot assign '{type(value)}' as parameter '{name}' \"\n                    \"(msgflux.nn.Parameter or None expected)\"\n                )\n            self.register_parameter(name, value)\n        else:\n            modules = self.__dict__.get(\"_modules\")\n            if isinstance(value, Module):\n                if modules is None:\n                    raise AttributeError(\n                        \"cannot assign module before Module.__init__() call\"\n                    )\n                remove_from(\n                    self.__dict__,\n                    self._parameters,\n                    self._buffers,\n                    self._non_persistent_buffers_set,\n                )\n                for hook in _global_module_registration_hooks.values():\n                    output = hook(self, name, value)\n                    if output is not None:\n                        value = output\n                modules[name] = value\n            elif modules is not None and name in modules:\n                if value is not None:\n                    raise TypeError(\n                        f\"cannot assign `{type(value)}` as child module `{name}` \"\n                        \"(msgflux.nn.Module or None expected)\"\n                    )\n                for hook in _global_module_registration_hooks.values():\n                    output = hook(self, name, value)\n                    if output is not None:\n                        value = output\n                modules[name] = value\n            else:\n                super().__setattr__(name, value)\n\n    def __delattr__(self, name: str):\n        if name in self._parameters:\n            del self._parameters[name]\n        elif name in self._buffers:\n            del self._buffers[name]\n            self._non_persistent_buffers_set.discard(name)\n        elif name in self._modules:\n            del self._modules[name]\n        else:\n            super().__delattr__(name)\n\n    def _register_state_dict_hook(self, hook: Callable):\n        \"\"\"Register a post-hook for the :meth:`~msgflux.nn.Module.state_dict` method.\n\n        It should have the following signature::\n            hook(module, state_dict, prefix, local_metadata) -&gt; None or state_dict\n\n        The registered hooks can modify the ``state_dict`` inplace or return a new one.\n        If a new ``state_dict`` is returned, it will only be respected if it is the root\n        module that :meth:`~nn.Module.state_dict` is called from.\n        \"\"\"\n        if getattr(hook, \"_from_public_api\", False):\n            raise RuntimeError(\n                \"Cannot register the same function as the state dict post\"\n                \"hook that was previously registered via \"\n                \"register_state_dict_post_hook\"\n            )\n        handle = RemovableHandle(self._state_dict_hooks)\n        self._state_dict_hooks[handle.id] = hook\n        return handle\n\n    def register_state_dict_post_hook(self, hook: Callable):\n        \"\"\"Register a post-hook for the :meth:`~msgflux.nn.Module.state_dict` method.\n\n        It should have the following signature::\n            hook(module, state_dict, prefix, local_metadata) -&gt; None\n\n        The registered hooks can modify the ``state_dict`` inplace.\n        \"\"\"\n        hook._from_public_api = True\n        handle = RemovableHandle(self._state_dict_hooks)\n        self._state_dict_hooks[handle.id] = hook\n        return handle\n\n    def register_state_dict_pre_hook(self, hook: Callable):\n        \"\"\"Register a pre-hook for the :meth:`~msgflux.nn.Module.state_dict`\n        method.\n\n        It should have the following signature::\n            hook(module, prefix, keep_vars) -&gt; None\n\n        The registered hooks can be used to perform pre-processing\n        before the ``state_dict`` call is made.\n        \"\"\"\n        handle = RemovableHandle(self._state_dict_pre_hooks)\n        self._state_dict_pre_hooks[handle.id] = hook\n        return handle\n\n    def _get_serializable_value(self, obj: object):\n        \"\"\"Get serializable value from an object.\"\"\"\n        if is_builtin_type(obj):\n            return obj\n        elif is_subclass_of(obj, msgspec.Struct):\n            return msgspec.json.schema(obj)\n        elif hasattr(obj, \"serialize\"):\n            return obj.serialize()\n        else:\n            return None  # Fallback\n\n    def _save_to_state_dict(self, destination: str, prefix: str):\n        \"\"\"Save parameters and buffers to state dict.\"\"\"\n        # Save parameters (only the data string)\n        for name, param in self._parameters.items():\n            if param is not None:\n                destination[prefix + name] = param.data\n\n        # Save buffers (handle different data types)\n        for name, buf in self._buffers.items():\n            destination[prefix + name] = self._get_serializable_value(buf)\n\n    def state_dict(\n        self, destination: Optional[Dict[str, Any]] = None, prefix: Optional[str] = \"\"\n    ):\n        \"\"\"Returns a dictionary containing module's state.\n\n        Args:\n            destination:\n                If provided, the state will be updated into\n                the given dict. Default: None\n            prefix:\n                Prefix added to parameter and buffer names.\n                Default: \"\"\n        \"\"\"\n        if destination is None:\n            destination = {}\n\n        # Save current module's state\n        self._save_to_state_dict(destination, prefix)\n\n        # Save states from child modules\n        for name, module in self._modules.items():\n            if module is not None:\n                module.state_dict(destination=destination, prefix=prefix + name + \".\")\n\n        return destination\n\n    def _register_load_state_dict_pre_hook(\n        self, hook: Callable, *, with_module: Optional[bool] = False\n    ):\n        \"\"\"See :meth:`~msgflux.nn.Module.register_load_state_dict_pre_hook` for details.\n\n        A subtle difference is that if ``with_module`` is set to ``False``, then the\n        hook will not take the ``module`` as the first argument whereas\n        :meth:`~msgflux.nn.Module.register_load_state_dict_pre_hook` always takes the\n        ``module`` as the first argument.\n\n        Args:\n            hook (Callable): Callable hook that will be invoked before\n                loading the state dict.\n            with_module (bool, optional): Whether or not to pass the module\n                instance to the hook as the first parameter.\n        \"\"\"\n        handle = RemovableHandle(self._load_state_dict_pre_hooks)\n        self._load_state_dict_pre_hooks[handle.id] = _WrappedHook(\n            hook, self if with_module else None\n        )\n        return handle\n\n    def register_load_state_dict_pre_hook(self, hook: Callable):\n        \"\"\"Register a pre-hook to be run before module's\n        :meth:`~nn.Module.load_state_dict` is called.\n\n        It should have the following signature::\n            hook(module, state_dict, prefix, local_metadata, strict,\n            missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950\n\n        Args:\n            hook:\n                Callable hook that will be invoked before\n                loading the state dict.\n        \"\"\"\n        return self._register_load_state_dict_pre_hook(hook, with_module=True)\n\n    def register_load_state_dict_post_hook(self, hook: Callable):\n        \"\"\"Register a post-hook to be run after module's\n        :meth:`~nn.Module.load_state_dict` is called.\n\n        It should have the following signature::\n            hook(module, incompatible_keys) -&gt; None\n\n        The ``module`` argument is the current module that this hook is registered\n        on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n        of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n        is a ``list`` of ``str`` containing the missing keys and\n        ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\n        The given incompatible_keys can be modified inplace if needed.\n\n        Note that the checks performed when calling :func:`load_state_dict` with\n        ``strict=True`` are affected by modifications the hook makes to\n        ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n        set of keys will result in an error being thrown when ``strict=True``, and\n        clearing out both missing and unexpected keys will avoid an error.\n\n        Returns:\n            :class:`msgflux.utils.hooks.RemovableHandle`:\n                a handle that can be used to remove the added hook by calling\n                ``handle.remove()``\n        \"\"\"\n        handle = RemovableHandle(self._load_state_dict_post_hooks)\n        self._load_state_dict_post_hooks[handle.id] = hook\n        return handle\n\n    def _load_from_state_dict(  # noqa: C901\n        self, state_dict: Dict[str, Any], prefix: Optional[str] = \"\"\n    ) -&gt; None:\n        \"\"\"Loads the module state from a state dict.\n\n        Args:\n            state_dict: Dictionary containing the state\n            prefix: Prefix used for parameter/buffer names\n        \"\"\"\n        # Load parameters\n        for name, param in self._parameters.items():\n            if param is not None:\n                key = prefix + name\n                if key in state_dict:\n                    self._parameters[name].copy_to_data(state_dict[key])\n\n        # Load buffers\n        for name, _ in self._buffers.items():\n            key = prefix + name\n            if key in state_dict:\n                data = state_dict[key]\n                # Check if it is a msgflux serializable class\n                if isinstance(data, dict) and \"msgflux_type\" in data:\n                    msgflux_type = data.pop(\"msgflux_type\")\n                    if msgflux_type in MSGFLUX_DESERIALIZABLE_CLS:\n                        # TODO not recreate if same type\n                        cls = MSGFLUX_DESERIALIZABLE_CLS[msgflux_type]\n                        instance = cls.from_serialized(**data)\n                        self._buffers[name] = instance\n                    elif msgflux_type == \"generation_schema\":\n                        state = data.pop(\"state\")\n                        generation_schema = StructFactory.from_schema(state)\n                        self._buffers[name] = generation_schema\n                else:  # Otherwise, load the value directly\n                    self._buffers[name] = data\n\n        # Load submodules recursively\n        for name, module in self._modules.items():\n            if module is not None:\n                module_prefix = prefix + name + \".\"\n                module_dict = {\n                    k.replace(module_prefix, \"\"): v\n                    for k, v in state_dict.items()\n                    if k.startswith(module_prefix)\n                }\n                if module_dict:\n                    module._load_from_state_dict(module_dict)\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n        \"\"\"Loads the state of the module and its submodules.\n\n        Args:\n            state_dict: Dictionary containing the complete state\n        \"\"\"\n        if not isinstance(state_dict, dict):\n            raise TypeError(\n                f\"`state_dict` to be dict, given {type(state_dict).__name__}\"\n            )\n\n        self._load_from_state_dict(state_dict)\n\n    def _named_members(  # ok?\n        self,\n        get_members_fn,\n        prefix: Optional[str] = \"\",\n        *,\n        recurse: Optional[bool] = True,\n        remove_duplicate: Optional[bool] = True,\n    ):\n        \"\"\"Help yield various names + members of modules.\"\"\"\n        memo = set()\n        modules = (\n            self.named_modules(prefix=prefix, remove_duplicate=remove_duplicate)\n            if recurse\n            else [(prefix, self)]\n        )\n        for module_prefix, module in modules:\n            members = get_members_fn(module)\n            for k, v in members:\n                if v is None or v in memo:\n                    continue\n                if remove_duplicate:\n                    memo.add(v)\n                name = module_prefix + (\".\" if module_prefix else \"\") + k\n                yield name, v\n\n    def parameters(self, *, recurse: Optional[bool] = True) -&gt; Iterator[Parameter]:\n        \"\"\"Return an iterator over module parameters.\n\n        This is typically passed to an optimizer.\n\n        Args:\n            recurse: If True, then yields parameters of this module\n                and all submodules. Otherwise, yields only parameters that\n                are direct members of this module.\n\n        Returns:\n            Parameter: module parameter\n\n        !!! example\n            # TODO\n            ```python\n            for param in model.parameters():\n                print(type(param), param.size())\n            &gt;&gt;&gt; &lt;class 'torch.Tensor'&gt; (20L,)\n            &gt;&gt;&gt; &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)\n            ```\n        \"\"\"\n        for _name, param in self.named_parameters(recurse=recurse):\n            yield param\n\n    def named_parameters(\n        self, prefix: str = \"\", *, recurse: bool = True, remove_duplicate: bool = True\n    ) -&gt; Iterator[Tuple[str, Parameter]]:\n        # TODO: docstring\n        \"\"\"Return an iterator over module parameters, yielding both the name of the\n            parameter as well as the parameter itself.\n\n        Args:\n            prefix (str): prefix to prepend to all parameter names.\n            recurse (bool): if True, then yields parameters of this module\n                and all submodules. Otherwise, yields only parameters that\n                are direct members of this module.\n            remove_duplicate (bool, optional): whether to remove the duplicated\n                parameters in the result. Defaults to True.\n\n        Yields:\n            (str, Parameter): Tuple containing the name and parameter\n\n        Example::\n\n            &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n            &gt;&gt;&gt; for name, param in self.named_parameters():\n            &gt;&gt;&gt;     if name in ['bias']:\n            &gt;&gt;&gt;         print(param.size())\n\n        \"\"\"\n        gen = self._named_members(\n            lambda module: module._parameters.items(),\n            prefix=prefix,\n            recurse=recurse,\n            remove_duplicate=remove_duplicate,\n        )\n        yield from gen\n\n    def buffers(self, *, recurse: Optional[bool] = True) -&gt; Iterator[Any]:  # TODO doc\n        \"\"\"Return an iterator over module buffers.\n\n        Args:\n            recurse (bool): if True, then yields buffers of this module\n                and all submodules. Otherwise, yields only buffers that\n                are direct members of this module.\n\n        Returns:\n            torch.Tensor: module buffer\n\n        Example::\n\n            &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n            &gt;&gt;&gt; for buf in model.buffers():\n            &gt;&gt;&gt;     print(type(buf), buf.size())\n            &lt;class 'torch.Tensor'&gt; (20L,)\n            &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)\n\n        \"\"\"\n        for _, buf in self.named_buffers(recurse=recurse):\n            yield buf\n\n    def named_buffers(\n        self,\n        prefix: Optional[str] = \"\",\n        *,\n        recurse: Optional[bool] = True,\n        remove_duplicate: Optional[bool] = True,\n    ) -&gt; Iterator[Tuple[str, Any]]:  # TODO docstring\n        \"\"\"Return an iterator over module buffers, yielding both the name of the\n            buffer as well as the buffer itself.\n\n        Args:\n            prefix:\n                Prefix to prepend to all buffer names.\n            recurse:\n                If True, then yields buffers of this module\n                and all submodules. Otherwise, yields only buffers that\n                are direct members of this module. Defaults to True.\n            remove_duplicat:\n                Whether to remove the duplicated buffers in the result.\n                Defaults to True.\n\n        Yields:\n            Tuple containing the name and buffer\n\n        Example::\n\n            &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n            &gt;&gt;&gt; for name, buf in self.named_buffers():\n            &gt;&gt;&gt;     if name in ['running_var']:\n            &gt;&gt;&gt;         print(buf.size())\n\n        \"\"\"\n        gen = self._named_members(\n            lambda module: module._buffers.items(),\n            prefix=prefix,\n            recurse=recurse,\n            remove_duplicate=remove_duplicate,\n        )\n        yield from gen\n\n    def children(self) -&gt; Iterator[\"Module\"]:\n        \"\"\"Return an iterator over immediate children modules.\n\n        Yields:\n            Module: a child module\n        \"\"\"\n        for _, module in self.named_children():\n            yield module\n\n    def named_children(self) -&gt; Iterator[Tuple[str, \"Module\"]]:\n        \"\"\"Return an iterator over immediate children modules, yielding\n            both the name of the module as well as the module itself.\n\n        Yields:\n            (str, Module): Tuple containing a name and child module\n\n        Example::\n\n            &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n            &gt;&gt;&gt; for name, module in model.named_children():\n            &gt;&gt;&gt;     if name in ['conv4', 'conv5']:\n            &gt;&gt;&gt;         print(module)\n\n        \"\"\"\n        memo = set()\n        for name, module in self._modules.items():\n            if module is not None and module not in memo:\n                memo.add(module)\n                yield name, module\n\n    def modules(self) -&gt; Iterator[\"Module\"]:  # TODO DOC\n        \"\"\"Return an iterator over all modules in the network.\n\n        Yields:\n            Module: a module in the network\n\n        Note:\n            Duplicate modules are returned only once. In the following\n            example, ``l`` will be returned only once.\n\n        Example::\n\n            &gt;&gt;&gt; l = nn.Linear(2, 2)\n            &gt;&gt;&gt; net = nn.Sequential(l, l)\n            &gt;&gt;&gt; for idx, m in enumerate(net.modules()):\n            ...     print(idx, '-&gt;', m)\n\n            0 -&gt; Sequential(\n              (0): Linear(in_features=2, out_features=2, bias=True)\n              (1): Linear(in_features=2, out_features=2, bias=True)\n            )\n            1 -&gt; Linear(in_features=2, out_features=2, bias=True)\n\n        \"\"\"\n        for _, module in self.named_modules():\n            yield module\n\n    def named_modules(\n        self,\n        memo: Optional[Set[\"Module\"]] = None,\n        prefix: Optional[str] = \"\",\n        *,\n        remove_duplicate: Optional[bool] = True,\n    ) -&gt; Iterator[Tuple[str, \"Module\"]]:\n        \"\"\"Return an iterator over all modules in the network, yielding\n        both the name of the module as well as the module itself.\n\n        Args:\n            memo:\n                A memo to store the set of modules already added to the result.\n            prefix:\n                A prefix that will be added to the name of the module.\n            remove_duplicate:\n                Whether to remove the duplicated module instances in the result\n                or not.\n\n        Yields:\n            Tuple of name and module\n\n        Note:\n            Duplicate modules are returned only once. In the following\n            example, ``l`` will be returned only once.\n        \"\"\"\n        if memo is None:\n            memo = set()\n        if self not in memo:\n            if remove_duplicate:\n                memo.add(self)\n            yield prefix, self\n            for name, module in self._modules.items():\n                if module is None:\n                    continue\n                submodule_prefix = prefix + (\".\" if prefix else \"\") + name\n                yield from module.named_modules(\n                    memo, submodule_prefix, remove_duplicate=remove_duplicate\n                )\n\n    def train(self: T, *, mode: Optional[bool] = True) -&gt; T:\n        \"\"\"Set the module in training mode.\n\n        This has an effect only on certain modules. See the documentation of\n        particular modules for details of their behaviors in training/evaluation\n        mode.\n\n        Args:\n            mode:\n                Whether to set training mode (``True``) or evaluation\n                mode (``False``). Default: ``True``.\n\n        Returns:\n            Self.\n        \"\"\"\n        if not isinstance(mode, bool):\n            raise ValueError(\"training mode is expected to be boolean\")\n        self.training = mode\n        for module in self.children():\n            module.train(mode=mode)\n        return self\n\n    def eval(self: T) -&gt; T:\n        \"\"\"Set the module in evaluation mode.\n\n        This has an effect only on certain modules. See the documentation of\n        particular modules for details of their behaviors in training/evaluation\n        mode, i.e. whether they are affected.\n\n        This is equivalent with :meth:`self.train(False) &lt;msgflux.nn.Module.train&gt;`.\n\n        See :ref:`locally-disable-grad-doc` for a comparison between\n        `.eval()` and several similar mechanisms that may be confused with it.\n\n        Returns:\n            Self.\n        \"\"\"\n        return self.train(mode=False)\n\n    def requires_grad_(self: T, *, requires_pgrad: Optional[bool] = True) -&gt; T:\n        \"\"\"Change if autograd should record operations on parameters in this module.\n\n        This method sets the parameters' :attr:`requires_grad` attributes\n        in-place.\n\n        This method is helpful for freezing part of the module for finetuning\n        or training parts of a model individually (e.g., GAN training).\n\n        See :ref:`locally-disable-grad-doc` for a comparison between\n        `.requires_grad_()` and several similar mechanisms that may be confused with it.\n\n        Args:\n            requires_grad:\n                Whether autograd should record operations on\n                parameters in this module.\n\n        Returns:\n            Module: self\n        \"\"\"\n        for p in self.parameters():\n            p.requires_grad_(requires_grad=requires_pgrad)\n        return self\n\n    def zero_pgrad(\n        self, *, set_to_none: Optional[bool] = True\n    ) -&gt; None:  # TODO isso \u00e9 interessante mas vai mudar\n        \"\"\"Reset gradients of all model parameters.\n\n        See similar function under :class:`msgflux.optim.Optimizer` for more context.\n\n        Args:\n            set_to_none (bool): instead of setting to zero, set the grads to None.\n                See :meth:`msgflux.optim.Optimizer.zero_grad` for details.\n        \"\"\"\n        for p in self.parameters():\n            if p.pgrad is not None:\n                if set_to_none:\n                    p.pgrad = None\n                else:  # TODO revisar abaixo\n                    if p.pgrad.grad_fn is not None:\n                        p.pgrad.detach_()\n                    else:\n                        p.pgrad.requires_grad_(False)\n                    p.pgrad.zero_()\n\n    def _get_name(self):\n        return self.__class__.__name__\n\n    def extra_repr(self) -&gt; str:\n        \"\"\"Return the extra representation of the module.\n\n        To print customized extra information, you should re-implement\n        this method in your own modules. Both single-line and multi-line\n        strings are acceptable.\n        \"\"\"\n        return \"\"\n\n    def __repr__(self):\n        # We treat the extra repr like the sub-module, one item per line\n        extra_lines = []\n        extra_repr = self.extra_repr()\n        # empty string will be split into list ['']\n        if extra_repr:\n            extra_lines = extra_repr.split(\"\\n\")\n        child_lines = []\n        for key, module in self._modules.items():\n            mod_str = repr(module)\n            mod_str = _addindent(mod_str, 2)\n            child_lines.append(\"(\" + key + \"): \" + mod_str)\n        lines = extra_lines + child_lines\n\n        main_str = self._get_name() + \"(\"\n        if lines:\n            # simple one-liner info, which most builtin Modules will use\n            if len(extra_lines) == 1 and not child_lines:\n                main_str += extra_lines[0]\n            else:\n                main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n\n        main_str += \")\"\n        return main_str\n\n    def __dir__(self):\n        module_attrs = dir(self.__class__)\n        attrs = list(self.__dict__.keys())\n        parameters = list(self._parameters.keys())\n        modules = list(self._modules.keys())\n        buffers = list(self._buffers.keys())\n        keys = module_attrs + attrs + parameters + modules + buffers\n\n        # Eliminate attrs that are not legal Python variable names\n        keys = [key for key in keys if not key[0].isdigit()]\n\n        return sorted(keys)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.__call__","title":"__call__  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>__call__ = _call_impl\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.aforward","title":"aforward  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>aforward = _aforward_unimplemented\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.call_super_init","title":"call_super_init  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>call_super_init = False\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.forward","title":"forward  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>forward = _forward_unimplemented\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.training","title":"training  <code>instance-attribute</code>","text":"<pre><code>training\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.__delattr__","title":"__delattr__","text":"<pre><code>__delattr__(name)\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def __delattr__(self, name: str):\n    if name in self._parameters:\n        del self._parameters[name]\n    elif name in self._buffers:\n        del self._buffers[name]\n        self._non_persistent_buffers_set.discard(name)\n    elif name in self._modules:\n        del self._modules[name]\n    else:\n        super().__delattr__(name)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.__dir__","title":"__dir__","text":"<pre><code>__dir__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def __dir__(self):\n    module_attrs = dir(self.__class__)\n    attrs = list(self.__dict__.keys())\n    parameters = list(self._parameters.keys())\n    modules = list(self._modules.keys())\n    buffers = list(self._buffers.keys())\n    keys = module_attrs + attrs + parameters + modules + buffers\n\n    # Eliminate attrs that are not legal Python variable names\n    keys = [key for key in keys if not key[0].isdigit()]\n\n    return sorted(keys)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.__getattr__","title":"__getattr__","text":"<pre><code>__getattr__(name)\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Union[Any, \"Module\"]:\n    # This is now only called when attribute truly doesn't exist\n    # (after __getattribute__ doesn't find it)\n    raise AttributeError(\n        f\"`{type(self).__name__}` object has no attribute `{name}`\"\n    )\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.__getattribute__","title":"__getattribute__","text":"<pre><code>__getattribute__(name)\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def __getattribute__(self, name: str) -&gt; Union[Any, \"Module\"]:\n    # Don't intercept special attributes or private attributes\n    if name.startswith(\"_\"):\n        return super().__getattribute__(name)\n\n    # Check if this is a registered parameter, buffer, or module\n    # These should take priority over class attributes\n    try:\n        _dict = super().__getattribute__(\"__dict__\")\n\n        if \"_parameters\" in _dict:\n            _parameters = _dict[\"_parameters\"]\n            if name in _parameters:\n                return _parameters[name]\n\n        if \"_buffers\" in _dict:\n            _buffers = _dict[\"_buffers\"]\n            if name in _buffers:\n                return _buffers[name]\n\n        if \"_modules\" in _dict:\n            _modules = _dict[\"_modules\"]\n            if name in _modules:\n                return _modules[name]\n    except AttributeError:\n        pass\n\n    # Fall back to normal attribute access\n    return super().__getattribute__(name)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.__getstate__","title":"__getstate__","text":"<pre><code>__getstate__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def __getstate__(self):\n    state = self.__dict__.copy()\n    return state\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.__init__","title":"__init__","text":"<pre><code>__init__(*args, **kwargs)\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n    if self.call_super_init is False and bool(kwargs):\n        raise TypeError(\n            f\"{type(self).__name__}.__init__() got an unexpected \"\n            f\"keyword argument `{next(iter(kwargs))}`\"\n        )\n\n    if self.call_super_init is False and bool(args):\n        raise TypeError(\n            f\"{type(self).__name__}.__init__() takes 1 positional \"\n            f\"argument but {len(args) + 1} were\"\n        )\n\n    \"\"\"\n    Calls super().__setattr__('a', a) instead of the typical self.a = a\n    to avoid Module.__setattr__ overhead. Module's __setattr__ has special\n    handling for parameters, submodules, and buffers but simply calls into\n    super().__setattr__ for all other attributes.\n    \"\"\"\n    super().__setattr__(\"training\", True)\n    super().__setattr__(\"_parameters\", {})\n    super().__setattr__(\"_buffers\", {})\n    super().__setattr__(\"_non_persistent_buffers_set\", set())  # ?\n    super().__setattr__(\"_forward_pre_hooks\", OrderedDict())\n    super().__setattr__(\"_forward_hooks\", OrderedDict())\n    super().__setattr__(\"_forward_hooks_always_called\", OrderedDict())\n    super().__setattr__(\"_state_dict_hooks\", OrderedDict())\n    super().__setattr__(\"_state_dict_pre_hooks\", OrderedDict())\n    super().__setattr__(\"_load_state_dict_pre_hooks\", OrderedDict())\n    super().__setattr__(\"_load_state_dict_post_hooks\", OrderedDict())\n    super().__setattr__(\"_modules\", {})\n\n    if self.call_super_init:\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def __repr__(self):\n    # We treat the extra repr like the sub-module, one item per line\n    extra_lines = []\n    extra_repr = self.extra_repr()\n    # empty string will be split into list ['']\n    if extra_repr:\n        extra_lines = extra_repr.split(\"\\n\")\n    child_lines = []\n    for key, module in self._modules.items():\n        mod_str = repr(module)\n        mod_str = _addindent(mod_str, 2)\n        child_lines.append(\"(\" + key + \"): \" + mod_str)\n    lines = extra_lines + child_lines\n\n    main_str = self._get_name() + \"(\"\n    if lines:\n        # simple one-liner info, which most builtin Modules will use\n        if len(extra_lines) == 1 and not child_lines:\n            main_str += extra_lines[0]\n        else:\n            main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n\n    main_str += \")\"\n    return main_str\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.__setattr__","title":"__setattr__","text":"<pre><code>__setattr__(name, value)\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def __setattr__(self, name: str, value: Union[Any, \"Module\"]) -&gt; None:  # noqa: C901\n    def remove_from(*dicts_or_sets):\n        for d in dicts_or_sets:\n            if name in d:\n                if isinstance(d, dict):\n                    del d[name]\n                else:\n                    d.discard(name)\n\n    params = self.__dict__.get(\"_parameters\")\n    if isinstance(value, Parameter):\n        if params is None:\n            raise AttributeError(\n                \"cannot assign parameters before Module.__init__() call\"\n            )\n        remove_from(\n            self.__dict__,\n            self._buffers,\n            self._modules,\n            self._non_persistent_buffers_set,\n        )\n        self.register_parameter(name, value)\n    elif params is not None and name in params:\n        if value is not None:\n            raise TypeError(\n                f\"cannot assign '{type(value)}' as parameter '{name}' \"\n                \"(msgflux.nn.Parameter or None expected)\"\n            )\n        self.register_parameter(name, value)\n    else:\n        modules = self.__dict__.get(\"_modules\")\n        if isinstance(value, Module):\n            if modules is None:\n                raise AttributeError(\n                    \"cannot assign module before Module.__init__() call\"\n                )\n            remove_from(\n                self.__dict__,\n                self._parameters,\n                self._buffers,\n                self._non_persistent_buffers_set,\n            )\n            for hook in _global_module_registration_hooks.values():\n                output = hook(self, name, value)\n                if output is not None:\n                    value = output\n            modules[name] = value\n        elif modules is not None and name in modules:\n            if value is not None:\n                raise TypeError(\n                    f\"cannot assign `{type(value)}` as child module `{name}` \"\n                    \"(msgflux.nn.Module or None expected)\"\n                )\n            for hook in _global_module_registration_hooks.values():\n                output = hook(self, name, value)\n                if output is not None:\n                    value = output\n            modules[name] = value\n        else:\n            super().__setattr__(name, value)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.__setstate__","title":"__setstate__","text":"<pre><code>__setstate__(state)\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def __setstate__(self, state: Dict[str, Any]):\n    self.__dict__.update(state)\n\n    # Support loading old checkpoints that don't have the following attrs:\n    if \"_forward_pre_hooks\" not in self.__dict__:\n        self._forward_pre_hooks = OrderedDict()\n    if \"_forward_hooks_always_called\" not in self.__dict__:\n        self._forward_hooks_always_called = OrderedDict()\n    if \"_state_dict_hooks\" not in self.__dict__:\n        self._state_dict_hooks = OrderedDict()\n    if \"_state_dict_pre_hooks\" not in self.__dict__:\n        self._state_dict_pre_hooks = OrderedDict()\n    if \"_load_state_dict_pre_hooks\" not in self.__dict__:\n        self._load_state_dict_pre_hooks = OrderedDict()\n    if \"_load_state_dict_post_hooks\" not in self.__dict__:\n        self._load_state_dict_post_hooks = OrderedDict()\n    if \"_non_persistent_buffers_set\" not in self.__dict__:\n        self._non_persistent_buffers_set = set()\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.acall","title":"acall  <code>async</code>","text":"<pre><code>acall(*args, **kwargs)\n</code></pre> <p>Async interface using aforward.</p> <p>If aforward is not implemented, falls back to running call in executor.</p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>async def acall(self, *args, **kwargs):\n    \"\"\"Async interface using aforward.\n\n    If aforward is not implemented, falls back to running __call__ in executor.\n    \"\"\"\n    # Check if aforward is implemented by comparing with the unimplemented version\n    if type(self).aforward is _aforward_unimplemented:\n        loop = asyncio.get_event_loop()\n        executor = Executor.get_instance()\n        return await loop.run_in_executor(\n            executor, lambda: self.__call__(*args, **kwargs)\n        )\n    else:\n        # Use native async implementation\n        return await self._acall_impl(*args, **kwargs)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.add_module","title":"add_module","text":"<pre><code>add_module(name, module)\n</code></pre> <p>Add a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the child module. The child module can be accessed from this module using the given name</p> required <code>module</code> <code>Module</code> <p>child module to be added to the module.</p> required Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def add_module(self, name: str, module: \"Module\") -&gt; None:\n    \"\"\"Add a child module to the current module.\n\n    The module can be accessed as an attribute using the given name.\n\n    Args:\n        name (str): name of the child module. The child module can be\n            accessed from this module using the given name\n        module (Module): child module to be added to the module.\n    \"\"\"\n    if not isinstance(module, Module) and module is not None:\n        raise TypeError(f\"{type(module)} is not a Module subclass\")\n    elif not isinstance(name, str):\n        raise TypeError(f\"module name should be a string. Got {type(name)}\")\n    elif name in self.__dict__ and name not in self._modules:\n        raise KeyError(f\"attribute `{name}` already exists\")\n    elif \".\" in name:\n        raise KeyError(f\"module name can't contain '.', got: {name}\")\n    elif name == \"\":\n        raise KeyError(\"module name can't be empty string \")\n    for hook in _global_module_registration_hooks.values():\n        output = hook(self, name, module)\n        if output is not None:\n            module = output\n    self._modules[name] = module\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.buffers","title":"buffers","text":"<pre><code>buffers(*, recurse=True)\n</code></pre> <p>Return an iterator over module buffers.</p> <p>Parameters:</p> Name Type Description Default <code>recurse</code> <code>bool</code> <p>if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</p> <code>True</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>torch.Tensor: module buffer</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for buf in model.buffers():\n&gt;&gt;&gt;     print(type(buf), buf.size())\n&lt;class 'torch.Tensor'&gt; (20L,)\n&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def buffers(self, *, recurse: Optional[bool] = True) -&gt; Iterator[Any]:  # TODO doc\n    \"\"\"Return an iterator over module buffers.\n\n    Args:\n        recurse (bool): if True, then yields buffers of this module\n            and all submodules. Otherwise, yields only buffers that\n            are direct members of this module.\n\n    Returns:\n        torch.Tensor: module buffer\n\n    Example::\n\n        &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n        &gt;&gt;&gt; for buf in model.buffers():\n        &gt;&gt;&gt;     print(type(buf), buf.size())\n        &lt;class 'torch.Tensor'&gt; (20L,)\n        &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)\n\n    \"\"\"\n    for _, buf in self.named_buffers(recurse=recurse):\n        yield buf\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.children","title":"children","text":"<pre><code>children()\n</code></pre> <p>Return an iterator over immediate children modules.</p> <p>Yields:</p> Name Type Description <code>Module</code> <code>Module</code> <p>a child module</p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def children(self) -&gt; Iterator[\"Module\"]:\n    \"\"\"Return an iterator over immediate children modules.\n\n    Yields:\n        Module: a child module\n    \"\"\"\n    for _, module in self.named_children():\n        yield module\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.eval","title":"eval","text":"<pre><code>eval()\n</code></pre> <p>Set the module in evaluation mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode, i.e. whether they are affected.</p> <p>This is equivalent with :meth:<code>self.train(False) &lt;msgflux.nn.Module.train&gt;</code>.</p> <p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.eval()</code> and several similar mechanisms that may be confused with it.</p> <p>Returns:</p> Type Description <code>T</code> <p>Self.</p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def eval(self: T) -&gt; T:\n    \"\"\"Set the module in evaluation mode.\n\n    This has an effect only on certain modules. See the documentation of\n    particular modules for details of their behaviors in training/evaluation\n    mode, i.e. whether they are affected.\n\n    This is equivalent with :meth:`self.train(False) &lt;msgflux.nn.Module.train&gt;`.\n\n    See :ref:`locally-disable-grad-doc` for a comparison between\n    `.eval()` and several similar mechanisms that may be confused with it.\n\n    Returns:\n        Self.\n    \"\"\"\n    return self.train(mode=False)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.extra_repr","title":"extra_repr","text":"<pre><code>extra_repr()\n</code></pre> <p>Return the extra representation of the module.</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def extra_repr(self) -&gt; str:\n    \"\"\"Return the extra representation of the module.\n\n    To print customized extra information, you should re-implement\n    this method in your own modules. Both single-line and multi-line\n    strings are acceptable.\n    \"\"\"\n    return \"\"\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.get_buffer","title":"get_buffer","text":"<pre><code>get_buffer(target)\n</code></pre> <p>Return the buffer given by <code>target</code> if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>str</code> <p>The fully-qualified string name of the buffer to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</p> required <p>TODO</p> Type Description <code>Any</code> <p>torch.Tensor: The buffer referenced by <code>target</code></p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the target string references an invalid path or resolves to something that is not a buffer</p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def get_buffer(self, target: str) -&gt; Any:\n    \"\"\"Return the buffer given by ``target`` if it exists, otherwise throw an error.\n\n    See the docstring for ``get_submodule`` for a more detailed\n    explanation of this method's functionality as well as how to\n    correctly specify ``target``.\n\n    Args:\n        target: The fully-qualified string name of the buffer\n            to look for. (See ``get_submodule`` for how to specify a\n            fully-qualified string.)\n\n    Returns: TODO\n        torch.Tensor: The buffer referenced by ``target``\n\n    Raises:\n        AttributeError: If the target string references an invalid\n            path or resolves to something that is not a\n            buffer\n    \"\"\"\n    module_path, _, buffer_name = target.rpartition(\".\")\n\n    mod: Module = self.get_submodule(module_path)\n\n    if not hasattr(mod, buffer_name):\n        raise AttributeError(\n            mod._get_name() + \" has no attribute `\" + buffer_name + \"`\"\n        )\n\n    buffer: Any = getattr(mod, buffer_name)\n\n    if buffer_name not in mod._buffers:\n        raise AttributeError(\"`\" + buffer_name + \"` is not a buffer\")\n\n    return buffer\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.get_model_preference_from_message","title":"get_model_preference_from_message","text":"<pre><code>get_model_preference_from_message(message)\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def get_model_preference_from_message(self, message: Message) -&gt; Optional[str]:\n    if isinstance(message, Message) and isinstance(self.model_preference, str):\n        return message.get(self.model_preference)\n    else:\n        return None\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.get_module_annotations","title":"get_module_annotations","text":"<pre><code>get_module_annotations()\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def get_module_annotations(self):\n    module_annotations = getattr(self, \"annotations\", None)\n    if module_annotations is None:\n        module_annotations = self.__class__.__annotations__\n    return module_annotations\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.get_module_description","title":"get_module_description","text":"<pre><code>get_module_description()\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def get_module_description(self):\n    module_description = getattr(self, \"description\", None)\n    if module_description is None:\n        module_description = self.__class__.__doc__\n    return module_description\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.get_module_name","title":"get_module_name","text":"<pre><code>get_module_name()\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def get_module_name(self):\n    module_name = getattr(self, \"name\", None)\n    if module_name is None:\n        module_name = self._get_name()\n    return module_name\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.get_parameter","title":"get_parameter","text":"<pre><code>get_parameter(target)\n</code></pre> <p>Return the parameter given by <code>target</code>if it exists, otherwise throw an error.</p> <p>See the docstring for <code>get_submodule</code> for a more detailed explanation of this method's functionality as well as how to correctly specify <code>target</code>.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>str</code> <p>The fully-qualified string name of the Parameter to look for. (See <code>get_submodule</code> for how to specify a fully-qualified string.)</p> required <p>Returns:</p> Type Description <code>Parameter</code> <p>msgflux.nn.Parameter: The Parameter referenced by <code>target</code></p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the target string references an invalid path or resolves to something that is not an <code>nn.Parameter</code></p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def get_parameter(self, target: str) -&gt; \"Parameter\":\n    \"\"\"Return the parameter given by ``target``if\n    it exists, otherwise throw an error.\n\n    See the docstring for ``get_submodule`` for a more detailed\n    explanation of this method's functionality as well as how to\n    correctly specify ``target``.\n\n    Args:\n        target: The fully-qualified string name of the Parameter\n            to look for. (See ``get_submodule`` for how to specify a\n            fully-qualified string.)\n\n    Returns:\n        msgflux.nn.Parameter: The Parameter referenced by ``target``\n\n    Raises:\n        AttributeError: If the target string references an invalid\n            path or resolves to something that is not an\n            ``nn.Parameter``\n    \"\"\"\n    module_path, _, param_name = target.rpartition(\".\")\n\n    mod: Module = self.get_submodule(module_path)\n\n    if not hasattr(mod, param_name):\n        raise AttributeError(\n            mod._get_name() + \" has no attribute `\" + param_name + \"`\"\n        )\n\n    param: Parameter = getattr(mod, param_name)\n\n    if not isinstance(param, Parameter):\n        raise AttributeError(\"`\" + param_name + \"` is not an nn.Parameter\")\n\n    return param\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.get_submodule","title":"get_submodule","text":"<pre><code>get_submodule(target)\n</code></pre> <p>Return the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p> <p>.. code-block:: text</p> <pre><code>A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n</code></pre> <p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> which has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p> <p>To check whether or not we have the <code>linear</code> submodule, we would call <code>get_submodule(\"net_b.linear\")</code>. To check whether we have the <code>conv</code> submodule, we would call <code>get_submodule(\"net_b.net_c.conv\")</code>.</p> <p>The runtime of <code>get_submodule</code> is bounded by the degree of module nesting in <code>target</code>. A query against <code>named_modules</code> achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, <code>get_submodule</code> should always be used.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>str</code> <p>The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</p> required <p>Returns:</p> Type Description <code>Module</code> <p>msgflux.nn.Module: The submodule referenced by <code>target</code></p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the target string references an invalid path or resolves to something that is not an <code>nn.Module</code></p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def get_submodule(self, target: str) -&gt; \"Module\":\n    \"\"\"Return the submodule given by ``target``\n    if it exists, otherwise throw an error.\n\n    For example, let's say you have an ``nn.Module`` ``A`` that\n    looks like this:\n\n    .. code-block:: text\n\n        A(\n            (net_b): Module(\n                (net_c): Module(\n                    (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n                )\n                (linear): Linear(in_features=100, out_features=200, bias=True)\n            )\n        )\n\n    (The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\n    submodule ``net_b``, which itself has two submodules ``net_c``\n    and ``linear``. ``net_c`` then has a submodule ``conv``.)\n\n    To check whether or not we have the ``linear`` submodule, we\n    would call ``get_submodule(\"net_b.linear\")``. To check whether\n    we have the ``conv`` submodule, we would call\n    ``get_submodule(\"net_b.net_c.conv\")``.\n\n    The runtime of ``get_submodule`` is bounded by the degree\n    of module nesting in ``target``. A query against\n    ``named_modules`` achieves the same result, but it is O(N) in\n    the number of transitive modules. So, for a simple check to see\n    if some submodule exists, ``get_submodule`` should always be\n    used.\n\n    Args:\n        target: The fully-qualified string name of the submodule\n            to look for. (See above example for how to specify a\n            fully-qualified string.)\n\n    Returns:\n        msgflux.nn.Module: The submodule referenced by ``target``\n\n    Raises:\n        AttributeError: If the target string references an invalid\n            path or resolves to something that is not an\n            ``nn.Module``\n    \"\"\"\n    if target == \"\":\n        return self\n\n    atoms: List[str] = target.split(\".\")\n    mod: Module = self\n\n    for item in atoms:\n        if not hasattr(mod, item):\n            raise AttributeError(\n                mod._get_name() + \" has no attribute `\" + item + \"`\"\n            )\n\n        mod = getattr(mod, item)\n\n        if not isinstance(mod, Module):\n            raise AttributeError(\"`\" + item + \"` is not an nn.Module\")\n\n    return mod\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict)\n</code></pre> <p>Loads the state of the module and its submodules.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing the complete state</p> required Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def load_state_dict(self, state_dict: Dict[str, Any]) -&gt; None:\n    \"\"\"Loads the state of the module and its submodules.\n\n    Args:\n        state_dict: Dictionary containing the complete state\n    \"\"\"\n    if not isinstance(state_dict, dict):\n        raise TypeError(\n            f\"`state_dict` to be dict, given {type(state_dict).__name__}\"\n        )\n\n    self._load_from_state_dict(state_dict)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.modules","title":"modules","text":"<pre><code>modules()\n</code></pre> <p>Return an iterator over all modules in the network.</p> <p>Yields:</p> Name Type Description <code>Module</code> <code>Module</code> <p>a module in the network</p> Note <p>Duplicate modules are returned only once. In the following example, <code>l</code> will be returned only once.</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; l = nn.Linear(2, 2)\n&gt;&gt;&gt; net = nn.Sequential(l, l)\n&gt;&gt;&gt; for idx, m in enumerate(net.modules()):\n...     print(idx, '-&gt;', m)\n\n0 -&gt; Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 -&gt; Linear(in_features=2, out_features=2, bias=True)\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def modules(self) -&gt; Iterator[\"Module\"]:  # TODO DOC\n    \"\"\"Return an iterator over all modules in the network.\n\n    Yields:\n        Module: a module in the network\n\n    Note:\n        Duplicate modules are returned only once. In the following\n        example, ``l`` will be returned only once.\n\n    Example::\n\n        &gt;&gt;&gt; l = nn.Linear(2, 2)\n        &gt;&gt;&gt; net = nn.Sequential(l, l)\n        &gt;&gt;&gt; for idx, m in enumerate(net.modules()):\n        ...     print(idx, '-&gt;', m)\n\n        0 -&gt; Sequential(\n          (0): Linear(in_features=2, out_features=2, bias=True)\n          (1): Linear(in_features=2, out_features=2, bias=True)\n        )\n        1 -&gt; Linear(in_features=2, out_features=2, bias=True)\n\n    \"\"\"\n    for _, module in self.named_modules():\n        yield module\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.named_buffers","title":"named_buffers","text":"<pre><code>named_buffers(\n    prefix=\"\", *, recurse=True, remove_duplicate=True\n)\n</code></pre> <p>Return an iterator over module buffers, yielding both the name of the     buffer as well as the buffer itself.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>Optional[str]</code> <p>Prefix to prepend to all buffer names.</p> <code>''</code> <code>recurse</code> <code>Optional[bool]</code> <p>If True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Defaults to True.</p> <code>True</code> <code>remove_duplicat</code> <p>Whether to remove the duplicated buffers in the result. Defaults to True.</p> required <p>Yields:</p> Type Description <code>Tuple[str, Any]</code> <p>Tuple containing the name and buffer</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, buf in self.named_buffers():\n&gt;&gt;&gt;     if name in ['running_var']:\n&gt;&gt;&gt;         print(buf.size())\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def named_buffers(\n    self,\n    prefix: Optional[str] = \"\",\n    *,\n    recurse: Optional[bool] = True,\n    remove_duplicate: Optional[bool] = True,\n) -&gt; Iterator[Tuple[str, Any]]:  # TODO docstring\n    \"\"\"Return an iterator over module buffers, yielding both the name of the\n        buffer as well as the buffer itself.\n\n    Args:\n        prefix:\n            Prefix to prepend to all buffer names.\n        recurse:\n            If True, then yields buffers of this module\n            and all submodules. Otherwise, yields only buffers that\n            are direct members of this module. Defaults to True.\n        remove_duplicat:\n            Whether to remove the duplicated buffers in the result.\n            Defaults to True.\n\n    Yields:\n        Tuple containing the name and buffer\n\n    Example::\n\n        &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n        &gt;&gt;&gt; for name, buf in self.named_buffers():\n        &gt;&gt;&gt;     if name in ['running_var']:\n        &gt;&gt;&gt;         print(buf.size())\n\n    \"\"\"\n    gen = self._named_members(\n        lambda module: module._buffers.items(),\n        prefix=prefix,\n        recurse=recurse,\n        remove_duplicate=remove_duplicate,\n    )\n    yield from gen\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.named_children","title":"named_children","text":"<pre><code>named_children()\n</code></pre> <p>Return an iterator over immediate children modules, yielding     both the name of the module as well as the module itself.</p> <p>Yields:</p> Type Description <code>(str, Module)</code> <p>Tuple containing a name and child module</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, module in model.named_children():\n&gt;&gt;&gt;     if name in ['conv4', 'conv5']:\n&gt;&gt;&gt;         print(module)\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def named_children(self) -&gt; Iterator[Tuple[str, \"Module\"]]:\n    \"\"\"Return an iterator over immediate children modules, yielding\n        both the name of the module as well as the module itself.\n\n    Yields:\n        (str, Module): Tuple containing a name and child module\n\n    Example::\n\n        &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n        &gt;&gt;&gt; for name, module in model.named_children():\n        &gt;&gt;&gt;     if name in ['conv4', 'conv5']:\n        &gt;&gt;&gt;         print(module)\n\n    \"\"\"\n    memo = set()\n    for name, module in self._modules.items():\n        if module is not None and module not in memo:\n            memo.add(module)\n            yield name, module\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.named_modules","title":"named_modules","text":"<pre><code>named_modules(\n    memo=None, prefix=\"\", *, remove_duplicate=True\n)\n</code></pre> <p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <p>Parameters:</p> Name Type Description Default <code>memo</code> <code>Optional[Set[Module]]</code> <p>A memo to store the set of modules already added to the result.</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>A prefix that will be added to the name of the module.</p> <code>''</code> <code>remove_duplicate</code> <code>Optional[bool]</code> <p>Whether to remove the duplicated module instances in the result or not.</p> <code>True</code> <p>Yields:</p> Type Description <code>Tuple[str, Module]</code> <p>Tuple of name and module</p> Note <p>Duplicate modules are returned only once. In the following example, <code>l</code> will be returned only once.</p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def named_modules(\n    self,\n    memo: Optional[Set[\"Module\"]] = None,\n    prefix: Optional[str] = \"\",\n    *,\n    remove_duplicate: Optional[bool] = True,\n) -&gt; Iterator[Tuple[str, \"Module\"]]:\n    \"\"\"Return an iterator over all modules in the network, yielding\n    both the name of the module as well as the module itself.\n\n    Args:\n        memo:\n            A memo to store the set of modules already added to the result.\n        prefix:\n            A prefix that will be added to the name of the module.\n        remove_duplicate:\n            Whether to remove the duplicated module instances in the result\n            or not.\n\n    Yields:\n        Tuple of name and module\n\n    Note:\n        Duplicate modules are returned only once. In the following\n        example, ``l`` will be returned only once.\n    \"\"\"\n    if memo is None:\n        memo = set()\n    if self not in memo:\n        if remove_duplicate:\n            memo.add(self)\n        yield prefix, self\n        for name, module in self._modules.items():\n            if module is None:\n                continue\n            submodule_prefix = prefix + (\".\" if prefix else \"\") + name\n            yield from module.named_modules(\n                memo, submodule_prefix, remove_duplicate=remove_duplicate\n            )\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.named_parameters","title":"named_parameters","text":"<pre><code>named_parameters(\n    prefix=\"\", *, recurse=True, remove_duplicate=True\n)\n</code></pre> <p>Return an iterator over module parameters, yielding both the name of the     parameter as well as the parameter itself.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>prefix to prepend to all parameter names.</p> <code>''</code> <code>recurse</code> <code>bool</code> <p>if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</p> <code>True</code> <code>remove_duplicate</code> <code>bool</code> <p>whether to remove the duplicated parameters in the result. Defaults to True.</p> <code>True</code> <p>Yields:</p> Type Description <code>(str, Parameter)</code> <p>Tuple containing the name and parameter</p> <p>Example::</p> <pre><code>&gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n&gt;&gt;&gt; for name, param in self.named_parameters():\n&gt;&gt;&gt;     if name in ['bias']:\n&gt;&gt;&gt;         print(param.size())\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def named_parameters(\n    self, prefix: str = \"\", *, recurse: bool = True, remove_duplicate: bool = True\n) -&gt; Iterator[Tuple[str, Parameter]]:\n    # TODO: docstring\n    \"\"\"Return an iterator over module parameters, yielding both the name of the\n        parameter as well as the parameter itself.\n\n    Args:\n        prefix (str): prefix to prepend to all parameter names.\n        recurse (bool): if True, then yields parameters of this module\n            and all submodules. Otherwise, yields only parameters that\n            are direct members of this module.\n        remove_duplicate (bool, optional): whether to remove the duplicated\n            parameters in the result. Defaults to True.\n\n    Yields:\n        (str, Parameter): Tuple containing the name and parameter\n\n    Example::\n\n        &gt;&gt;&gt; # xdoctest: +SKIP(\"undefined vars\")\n        &gt;&gt;&gt; for name, param in self.named_parameters():\n        &gt;&gt;&gt;     if name in ['bias']:\n        &gt;&gt;&gt;         print(param.size())\n\n    \"\"\"\n    gen = self._named_members(\n        lambda module: module._parameters.items(),\n        prefix=prefix,\n        recurse=recurse,\n        remove_duplicate=remove_duplicate,\n    )\n    yield from gen\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.parameters","title":"parameters","text":"<pre><code>parameters(*, recurse=True)\n</code></pre> <p>Return an iterator over module parameters.</p> <p>This is typically passed to an optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>recurse</code> <code>Optional[bool]</code> <p>If True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Parameter</code> <code>Iterator[Parameter]</code> <p>module parameter</p> <p>Example</p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def parameters(self, *, recurse: Optional[bool] = True) -&gt; Iterator[Parameter]:\n    \"\"\"Return an iterator over module parameters.\n\n    This is typically passed to an optimizer.\n\n    Args:\n        recurse: If True, then yields parameters of this module\n            and all submodules. Otherwise, yields only parameters that\n            are direct members of this module.\n\n    Returns:\n        Parameter: module parameter\n\n    !!! example\n        # TODO\n        ```python\n        for param in model.parameters():\n            print(type(param), param.size())\n        &gt;&gt;&gt; &lt;class 'torch.Tensor'&gt; (20L,)\n        &gt;&gt;&gt; &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)\n        ```\n    \"\"\"\n    for _name, param in self.named_parameters(recurse=recurse):\n        yield param\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.parameters--todo","title":"TODO","text":"<pre><code>for param in model.parameters():\n    print(type(param), param.size())\n&gt;&gt;&gt; &lt;class 'torch.Tensor'&gt; (20L,)\n&gt;&gt;&gt; &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.plot","title":"plot","text":"<pre><code>plot(title=None, orientation='TD', *, remove_self=True)\n</code></pre> <p>Generates and renders a Mermaid diagram of the <code>forward</code> method.</p> <p>This method extracts the source code of the <code>forward</code> method and converts it into a Mermaid diagram for visualization. Optionally, it can clean up the code by removing references to <code>self</code> to produce a cleaner diagram.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>Optional[str]</code> <p>Title to display at the top of the Mermaid diagram.</p> <code>None</code> <code>orientation</code> <code>Optional[str]</code> <p>Diagram orientation. Options include \"TD\" (top-down), \"LR\" (left-right), etc.</p> <code>'TD'</code> <code>remove_self</code> <code>Optional[bool]</code> <p>Whether to remove references to <code>self</code> from the code before generating the diagram. Useful for cleaner output.</p> <code>True</code> <p>Returns:</p> Type Description <code>Mermaid</code> <p>The rendered Mermaid diagram.</p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def plot(\n    self,\n    title: Optional[str] = None,\n    orientation: Optional[str] = \"TD\",\n    *,\n    remove_self: Optional[bool] = True,\n) -&gt; Mermaid:\n    \"\"\"Generates and renders a Mermaid diagram of the `forward` method.\n\n    This method extracts the source code of the `forward` method and converts\n    it into a Mermaid diagram for visualization. Optionally, it can clean up\n    the code by removing references to `self` to produce a cleaner diagram.\n\n    Args:\n        title:\n            Title to display at the top of the Mermaid diagram.\n        orientation:\n            Diagram orientation. Options include \"TD\" (top-down),\n            \"LR\" (left-right), etc.\n        remove_self:\n            Whether to remove references to `self` from the code before\n            generating the diagram. Useful for cleaner output.\n\n    Returns:\n        The rendered Mermaid diagram.\n    \"\"\"\n    mermaid = self._get_mermaid(title, orientation, remove_self)\n    return plot_mermaid(mermaid)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.register_buffer","title":"register_buffer","text":"<pre><code>register_buffer(name, data)\n</code></pre> <p>Add a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's <code>running_mean</code> is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr:<code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr:<code>state_dict</code>.</p> <p>Buffers can be accessed as attributes using given names.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the buffer. The buffer can be accessed from this module using the given name</p> required <code>data</code> <code>Any</code> <p>buffer to be registered.</p> required <p>Example::     &gt;&gt;&gt; self.register_buffer(\"name\", \"agent\")</p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def register_buffer(self, name: str, data: Any) -&gt; None:\n    # TODO: muito trabalho pra ajeitar a docstring\n    # mudei de tensor para data\n    \"\"\"Add a buffer to the module.\n\n    This is typically used to register a buffer that should not to be\n    considered a model parameter. For example, BatchNorm's ``running_mean``\n    is not a parameter, but is part of the module's state. Buffers, by\n    default, are persistent and will be saved alongside parameters. This\n    behavior can be changed by setting :attr:`persistent` to ``False``. The\n    only difference between a persistent buffer and a non-persistent buffer\n    is that the latter will not be a part of this module's\n    :attr:`state_dict`.\n\n    Buffers can be accessed as attributes using given names.\n\n    Args:\n        name:\n            Name of the buffer. The buffer can be accessed\n            from this module using the given name\n        data:\n            buffer to be registered.\n    Example::\n        &gt;&gt;&gt; self.register_buffer(\"name\", \"agent\")\n    \"\"\"\n    if \"_buffers\" not in self.__dict__:\n        raise AttributeError(\"cannot assign buffer before Module.__init__() call\")\n    elif not isinstance(name, str):\n        raise TypeError(f\"buffer name should be a string. Got {type(name)}\")\n    elif \".\" in name:\n        raise KeyError(\"buffer name can't contain '.'\")\n    elif name == \"\":\n        raise KeyError(\"buffer name can't be empty string\")\n    else:\n        for hook in _global_buffer_registration_hooks.values():\n            output = hook(self, name, data)\n            if output is not None:\n                data = output\n\n        self._buffers[name] = data\n        self._non_persistent_buffers_set.discard(name)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.register_forward_hook","title":"register_forward_hook","text":"<pre><code>register_forward_hook(\n    hook, *, prepend=False, always_call=False\n)\n</code></pre> <p>Register a forward hook on the module.</p> <p>The hook will be called every time after :func:<code>forward</code> has computed an output. The hook receives both positional arguments (<code>args</code>), keyword arguments (<code>kwargs</code>), and the output of the forward call. The hook can modify <code>args</code>, <code>kwargs</code>, and the output. It should have the following signature::</p> <pre><code>hook(module, args, kwargs, output) -&gt; None, modified_output,\nor (modified_args, modified_kwargs, modified_output)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable</code> <p>The user defined hook to be registered.</p> required <code>prepend</code> <code>bool</code> <p>If <code>True</code>, the provided <code>hook</code> will be fired before all existing <code>forward</code> hooks on this :class:<code>msgflux.nn.modules.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward</code> hooks on this :class:<code>msgflux.nn.modules.Module</code>. Note that global <code>forward</code> hooks registered with :func:<code>register_module_forward_hook</code> will fire before all hooks registered by this method. Default: <code>False</code></p> <code>False</code> <code>always_call</code> <code>bool</code> <p>If <code>True</code> the <code>hook</code> will be run regardless of whether an exception is raised while calling the Module. Default: <code>False</code></p> <code>False</code> <p>Returns:</p> Type Description <code>RemovableHandle</code> <p>class:<code>msgflux.utils.hooks.RemovableHandle</code>: a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def register_forward_hook(\n    self,\n    hook: Union[\n        Callable[[T, tuple[Any, ...], Any], Optional[Any]],\n        Callable[[T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]],\n    ],\n    *,\n    prepend: bool = False,\n    always_call: bool = False,\n) -&gt; RemovableHandle:\n    \"\"\"Register a forward hook on the module.\n\n    The hook will be called every time after :func:`forward`\n    has computed an output. The hook receives both positional\n    arguments (`args`), keyword arguments (`kwargs`), and the\n    output of the forward call. The hook can modify `args`,\n    `kwargs`, and the output. It should have the following signature::\n\n        hook(module, args, kwargs, output) -&gt; None, modified_output,\n        or (modified_args, modified_kwargs, modified_output)\n\n    Args:\n        hook (Callable): The user defined hook to be registered.\n        prepend (bool): If ``True``, the provided ``hook`` will be fired\n            before all existing ``forward`` hooks on this\n            :class:`msgflux.nn.modules.Module`. Otherwise, the provided\n            ``hook`` will be fired after all existing ``forward`` hooks on\n            this :class:`msgflux.nn.modules.Module`. Note that global\n            ``forward`` hooks registered with\n            :func:`register_module_forward_hook` will fire before all hooks\n            registered by this method.\n            Default: ``False``\n        always_call (bool): If ``True`` the ``hook`` will be run regardless of\n            whether an exception is raised while calling the Module.\n            Default: ``False``\n\n    Returns:\n        :class:`msgflux.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n    \"\"\"\n    handle = RemovableHandle(\n        self._forward_hooks,\n        extra_dict=[self._forward_hooks_always_called],\n    )\n    self._forward_hooks[handle.id] = hook\n    if always_call:\n        self._forward_hooks_always_called[handle.id] = True\n    if prepend:\n        self._forward_hooks.move_to_end(handle.id, last=False)\n    return handle\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.register_forward_pre_hook","title":"register_forward_pre_hook","text":"<pre><code>register_forward_pre_hook(hook, *, prepend=False)\n</code></pre> <p>Register a forward pre-hook on the module.</p> <p>The hook will be called every time before :func:<code>forward</code> is invoked.</p> <p>If <code>with_kwargs</code> is false or not specified, the input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned (unless that value is already a tuple). The hook should have the following signature::</p> <pre><code>hook(module, args) -&gt; None or modified input\n</code></pre> <p>If <code>with_kwargs</code> is true, the forward pre-hook will be passed the kwargs given to the forward function. And if the hook modifies the input, both the args and kwargs should be returned. The hook should have the following signature::</p> <pre><code>hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable</code> <p>The user defined hook to be registered.</p> required <code>prepend</code> <code>bool</code> <p>If true, the provided <code>hook</code> will be fired before all existing <code>forward_pre</code> hooks on this :class:<code>msgflux.nn.modules.Module</code>. Otherwise, the provided <code>hook</code> will be fired after all existing <code>forward_pre</code> hooks on this :class:<code>msgflux.nn.modules.Module</code>. Note that global <code>forward_pre</code> hooks registered with :func:<code>register_module_forward_pre_hook</code> will fire before all hooks registered by this method. Default: <code>False</code></p> <code>False</code> <p>Returns:</p> Type Description <code>RemovableHandle</code> <p>class:<code>msgflux.utils.hooks.RemovableHandle</code>: a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def register_forward_pre_hook(\n    self,\n    hook: Callable[\n        [T, Tuple[Any, ...], Dict[str, Any]],\n        Optional[Tuple[Any, Dict[str, Any]]],\n    ],\n    *,\n    prepend: bool = False,\n) -&gt; RemovableHandle:\n    r\"\"\"Register a forward pre-hook on the module.\n\n    The hook will be called every time before :func:`forward` is invoked.\n\n\n    If ``with_kwargs`` is false or not specified, the input contains only\n    the positional arguments given to the module. Keyword arguments won't be\n    passed to the hooks and only to the ``forward``. The hook can modify the\n    input. User can either return a tuple or a single modified value in the\n    hook. We will wrap the value into a tuple if a single value is returned\n    (unless that value is already a tuple). The hook should have the\n    following signature::\n\n        hook(module, args) -&gt; None or modified input\n\n    If ``with_kwargs`` is true, the forward pre-hook will be passed the\n    kwargs given to the forward function. And if the hook modifies the\n    input, both the args and kwargs should be returned. The hook should have\n    the following signature::\n\n        hook(module, args, kwargs) -&gt; None or a tuple of modified input and kwargs\n\n    Args:\n        hook (Callable): The user defined hook to be registered.\n        prepend (bool): If true, the provided ``hook`` will be fired before\n            all existing ``forward_pre`` hooks on this\n            :class:`msgflux.nn.modules.Module`. Otherwise, the provided\n            ``hook`` will be fired after all existing ``forward_pre`` hooks\n            on this :class:`msgflux.nn.modules.Module`. Note that global\n            ``forward_pre`` hooks registered with\n            :func:`register_module_forward_pre_hook` will fire before all\n            hooks registered by this method.\n            Default: ``False``\n\n    Returns:\n        :class:`msgflux.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n    \"\"\"\n    handle = RemovableHandle(self._forward_pre_hooks)\n    self._forward_pre_hooks[handle.id] = hook\n    if prepend:\n        self._forward_pre_hooks.move_to_end(handle.id, last=False)\n    return handle\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.register_load_state_dict_post_hook","title":"register_load_state_dict_post_hook","text":"<pre><code>register_load_state_dict_post_hook(hook)\n</code></pre> <p>Register a post-hook to be run after module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, incompatible_keys) -&gt; None</p> <p>The <code>module</code> argument is the current module that this hook is registered on, and the <code>incompatible_keys</code> argument is a <code>NamedTuple</code> consisting of attributes <code>missing_keys</code> and <code>unexpected_keys</code>. <code>missing_keys</code> is a <code>list</code> of <code>str</code> containing the missing keys and <code>unexpected_keys</code> is a <code>list</code> of <code>str</code> containing the unexpected keys.</p> <p>The given incompatible_keys can be modified inplace if needed.</p> <p>Note that the checks performed when calling :func:<code>load_state_dict</code> with <code>strict=True</code> are affected by modifications the hook makes to <code>missing_keys</code> or <code>unexpected_keys</code>, as expected. Additions to either set of keys will result in an error being thrown when <code>strict=True</code>, and clearing out both missing and unexpected keys will avoid an error.</p> <p>Returns:</p> Type Description <p>class:<code>msgflux.utils.hooks.RemovableHandle</code>: a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def register_load_state_dict_post_hook(self, hook: Callable):\n    \"\"\"Register a post-hook to be run after module's\n    :meth:`~nn.Module.load_state_dict` is called.\n\n    It should have the following signature::\n        hook(module, incompatible_keys) -&gt; None\n\n    The ``module`` argument is the current module that this hook is registered\n    on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n    of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n    is a ``list`` of ``str`` containing the missing keys and\n    ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n\n    The given incompatible_keys can be modified inplace if needed.\n\n    Note that the checks performed when calling :func:`load_state_dict` with\n    ``strict=True`` are affected by modifications the hook makes to\n    ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n    set of keys will result in an error being thrown when ``strict=True``, and\n    clearing out both missing and unexpected keys will avoid an error.\n\n    Returns:\n        :class:`msgflux.utils.hooks.RemovableHandle`:\n            a handle that can be used to remove the added hook by calling\n            ``handle.remove()``\n    \"\"\"\n    handle = RemovableHandle(self._load_state_dict_post_hooks)\n    self._load_state_dict_post_hooks[handle.id] = hook\n    return handle\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.register_load_state_dict_pre_hook","title":"register_load_state_dict_pre_hook","text":"<pre><code>register_load_state_dict_pre_hook(hook)\n</code></pre> <p>Register a pre-hook to be run before module's :meth:<code>~nn.Module.load_state_dict</code> is called.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata, strict,     missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable</code> <p>Callable hook that will be invoked before loading the state dict.</p> required Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def register_load_state_dict_pre_hook(self, hook: Callable):\n    \"\"\"Register a pre-hook to be run before module's\n    :meth:`~nn.Module.load_state_dict` is called.\n\n    It should have the following signature::\n        hook(module, state_dict, prefix, local_metadata, strict,\n        missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950\n\n    Args:\n        hook:\n            Callable hook that will be invoked before\n            loading the state dict.\n    \"\"\"\n    return self._register_load_state_dict_pre_hook(hook, with_module=True)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.register_module","title":"register_module","text":"<pre><code>register_module(name, module)\n</code></pre> <p>Alias for :func:<code>add_module</code>.</p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def register_module(self, name: str, module: \"Module\") -&gt; None:\n    \"\"\"Alias for :func:`add_module`.\"\"\"\n    self.add_module(name, module)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.register_parameter","title":"register_parameter","text":"<pre><code>register_parameter(name, param)\n</code></pre> <p>Add a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of the parameter. The parameter can be accessed from this module using the given name</p> required <code>param</code> <code>Parameter or None</code> <p>parameter to be added to the module. If <code>None</code>, then operations that run on parameters, such as :attr:<code>cuda</code>, are ignored. If <code>None</code>, the parameter is not included in the module's :attr:<code>state_dict</code>.</p> required Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def register_parameter(self, name: str, param: Parameter) -&gt; None:\n    \"\"\"Add a parameter to the module.\n\n    The parameter can be accessed as an attribute using given name.\n\n    Args:\n        name (str): name of the parameter. The parameter can be accessed\n            from this module using the given name\n        param (Parameter or None): parameter to be added to the module. If\n            ``None``, then operations that run on parameters, such as :attr:`cuda`,\n            are ignored. If ``None``, the parameter is **not** included in the\n            module's :attr:`state_dict`.\n    \"\"\"\n    if \"_parameters\" not in self.__dict__:\n        raise AttributeError(\n            \"cannot assign parameter before Module.__init__() call\"\n        )\n\n    elif not isinstance(name, str):\n        raise TypeError(f\"parameter name should be a string. Got {type(name)}\")\n    elif \".\" in name:\n        raise KeyError(\"parameter name can't contain '.'\")\n    elif name == \"\":\n        raise KeyError(\"parameter name can't be empty string\")\n    elif name in self.__dict__ and name not in self._parameters:\n        raise KeyError(f\"attribute '{name}' already exists\")\n    elif param is None:\n        self._parameters[name] = None\n    elif not isinstance(param, Parameter):\n        raise TypeError(\n            f\"cannot assign `{type(param)}` object to parameter `{name}` \"\n            \"(msgflux.nn.Parameter required)\"\n        )\n    else:\n        for hook in _global_parameter_registration_hooks.values():\n            output = hook(self, name, param)\n            if output is not None:\n                param = output\n        self._parameters[name] = param\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.register_state_dict_post_hook","title":"register_state_dict_post_hook","text":"<pre><code>register_state_dict_post_hook(hook)\n</code></pre> <p>Register a post-hook for the :meth:<code>~msgflux.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, state_dict, prefix, local_metadata) -&gt; None</p> <p>The registered hooks can modify the <code>state_dict</code> inplace.</p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def register_state_dict_post_hook(self, hook: Callable):\n    \"\"\"Register a post-hook for the :meth:`~msgflux.nn.Module.state_dict` method.\n\n    It should have the following signature::\n        hook(module, state_dict, prefix, local_metadata) -&gt; None\n\n    The registered hooks can modify the ``state_dict`` inplace.\n    \"\"\"\n    hook._from_public_api = True\n    handle = RemovableHandle(self._state_dict_hooks)\n    self._state_dict_hooks[handle.id] = hook\n    return handle\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.register_state_dict_pre_hook","title":"register_state_dict_pre_hook","text":"<pre><code>register_state_dict_pre_hook(hook)\n</code></pre> <p>Register a pre-hook for the :meth:<code>~msgflux.nn.Module.state_dict</code> method.</p> <p>It should have the following signature::     hook(module, prefix, keep_vars) -&gt; None</p> <p>The registered hooks can be used to perform pre-processing before the <code>state_dict</code> call is made.</p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def register_state_dict_pre_hook(self, hook: Callable):\n    \"\"\"Register a pre-hook for the :meth:`~msgflux.nn.Module.state_dict`\n    method.\n\n    It should have the following signature::\n        hook(module, prefix, keep_vars) -&gt; None\n\n    The registered hooks can be used to perform pre-processing\n    before the ``state_dict`` call is made.\n    \"\"\"\n    handle = RemovableHandle(self._state_dict_pre_hooks)\n    self._state_dict_pre_hooks[handle.id] = hook\n    return handle\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.requires_grad_","title":"requires_grad_","text":"<pre><code>requires_grad_(*, requires_pgrad=True)\n</code></pre> <p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters' :attr:<code>requires_grad</code> attributes in-place.</p> <p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p> <p>See :ref:<code>locally-disable-grad-doc</code> for a comparison between <code>.requires_grad_()</code> and several similar mechanisms that may be confused with it.</p> <p>Parameters:</p> Name Type Description Default <code>requires_grad</code> <p>Whether autograd should record operations on parameters in this module.</p> required <p>Returns:</p> Name Type Description <code>Module</code> <code>T</code> <p>self</p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def requires_grad_(self: T, *, requires_pgrad: Optional[bool] = True) -&gt; T:\n    \"\"\"Change if autograd should record operations on parameters in this module.\n\n    This method sets the parameters' :attr:`requires_grad` attributes\n    in-place.\n\n    This method is helpful for freezing part of the module for finetuning\n    or training parts of a model individually (e.g., GAN training).\n\n    See :ref:`locally-disable-grad-doc` for a comparison between\n    `.requires_grad_()` and several similar mechanisms that may be confused with it.\n\n    Args:\n        requires_grad:\n            Whether autograd should record operations on\n            parameters in this module.\n\n    Returns:\n        Module: self\n    \"\"\"\n    for p in self.parameters():\n        p.requires_grad_(requires_grad=requires_pgrad)\n    return self\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.set_annotations","title":"set_annotations","text":"<pre><code>set_annotations(annotations)\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def set_annotations(self, annotations: Dict[str, type]):\n    if isinstance(annotations, dict):\n        super().__setattr__(\"annotations\", annotations)\n    else:\n        raise TypeError(f\"`annotations` need be a `dict` given {type(annotations)}\")\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.set_description","title":"set_description","text":"<pre><code>set_description(description=None)\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def set_description(self, description: Optional[str] = None):\n    if isinstance(description, str) or description is None:\n        self.register_buffer(\"description\", description)\n    else:\n        raise ValueError(\"`description` requires a string not empty\")\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.set_name","title":"set_name","text":"<pre><code>set_name(name)\n</code></pre> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def set_name(self, name: str):\n    if isinstance(name, str):\n        if name != \"\":\n            self.register_buffer(\"name\", name)\n        else:\n            raise ValueError(\"`name` requires a string not empty\")\n    else:\n        raise TypeError(f\"`name` need be a `str` given {type(name)}\")\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.set_submodule","title":"set_submodule","text":"<pre><code>set_submodule(target, module)\n</code></pre> <p>Set the submodule given by <code>target</code> if it exists, otherwise throw an error.</p> <p>For example, let's say you have an <code>nn.Module</code> <code>A</code> that looks like this:</p> <p>.. code-block:: text</p> <pre><code>A(\n    (net_b): Module(\n        (net_c): Module(\n            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n        )\n        (linear): Linear(in_features=100, out_features=200, bias=True)\n    )\n)\n</code></pre> <p>(The diagram shows an <code>nn.Module</code> <code>A</code>. <code>A</code> has a nested submodule <code>net_b</code>, which itself has two submodules <code>net_c</code> and <code>linear</code>. <code>net_c</code> then has a submodule <code>conv</code>.)</p> <p>To overide the <code>Conv2d</code> with a new submodule <code>Linear</code>, you would call <code>set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))</code>.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>str</code> <p>The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.)</p> required <code>module</code> <code>Module</code> <p>The module to set the submodule to.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the target string is empty</p> <code>AttributeError</code> <p>If the target string references an invalid path or resolves to something that is not an <code>nn.Module</code></p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def set_submodule(self, target: str, module: \"Module\") -&gt; None:\n    \"\"\"Set the submodule given by ``target`` if it exists, otherwise throw an error.\n\n    For example, let's say you have an ``nn.Module`` ``A`` that\n    looks like this:\n\n    .. code-block:: text\n\n        A(\n            (net_b): Module(\n                (net_c): Module(\n                    (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n                )\n                (linear): Linear(in_features=100, out_features=200, bias=True)\n            )\n        )\n\n    (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n    submodule ``net_b``, which itself has two submodules ``net_c``\n    and ``linear``. ``net_c`` then has a submodule ``conv``.)\n\n    To overide the ``Conv2d`` with a new submodule ``Linear``, you\n    would call\n    ``set_submodule(\"net_b.net_c.conv\", nn.Linear(33, 16))``.\n\n    Args:\n        target: The fully-qualified string name of the submodule\n            to look for. (See above example for how to specify a\n            fully-qualified string.)\n        module: The module to set the submodule to.\n\n    Raises:\n        ValueError: If the target string is empty\n        AttributeError: If the target string references an invalid\n            path or resolves to something that is not an\n            ``nn.Module``\n    \"\"\"\n    if target == \"\":\n        raise ValueError(\"Cannot set the submodule without a target name!\")\n\n    atoms: List[str] = target.split(\".\")\n    name = atoms.pop(-1)\n    mod: Module = self\n\n    for item in atoms:\n        if not hasattr(mod, item):\n            raise AttributeError(\n                mod._get_name() + \" has no attribute `\" + item + \"`\"\n            )\n\n        mod = getattr(mod, item)\n\n        # Use isinstance instead of type here to also handle subclass of nn.Module\n        if not isinstance(mod, Module):\n            raise AttributeError(\"`\" + item + \"` is not an nn.Module\")\n\n    setattr(mod, name, module)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.state_dict","title":"state_dict","text":"<pre><code>state_dict(destination=None, prefix='')\n</code></pre> <p>Returns a dictionary containing module's state.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <code>Optional[Dict[str, Any]]</code> <p>If provided, the state will be updated into the given dict. Default: None</p> <code>None</code> <code>prefix</code> <code>Optional[str]</code> <p>Prefix added to parameter and buffer names. Default: \"\"</p> <code>''</code> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def state_dict(\n    self, destination: Optional[Dict[str, Any]] = None, prefix: Optional[str] = \"\"\n):\n    \"\"\"Returns a dictionary containing module's state.\n\n    Args:\n        destination:\n            If provided, the state will be updated into\n            the given dict. Default: None\n        prefix:\n            Prefix added to parameter and buffer names.\n            Default: \"\"\n    \"\"\"\n    if destination is None:\n        destination = {}\n\n    # Save current module's state\n    self._save_to_state_dict(destination, prefix)\n\n    # Save states from child modules\n    for name, module in self._modules.items():\n        if module is not None:\n            module.state_dict(destination=destination, prefix=prefix + name + \".\")\n\n    return destination\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.train","title":"train","text":"<pre><code>train(*, mode=True)\n</code></pre> <p>Set the module in training mode.</p> <p>This has an effect only on certain modules. See the documentation of particular modules for details of their behaviors in training/evaluation mode.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Optional[bool]</code> <p>Whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>T</code> <p>Self.</p> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def train(self: T, *, mode: Optional[bool] = True) -&gt; T:\n    \"\"\"Set the module in training mode.\n\n    This has an effect only on certain modules. See the documentation of\n    particular modules for details of their behaviors in training/evaluation\n    mode.\n\n    Args:\n        mode:\n            Whether to set training mode (``True``) or evaluation\n            mode (``False``). Default: ``True``.\n\n    Returns:\n        Self.\n    \"\"\"\n    if not isinstance(mode, bool):\n        raise ValueError(\"training mode is expected to be boolean\")\n    self.training = mode\n    for module in self.children():\n        module.train(mode=mode)\n    return self\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Module.zero_pgrad","title":"zero_pgrad","text":"<pre><code>zero_pgrad(*, set_to_none=True)\n</code></pre> <p>Reset gradients of all model parameters.</p> <p>See similar function under :class:<code>msgflux.optim.Optimizer</code> for more context.</p> <p>Parameters:</p> Name Type Description Default <code>set_to_none</code> <code>bool</code> <p>instead of setting to zero, set the grads to None. See :meth:<code>msgflux.optim.Optimizer.zero_grad</code> for details.</p> <code>True</code> Source code in <code>src/msgflux/nn/modules/module.py</code> <pre><code>def zero_pgrad(\n    self, *, set_to_none: Optional[bool] = True\n) -&gt; None:  # TODO isso \u00e9 interessante mas vai mudar\n    \"\"\"Reset gradients of all model parameters.\n\n    See similar function under :class:`msgflux.optim.Optimizer` for more context.\n\n    Args:\n        set_to_none (bool): instead of setting to zero, set the grads to None.\n            See :meth:`msgflux.optim.Optimizer.zero_grad` for details.\n    \"\"\"\n    for p in self.parameters():\n        if p.pgrad is not None:\n            if set_to_none:\n                p.pgrad = None\n            else:  # TODO revisar abaixo\n                if p.pgrad.grad_fn is not None:\n                    p.pgrad.detach_()\n                else:\n                    p.pgrad.requires_grad_(False)\n                p.pgrad.zero_()\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict","title":"ModuleDict","text":"<p>               Bases: <code>Module</code>, <code>Mapping</code></p> <p>Holds submodules in a dictionary.</p> <p><code>msgflux.nn.ModuleDict</code> can be indexed like a regular Python dictionary, but modules it contains are properly registered, and will be visible by all <code>msgflux.nn.Module</code> methods.</p> <p><code>msgflux.nn.ModuleDict</code> is an ordered dictionary that respects:</p> <pre><code>* the order of insertion, and\n\n* in `msgflux.nn.ModuleDict.update` the order of the merged\n`OrderedDict`, `dict` (started from Python 3.6) or another\n`msgflux.nn.ModuleDict` (the argument to `msgflux.nn.ModuleDict.update`).\n</code></pre> <p>Note that <code>msgflux.nn.ModuleDict.update</code> with other unordered mapping types (e.g., Python's plain <code>dict</code> before Python version 3.6) does not preserve the order of the merged mapping.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>class ModuleDict(Module, container_abcs.Mapping):\n    \"\"\"Holds submodules in a dictionary.\n\n    `msgflux.nn.ModuleDict` can be indexed like a regular Python dictionary,\n    but modules it contains are properly registered, and will be visible by all\n    `msgflux.nn.Module` methods.\n\n    `msgflux.nn.ModuleDict` is an **ordered** dictionary that respects:\n\n        * the order of insertion, and\n\n        * in `msgflux.nn.ModuleDict.update` the order of the merged\n        `OrderedDict`, `dict` (started from Python 3.6) or another\n        `msgflux.nn.ModuleDict` (the argument to `msgflux.nn.ModuleDict.update`).\n\n    Note that `msgflux.nn.ModuleDict.update` with other unordered mapping\n    types (e.g., Python's plain `dict` before Python version 3.6) does not\n    preserve the order of the merged mapping.\n    \"\"\"\n\n    _modules: Dict[str, Module]  # type: ignore[assignment]\n\n    def __init__(self, modules: Optional[Mapping[str, Module]] = None) -&gt; None:\n        \"\"\"Args:\n            modules (iterable, optional): a mapping (dictionary) of (string: module)\n                or an iterable of key-value pairs of type (string, module).\n\n        !!! example\n            ```python\n            import random\n            import msgflux.nn as nn\n\n            class ExpertSales(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n                def forward(self, msg: str):\n                    return msg + self.response\n\n            class ExpertSupport(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.register_buffer(\"response\", \"Hi, call 190\")\n\n                def forward(self, msg: str):\n                    return msg + self.response\n\n            def draw_choice(choices: list[str]) -&gt; str:\n                return random.choice(choices)\n\n            class Router(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.choices = nn.ModuleDict({\n                        \"sales\": ExpertSales(),\n                        \"support\": ExpertSupport()\n                    })\n\n                def forward(self, msg: str) -&gt; str:\n                    choice = draw_choice(list(self.choices.keys()))\n                    msg = self.choices[choice](msg)\n                    return msg\n\n            router = Router()\n            router(\"I need help with my tv.\")\n            ```\n        \"\"\"\n        super().__init__()\n        if modules is not None:\n            self.update(modules)\n\n    def __getitem__(self, key: str) -&gt; Module:\n        return self._modules[key]\n\n    def __setitem__(self, key: str, module: Module) -&gt; None:\n        self.add_module(key, module)\n\n    def __delitem__(self, key: str) -&gt; None:\n        del self._modules[key]\n\n    def __len__(self) -&gt; int:\n        return len(self._modules)\n\n    def __iter__(self) -&gt; Iterator[str]:\n        return iter(self._modules)\n\n    def __contains__(self, key: str) -&gt; bool:\n        return key in self._modules\n\n    def clear(self) -&gt; None:\n        \"\"\"Remove all items from the ModuleDict.\"\"\"\n        self._modules.clear()\n\n    def pop(self, key: str) -&gt; Module:\n        \"\"\"Remove key from the ModuleDict and return its module.\n\n        Args:\n            key:\n                key to pop from the ModuleDict.\n        \"\"\"\n        v = self[key]\n        del self[key]\n        return v\n\n    def keys(self) -&gt; Iterable[str]:\n        \"\"\"Return an iterable of the ModuleDict keys.\"\"\"\n        return self._modules.keys()\n\n    def items(self) -&gt; Iterable[Tuple[str, Module]]:\n        \"\"\"Return an iterable of the ModuleDict key/value pairs.\"\"\"\n        return self._modules.items()\n\n    def values(self) -&gt; Iterable[Module]:\n        \"\"\"Return an iterable of the ModuleDict values.\"\"\"\n        return self._modules.values()\n\n    def update(self, modules: Mapping[str, Module]) -&gt; None:\n        \"\"\"Update the class **msgflux.nn.ModuleDict** with\n        key-value pairs from a mapping, overwriting existing keys.\n\n        !!! note\n\n            If `modules` is an `OrderedDict`, a `msgflux.nn.ModuleDict`, or\n            an iterable of key-value pairs, the order of new elements in\n            it is preserved.\n\n        Args:\n            modules:\n                A mapping (dictionary) from string to `msgflux.nn.Module`,\n                or an iterable of key-value pairs of type (string, `msgflux.nn.Module`).\n        \"\"\"\n        if not isinstance(modules, container_abcs.Iterable):\n            raise TypeError(\n                \"ModuleDict.update should be called with an \"\n                \"iterable of key/value pairs, but got \" + type(modules).__name__\n            )\n\n        if isinstance(modules, (OrderedDict, ModuleDict, container_abcs.Mapping)):\n            for key, module in modules.items():\n                self[key] = module\n        else:\n            # modules here can be a list with two items\n            for j, m in enumerate(modules):\n                if not isinstance(m, container_abcs.Iterable):\n                    raise TypeError(\n                        \"ModuleDict update sequence element \"\n                        \"#\" + str(j) + \" should be Iterable; is\" + type(m).__name__\n                    )\n                if not len(m) == 2:\n                    raise ValueError(\n                        \"ModuleDict update sequence element \"\n                        \"#\" + str(j) + \" has length \" + str(len(m)) + \"; 2 is required\"\n                    )\n                # modules can be Mapping (what it's typed at),\n                # or a list: [(name1, module1), (name2, module2)]\n                # that's too cumbersome to type correctly with overloads,\n                # so we add an ignore here\n                self[m[0]] = m[1]  # type: ignore[assignment]islice\n\n    def get(self, key: str, default: Optional[Module] = None) -&gt; Optional[Module]:\n        \"\"\"Return the module for the given key if it exists,\n        else return the default value.\n\n        Args:\n            key:\n                The key to look up in the ModuleDict.\n            default:\n                The value to return if the key is not found. Defaults to None.\n\n        Returns:\n            The module associated with the key, or the default\n            value if the key is not found.\n        \"\"\"\n        return self._modules.get(key, default)\n\n    def set(self, key: str, module: Module) -&gt; None:\n        \"\"\"Set a single key-value pair in the ModuleDict.\n\n        Args:\n            key:\n                The key to set in the ModuleDict.\n            module:\n                The module to associate with the key.\n\n        !!! note\n            This method registers the module using `add_module` to ensure\n            proper registration and preserves the order of insertion,\n            consistent with `ModuleDict` behavior.\n        \"\"\"\n        self.add_module(key, module)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.__contains__","title":"__contains__","text":"<pre><code>__contains__(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __contains__(self, key: str) -&gt; bool:\n    return key in self._modules\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __delitem__(self, key: str) -&gt; None:\n    del self._modules[key]\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Module:\n    return self._modules[key]\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.__init__","title":"__init__","text":"<pre><code>__init__(modules=None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>iterable</code> <p>a mapping (dictionary) of (string: module) or an iterable of key-value pairs of type (string, module).</p> <code>None</code> <p>Example</p> <pre><code>import random\nimport msgflux.nn as nn\n\nclass ExpertSales(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass ExpertSupport(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, call 190\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\ndef draw_choice(choices: list[str]) -&gt; str:\n    return random.choice(choices)\n\nclass Router(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.choices = nn.ModuleDict({\n            \"sales\": ExpertSales(),\n            \"support\": ExpertSupport()\n        })\n\n    def forward(self, msg: str) -&gt; str:\n        choice = draw_choice(list(self.choices.keys()))\n        msg = self.choices[choice](msg)\n        return msg\n\nrouter = Router()\nrouter(\"I need help with my tv.\")\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __init__(self, modules: Optional[Mapping[str, Module]] = None) -&gt; None:\n    \"\"\"Args:\n        modules (iterable, optional): a mapping (dictionary) of (string: module)\n            or an iterable of key-value pairs of type (string, module).\n\n    !!! example\n        ```python\n        import random\n        import msgflux.nn as nn\n\n        class ExpertSales(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        class ExpertSupport(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, call 190\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        def draw_choice(choices: list[str]) -&gt; str:\n            return random.choice(choices)\n\n        class Router(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.choices = nn.ModuleDict({\n                    \"sales\": ExpertSales(),\n                    \"support\": ExpertSupport()\n                })\n\n            def forward(self, msg: str) -&gt; str:\n                choice = draw_choice(list(self.choices.keys()))\n                msg = self.choices[choice](msg)\n                return msg\n\n        router = Router()\n        router(\"I need help with my tv.\")\n        ```\n    \"\"\"\n    super().__init__()\n    if modules is not None:\n        self.update(modules)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iter__(self) -&gt; Iterator[str]:\n    return iter(self._modules)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._modules)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key, module)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __setitem__(self, key: str, module: Module) -&gt; None:\n    self.add_module(key, module)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Remove all items from the ModuleDict.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Remove all items from the ModuleDict.\"\"\"\n    self._modules.clear()\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.get","title":"get","text":"<pre><code>get(key, default=None)\n</code></pre> <p>Return the module for the given key if it exists, else return the default value.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to look up in the ModuleDict.</p> required <code>default</code> <code>Optional[Module]</code> <p>The value to return if the key is not found. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Module]</code> <p>The module associated with the key, or the default</p> <code>Optional[Module]</code> <p>value if the key is not found.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def get(self, key: str, default: Optional[Module] = None) -&gt; Optional[Module]:\n    \"\"\"Return the module for the given key if it exists,\n    else return the default value.\n\n    Args:\n        key:\n            The key to look up in the ModuleDict.\n        default:\n            The value to return if the key is not found. Defaults to None.\n\n    Returns:\n        The module associated with the key, or the default\n        value if the key is not found.\n    \"\"\"\n    return self._modules.get(key, default)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.items","title":"items","text":"<pre><code>items()\n</code></pre> <p>Return an iterable of the ModuleDict key/value pairs.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def items(self) -&gt; Iterable[Tuple[str, Module]]:\n    \"\"\"Return an iterable of the ModuleDict key/value pairs.\"\"\"\n    return self._modules.items()\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.keys","title":"keys","text":"<pre><code>keys()\n</code></pre> <p>Return an iterable of the ModuleDict keys.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def keys(self) -&gt; Iterable[str]:\n    \"\"\"Return an iterable of the ModuleDict keys.\"\"\"\n    return self._modules.keys()\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.pop","title":"pop","text":"<pre><code>pop(key)\n</code></pre> <p>Remove key from the ModuleDict and return its module.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>key to pop from the ModuleDict.</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def pop(self, key: str) -&gt; Module:\n    \"\"\"Remove key from the ModuleDict and return its module.\n\n    Args:\n        key:\n            key to pop from the ModuleDict.\n    \"\"\"\n    v = self[key]\n    del self[key]\n    return v\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.set","title":"set","text":"<pre><code>set(key, module)\n</code></pre> <p>Set a single key-value pair in the ModuleDict.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key to set in the ModuleDict.</p> required <code>module</code> <code>Module</code> <p>The module to associate with the key.</p> required <p>Note</p> <p>This method registers the module using <code>add_module</code> to ensure proper registration and preserves the order of insertion, consistent with <code>ModuleDict</code> behavior.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def set(self, key: str, module: Module) -&gt; None:\n    \"\"\"Set a single key-value pair in the ModuleDict.\n\n    Args:\n        key:\n            The key to set in the ModuleDict.\n        module:\n            The module to associate with the key.\n\n    !!! note\n        This method registers the module using `add_module` to ensure\n        proper registration and preserves the order of insertion,\n        consistent with `ModuleDict` behavior.\n    \"\"\"\n    self.add_module(key, module)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.update","title":"update","text":"<pre><code>update(modules)\n</code></pre> <p>Update the class msgflux.nn.ModuleDict with key-value pairs from a mapping, overwriting existing keys.</p> <p>Note</p> <p>If <code>modules</code> is an <code>OrderedDict</code>, a <code>msgflux.nn.ModuleDict</code>, or an iterable of key-value pairs, the order of new elements in it is preserved.</p> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>Mapping[str, Module]</code> <p>A mapping (dictionary) from string to <code>msgflux.nn.Module</code>, or an iterable of key-value pairs of type (string, <code>msgflux.nn.Module</code>).</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def update(self, modules: Mapping[str, Module]) -&gt; None:\n    \"\"\"Update the class **msgflux.nn.ModuleDict** with\n    key-value pairs from a mapping, overwriting existing keys.\n\n    !!! note\n\n        If `modules` is an `OrderedDict`, a `msgflux.nn.ModuleDict`, or\n        an iterable of key-value pairs, the order of new elements in\n        it is preserved.\n\n    Args:\n        modules:\n            A mapping (dictionary) from string to `msgflux.nn.Module`,\n            or an iterable of key-value pairs of type (string, `msgflux.nn.Module`).\n    \"\"\"\n    if not isinstance(modules, container_abcs.Iterable):\n        raise TypeError(\n            \"ModuleDict.update should be called with an \"\n            \"iterable of key/value pairs, but got \" + type(modules).__name__\n        )\n\n    if isinstance(modules, (OrderedDict, ModuleDict, container_abcs.Mapping)):\n        for key, module in modules.items():\n            self[key] = module\n    else:\n        # modules here can be a list with two items\n        for j, m in enumerate(modules):\n            if not isinstance(m, container_abcs.Iterable):\n                raise TypeError(\n                    \"ModuleDict update sequence element \"\n                    \"#\" + str(j) + \" should be Iterable; is\" + type(m).__name__\n                )\n            if not len(m) == 2:\n                raise ValueError(\n                    \"ModuleDict update sequence element \"\n                    \"#\" + str(j) + \" has length \" + str(len(m)) + \"; 2 is required\"\n                )\n            # modules can be Mapping (what it's typed at),\n            # or a list: [(name1, module1), (name2, module2)]\n            # that's too cumbersome to type correctly with overloads,\n            # so we add an ignore here\n            self[m[0]] = m[1]  # type: ignore[assignment]islice\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleDict.values","title":"values","text":"<pre><code>values()\n</code></pre> <p>Return an iterable of the ModuleDict values.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def values(self) -&gt; Iterable[Module]:\n    \"\"\"Return an iterable of the ModuleDict values.\"\"\"\n    return self._modules.values()\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList","title":"ModuleList","text":"<p>               Bases: <code>Module</code></p> <p>Holds submodules in a list.</p> <p><code>msgflux.nn.ModuleList</code> can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all <code>msgflux.nn.Module</code> methods.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>class ModuleList(Module):\n    \"\"\"Holds submodules in a list.\n\n    `msgflux.nn.ModuleList` can be indexed like a regular Python list, but\n    modules it contains are properly registered, and will be visible by all\n    `msgflux.nn.Module` methods.\n    \"\"\"\n\n    _modules: Dict[str, Module]\n\n    def __init__(self, modules: Optional[Iterable[Module]] = None) -&gt; None:\n        \"\"\"Args:\n            modules (iterable, optional):\n                An iterable of modules to add.\n\n        !!! example\n\n            ```python\n\n            class ExpertSales(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n                def forward(self, msg: str):\n                    return msg + self.response\n\n            class ExpertSupport(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.register_buffer(\"response\", \"Hi, call 190\")\n\n                def forward(self, msg: str):\n                    return msg + self.response\n\n            class Expert(nn.Module):\n                def __init__(self):\n                    super().__init__()\n                    self.experts = nn.ModuleList([ExpertSales(), ExpertSupport()])\n\n                def forward(self, msg: str) -&gt; str:\n                    # ModuleList can act as an iterable, or be indexed using ints\n                    for i, l in enumerate(self.experts):\n                        msg = self.experts[i](msg)\n                    return msg\n\n            expert = Expert()\n            expert(\"I need help with my tv.\")\n            ```\n        \"\"\"\n        super().__init__()\n        if modules is not None:\n            self += modules\n\n    def _get_abs_string_index(self, idx):\n        \"\"\"Get the absolute index for the list of modules.\"\"\"\n        idx = operator.index(idx)\n        if not (-len(self) &lt;= idx &lt; len(self)):\n            raise IndexError(f\"index {idx} is out of range\")\n        if idx &lt; 0:\n            idx += len(self)\n        return str(idx)\n\n    def __getitem__(self, idx: Union[int, slice]) -&gt; Union[Module, \"ModuleList\"]:\n        if isinstance(idx, slice):\n            return self.__class__(list(self._modules.values())[idx])\n        else:\n            return self._modules[self._get_abs_string_index(idx)]\n\n    def __setitem__(self, idx: int, module: Module) -&gt; None:\n        idx = self._get_abs_string_index(idx)\n        return setattr(self, str(idx), module)\n\n    def __delitem__(self, idx: Union[int, slice]) -&gt; None:\n        if isinstance(idx, slice):\n            for k in range(len(self._modules))[idx]:\n                delattr(self, str(k))\n        else:\n            delattr(self, self._get_abs_string_index(idx))\n        # To preserve numbering, self._modules is being\n        # reconstructed with modules after deletion\n        str_indices = [str(i) for i in range(len(self._modules))]\n        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n\n    def __len__(self) -&gt; int:\n        return len(self._modules)\n\n    def __iter__(self) -&gt; Iterator[Module]:\n        return iter(self._modules.values())\n\n    def __iadd__(self, modules: Iterable[Module]) -&gt; Self:\n        return self.extend(modules)\n\n    def __add__(self, other: Iterable[Module]) -&gt; \"ModuleList\":\n        combined = ModuleList()\n        for i, module in enumerate(chain(self, other)):\n            combined.add_module(str(i), module)\n        return combined\n\n    def __repr__(self):\n        \"\"\"Return a custom repr for ModuleList that compresses\n        repeated module representations.\n        \"\"\"\n        list_of_reprs = [repr(item) for item in self]\n        if len(list_of_reprs) == 0:\n            return self._get_name() + \"()\"\n\n        start_end_indices = [[0, 0]]\n        repeated_blocks = [list_of_reprs[0]]\n        for i, r in enumerate(list_of_reprs[1:], 1):\n            if r == repeated_blocks[-1]:\n                start_end_indices[-1][1] += 1\n                continue\n\n            start_end_indices.append([i, i])\n            repeated_blocks.append(r)\n\n        lines = []\n        main_str = self._get_name() + \"(\"\n        for (start_id, end_id), b in zip(start_end_indices, repeated_blocks):\n            local_repr = f\"({start_id}): {b}\"  # default repr\n\n            if start_id != end_id:\n                n = end_id - start_id + 1\n                local_repr = f\"({start_id}-{end_id}): {n} x {b}\"\n\n            local_repr = _addindent(local_repr, 2)\n            lines.append(local_repr)\n\n        main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n        main_str += \")\"\n        return main_str\n\n    def __dir__(self):\n        keys = super().__dir__()\n        keys = [key for key in keys if not key.isdigit()]\n        return keys\n\n    def insert(self, index: int, module: Module) -&gt; None:\n        \"\"\"Insert a given module before a given index in the list.\n\n        Args:\n            index (int): index to insert.\n            module (nn.Module): module to insert\n        \"\"\"\n        for i in range(len(self._modules), index, -1):\n            self._modules[str(i)] = self._modules[str(i - 1)]\n        self._modules[str(index)] = module\n\n    def append(self, module: Module) -&gt; \"ModuleList\":\n        \"\"\"Append a given module to the end of the list.\n\n        Args:\n            module (nn.Module): module to append\n        \"\"\"\n        self.add_module(str(len(self)), module)\n        return self\n\n    def pop(self, key: Union[int, slice]) -&gt; Module:\n        v = self[key]\n        del self[key]\n        return v\n\n    def extend(self, modules: Iterable[Module]) -&gt; Self:\n        \"\"\"Append modules from a Python iterable to the end of the list.\n\n        Args:\n            modules (iterable): iterable of modules to append\n        \"\"\"\n        if not isinstance(modules, container_abcs.Iterable):\n            raise TypeError(\n                \"ModuleList.extend should be called with an \"\n                \"iterable, but got \" + type(modules).__name__\n            )\n        offset = len(self)\n        for i, module in enumerate(modules):\n            self.add_module(str(offset + i), module)\n        return self\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __add__(self, other: Iterable[Module]) -&gt; \"ModuleList\":\n    combined = ModuleList()\n    for i, module in enumerate(chain(self, other)):\n        combined.add_module(str(i), module)\n    return combined\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(idx)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __delitem__(self, idx: Union[int, slice]) -&gt; None:\n    if isinstance(idx, slice):\n        for k in range(len(self._modules))[idx]:\n            delattr(self, str(k))\n    else:\n        delattr(self, self._get_abs_string_index(idx))\n    # To preserve numbering, self._modules is being\n    # reconstructed with modules after deletion\n    str_indices = [str(i) for i in range(len(self._modules))]\n    self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList.__dir__","title":"__dir__","text":"<pre><code>__dir__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __dir__(self):\n    keys = super().__dir__()\n    keys = [key for key in keys if not key.isdigit()]\n    return keys\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __getitem__(self, idx: Union[int, slice]) -&gt; Union[Module, \"ModuleList\"]:\n    if isinstance(idx, slice):\n        return self.__class__(list(self._modules.values())[idx])\n    else:\n        return self._modules[self._get_abs_string_index(idx)]\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList.__iadd__","title":"__iadd__","text":"<pre><code>__iadd__(modules)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iadd__(self, modules: Iterable[Module]) -&gt; Self:\n    return self.extend(modules)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList.__init__","title":"__init__","text":"<pre><code>__init__(modules=None)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>iterable</code> <p>An iterable of modules to add.</p> <code>None</code> <p>Example</p> <pre><code>class ExpertSales(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass ExpertSupport(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, call 190\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass Expert(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.experts = nn.ModuleList([ExpertSales(), ExpertSupport()])\n\n    def forward(self, msg: str) -&gt; str:\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.experts):\n            msg = self.experts[i](msg)\n        return msg\n\nexpert = Expert()\nexpert(\"I need help with my tv.\")\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __init__(self, modules: Optional[Iterable[Module]] = None) -&gt; None:\n    \"\"\"Args:\n        modules (iterable, optional):\n            An iterable of modules to add.\n\n    !!! example\n\n        ```python\n\n        class ExpertSales(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        class ExpertSupport(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, call 190\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        class Expert(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.experts = nn.ModuleList([ExpertSales(), ExpertSupport()])\n\n            def forward(self, msg: str) -&gt; str:\n                # ModuleList can act as an iterable, or be indexed using ints\n                for i, l in enumerate(self.experts):\n                    msg = self.experts[i](msg)\n                return msg\n\n        expert = Expert()\n        expert(\"I need help with my tv.\")\n        ```\n    \"\"\"\n    super().__init__()\n    if modules is not None:\n        self += modules\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Module]:\n    return iter(self._modules.values())\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._modules)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Return a custom repr for ModuleList that compresses repeated module representations.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a custom repr for ModuleList that compresses\n    repeated module representations.\n    \"\"\"\n    list_of_reprs = [repr(item) for item in self]\n    if len(list_of_reprs) == 0:\n        return self._get_name() + \"()\"\n\n    start_end_indices = [[0, 0]]\n    repeated_blocks = [list_of_reprs[0]]\n    for i, r in enumerate(list_of_reprs[1:], 1):\n        if r == repeated_blocks[-1]:\n            start_end_indices[-1][1] += 1\n            continue\n\n        start_end_indices.append([i, i])\n        repeated_blocks.append(r)\n\n    lines = []\n    main_str = self._get_name() + \"(\"\n    for (start_id, end_id), b in zip(start_end_indices, repeated_blocks):\n        local_repr = f\"({start_id}): {b}\"  # default repr\n\n        if start_id != end_id:\n            n = end_id - start_id + 1\n            local_repr = f\"({start_id}-{end_id}): {n} x {b}\"\n\n        local_repr = _addindent(local_repr, 2)\n        lines.append(local_repr)\n\n    main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n    main_str += \")\"\n    return main_str\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(idx, module)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __setitem__(self, idx: int, module: Module) -&gt; None:\n    idx = self._get_abs_string_index(idx)\n    return setattr(self, str(idx), module)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList.append","title":"append","text":"<pre><code>append(module)\n</code></pre> <p>Append a given module to the end of the list.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module to append</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def append(self, module: Module) -&gt; \"ModuleList\":\n    \"\"\"Append a given module to the end of the list.\n\n    Args:\n        module (nn.Module): module to append\n    \"\"\"\n    self.add_module(str(len(self)), module)\n    return self\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList.extend","title":"extend","text":"<pre><code>extend(modules)\n</code></pre> <p>Append modules from a Python iterable to the end of the list.</p> <p>Parameters:</p> Name Type Description Default <code>modules</code> <code>iterable</code> <p>iterable of modules to append</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def extend(self, modules: Iterable[Module]) -&gt; Self:\n    \"\"\"Append modules from a Python iterable to the end of the list.\n\n    Args:\n        modules (iterable): iterable of modules to append\n    \"\"\"\n    if not isinstance(modules, container_abcs.Iterable):\n        raise TypeError(\n            \"ModuleList.extend should be called with an \"\n            \"iterable, but got \" + type(modules).__name__\n        )\n    offset = len(self)\n    for i, module in enumerate(modules):\n        self.add_module(str(offset + i), module)\n    return self\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList.insert","title":"insert","text":"<pre><code>insert(index, module)\n</code></pre> <p>Insert a given module before a given index in the list.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>index to insert.</p> required <code>module</code> <code>Module</code> <p>module to insert</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def insert(self, index: int, module: Module) -&gt; None:\n    \"\"\"Insert a given module before a given index in the list.\n\n    Args:\n        index (int): index to insert.\n        module (nn.Module): module to insert\n    \"\"\"\n    for i in range(len(self._modules), index, -1):\n        self._modules[str(i)] = self._modules[str(i - 1)]\n    self._modules[str(index)] = module\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.ModuleList.pop","title":"pop","text":"<pre><code>pop(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def pop(self, key: Union[int, slice]) -&gt; Module:\n    v = self[key]\n    del self[key]\n    return v\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential","title":"Sequential","text":"<p>               Bases: <code>Module</code></p> <p>A sequential container.</p> <p>Modules will be added to it in the order they are passed in the constructor. Alternatively, an <code>OrderedDict</code> of modules can be passed in. The <code>forward()</code> method of <code>Sequential</code> accepts any input and forwards it to the first module it contains. It then \"chains\" outputs to inputs sequentially for each subsequent module, finally returning the output of the last module.</p> <p>The value a <code>Sequential</code> provides over manually calling a sequence of modules is that it allows treating the whole container as a single module, such that performing a transformation on the <code>Sequential</code> applies to each of the modules it stores (which are each a registered submodule of the <code>Sequential</code>).</p> <p>What's the difference between a <code>Sequential</code> and a <code>msgflux.nn.ModuleList</code>? A <code>ModuleList</code> is exactly what it sounds like--a list for storing <code>Module</code>s! On the other hand, the layers in a <code>Sequential</code> are connected in a cascading way.</p> <p>Example</p> <pre><code>from collections import OrderedDict\nimport msgflux.nn as nn\nclass ExpertSales(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\nclass ExpertSupport(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"response\", \"Hi, call 190\")\n\n    def forward(self, msg: str):\n        return msg + self.response\n\n# Using Sequential to create a small workflow. When **expert** is run,\n# input will first be passed to **ExpertSales**. The output of\n# **ExpertSales** will be used as the input to the first\n# **ExpertSupport**; Finally, the output of\n# **ExpertSupport** will be the experts response.\nexperts = nn.Sequential(ExpertSales(), ExpertSupport())\nexperts(\"I need help with my tv.\")\n\n# Using Sequential with OrderedDict. This is functionally the\n# same as the above code\nexperts_dict = nn.Sequential(OrderedDict([\n    (\"expert_sales\", ExpertSales()),\n    (\"expert_support\", ExpertSupport())\n]))\nexperts_dict(\"I need help with my tv.\")\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>class Sequential(Module):\n    \"\"\"A sequential container.\n\n    Modules will be added to it in the order they are passed in the\n    constructor. Alternatively, an `OrderedDict` of modules can be\n    passed in. The `forward()` method of `Sequential` accepts any\n    input and forwards it to the first module it contains. It then\n    \"chains\" outputs to inputs sequentially for each subsequent module,\n    finally returning the output of the last module.\n\n    The value a `Sequential` provides over manually calling a sequence\n    of modules is that it allows treating the whole container as a\n    single module, such that performing a transformation on the\n    `Sequential` applies to each of the modules it stores (which are\n    each a registered submodule of the `Sequential`).\n\n    What's the difference between a `Sequential` and a\n    `msgflux.nn.ModuleList`? A `ModuleList` is exactly what it\n    sounds like--a list for storing `Module`s! On the other hand,\n    the layers in a `Sequential` are connected in a cascading way.\n\n    !!! example\n        ```python\n        from collections import OrderedDict\n        import msgflux.nn as nn\n        class ExpertSales(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, let's talk?\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        class ExpertSupport(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.register_buffer(\"response\", \"Hi, call 190\")\n\n            def forward(self, msg: str):\n                return msg + self.response\n\n        # Using Sequential to create a small workflow. When **expert** is run,\n        # input will first be passed to **ExpertSales**. The output of\n        # **ExpertSales** will be used as the input to the first\n        # **ExpertSupport**; Finally, the output of\n        # **ExpertSupport** will be the experts response.\n        experts = nn.Sequential(ExpertSales(), ExpertSupport())\n        experts(\"I need help with my tv.\")\n\n        # Using Sequential with OrderedDict. This is functionally the\n        # same as the above code\n        experts_dict = nn.Sequential(OrderedDict([\n            (\"expert_sales\", ExpertSales()),\n            (\"expert_support\", ExpertSupport())\n        ]))\n        experts_dict(\"I need help with my tv.\")\n        ```\n    \"\"\"\n\n    _modules: Dict[str, Module] = OrderedDict()\n\n    def __init__(self, *args: Union[Module, OrderedDict[str, Module]]):\n        super().__init__()\n        if len(args) == 1 and isinstance(args[0], OrderedDict):\n            for key, module in args[0].items():\n                self.add_module(key, module)\n        else:\n            for idx, module in enumerate(args):\n                self.add_module(str(idx), module)\n\n    def forward(self, *args, **kwargs) -&gt; Any:\n        modules_iter = iter(self._modules.values())\n        first_module = next(modules_iter)\n        output = first_module(*args, **kwargs)\n\n        for module in modules_iter:\n            output = module(output)\n\n        return output\n\n    async def aforward(self, *args, **kwargs) -&gt; Any:\n        \"\"\"Async version of forward. Executes modules sequentially with async\n        support.\n        \"\"\"\n        modules_iter = iter(self._modules.values())\n        first_module = next(modules_iter)\n\n        # Check for acall method first, then coroutine function\n        if hasattr(first_module, \"acall\"):\n            output = await first_module.acall(*args, **kwargs)\n        elif asyncio.iscoroutinefunction(first_module):\n            output = await first_module(*args, **kwargs)\n        else:\n            # Fallback to sync call\n            output = first_module(*args, **kwargs)\n\n        for module in modules_iter:\n            # Check for acall method first, then coroutine function\n            if hasattr(module, \"acall\"):\n                output = await module.acall(output)\n            elif asyncio.iscoroutinefunction(module):\n                output = await module(output)\n            else:\n                # Fallback to sync call\n                output = module(output)\n\n        return output\n\n    def _get_mermaid(\n        self,\n        title: Optional[str] = None,\n        orientation: Optional[str] = \"TD\",\n    ) -&gt; str:\n        mermaid_code = [\n            \"%%{\",\n            \"        init: {\",\n            \"            'theme': 'base',\",\n            \"            'themeVariables': {\",\n            \"            'primaryColor': '#E9E7E7',\",\n            \"            'primaryTextColor': '#000000',\",\n            \"            'primaryBorderColor': '#C0000',\",\n            \"            'lineColor': '#F8B229',\",\n            \"            'secondaryColor': '#91939C',\",\n            \"            'tertiaryColor': '#fff'\",\n            \"            }\",\n            \"        }\",\n            \"    }%%\",\n            f\"flowchart {orientation}\",\n        ]\n\n        if title:\n            mermaid_code.insert(0, f\"%% Title: {title}\")\n\n        mermaid_code.append(\"subgraph PARAMETERS\")\n        mermaid_code.append(\"direction LR\")\n        mermaid_code.append(\"param_msg([**msg**])\")\n        mermaid_code.append(\"end\")\n\n        first_node = None\n        for i, module_name in enumerate(self._modules.keys()):\n            node_id = f\"node_{i}\"\n            if i == 0:\n                first_node = node_id\n            mermaid_code.append(f\"{node_id}[ **msg = {module_name}\ufe59msg\ufe5a** ]\")\n\n        for i in range(len(self._modules) - 1):\n            mermaid_code.append(f\"node_{i} --&gt; node_{i + 1}\")\n\n        last_node = f\"node_{len(self._modules) - 1}\"\n        mermaid_code.append(f\"{last_node} --&gt; node_return\")\n        mermaid_code.append(\"node_return([**return msg**])\")\n\n        if first_node:\n            mermaid_code.append(f\"PARAMETERS --&gt; {first_node}\")\n\n        mermaid_code.append(\"%% Styles\")\n        mermaid_code.append(\n            \"classDef terminal fill:#FFF4DD,stroke:#333,stroke-width:2px;\"\n        )\n        mermaid_code.append(\n            \"classDef parameter fill:#FFF4DD,stroke:#333,stroke-width:px;\"\n        )\n\n        for i in range(len(self._modules)):\n            mermaid_code.append(f\"class node_{i} default;\")\n        mermaid_code.append(\"class node_return terminal;\")\n\n        mermaid_code.append(\"class param_msg parameter;\")\n\n        return \"\\n\".join(mermaid_code)\n\n    def _get_item_by_idx(self, iterator, idx) -&gt; T:\n        \"\"\"Get the idx-th item of the iterator.\"\"\"\n        size = len(self)\n        idx = operator.index(idx)\n        if not -size &lt;= idx &lt; size:\n            raise IndexError(f\"index {idx} is out of range\")\n        idx %= size\n        return next(islice(iterator, idx, None))\n\n    def __getitem__(self, idx: Union[slice, int]) -&gt; Union[\"Sequential\", T]:\n        if isinstance(idx, slice):\n            return self.__class__(OrderedDict(list(self._modules.items())[idx]))\n        else:\n            return self._get_item_by_idx(self._modules.values(), idx)\n\n    def __setitem__(self, idx: int, module: Module) -&gt; None:\n        key: str = self._get_item_by_idx(self._modules.keys(), idx)\n        return setattr(self, key, module)\n\n    def __delitem__(self, idx: Union[slice, int]) -&gt; None:\n        if isinstance(idx, slice):\n            for key in list(self._modules.keys())[idx]:\n                delattr(self, key)\n        else:\n            key = self._get_item_by_idx(self._modules.keys(), idx)\n            delattr(self, key)\n        # To preserve numbering\n        str_indices = [str(i) for i in range(len(self._modules))]\n        self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n\n    def __len__(self) -&gt; int:\n        return len(self._modules)\n\n    def __add__(self, other) -&gt; \"Sequential\":\n        if isinstance(other, Sequential):\n            ret = Sequential()\n            for layer in self:\n                ret.append(layer)\n            for layer in other:\n                ret.append(layer)\n            return ret\n        else:\n            raise ValueError(\n                \"add operator supports only objects \"\n                f\"of Sequential class, but {type(other)!s} is given.\"\n            )\n\n    def pop(self, key: Union[int, slice]) -&gt; Module:\n        v = self[key]\n        del self[key]\n        return v\n\n    def __iadd__(self, other) -&gt; Self:\n        if isinstance(other, Sequential):\n            offset = len(self)\n            for i, module in enumerate(other):\n                self.add_module(str(i + offset), module)\n            return self\n        else:\n            raise ValueError(\n                \"add operator supports only objects \"\n                f\"of Sequential class, but {type(other)!s} is given.\"\n            )\n\n    def __dir__(self):\n        keys = super().__dir__()\n        keys = [key for key in keys if not key.isdigit()]\n        return keys\n\n    def __iter__(self) -&gt; Iterator[Module]:\n        return iter(self._modules.values())\n\n    def append(self, module: Module) -&gt; \"Sequential\":\n        r\"\"\"Append a given module to the end.\n\n        Args:\n            module (nn.Module): module to append\n        \"\"\"\n        self.add_module(str(len(self)), module)\n        return self\n\n    def insert(self, index: int, module: Module) -&gt; \"Sequential\":\n        if not isinstance(module, Module):\n            raise AssertionError(f\"module should be of type: {Module}\")\n        n = len(self._modules)\n        if not (-n &lt;= index &lt;= n):\n            raise IndexError(f\"Index out of range: {index}\")\n        if index &lt; 0:\n            index += n\n        for i in range(n, index, -1):\n            self._modules[str(i)] = self._modules[str(i - 1)]\n        self._modules[str(index)] = module\n        return self\n\n    def extend(self, sequential) -&gt; \"Sequential\":\n        for layer in sequential:\n            self.append(layer)\n        return self\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.__add__","title":"__add__","text":"<pre><code>__add__(other)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __add__(self, other) -&gt; \"Sequential\":\n    if isinstance(other, Sequential):\n        ret = Sequential()\n        for layer in self:\n            ret.append(layer)\n        for layer in other:\n            ret.append(layer)\n        return ret\n    else:\n        raise ValueError(\n            \"add operator supports only objects \"\n            f\"of Sequential class, but {type(other)!s} is given.\"\n        )\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(idx)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __delitem__(self, idx: Union[slice, int]) -&gt; None:\n    if isinstance(idx, slice):\n        for key in list(self._modules.keys())[idx]:\n            delattr(self, key)\n    else:\n        key = self._get_item_by_idx(self._modules.keys(), idx)\n        delattr(self, key)\n    # To preserve numbering\n    str_indices = [str(i) for i in range(len(self._modules))]\n    self._modules = OrderedDict(list(zip(str_indices, self._modules.values())))\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.__dir__","title":"__dir__","text":"<pre><code>__dir__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __dir__(self):\n    keys = super().__dir__()\n    keys = [key for key in keys if not key.isdigit()]\n    return keys\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(idx)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __getitem__(self, idx: Union[slice, int]) -&gt; Union[\"Sequential\", T]:\n    if isinstance(idx, slice):\n        return self.__class__(OrderedDict(list(self._modules.items())[idx]))\n    else:\n        return self._get_item_by_idx(self._modules.values(), idx)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.__iadd__","title":"__iadd__","text":"<pre><code>__iadd__(other)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iadd__(self, other) -&gt; Self:\n    if isinstance(other, Sequential):\n        offset = len(self)\n        for i, module in enumerate(other):\n            self.add_module(str(i + offset), module)\n        return self\n    else:\n        raise ValueError(\n            \"add operator supports only objects \"\n            f\"of Sequential class, but {type(other)!s} is given.\"\n        )\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.__init__","title":"__init__","text":"<pre><code>__init__(*args)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __init__(self, *args: Union[Module, OrderedDict[str, Module]]):\n    super().__init__()\n    if len(args) == 1 and isinstance(args[0], OrderedDict):\n        for key, module in args[0].items():\n            self.add_module(key, module)\n    else:\n        for idx, module in enumerate(args):\n            self.add_module(str(idx), module)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Module]:\n    return iter(self._modules.values())\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __len__(self) -&gt; int:\n    return len(self._modules)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(idx, module)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def __setitem__(self, idx: int, module: Module) -&gt; None:\n    key: str = self._get_item_by_idx(self._modules.keys(), idx)\n    return setattr(self, key, module)\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.aforward","title":"aforward  <code>async</code>","text":"<pre><code>aforward(*args, **kwargs)\n</code></pre> <p>Async version of forward. Executes modules sequentially with async support.</p> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>async def aforward(self, *args, **kwargs) -&gt; Any:\n    \"\"\"Async version of forward. Executes modules sequentially with async\n    support.\n    \"\"\"\n    modules_iter = iter(self._modules.values())\n    first_module = next(modules_iter)\n\n    # Check for acall method first, then coroutine function\n    if hasattr(first_module, \"acall\"):\n        output = await first_module.acall(*args, **kwargs)\n    elif asyncio.iscoroutinefunction(first_module):\n        output = await first_module(*args, **kwargs)\n    else:\n        # Fallback to sync call\n        output = first_module(*args, **kwargs)\n\n    for module in modules_iter:\n        # Check for acall method first, then coroutine function\n        if hasattr(module, \"acall\"):\n            output = await module.acall(output)\n        elif asyncio.iscoroutinefunction(module):\n            output = await module(output)\n        else:\n            # Fallback to sync call\n            output = module(output)\n\n    return output\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.append","title":"append","text":"<pre><code>append(module)\n</code></pre> <p>Append a given module to the end.</p> <p>Parameters:</p> Name Type Description Default <code>module</code> <code>Module</code> <p>module to append</p> required Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def append(self, module: Module) -&gt; \"Sequential\":\n    r\"\"\"Append a given module to the end.\n\n    Args:\n        module (nn.Module): module to append\n    \"\"\"\n    self.add_module(str(len(self)), module)\n    return self\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.extend","title":"extend","text":"<pre><code>extend(sequential)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def extend(self, sequential) -&gt; \"Sequential\":\n    for layer in sequential:\n        self.append(layer)\n    return self\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.forward","title":"forward","text":"<pre><code>forward(*args, **kwargs)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def forward(self, *args, **kwargs) -&gt; Any:\n    modules_iter = iter(self._modules.values())\n    first_module = next(modules_iter)\n    output = first_module(*args, **kwargs)\n\n    for module in modules_iter:\n        output = module(output)\n\n    return output\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.insert","title":"insert","text":"<pre><code>insert(index, module)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def insert(self, index: int, module: Module) -&gt; \"Sequential\":\n    if not isinstance(module, Module):\n        raise AssertionError(f\"module should be of type: {Module}\")\n    n = len(self._modules)\n    if not (-n &lt;= index &lt;= n):\n        raise IndexError(f\"Index out of range: {index}\")\n    if index &lt; 0:\n        index += n\n    for i in range(n, index, -1):\n        self._modules[str(i)] = self._modules[str(i - 1)]\n    self._modules[str(index)] = module\n    return self\n</code></pre>"},{"location":"learn/nn/module/#msgflux.nn.Sequential.pop","title":"pop","text":"<pre><code>pop(key)\n</code></pre> Source code in <code>src/msgflux/nn/modules/container.py</code> <pre><code>def pop(self, key: Union[int, slice]) -&gt; Module:\n    v = self[key]\n    del self[key]\n    return v\n</code></pre>"},{"location":"learn/nn/predictor/","title":"Predictor: ML Model Wrapper Module","text":"<p>The <code>Predictor</code> module provides a unified interface for wrapping machine learning models (classifiers, regressors, detectors, segmenters) into msgflux workflows.</p> <p>All code examples use the recommended import pattern:</p> <pre><code>import msgflux as mf\n</code></pre>"},{"location":"learn/nn/predictor/#quick-start","title":"Quick Start","text":""},{"location":"learn/nn/predictor/#traditional-initialization","title":"Traditional Initialization","text":"<pre><code>import msgflux as mf\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\n# Train a scikit-learn model\nX_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny_train = np.array([0, 0, 1, 1])\n\nrf_model = RandomForestClassifier()\nrf_model.fit(X_train, y_train)\n\n# Wrap in msgflux BaseModel (example - you'd create custom wrapper)\nwrapped_model = CustomSklearnModel(rf_model)\n\n# Traditional predictor initialization\npredictor = mf.nn.Predictor(\n    model=wrapped_model,\n    config={\"return_probabilities\": True}\n)\n\n# Make predictions\nresult = predictor([[2, 3]])\nprint(result)\n</code></pre>"},{"location":"learn/nn/predictor/#autoparams-initialization-recommended","title":"AutoParams Initialization (Recommended)","text":"<p>This is the preferred and recommended way to define predictors in msgflux.</p> <pre><code>import msgflux as mf\n\nclass SentimentPredictor(mf.nn.Predictor):\n    \"\"\"Predicts sentiment (positive/negative) from text.\"\"\"\n\n    # AutoParams automatically uses class name as 'name'\n    response_mode = \"plain_response\"\n\n# Create or load your ML model\nsentiment_model = load_sentiment_model()\n\n# Create predictor with defaults\npredictor = SentimentPredictor(\n    model=sentiment_model,\n    config={\"threshold\": 0.5}\n)\n\n# Predict\ntext = \"This product is amazing!\"\nsentiment = predictor(text)\nprint(sentiment)  # \"positive\"\n\n# Create variant with different threshold\nstrict_predictor = SentimentPredictor(\n    model=sentiment_model,\n    config={\"threshold\": 0.7}  # Higher confidence required\n)\n</code></pre>"},{"location":"learn/nn/predictor/#why-use-autoparams","title":"Why Use AutoParams?","text":"<ol> <li>Model-Specific Predictors: Create specialized predictors for different ML tasks</li> <li>Reusable Configuration: Share model configuration across predictions</li> <li>Clear Purpose: Class name and docstring document predictor's task</li> <li>Easy Variants: Create predictor variations with different thresholds/parameters</li> <li>Better Organization: Group predictors by ML task or domain</li> </ol>"},{"location":"learn/nn/predictor/#use-cases","title":"Use Cases","text":""},{"location":"learn/nn/predictor/#1-text-classification","title":"1. Text Classification","text":"<pre><code>import msgflux as mf\n\nclass SpamClassifier(mf.nn.Predictor):\n    \"\"\"Classifies emails as spam or not spam.\"\"\"\n\n    response_mode = \"plain_response\"\n\nclass TopicClassifier(mf.nn.Predictor):\n    \"\"\"Classifies documents into topic categories.\"\"\"\n\n    response_mode = \"plain_response\"\n\n# Load pre-trained models\nspam_model = load_model(\"spam_classifier.pkl\")\ntopic_model = load_model(\"topic_classifier.pkl\")\n\n# Create predictors\nspam_detector = SpamClassifier(\n    model=spam_model,\n    config={\"threshold\": 0.8}  # High confidence for spam\n)\n\ntopic_detector = TopicClassifier(\n    model=topic_model,\n    config={\"top_k\": 3}  # Return top 3 topics\n)\n\n# Use\nemail_text = \"Congratulations! You've won $1,000,000!\"\nis_spam = spam_detector(email_text)\n\narticle_text = \"Recent advances in quantum computing have...\"\ntopics = topic_detector(article_text)\n</code></pre>"},{"location":"learn/nn/predictor/#2-image-classification","title":"2. Image Classification","text":"<pre><code>import msgflux as mf\n\nclass ObjectDetector(mf.nn.Predictor):\n    \"\"\"Detects objects in images.\"\"\"\n\n    response_mode = \"plain_response\"\n\nclass ImageClassifier(mf.nn.Predictor):\n    \"\"\"Classifies images into categories.\"\"\"\n\n    response_mode = \"plain_response\"\n\n# Load computer vision models\ndetector_model = load_model(\"yolo_detector.pt\")\nclassifier_model = load_model(\"resnet_classifier.pt\")\n\n# Create predictors\ndetector = ObjectDetector(\n    model=detector_model,\n    config={\"confidence\": 0.5, \"iou_threshold\": 0.45}\n)\n\nclassifier = ImageClassifier(\n    model=classifier_model,\n    config={\"top_k\": 5}\n)\n\n# Use\nimage_path = \"photo.jpg\"\nobjects = detector(image_path)\ncategories = classifier(image_path)\n</code></pre>"},{"location":"learn/nn/predictor/#3-regression","title":"3. Regression","text":"<pre><code>import msgflux as mf\n\nclass PricePredictor(mf.nn.Predictor):\n    \"\"\"Predicts house prices based on features.\"\"\"\n\n    response_mode = \"plain_response\"\n\nclass DemandForecaster(mf.nn.Predictor):\n    \"\"\"Forecasts product demand.\"\"\"\n\n    response_mode = \"plain_response\"\n\n# Load regression models\nprice_model = load_model(\"price_regressor.pkl\")\ndemand_model = load_model(\"demand_forecaster.pkl\")\n\n# Create predictors\nprice_predictor = PricePredictor(model=price_model)\ndemand_forecaster = DemandForecaster(model=demand_model)\n\n# Use\nhouse_features = {\"sqft\": 2000, \"bedrooms\": 3, \"location\": \"downtown\"}\npredicted_price = price_predictor(house_features)\n\nproduct_features = {\"day_of_week\": 5, \"promotions\": True, \"weather\": \"sunny\"}\npredicted_demand = demand_forecaster(product_features)\n</code></pre>"},{"location":"learn/nn/predictor/#4-time-series-analysis","title":"4. Time Series Analysis","text":"<pre><code>import msgflux as mf\n\nclass StockPredictor(mf.nn.Predictor):\n    \"\"\"Predicts stock price movements.\"\"\"\n\n    response_mode = \"plain_response\"\n\nclass AnomalyDetector(mf.nn.Predictor):\n    \"\"\"Detects anomalies in time series data.\"\"\"\n\n    response_mode = \"plain_response\"\n\n# Load time series models\nstock_model = load_model(\"lstm_stock.pt\")\nanomaly_model = load_model(\"isolation_forest.pkl\")\n\n# Create predictors\nstock_predictor = StockPredictor(\n    model=stock_model,\n    config={\"horizon\": 5}  # Predict 5 days ahead\n)\n\nanomaly_detector = AnomalyDetector(\n    model=anomaly_model,\n    config={\"contamination\": 0.1}\n)\n\n# Use\nhistorical_prices = [100, 102, 101, 103, 105]\nfuture_prices = stock_predictor(historical_prices)\n\nsensor_data = get_sensor_readings()\nanomalies = anomaly_detector(sensor_data)\n</code></pre>"},{"location":"learn/nn/predictor/#message-field-mapping","title":"Message Field Mapping","text":"<p>Use Message objects for structured input/output:</p> <pre><code>import msgflux as mf\n\nclass StructuredPredictor(mf.nn.Predictor):\n    \"\"\"Predictor that processes structured message inputs.\"\"\"\n\n    response_mode = \"message\"\n\nmodel = load_model(\"classifier.pkl\")\n\npredictor = StructuredPredictor(\n    model=model,\n    message_fields={\n        \"task_inputs\": \"features.data\"\n    }\n)\n\n# Create message with features\nmsg = mf.Message()\nmsg.set(\"features.data\", [1.5, 2.3, 0.8, 4.1])\nmsg.set(\"metadata.id\", \"prediction_123\")\n\n# Predict and get result in message\nresult_msg = predictor(msg)\nprediction = result_msg.get(\"predictor.result\")\nmetadata_id = result_msg.get(\"metadata.id\")\n\nprint(f\"Prediction {metadata_id}: {prediction}\")\n</code></pre>"},{"location":"learn/nn/predictor/#response-templates","title":"Response Templates","text":"<p>Format predictions using Jinja templates:</p> <pre><code>import msgflux as mf\n\nclass FormattedPredictor(mf.nn.Predictor):\n    \"\"\"Predictor with custom output formatting.\"\"\"\n\n    response_mode = \"plain_response\"\n\nmodel = load_model(\"sentiment_model.pkl\")\n\npredictor = FormattedPredictor(\n    model=model,\n    response_template=\"\"\"\n    Sentiment Analysis Result:\n    - Sentiment: {{ prediction }}\n    - Confidence: {{ confidence }}%\n    - Model: {{ model_name }}\n    \"\"\"\n)\n\nformatted_result = predictor(\"This is great!\")\nprint(formatted_result)\n</code></pre>"},{"location":"learn/nn/predictor/#creating-predictor-hierarchies","title":"Creating Predictor Hierarchies","text":"<p>Build specialized predictors through inheritance:</p> <pre><code>import msgflux as mf\n\n# Base predictor for all classifiers\nclass BaseClassifier(mf.nn.Predictor):\n    \"\"\"Base predictor for classification tasks.\"\"\"\n\n    response_mode = \"plain_response\"\n\n# Binary classifier\nclass BinaryClassifier(BaseClassifier):\n    \"\"\"Binary classification (yes/no, spam/ham, etc.).\"\"\"\n\n    # Inherits response_mode from BaseClassifier\n\n# Multi-class classifier\nclass MultiClassClassifier(BaseClassifier):\n    \"\"\"Multi-class classification.\"\"\"\n\n    # Different configuration for multi-class\n\n# Load models\nbinary_model = load_model(\"binary_classifier.pkl\")\nmulticlass_model = load_model(\"multiclass_classifier.pkl\")\n\n# Create instances\nbinary = BinaryClassifier(\n    model=binary_model,\n    config={\"threshold\": 0.5}\n)\n\nmulticlass = MultiClassClassifier(\n    model=multiclass_model,\n    config={\"top_k\": 1}  # Return top class only\n)\n\n# Use appropriately\nis_fraudulent = binary(\"Transaction details...\")\ndocument_category = multiclass(\"Document text...\")\n</code></pre>"},{"location":"learn/nn/predictor/#integration-with-agents","title":"Integration with Agents","text":"<p>Predictors can be used as tools for agents:</p> <pre><code>import msgflux as mf\n\n# Define predictor\nclass SentimentAnalyzer(mf.nn.Predictor):\n    \"\"\"Analyzes sentiment of text.\"\"\"\n\n    response_mode = \"plain_response\"\n\nsentiment_model = load_model(\"sentiment.pkl\")\n\nanalyzer = SentimentAnalyzer(model=sentiment_model)\n\n# Define predictor as a tool function\ndef analyze_sentiment(text: str) -&gt; str:\n    \"\"\"Analyze the sentiment of the given text.\"\"\"\n    result = analyzer(text)\n    return result\n\n# Create agent with predictor tool\nclass CustomerSupportAgent(mf.nn.Agent):\n    \"\"\"Customer support agent with sentiment analysis.\"\"\"\n\n    temperature = 0.7\n    max_tokens = 2000\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4\")\n\nsupport_agent = CustomerSupportAgent(\n    model=model,\n    tools=[analyze_sentiment]\n)\n\n# Agent can now use predictor to analyze customer sentiment\nresponse = support_agent(\"\"\"\nCustomer message: \"I'm very disappointed with your service.\"\nAnalyze sentiment and respond appropriately.\n\"\"\")\nprint(response)\n</code></pre>"},{"location":"learn/nn/predictor/#batch-prediction","title":"Batch Prediction","text":"<p>Make predictions on multiple inputs efficiently:</p> <pre><code>import msgflux as mf\nimport asyncio\n\nclass BatchPredictor(mf.nn.Predictor):\n    \"\"\"Predictor for batch processing.\"\"\"\n\n    response_mode = \"plain_response\"\n\nmodel = load_model(\"classifier.pkl\")\n\npredictor = BatchPredictor(model=model)\n\nasync def predict_batch(inputs):\n    \"\"\"Predict on multiple inputs concurrently.\"\"\"\n    tasks = [predictor.aforward(inp) for inp in inputs]\n    return await asyncio.gather(*tasks)\n\n# Batch predict\ninputs = [\"text 1\", \"text 2\", \"text 3\", \"text 4\"]\nresults = asyncio.run(predict_batch(inputs))\n\nfor inp, result in zip(inputs, results):\n    print(f\"{inp}: {result}\")\n</code></pre>"},{"location":"learn/nn/predictor/#model-gateway-support","title":"Model Gateway Support","text":"<p>Use multiple models with fallback:</p> <pre><code>import msgflux as mf\n\nclass EnsemblePredictor(mf.nn.Predictor):\n    \"\"\"Predictor using multiple models for ensemble.\"\"\"\n\n    response_mode = \"plain_response\"\n\n# Create model gateway with multiple models\ngateway = mf.ModelGateway(\n    models={\n        \"primary\": load_model(\"model_v2.pkl\"),\n        \"fallback\": load_model(\"model_v1.pkl\")\n    }\n)\n\npredictor = EnsemblePredictor(\n    model=gateway,\n    message_fields={\n        \"model_preference\": \"config.preferred_model\"\n    }\n)\n\n# Predict with model preference\nmsg = mf.Message()\nmsg.set(\"features\", [1, 2, 3])\nmsg.set(\"config.preferred_model\", \"primary\")\n\nresult = predictor(msg)\n</code></pre>"},{"location":"learn/nn/predictor/#configuration-options","title":"Configuration Options","text":""},{"location":"learn/nn/predictor/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<pre><code>import msgflux as mf\n\nclass FullyConfiguredPredictor(mf.nn.Predictor):\n    \"\"\"Predictor with all configuration options.\"\"\"\n\n    # Response behavior\n    response_mode = \"plain_response\"  # or \"message\"\n\nmodel = load_model(\"model.pkl\")\n\npredictor = FullyConfiguredPredictor(\n    model=model,                         # Required: ML model\n    message_fields={                     # Optional: Message field mapping\n        \"task_inputs\": \"data.features\",\n        \"model_preference\": \"config.model\"\n    },\n    response_template=\"...\",             # Optional: Jinja template\n    config={                             # Optional: model-specific config\n        # All parameters passed directly to model\n        \"threshold\": 0.5,\n        \"top_k\": 3,\n        \"temperature\": 0.7\n    },\n    name=\"custom_predictor\"              # Optional: custom name\n)\n</code></pre>"},{"location":"learn/nn/predictor/#best-practices","title":"Best Practices","text":""},{"location":"learn/nn/predictor/#1-use-task-specific-predictors","title":"1. Use Task-Specific Predictors","text":"<pre><code># Good - Clear, specialized predictors\nclass FraudDetector(mf.nn.Predictor):\n    \"\"\"Detects fraudulent transactions.\"\"\"\n    response_mode = \"plain_response\"\n\nclass ChurnPredictor(mf.nn.Predictor):\n    \"\"\"Predicts customer churn risk.\"\"\"\n    response_mode = \"plain_response\"\n\nclass RecommendationEngine(mf.nn.Predictor):\n    \"\"\"Generates product recommendations.\"\"\"\n    response_mode = \"plain_response\"\n</code></pre>"},{"location":"learn/nn/predictor/#2-validate-inputs","title":"2. Validate Inputs","text":"<pre><code>class ValidatedPredictor(mf.nn.Predictor):\n    \"\"\"Predictor with input validation.\"\"\"\n    response_mode = \"plain_response\"\n\ndef validate_features(features):\n    \"\"\"Validate features before prediction.\"\"\"\n    if not isinstance(features, list):\n        raise ValueError(\"Features must be a list\")\n    if len(features) != 10:\n        raise ValueError(\"Expected 10 features\")\n    return features\n\n# Use validation in preprocessing\npredictor = ValidatedPredictor(model=model)\n\ntry:\n    result = predictor(validate_features(user_input))\nexcept ValueError as e:\n    print(f\"Invalid input: {e}\")\n</code></pre>"},{"location":"learn/nn/predictor/#3-handle-uncertainty","title":"3. Handle Uncertainty","text":"<pre><code>class ConfidencePredictor(mf.nn.Predictor):\n    \"\"\"Predictor that returns confidence scores.\"\"\"\n    response_mode = \"plain_response\"\n\npredictor = ConfidencePredictor(\n    model=model,\n    config={\"return_confidence\": True}\n)\n\nresult = predictor(input_data)\n\nif result[\"confidence\"] &lt; 0.7:\n    print(\"Low confidence - manual review recommended\")\nelse:\n    print(f\"Prediction: {result['prediction']} (confidence: {result['confidence']})\")\n</code></pre>"},{"location":"learn/nn/predictor/#migration-guide","title":"Migration Guide","text":""},{"location":"learn/nn/predictor/#from-traditional-to-autoparams","title":"From Traditional to AutoParams","text":"<p>Before (Traditional): <pre><code>predictor = mf.nn.Predictor(\n    model=ml_model,\n    config={\"threshold\": 0.5, \"top_k\": 3}\n)\n</code></pre></p> <p>After (AutoParams - Recommended): <pre><code>class MyPredictor(mf.nn.Predictor):\n    \"\"\"Predictor for my specific ML task.\"\"\"\n    response_mode = \"plain_response\"\n\npredictor = MyPredictor(\n    model=ml_model,\n    config={\"threshold\": 0.5, \"top_k\": 3}\n)\n</code></pre></p>"},{"location":"learn/nn/predictor/#summary","title":"Summary","text":"<ul> <li>Use AutoParams for defining predictors - create specialized predictors for different ML tasks</li> <li>Traditional initialization works for quick, one-off predictions</li> <li>Supports any ML model (classifiers, regressors, detectors, segmenters)</li> <li>Works with scikit-learn, PyTorch, TensorFlow, and custom models</li> <li>Configure via message_fields, response_template, and config options</li> <li>Commonly integrated with Agents as tools</li> <li>Async support for batch predictions</li> <li>Model Gateway support for ensemble and fallback</li> </ul> <p>The Predictor module provides a unified interface for ML models - use AutoParams to organize predictors by task and domain.</p>"},{"location":"learn/nn/retriever/","title":"Retriever: Information Retrieval Module","text":"<p>The <code>Retriever</code> module provides a unified interface for information retrieval using web search, lexical search, semantic search, or vector databases.</p> <p>All code examples use the recommended import pattern:</p> <pre><code>import msgflux as mf\n</code></pre>"},{"location":"learn/nn/retriever/#quick-start","title":"Quick Start","text":""},{"location":"learn/nn/retriever/#traditional-initialization","title":"Traditional Initialization","text":"<pre><code>import msgflux as mf\n\n# Create a vector database retriever\nvector_db = mf.data.VectorDB.qdrant(\n    collection_name=\"documents\",\n    url=\"http://localhost:6333\"\n)\n\n# Create embedding model for semantic search\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\n# Traditional retriever initialization\nretriever = mf.nn.Retriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\"top_k\": 5, \"threshold\": 0.7}\n)\n\n# Use the retriever\nresults = retriever(\"What is machine learning?\")\nprint(results)\n</code></pre>"},{"location":"learn/nn/retriever/#autoparams-initialization-recommended","title":"AutoParams Initialization (Recommended)","text":"<p>This is the preferred and recommended way to define retrievers in msgflux.</p> <pre><code>import msgflux as mf\n\nclass DocumentRetriever(mf.nn.Retriever):\n    \"\"\"Semantic retriever for technical documentation.\"\"\"\n\n    # AutoParams automatically uses class name as 'name'\n    # Define configuration as class attributes\n    response_mode = \"plain_response\"\n\n# Setup\nvector_db = mf.data.VectorDB.qdrant(\n    collection_name=\"documents\",\n    url=\"http://localhost:6333\"\n)\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\n# Create instance with defaults\ndoc_retriever = DocumentRetriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\"top_k\": 5, \"threshold\": 0.7}\n)\n\n# Use the retriever\nresults = doc_retriever(\"What is machine learning?\")\nprint(results)\n\n# Override configuration at runtime\ndetailed_results = DocumentRetriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\"top_k\": 10, \"threshold\": 0.5, \"return_score\": True}\n)\n</code></pre>"},{"location":"learn/nn/retriever/#why-use-autoparams","title":"Why Use AutoParams?","text":"<ol> <li>Domain-Specific Retrievers: Create specialized retrievers for different knowledge domains</li> <li>Reusable Configuration: Share configuration across retriever instances</li> <li>Clear Intent: Class name and docstring document the retriever's purpose</li> <li>Easy Customization: Override defaults per instance or create retriever hierarchies</li> <li>Better Organization: Group related retrievers by domain or use case</li> </ol>"},{"location":"learn/nn/retriever/#retriever-types","title":"Retriever Types","text":""},{"location":"learn/nn/retriever/#1-vector-database-retrieval-semantic","title":"1. Vector Database Retrieval (Semantic)","text":"<p>Most common type - uses embeddings for semantic similarity search.</p>"},{"location":"learn/nn/retriever/#traditional-approach","title":"Traditional Approach","text":"<pre><code>import msgflux as mf\n\n# Setup vector database\nvector_db = mf.data.VectorDB.qdrant(\n    collection_name=\"knowledge_base\",\n    url=\"http://localhost:6333\"\n)\n\n# Create embedding model\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\n# Create retriever\nretriever = mf.nn.Retriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\n        \"top_k\": 5,\n        \"threshold\": 0.75,\n        \"return_score\": True\n    }\n)\n\n# Search\nresults = retriever(\"How do I use async functions in Python?\")\nfor result in results[\"results\"]:\n    print(f\"Score: {result['score']}\")\n    print(f\"Content: {result['data']}\")\n</code></pre>"},{"location":"learn/nn/retriever/#autoparams-approach-recommended","title":"AutoParams Approach (Recommended)","text":"<pre><code>import msgflux as mf\n\nclass KnowledgeBaseRetriever(mf.nn.Retriever):\n    \"\"\"Semantic retriever for company knowledge base with high precision.\"\"\"\n\n    response_mode = \"plain_response\"\n\n# Setup\nvector_db = mf.data.VectorDB.qdrant(\n    collection_name=\"knowledge_base\",\n    url=\"http://localhost:6333\"\n)\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\n# Create with strict thresholds for high precision\nkb_retriever = KnowledgeBaseRetriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\"top_k\": 5, \"threshold\": 0.75, \"return_score\": True}\n)\n\nresults = kb_retriever(\"How do I use async functions in Python?\")\n</code></pre>"},{"location":"learn/nn/retriever/#2-web-retrieval","title":"2. Web Retrieval","text":"<p>Search the web for information.</p> <pre><code>import msgflux as mf\n\nclass WebResearcher(mf.nn.Retriever):\n    \"\"\"Web search retriever for real-time information.\"\"\"\n\n    response_mode = \"plain_response\"\n\n# Create web retriever\nweb_search = mf.data.WebRetriever(api_key=\"your-api-key\")\n\nresearcher = WebResearcher(\n    retriever=web_search,\n    config={\"top_k\": 10}\n)\n\n# Search current events\nresults = researcher(\"Latest news on artificial intelligence\")\nprint(results)\n</code></pre>"},{"location":"learn/nn/retriever/#3-lexical-retrieval","title":"3. Lexical Retrieval","text":"<p>Traditional keyword-based search (BM25, TF-IDF).</p> <pre><code>import msgflux as mf\n\nclass DocumentSearcher(mf.nn.Retriever):\n    \"\"\"Fast lexical search for exact keyword matching.\"\"\"\n\n    response_mode = \"plain_response\"\n\n# Create lexical retriever\nlexical_search = mf.data.LexicalRetriever(\n    documents=[\"doc1.txt\", \"doc2.txt\", \"doc3.txt\"]\n)\n\nsearcher = DocumentSearcher(\n    retriever=lexical_search,\n    config={\"top_k\": 3}\n)\n\nresults = searcher(\"Python async await\")\n</code></pre>"},{"location":"learn/nn/retriever/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"learn/nn/retriever/#message-field-mapping","title":"Message Field Mapping","text":"<p>Use Message objects for structured input/output.</p> <pre><code>import msgflux as mf\n\nclass ContextualRetriever(mf.nn.Retriever):\n    \"\"\"Retriever that processes structured message inputs.\"\"\"\n\n    response_mode = \"message\"  # Return Message object\n\nvector_db = mf.data.VectorDB.qdrant(\n    collection_name=\"docs\",\n    url=\"http://localhost:6333\"\n)\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\nretriever = ContextualRetriever(\n    retriever=vector_db,\n    model=embedder,\n    message_fields={\n        \"task_inputs\": \"query.user\"  # Extract query from message.query.user\n    },\n    config={\"top_k\": 5}\n)\n\n# Use with Message\nmsg = mf.Message()\nmsg.set(\"query.user\", \"What is dependency injection?\")\n\nresult_msg = retriever(msg)\nprint(result_msg.get(\"retriever.results\"))\n</code></pre>"},{"location":"learn/nn/retriever/#response-templates","title":"Response Templates","text":"<p>Format retrieval results using Jinja templates.</p> <pre><code>import msgflux as mf\n\nclass FormattedRetriever(mf.nn.Retriever):\n    \"\"\"Retriever with custom result formatting.\"\"\"\n\n    response_mode = \"plain_response\"\n\nvector_db = mf.data.VectorDB.qdrant(\n    collection_name=\"articles\",\n    url=\"http://localhost:6333\"\n)\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\nretriever = FormattedRetriever(\n    retriever=vector_db,\n    model=embedder,\n    templates={\n        \"response\": \"\"\"\n        Found {{ results|length }} relevant articles:\n        {% for result in results %}\n        {{ loop.index }}. {{ result.data }} (Score: {{ result.score }})\n        {% endfor %}\n        \"\"\"\n    },\n    config={\"top_k\": 5, \"return_score\": True}\n)\n\nformatted_results = retriever(\"machine learning best practices\")\nprint(formatted_results)\n</code></pre>"},{"location":"learn/nn/retriever/#runtime-configuration-override","title":"Runtime Configuration Override","text":"<p>Override configuration at call time.</p> <pre><code>import msgflux as mf\n\nclass FlexibleRetriever(mf.nn.Retriever):\n    \"\"\"Retriever with runtime-configurable parameters.\"\"\"\n\n    response_mode = \"plain_response\"\n\nvector_db = mf.data.VectorDB.qdrant(\n    collection_name=\"docs\",\n    url=\"http://localhost:6333\"\n)\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\nretriever = FlexibleRetriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\"top_k\": 3}  # Default\n)\n\n# Normal retrieval with defaults\nresults = retriever(\"python tutorial\")\n\n# Override task_inputs at runtime\nresults = retriever(\n    \"machine learning\",\n    task_inputs=\"deep learning neural networks\"  # Override query\n)\n</code></pre>"},{"location":"learn/nn/retriever/#creating-retriever-hierarchies","title":"Creating Retriever Hierarchies","text":"<p>Build specialized retrievers through inheritance.</p> <pre><code>import msgflux as mf\n\n# Base retriever for all documentation\nclass BaseDocRetriever(mf.nn.Retriever):\n    \"\"\"Base retriever for documentation with common config.\"\"\"\n\n    response_mode = \"plain_response\"\n\n# High precision retriever for critical docs\nclass CriticalDocRetriever(BaseDocRetriever):\n    \"\"\"High precision retriever for critical documentation (compliance, security).\"\"\"\n\n    # Inherits response_mode from BaseDocRetriever\n    # Override defaults for stricter matching\n\n# Broad retriever for general exploration\nclass ExploratoryRetriever(BaseDocRetriever):\n    \"\"\"Broad retriever for exploratory searches.\"\"\"\n\n    # Lower threshold for diverse results\n\n# Setup\nvector_db = mf.data.VectorDB.qdrant(\n    collection_name=\"documentation\",\n    url=\"http://localhost:6333\"\n)\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\n# Create instances with different thresholds\ncritical = CriticalDocRetriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\"top_k\": 3, \"threshold\": 0.85}  # Strict\n)\n\nexploratory = ExploratoryRetriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\"top_k\": 10, \"threshold\": 0.5}  # Permissive\n)\n\n# Use appropriately\ncompliance_docs = critical(\"GDPR data retention requirements\")\nrelated_topics = exploratory(\"data privacy regulations\")\n</code></pre>"},{"location":"learn/nn/retriever/#integration-with-agents","title":"Integration with Agents","text":"<p>Retrievers are commonly used as tools for agents.</p> <pre><code>import msgflux as mf\n\n# Define retriever\nclass CompanyKnowledgeRetriever(mf.nn.Retriever):\n    \"\"\"Retrieves information from company knowledge base.\"\"\"\n\n    response_mode = \"plain_response\"\n\nvector_db = mf.data.VectorDB.qdrant(\n    collection_name=\"company_kb\",\n    url=\"http://localhost:6333\"\n)\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\nkb_retriever = CompanyKnowledgeRetriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\"top_k\": 5, \"threshold\": 0.7}\n)\n\n# Define retriever as a tool function\ndef search_knowledge_base(query: str) -&gt; str:\n    \"\"\"Search the company knowledge base for information.\"\"\"\n    results = kb_retriever(query)\n    return results\n\n# Create agent with retriever tool\nclass SupportAgent(mf.nn.Agent):\n    \"\"\"Customer support agent with access to knowledge base.\"\"\"\n\n    temperature = 0.7\n    max_tokens = 2000\n    max_tool_iterations = 3\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4\")\n\nsupport_agent = SupportAgent(\n    model=model,\n    tools=[search_knowledge_base]\n)\n\n# Agent can now use retriever to answer questions\nresponse = support_agent(\"How do I reset my password?\")\nprint(response)\n</code></pre>"},{"location":"learn/nn/retriever/#configuration-options","title":"Configuration Options","text":""},{"location":"learn/nn/retriever/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<pre><code>import msgflux as mf\n\nclass ConfiguredRetriever(mf.nn.Retriever):\n    \"\"\"Fully configured retriever example.\"\"\"\n\n    # Response behavior\n    response_mode = \"plain_response\"  # or \"message\"\n\n# Initialize with all options\nretriever = ConfiguredRetriever(\n    retriever=vector_db,              # Required: retriever backend\n    model=embedder,                   # Optional: for semantic retrieval\n    message_fields={                  # Optional: Message field mapping\n        \"task_inputs\": \"query.text\"\n    },\n    templates={                       # Optional: Jinja response templates\n        \"response\": \"Results: {{ content }}\"\n    },\n    config={                          # Optional: retriever-specific config\n        \"top_k\": 5,                   # Max results to return\n        \"threshold\": 0.7,             # Minimum similarity score\n        \"return_score\": True,         # Include scores in results\n        \"dict_key\": \"content\"         # Extract specific dict key from results\n    },\n    name=\"custom_retriever\"           # Optional: custom name\n)\n</code></pre>"},{"location":"learn/nn/retriever/#async-support","title":"Async Support","text":"<p>Retrievers support asynchronous operation.</p> <pre><code>import msgflux as mf\nimport asyncio\n\nclass AsyncRetriever(mf.nn.Retriever):\n    \"\"\"Async retriever for concurrent searches.\"\"\"\n\n    response_mode = \"plain_response\"\n\nvector_db = mf.data.VectorDB.qdrant(\n    collection_name=\"docs\",\n    url=\"http://localhost:6333\"\n)\n\nembedder = mf.Model.text_embedder(\"openai/text-embedding-3-small\")\n\nretriever = AsyncRetriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\"top_k\": 5}\n)\n\nasync def search_multiple():\n    \"\"\"Perform multiple searches concurrently.\"\"\"\n    results = await asyncio.gather(\n        retriever.aforward(\"machine learning\"),\n        retriever.aforward(\"deep learning\"),\n        retriever.aforward(\"neural networks\")\n    )\n    return results\n\n# Run async searches\nresults = asyncio.run(search_multiple())\n</code></pre>"},{"location":"learn/nn/retriever/#best-practices","title":"Best Practices","text":""},{"location":"learn/nn/retriever/#1-use-domain-specific-retrievers","title":"1. Use Domain-Specific Retrievers","text":"<pre><code># Good - Clear, specialized retrievers\nclass TechnicalDocRetriever(mf.nn.Retriever):\n    \"\"\"Retriever for technical documentation (APIs, guides).\"\"\"\n    response_mode = \"plain_response\"\n\nclass CustomerQueryRetriever(mf.nn.Retriever):\n    \"\"\"Retriever for customer support queries.\"\"\"\n    response_mode = \"plain_response\"\n\nclass ProductCatalogRetriever(mf.nn.Retriever):\n    \"\"\"Retriever for product information.\"\"\"\n    response_mode = \"plain_response\"\n</code></pre>"},{"location":"learn/nn/retriever/#2-tune-thresholds-by-use-case","title":"2. Tune Thresholds by Use Case","text":"<pre><code># High precision for critical applications\nclass ComplianceRetriever(mf.nn.Retriever):\n    \"\"\"High precision retriever for compliance documentation.\"\"\"\n    response_mode = \"plain_response\"\n\ncompliance = ComplianceRetriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\"top_k\": 3, \"threshold\": 0.9}  # Very strict\n)\n\n# High recall for exploratory search\nclass ResearchRetriever(mf.nn.Retriever):\n    \"\"\"Broad retriever for research and exploration.\"\"\"\n    response_mode = \"plain_response\"\n\nresearch = ResearchRetriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\"top_k\": 20, \"threshold\": 0.5}  # Permissive\n)\n</code></pre>"},{"location":"learn/nn/retriever/#3-return-scores-for-transparency","title":"3. Return Scores for Transparency","text":"<pre><code>class TransparentRetriever(mf.nn.Retriever):\n    \"\"\"Retriever that returns similarity scores for verification.\"\"\"\n    response_mode = \"plain_response\"\n\nretriever = TransparentRetriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\n        \"top_k\": 5,\n        \"return_score\": True  # Always include scores\n    }\n)\n\nresults = retriever(\"query\")\nfor r in results[\"results\"]:\n    if r[\"score\"] &lt; 0.7:\n        print(f\"Warning: Low confidence result (score: {r['score']})\")\n</code></pre>"},{"location":"learn/nn/retriever/#migration-guide","title":"Migration Guide","text":""},{"location":"learn/nn/retriever/#from-traditional-to-autoparams","title":"From Traditional to AutoParams","text":"<p>Before (Traditional): <pre><code>retriever = mf.nn.Retriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\"top_k\": 5, \"threshold\": 0.7},\n    response_mode=\"plain_response\"\n)\n</code></pre></p> <p>After (AutoParams - Recommended): <pre><code>class MyRetriever(mf.nn.Retriever):\n    \"\"\"Semantic retriever for my use case.\"\"\"\n    response_mode = \"plain_response\"\n\nretriever = MyRetriever(\n    retriever=vector_db,\n    model=embedder,\n    config={\"top_k\": 5, \"threshold\": 0.7}\n)\n</code></pre></p>"},{"location":"learn/nn/retriever/#summary","title":"Summary","text":"<ul> <li>Use AutoParams for defining retrievers - cleaner and more maintainable</li> <li>Traditional initialization works for quick, one-off retrievers</li> <li>Supports vector DB, web search, and lexical search</li> <li>Configure via message_fields, templates, and config options</li> <li>Commonly integrated with Agents as tools</li> <li>Async support for concurrent retrieval operations</li> </ul> <p>The Retriever module provides flexible, powerful information retrieval - use AutoParams to organize retrievers by domain and use case.</p>"},{"location":"learn/nn/speaker/","title":"Speaker: Text-to-Speech Module","text":"<p>The <code>Speaker</code> module converts text into natural-sounding speech using text-to-speech models.</p> <p>All code examples use the recommended import pattern:</p> <pre><code>import msgflux as mf\n</code></pre>"},{"location":"learn/nn/speaker/#quick-start","title":"Quick Start","text":""},{"location":"learn/nn/speaker/#traditional-initialization","title":"Traditional Initialization","text":"<pre><code>import msgflux as mf\n\n# Create TTS model\ntts_model = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# Traditional speaker initialization\nspeaker = mf.nn.Speaker(\n    model=tts_model,\n    response_format=\"mp3\",\n    config={\"voice\": \"alloy\"}\n)\n\n# Generate speech\naudio = speaker(\"Hello, welcome to msgFlux!\")\n\n# Save audio file\nwith open(\"output.mp3\", \"wb\") as f:\n    f.write(audio)\n</code></pre>"},{"location":"learn/nn/speaker/#autoparams-initialization-recommended","title":"AutoParams Initialization (Recommended)","text":"<p>This is the preferred and recommended way to define speakers in msgflux.</p> <pre><code>import msgflux as mf\n\nclass NaturalVoiceSpeaker(mf.nn.Speaker):\n    \"\"\"Natural-sounding speaker for user-facing applications.\"\"\"\n\n    # AutoParams automatically uses class name as 'name'\n    response_format = \"mp3\"\n    response_mode = \"plain_response\"\n\n# Create TTS model\ntts_model = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# Create speaker with defaults\nspeaker = NaturalVoiceSpeaker(\n    model=tts_model,\n    config={\"voice\": \"alloy\", \"speed\": 1.0}\n)\n\n# Generate speech\naudio = speaker(\"Hello, welcome to msgFlux!\")\n\n# Save audio\nwith open(\"welcome.mp3\", \"wb\") as f:\n    f.write(audio)\n\n# Create variant with different voice\nnova_speaker = NaturalVoiceSpeaker(\n    model=tts_model,\n    config={\"voice\": \"nova\", \"speed\": 1.1}  # Slightly faster\n)\n</code></pre>"},{"location":"learn/nn/speaker/#why-use-autoparams","title":"Why Use AutoParams?","text":"<ol> <li>Voice Personas: Create distinct speaker personas with consistent configuration</li> <li>Reusable Voices: Define voices once, use across application</li> <li>Clear Purpose: Class name and docstring document speaker characteristics</li> <li>Easy Variants: Create voice variations through inheritance</li> <li>Better Organization: Group speakers by use case or audience</li> </ol>"},{"location":"learn/nn/speaker/#audio-formats","title":"Audio Formats","text":"<p>Speakers support multiple output formats:</p> <pre><code>import msgflux as mf\n\nclass PodcastSpeaker(mf.nn.Speaker):\n    \"\"\"High-quality speaker for podcast production.\"\"\"\n\n    response_format = \"mp3\"  # Best for distribution\n    response_mode = \"plain_response\"\n\nclass LiveStreamSpeaker(mf.nn.Speaker):\n    \"\"\"Low-latency speaker for live streaming.\"\"\"\n\n    response_format = \"opus\"  # Low latency, good compression\n    response_mode = \"plain_response\"\n\nclass ArchivalSpeaker(mf.nn.Speaker):\n    \"\"\"Lossless speaker for archival purposes.\"\"\"\n\n    response_format = \"flac\"  # Lossless compression\n    response_mode = \"plain_response\"\n\n# Create instances\ntts = mf.Model.text_to_speech(\"openai/tts-1-hd\")\n\npodcast = PodcastSpeaker(model=tts, config={\"voice\": \"onyx\"})\nlivestream = LiveStreamSpeaker(model=tts, config={\"voice\": \"alloy\"})\narchival = ArchivalSpeaker(model=tts, config={\"voice\": \"echo\"})\n\ntext = \"This is a test of the text-to-speech system.\"\n\n# Generate in different formats\npodcast_audio = podcast(text)\nstream_audio = livestream(text)\narchive_audio = archival(text)\n</code></pre>"},{"location":"learn/nn/speaker/#voice-selection","title":"Voice Selection","text":"<p>Different voices for different contexts:</p> <pre><code>import msgflux as mf\n\nclass ProfessionalSpeaker(mf.nn.Speaker):\n    \"\"\"Professional, authoritative voice for business content.\"\"\"\n\n    response_format = \"mp3\"\n    response_mode = \"plain_response\"\n\nclass FriendlySpeaker(mf.nn.Speaker):\n    \"\"\"Warm, friendly voice for customer interactions.\"\"\"\n\n    response_format = \"mp3\"\n    response_mode = \"plain_response\"\n\nclass NarratorSpeaker(mf.nn.Speaker):\n    \"\"\"Clear, neutral voice for audiobook narration.\"\"\"\n\n    response_format = \"mp3\"\n    response_mode = \"plain_response\"\n\ntts = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# Create speakers with appropriate voices\nprofessional = ProfessionalSpeaker(model=tts, config={\"voice\": \"onyx\"})\nfriendly = FriendlySpeaker(model=tts, config={\"voice\": \"nova\"})\nnarrator = NarratorSpeaker(model=tts, config={\"voice\": \"echo\"})\n\n# Use contextually\nbusiness_intro = professional(\"Welcome to our quarterly earnings call.\")\ncustomer_greeting = friendly(\"Hi! How can I help you today?\")\naudiobook_chapter = narrator(\"Chapter 1: The Beginning.\")\n</code></pre>"},{"location":"learn/nn/speaker/#speed-control","title":"Speed Control","text":"<p>Control speech rate for different use cases:</p> <pre><code>import msgflux as mf\n\nclass SlowSpeaker(mf.nn.Speaker):\n    \"\"\"Slow, clear speech for learning content.\"\"\"\n\n    response_format = \"mp3\"\n    response_mode = \"plain_response\"\n\nclass NormalSpeaker(mf.nn.Speaker):\n    \"\"\"Normal speed for general content.\"\"\"\n\n    response_format = \"mp3\"\n    response_mode = \"plain_response\"\n\nclass FastSpeaker(mf.nn.Speaker):\n    \"\"\"Fast speech for efficient content delivery.\"\"\"\n\n    response_format = \"mp3\"\n    response_mode = \"plain_response\"\n\ntts = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# Create with different speeds\nslow = SlowSpeaker(model=tts, config={\"voice\": \"alloy\", \"speed\": 0.75})\nnormal = NormalSpeaker(model=tts, config={\"voice\": \"alloy\", \"speed\": 1.0})\nfast = FastSpeaker(model=tts, config={\"voice\": \"alloy\", \"speed\": 1.5})\n\ntext = \"This content is optimized for different listening speeds.\"\n\n# Generate at different rates\nslow_audio = slow(text)\nnormal_audio = normal(text)\nfast_audio = fast(text)\n</code></pre>"},{"location":"learn/nn/speaker/#streaming-audio","title":"Streaming Audio","text":"<p>For real-time applications, stream audio as it's generated:</p> <pre><code>import msgflux as mf\n\nclass StreamingSpeaker(mf.nn.Speaker):\n    \"\"\"Streaming speaker for real-time applications.\"\"\"\n\n    response_format = \"opus\"  # Best for streaming\n    response_mode = \"plain_response\"\n\ntts = mf.Model.text_to_speech(\"openai/tts-1\")\n\nspeaker = StreamingSpeaker(\n    model=tts,\n    config={\"voice\": \"nova\", \"stream\": True}\n)\n\n# Get streaming response\nstream = speaker(\"This audio will be streamed chunk by chunk.\")\n\n# Process chunks as they arrive\nasync for chunk in stream:\n    # Send chunk to client, play immediately, etc.\n    process_audio_chunk(chunk)\n</code></pre>"},{"location":"learn/nn/speaker/#guardrails","title":"Guardrails","text":"<p>Add input validation and filtering:</p> <pre><code>import msgflux as mf\n\ndef sanitize_input(text: str) -&gt; str:\n    \"\"\"Remove sensitive information before TTS.\"\"\"\n    # Remove credit card numbers, SSNs, etc.\n    sanitized = remove_pii(text)\n    return sanitized\n\ndef check_length(text: str) -&gt; str:\n    \"\"\"Ensure text is within reasonable length.\"\"\"\n    if len(text) &gt; 4096:\n        raise ValueError(\"Text too long for TTS\")\n    return text\n\nclass SafeSpeaker(mf.nn.Speaker):\n    \"\"\"Speaker with input guardrails for safety.\"\"\"\n\n    response_format = \"mp3\"\n    response_mode = \"plain_response\"\n\ntts = mf.Model.text_to_speech(\"openai/tts-1\")\n\nsafe_speaker = SafeSpeaker(\n    model=tts,\n    guardrails={\n        \"input\": lambda text: check_length(sanitize_input(text))\n    },\n    config={\"voice\": \"alloy\"}\n)\n\n# Input is automatically sanitized and validated\naudio = safe_speaker(\"User input that may contain sensitive data...\")\n</code></pre>"},{"location":"learn/nn/speaker/#message-field-mapping","title":"Message Field Mapping","text":"<p>Use Message objects for structured processing:</p> <pre><code>import msgflux as mf\n\nclass NotificationSpeaker(mf.nn.Speaker):\n    \"\"\"Speaker that processes structured notification messages.\"\"\"\n\n    response_format = \"mp3\"\n    response_mode = \"message\"\n\ntts = mf.Model.text_to_speech(\"openai/tts-1\")\n\nspeaker = NotificationSpeaker(\n    model=tts,\n    message_fields={\n        \"task_inputs\": \"notification.text\"\n    },\n    config={\"voice\": \"nova\"}\n)\n\n# Create notification message\nmsg = mf.Message()\nmsg.set(\"notification.text\", \"You have 3 new messages\")\nmsg.set(\"notification.priority\", \"high\")\n\n# Process and get audio in message\nresult_msg = speaker(msg)\naudio = result_msg.get(\"speaker.audio\")\n\n# Save\nwith open(\"notification.mp3\", \"wb\") as f:\n    f.write(audio)\n</code></pre>"},{"location":"learn/nn/speaker/#creating-speaker-hierarchies","title":"Creating Speaker Hierarchies","text":"<p>Build specialized speakers through inheritance:</p> <pre><code>import msgflux as mf\n\n# Base speaker for all announcements\nclass AnnouncementSpeaker(mf.nn.Speaker):\n    \"\"\"Base speaker for public announcements.\"\"\"\n\n    response_format = \"mp3\"\n    response_mode = \"plain_response\"\n\n# Emergency announcements - clear and authoritative\nclass EmergencySpeaker(AnnouncementSpeaker):\n    \"\"\"Urgent speaker for emergency announcements.\"\"\"\n\n    # Inherits format and mode from AnnouncementSpeaker\n\n# Casual announcements - friendly tone\nclass CasualSpeaker(AnnouncementSpeaker):\n    \"\"\"Friendly speaker for casual announcements.\"\"\"\n\n    # Different voice but same format\n\ntts = mf.Model.text_to_speech(\"openai/tts-1\")\n\n# Create instances\nemergency = EmergencySpeaker(\n    model=tts,\n    config={\"voice\": \"onyx\", \"speed\": 0.9}  # Slower for clarity\n)\n\ncasual = CasualSpeaker(\n    model=tts,\n    config={\"voice\": \"nova\", \"speed\": 1.1}  # Slightly faster\n)\n\n# Use appropriately\nalert = emergency(\"Attention: Building evacuation in progress.\")\nupdate = casual(\"Reminder: Team meeting at 3 PM.\")\n</code></pre>"},{"location":"learn/nn/speaker/#integration-with-agents","title":"Integration with Agents","text":"<p>Speakers can be used in agent workflows:</p> <pre><code>import msgflux as mf\n\nclass ResponseSpeaker(mf.nn.Speaker):\n    \"\"\"Converts agent responses to speech.\"\"\"\n\n    response_format = \"mp3\"\n    response_mode = \"plain_response\"\n\n# Create speaker\ntts = mf.Model.text_to_speech(\"openai/tts-1\")\nspeaker = ResponseSpeaker(model=tts, config={\"voice\": \"alloy\"})\n\n# Create agent\nclass VoiceAssistant(mf.nn.Agent):\n    \"\"\"Voice-enabled assistant.\"\"\"\n\n    temperature = 0.7\n    max_tokens = 150\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4\")\nassistant = VoiceAssistant(model=model)\n\n# Get text response from agent\ntext_response = assistant(\"What's the weather like today?\")\n\n# Convert to speech\naudio_response = speaker(text_response)\n\n# Play or save audio\nwith open(\"response.mp3\", \"wb\") as f:\n    f.write(audio_response)\n</code></pre>"},{"location":"learn/nn/speaker/#prompt-guidance","title":"Prompt Guidance","text":"<p>Guide speech generation patterns:</p> <pre><code>import msgflux as mf\n\nclass StorytellerSpeaker(mf.nn.Speaker):\n    \"\"\"Expressive speaker for storytelling.\"\"\"\n\n    response_format = \"mp3\"\n    response_mode = \"plain_response\"\n\ntts = mf.Model.text_to_speech(\"openai/tts-1-hd\")\n\nstoryteller = StorytellerSpeaker(\n    model=tts,\n    prompt=\"Speak with dramatic pauses and emotional variation, like narrating an audiobook.\",\n    config={\"voice\": \"echo\"}\n)\n\nstory = \"\"\"\nOnce upon a time, in a land far away, there lived a brave knight.\nHis adventures would become the stuff of legends.\n\"\"\"\n\naudio = storyteller(story)\n</code></pre>"},{"location":"learn/nn/speaker/#configuration-options","title":"Configuration Options","text":""},{"location":"learn/nn/speaker/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<pre><code>import msgflux as mf\n\nclass FullyConfiguredSpeaker(mf.nn.Speaker):\n    \"\"\"Speaker with all configuration options.\"\"\"\n\n    # Response behavior\n    response_format = \"mp3\"  # \"mp3\", \"opus\", \"aac\", \"flac\", \"wav\", \"pcm\"\n    response_mode = \"plain_response\"  # or \"message\"\n\ntts = mf.Model.text_to_speech(\"openai/tts-1\")\n\nspeaker = FullyConfiguredSpeaker(\n    model=tts,                           # Required: TTS model\n    guardrails={                         # Optional: input validation\n        \"input\": sanitize_function\n    },\n    message_fields={                     # Optional: Message field mapping\n        \"task_inputs\": \"text.content\"\n    },\n    prompt=\"Speaker guidance\",           # Optional: generation guidance\n    config={                             # Optional: TTS-specific config\n        \"voice\": \"alloy\",                # Voice selection\n        \"speed\": 1.0,                    # Speech rate (0.25 - 4.0)\n        \"stream\": False                  # Enable streaming\n    },\n    name=\"custom_speaker\"                # Optional: custom name\n)\n</code></pre>"},{"location":"learn/nn/speaker/#best-practices","title":"Best Practices","text":""},{"location":"learn/nn/speaker/#1-match-voice-to-purpose","title":"1. Match Voice to Purpose","text":"<pre><code># Good - Voice matches content type\nclass BusinessSpeaker(mf.nn.Speaker):\n    \"\"\"Professional voice for business content.\"\"\"\n    response_format = \"mp3\"\n\nbusiness = BusinessSpeaker(\n    model=tts,\n    config={\"voice\": \"onyx\"}  # Deep, authoritative\n)\n\nclass ChildrensSpeaker(mf.nn.Speaker):\n    \"\"\"Friendly voice for children's content.\"\"\"\n    response_format = \"mp3\"\n\nchildren = ChildrensSpeaker(\n    model=tts,\n    config={\"voice\": \"nova\"}  # Warm, friendly\n)\n</code></pre>"},{"location":"learn/nn/speaker/#2-choose-appropriate-format","title":"2. Choose Appropriate Format","text":"<pre><code># High quality for downloads\nclass DownloadSpeaker(mf.nn.Speaker):\n    \"\"\"High-quality speaker for downloadable content.\"\"\"\n    response_format = \"mp3\"  # Good quality, widely supported\n\n# Low latency for real-time\nclass RealtimeSpeaker(mf.nn.Speaker):\n    \"\"\"Low-latency speaker for real-time applications.\"\"\"\n    response_format = \"opus\"  # Fast, efficient\n</code></pre>"},{"location":"learn/nn/speaker/#3-use-guardrails-for-user-input","title":"3. Use Guardrails for User Input","text":"<pre><code>class UserInputSpeaker(mf.nn.Speaker):\n    \"\"\"Safe speaker for user-generated content.\"\"\"\n    response_format = \"mp3\"\n\nspeaker = UserInputSpeaker(\n    model=tts,\n    guardrails={\n        \"input\": lambda text: sanitize_and_validate(text)\n    }\n)\n</code></pre>"},{"location":"learn/nn/speaker/#migration-guide","title":"Migration Guide","text":""},{"location":"learn/nn/speaker/#from-traditional-to-autoparams","title":"From Traditional to AutoParams","text":"<p>Before (Traditional): <pre><code>speaker = mf.nn.Speaker(\n    model=tts,\n    response_format=\"mp3\",\n    config={\"voice\": \"alloy\", \"speed\": 1.0}\n)\n</code></pre></p> <p>After (AutoParams - Recommended): <pre><code>class MySpeaker(mf.nn.Speaker):\n    \"\"\"Natural speaker for general use.\"\"\"\n    response_format = \"mp3\"\n\nspeaker = MySpeaker(\n    model=tts,\n    config={\"voice\": \"alloy\", \"speed\": 1.0}\n)\n</code></pre></p>"},{"location":"learn/nn/speaker/#summary","title":"Summary","text":"<ul> <li>Use AutoParams for defining speakers - create voice personas with consistent configuration</li> <li>Traditional initialization works for quick, one-off TTS tasks</li> <li>Supports multiple audio formats (MP3, Opus, AAC, FLAC, WAV, PCM)</li> <li>Configure voice, speed, and streaming options</li> <li>Add guardrails for input validation</li> <li>Streaming support for real-time applications</li> </ul> <p>The Speaker module provides flexible text-to-speech - use AutoParams to create distinct voice personas for different contexts.</p>"},{"location":"learn/nn/transcriber/","title":"Transcriber: Speech-to-Text Module","text":"<p>The <code>Transcriber</code> module converts speech audio into text using speech-to-text models.</p> <p>All code examples use the recommended import pattern:</p> <pre><code>import msgflux as mf\n</code></pre>"},{"location":"learn/nn/transcriber/#quick-start","title":"Quick Start","text":""},{"location":"learn/nn/transcriber/#traditional-initialization","title":"Traditional Initialization","text":"<pre><code>import msgflux as mf\n\n# Create STT model\nstt_model = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# Traditional transcriber initialization\ntranscriber = mf.nn.Transcriber(\n    model=stt_model,\n    response_format=\"text\",\n    config={\"language\": \"en\"}\n)\n\n# Transcribe audio file\ntext = transcriber(\"audio.mp3\")\nprint(text)\n</code></pre>"},{"location":"learn/nn/transcriber/#autoparams-initialization-recommended","title":"AutoParams Initialization (Recommended)","text":"<p>This is the preferred and recommended way to define transcribers in msgflux.</p> <pre><code>import msgflux as mf\n\nclass PodcastTranscriber(mf.nn.Transcriber):\n    \"\"\"Transcriber optimized for podcast audio with high accuracy.\"\"\"\n\n    # AutoParams automatically uses class name as 'name'\n    response_format = \"text\"\n    response_mode = \"plain_response\"\n\n# Create STT model\nstt_model = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# Create transcriber with defaults\ntranscriber = PodcastTranscriber(\n    model=stt_model,\n    config={\"language\": \"en\", \"temperature\": 0.0}\n)\n\n# Transcribe\ntext = transcriber(\"podcast_episode.mp3\")\nprint(text)\n\n# Create variant for different language\nspanish_transcriber = PodcastTranscriber(\n    model=stt_model,\n    config={\"language\": \"es\", \"temperature\": 0.0}\n)\n</code></pre>"},{"location":"learn/nn/transcriber/#why-use-autoparams","title":"Why Use AutoParams?","text":"<ol> <li>Domain-Specific Transcribers: Create specialized transcribers for different audio types</li> <li>Language-Specific Configuration: Define transcribers per language with optimal settings</li> <li>Reusable Configurations: Share configuration across transcription tasks</li> <li>Clear Purpose: Class name and docstring document transcriber characteristics</li> <li>Easy Variants: Create transcriber variations through inheritance</li> </ol>"},{"location":"learn/nn/transcriber/#response-formats","title":"Response Formats","text":"<p>Transcribers support multiple output formats:</p> <pre><code>import msgflux as mf\n\nclass TextTranscriber(mf.nn.Transcriber):\n    \"\"\"Basic transcriber returning plain text.\"\"\"\n\n    response_format = \"text\"\n    response_mode = \"plain_response\"\n\nclass JSONTranscriber(mf.nn.Transcriber):\n    \"\"\"Transcriber with JSON output for structured processing.\"\"\"\n\n    response_format = \"json\"\n    response_mode = \"plain_response\"\n\nclass SubtitleTranscriber(mf.nn.Transcriber):\n    \"\"\"Transcriber generating SRT subtitles with timestamps.\"\"\"\n\n    response_format = \"srt\"\n    response_mode = \"plain_response\"\n\nclass DetailedTranscriber(mf.nn.Transcriber):\n    \"\"\"Transcriber with verbose JSON including word-level timestamps.\"\"\"\n\n    response_format = \"verbose_json\"\n    response_mode = \"plain_response\"\n\n# Create instances\nstt = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ntext_trans = TextTranscriber(model=stt)\njson_trans = JSONTranscriber(model=stt)\nsubtitle_trans = SubtitleTranscriber(model=stt)\ndetailed_trans = DetailedTranscriber(\n    model=stt,\n    config={\"timestamp_granularities\": \"word\"}\n)\n\n# Transcribe in different formats\ntext = text_trans(\"audio.mp3\")\njson_data = json_trans(\"audio.mp3\")\nsubtitles = subtitle_trans(\"audio.mp3\")\ndetailed = detailed_trans(\"audio.mp3\")\n</code></pre>"},{"location":"learn/nn/transcriber/#language-specific-transcribers","title":"Language-Specific Transcribers","text":"<p>Create transcribers optimized for specific languages:</p> <pre><code>import msgflux as mf\n\nclass EnglishTranscriber(mf.nn.Transcriber):\n    \"\"\"High-accuracy transcriber for English audio.\"\"\"\n\n    response_format = \"text\"\n    response_mode = \"plain_response\"\n\nclass SpanishTranscriber(mf.nn.Transcriber):\n    \"\"\"High-accuracy transcriber for Spanish audio.\"\"\"\n\n    response_format = \"text\"\n    response_mode = \"plain_response\"\n\nclass MultilingualTranscriber(mf.nn.Transcriber):\n    \"\"\"Transcriber for auto-detecting and transcribing multiple languages.\"\"\"\n\n    response_format = \"text\"\n    response_mode = \"plain_response\"\n\nstt = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# Create language-specific transcribers\nenglish = EnglishTranscriber(model=stt, config={\"language\": \"en\"})\nspanish = SpanishTranscriber(model=stt, config={\"language\": \"es\"})\nmultilingual = MultilingualTranscriber(model=stt)  # Auto-detect\n\n# Use appropriately\nen_text = english(\"english_audio.mp3\")\nes_text = spanish(\"spanish_audio.mp3\")\nmulti_text = multilingual(\"unknown_language.mp3\")\n</code></pre>"},{"location":"learn/nn/transcriber/#prompt-guidance","title":"Prompt Guidance","text":"<p>Guide transcription with prompts for better accuracy:</p> <pre><code>import msgflux as mf\n\nclass MedicalTranscriber(mf.nn.Transcriber):\n    \"\"\"Transcriber optimized for medical terminology.\"\"\"\n\n    response_format = \"text\"\n    response_mode = \"plain_response\"\n\nclass LegalTranscriber(mf.nn.Transcriber):\n    \"\"\"Transcriber optimized for legal terminology.\"\"\"\n\n    response_format = \"text\"\n    response_mode = \"plain_response\"\n\nstt = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# Create with domain-specific prompts\nmedical = MedicalTranscriber(\n    model=stt,\n    prompt=\"Medical consultation with terms like: patient, diagnosis, treatment, prescription, symptoms\",\n    config={\"language\": \"en\"}\n)\n\nlegal = LegalTranscriber(\n    model=stt,\n    prompt=\"Legal deposition with terms like: plaintiff, defendant, evidence, testimony, objection\",\n    config={\"language\": \"en\"}\n)\n\n# Transcribe with improved accuracy for domain terms\nmedical_transcript = medical(\"doctor_patient_conversation.mp3\")\nlegal_transcript = legal(\"court_hearing.mp3\")\n</code></pre>"},{"location":"learn/nn/transcriber/#timestamp-granularities","title":"Timestamp Granularities","text":"<p>Get word or segment-level timestamps:</p> <pre><code>import msgflux as mf\n\nclass TimestampedTranscriber(mf.nn.Transcriber):\n    \"\"\"Transcriber with word-level timestamps.\"\"\"\n\n    response_format = \"verbose_json\"\n    response_mode = \"plain_response\"\n\nstt = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ntranscriber = TimestampedTranscriber(\n    model=stt,\n    config={\n        \"language\": \"en\",\n        \"timestamp_granularities\": \"word\"  # or \"segment\"\n    }\n)\n\n# Get detailed transcription with timestamps\nresult = transcriber(\"speech.mp3\")\n\n# Access word-level timestamps\nfor word_info in result[\"words\"]:\n    print(f\"{word_info['word']}: {word_info['start']}s - {word_info['end']}s\")\n</code></pre>"},{"location":"learn/nn/transcriber/#streaming-transcription","title":"Streaming Transcription","text":"<p>For real-time applications, stream transcription as audio is processed:</p> <pre><code>import msgflux as mf\n\nclass StreamingTranscriber(mf.nn.Transcriber):\n    \"\"\"Real-time streaming transcriber.\"\"\"\n\n    response_format = \"text\"\n    response_mode = \"plain_response\"\n\nstt = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ntranscriber = StreamingTranscriber(\n    model=stt,\n    config={\"language\": \"en\", \"stream\": True}\n)\n\n# Get streaming response\nstream = transcriber(\"live_audio.mp3\")\n\n# Process chunks as they arrive\nasync for chunk in stream:\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"learn/nn/transcriber/#message-field-mapping","title":"Message Field Mapping","text":"<p>Use Message objects for structured processing:</p> <pre><code>import msgflux as mf\n\nclass CallTranscriber(mf.nn.Transcriber):\n    \"\"\"Transcriber that processes call recording messages.\"\"\"\n\n    response_format = \"text\"\n    response_mode = \"message\"\n\nstt = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ntranscriber = CallTranscriber(\n    model=stt,\n    message_fields={\n        \"task_multimodal_inputs\": \"call.recording\"\n    },\n    config={\"language\": \"en\"}\n)\n\n# Create call message\nmsg = mf.Message()\nmsg.set(\"call.recording\", \"customer_call_123.mp3\")\nmsg.set(\"call.id\", \"123\")\nmsg.set(\"call.timestamp\", \"2024-01-15T10:30:00\")\n\n# Transcribe and get result in message\nresult_msg = transcriber(msg)\ntranscript = result_msg.get(\"transcriber.text\")\ncall_id = result_msg.get(\"call.id\")\n\nprint(f\"Call {call_id}: {transcript}\")\n</code></pre>"},{"location":"learn/nn/transcriber/#response-templates","title":"Response Templates","text":"<p>Format transcription results using Jinja templates:</p> <pre><code>import msgflux as mf\n\nclass FormattedTranscriber(mf.nn.Transcriber):\n    \"\"\"Transcriber with custom output formatting.\"\"\"\n\n    response_format = \"json\"\n    response_mode = \"plain_response\"\n\nstt = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ntranscriber = FormattedTranscriber(\n    model=stt,\n    response_template=\"\"\"\n    Transcription completed:\n    Language: {{ language }}\n    Duration: {{ duration }}s\n    Text: {{ text }}\n    \"\"\",\n    config={\"language\": \"en\"}\n)\n\nformatted_result = transcriber(\"audio.mp3\")\nprint(formatted_result)\n</code></pre>"},{"location":"learn/nn/transcriber/#creating-transcriber-hierarchies","title":"Creating Transcriber Hierarchies","text":"<p>Build specialized transcribers through inheritance:</p> <pre><code>import msgflux as mf\n\n# Base transcriber for all meetings\nclass MeetingTranscriber(mf.nn.Transcriber):\n    \"\"\"Base transcriber for meeting recordings.\"\"\"\n\n    response_format = \"text\"\n    response_mode = \"plain_response\"\n\n# Internal meetings - detailed transcription\nclass InternalMeetingTranscriber(MeetingTranscriber):\n    \"\"\"Detailed transcriber for internal meetings.\"\"\"\n\n    response_format = \"verbose_json\"  # Override for timestamps\n\n# Client meetings - clean, formatted output\nclass ClientMeetingTranscriber(MeetingTranscriber):\n    \"\"\"Clean transcriber for client-facing meetings.\"\"\"\n\n    # Inherits text format from base\n\nstt = mf.Model.speech_to_text(\"openai/whisper-1\")\n\n# Create instances\ninternal = InternalMeetingTranscriber(\n    model=stt,\n    config={\n        \"language\": \"en\",\n        \"timestamp_granularities\": \"segment\"\n    }\n)\n\nclient = ClientMeetingTranscriber(\n    model=stt,\n    prompt=\"Professional business meeting. Clean transcription without filler words.\",\n    config={\"language\": \"en\"}\n)\n\n# Use appropriately\ninternal_notes = internal(\"team_standup.mp3\")\nclient_summary = client(\"client_presentation.mp3\")\n</code></pre>"},{"location":"learn/nn/transcriber/#integration-with-agents","title":"Integration with Agents","text":"<p>Transcribers can be used in agent workflows:</p> <pre><code>import msgflux as mf\n\nclass VoiceCommandTranscriber(mf.nn.Transcriber):\n    \"\"\"Transcribes voice commands for agent processing.\"\"\"\n\n    response_format = \"text\"\n    response_mode = \"plain_response\"\n\n# Create transcriber\nstt = mf.Model.speech_to_text(\"openai/whisper-1\")\ntranscriber = VoiceCommandTranscriber(\n    model=stt,\n    config={\"language\": \"en\"}\n)\n\n# Create agent\nclass VoiceAssistant(mf.nn.Agent):\n    \"\"\"Voice-controlled assistant.\"\"\"\n\n    temperature = 0.7\n    max_tokens = 150\n\nmodel = mf.Model.chat_completion(\"openai/gpt-4\")\nassistant = VoiceAssistant(model=model)\n\n# Process voice command\nvoice_input = \"voice_command.mp3\"\n\n# Transcribe\ncommand_text = transcriber(voice_input)\nprint(f\"Command: {command_text}\")\n\n# Process with agent\nresponse = assistant(command_text)\nprint(f\"Response: {response}\")\n</code></pre>"},{"location":"learn/nn/transcriber/#batch-transcription","title":"Batch Transcription","text":"<p>Transcribe multiple files efficiently:</p> <pre><code>import msgflux as mf\nimport asyncio\n\nclass BatchTranscriber(mf.nn.Transcriber):\n    \"\"\"Transcriber for batch processing multiple files.\"\"\"\n\n    response_format = \"text\"\n    response_mode = \"plain_response\"\n\nstt = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ntranscriber = BatchTranscriber(\n    model=stt,\n    config={\"language\": \"en\"}\n)\n\nasync def transcribe_batch(audio_files):\n    \"\"\"Transcribe multiple audio files concurrently.\"\"\"\n    tasks = [transcriber.aforward(file) for file in audio_files]\n    return await asyncio.gather(*tasks)\n\n# Transcribe batch\naudio_files = [\"file1.mp3\", \"file2.mp3\", \"file3.mp3\"]\ntranscripts = asyncio.run(transcribe_batch(audio_files))\n\nfor file, transcript in zip(audio_files, transcripts):\n    print(f\"{file}: {transcript}\")\n</code></pre>"},{"location":"learn/nn/transcriber/#configuration-options","title":"Configuration Options","text":""},{"location":"learn/nn/transcriber/#complete-parameter-reference","title":"Complete Parameter Reference","text":"<pre><code>import msgflux as mf\n\nclass FullyConfiguredTranscriber(mf.nn.Transcriber):\n    \"\"\"Transcriber with all configuration options.\"\"\"\n\n    # Response behavior\n    response_format = \"text\"  # \"text\", \"json\", \"srt\", \"verbose_json\", \"vtt\"\n    response_mode = \"plain_response\"  # or \"message\"\n\nstt = mf.Model.speech_to_text(\"openai/whisper-1\")\n\ntranscriber = FullyConfiguredTranscriber(\n    model=stt,                           # Required: STT model\n    message_fields={                     # Optional: Message field mapping\n        \"task_multimodal_inputs\": \"audio.file\",\n        \"model_preference\": \"model.choice\"\n    },\n    response_template=\"...\",             # Optional: Jinja template\n    prompt=\"Transcription guidance\",     # Optional: prompt for accuracy\n    config={                             # Optional: STT-specific config\n        \"language\": \"en\",                # Language code (en, es, fr, etc.)\n        \"stream\": False,                 # Enable streaming\n        \"timestamp_granularities\": \"word\",  # \"word\", \"segment\", or None\n        \"temperature\": 0.0               # Sampling temperature (0-1)\n    },\n    name=\"custom_transcriber\"            # Optional: custom name\n)\n</code></pre>"},{"location":"learn/nn/transcriber/#best-practices","title":"Best Practices","text":""},{"location":"learn/nn/transcriber/#1-use-language-specific-transcribers","title":"1. Use Language-Specific Transcribers","text":"<pre><code># Good - Optimized per language\nclass EnglishTranscriber(mf.nn.Transcriber):\n    \"\"\"Optimized for English audio.\"\"\"\n    response_format = \"text\"\n\nclass JapaneseTranscriber(mf.nn.Transcriber):\n    \"\"\"Optimized for Japanese audio.\"\"\"\n    response_format = \"text\"\n\nenglish = EnglishTranscriber(model=stt, config={\"language\": \"en\"})\njapanese = JapaneseTranscriber(model=stt, config={\"language\": \"ja\"})\n</code></pre>"},{"location":"learn/nn/transcriber/#2-use-prompts-for-domain-specific-audio","title":"2. Use Prompts for Domain-Specific Audio","text":"<pre><code># Good - Domain-specific prompts improve accuracy\nclass TechnicalTranscriber(mf.nn.Transcriber):\n    \"\"\"Transcriber for technical content.\"\"\"\n    response_format = \"text\"\n\ntechnical = TechnicalTranscriber(\n    model=stt,\n    prompt=\"Technical presentation with terms: API, database, microservices, kubernetes\",\n    config={\"language\": \"en\"}\n)\n</code></pre>"},{"location":"learn/nn/transcriber/#3-choose-appropriate-response-format","title":"3. Choose Appropriate Response Format","text":"<pre><code># Text for simple transcription\nclass SimpleTranscriber(mf.nn.Transcriber):\n    \"\"\"Simple text transcription.\"\"\"\n    response_format = \"text\"\n\n# Verbose JSON for detailed analysis\nclass DetailedTranscriber(mf.nn.Transcriber):\n    \"\"\"Detailed transcription with timestamps.\"\"\"\n    response_format = \"verbose_json\"\n\n# SRT for video subtitles\nclass SubtitleTranscriber(mf.nn.Transcriber):\n    \"\"\"Generate subtitles for videos.\"\"\"\n    response_format = \"srt\"\n</code></pre>"},{"location":"learn/nn/transcriber/#migration-guide","title":"Migration Guide","text":""},{"location":"learn/nn/transcriber/#from-traditional-to-autoparams","title":"From Traditional to AutoParams","text":"<p>Before (Traditional): <pre><code>transcriber = mf.nn.Transcriber(\n    model=stt,\n    response_format=\"text\",\n    config={\"language\": \"en\", \"temperature\": 0.0}\n)\n</code></pre></p> <p>After (AutoParams - Recommended): <pre><code>class MyTranscriber(mf.nn.Transcriber):\n    \"\"\"Accurate transcriber for English audio.\"\"\"\n    response_format = \"text\"\n\ntranscriber = MyTranscriber(\n    model=stt,\n    config={\"language\": \"en\", \"temperature\": 0.0}\n)\n</code></pre></p>"},{"location":"learn/nn/transcriber/#summary","title":"Summary","text":"<ul> <li>Use AutoParams for defining transcribers - create specialized transcribers for different domains</li> <li>Traditional initialization works for quick, one-off transcriptions</li> <li>Supports multiple output formats (text, JSON, SRT, verbose JSON, VTT)</li> <li>Configure language, timestamps, and streaming options</li> <li>Use prompts to improve accuracy for domain-specific audio</li> <li>Async support for batch transcription</li> <li>Integrate with Agents for voice-controlled workflows</li> </ul> <p>The Transcriber module provides flexible speech-to-text - use AutoParams to organize transcribers by language, domain, and use case.</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/agents/","title":"Agents","text":""},{"location":"blog/category/rag/","title":"RAG","text":""},{"location":"blog/category/multimodal/","title":"MultiModal","text":""},{"location":"blog/category/search/","title":"Search","text":""}]}