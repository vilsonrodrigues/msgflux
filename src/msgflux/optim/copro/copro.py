"""COPRO - Collaborative Prompt Optimization.

This optimizer uses an LLM to generate improved instructions based on
successful and failed examples from evaluation. Supports both synchronous
and asynchronous execution.
"""

import asyncio
import random
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union

from msgflux.examples import Example
from msgflux.generation.templates import PromptSpec
from msgflux.logger import init_logger
from msgflux.nn.modules.module import Module
from msgflux.nn.parameter import Parameter
from msgflux.optim.optimizer import Optimizer
from msgflux.optim.progress import OptimProgress, TrialInfo

logger = init_logger(__name__)


# Template for instruction generation
INSTRUCTION_GENERATION_TEMPLATE = """You are an expert prompt engineer. Your task is to generate better instructions for an AI assistant.

## Current Task Description
{task_description}

## Current Instructions
{current_instructions}

## Performance Summary
- Total examples evaluated: {total_examples}
- Success rate: {success_rate:.1%}

## Examples of Successful Predictions
{success_examples}

## Examples of Failed Predictions
{failure_examples}

## Your Task
Based on the successful and failed examples above, generate {num_candidates} improved versions of the instructions.
Each new instruction should:
1. Be clear and specific
2. Address patterns in the failures
3. Preserve what works in successful cases
4. Be concise but complete

Generate the instructions in the following format:
INSTRUCTION 1:
[Your first improved instruction]

INSTRUCTION 2:
[Your second improved instruction]

{additional_instructions}"""


@dataclass
class CoproCandidate:
    """A candidate instruction generated by COPRO."""

    instruction: str
    score: float = 0.0
    evaluated: bool = False


@dataclass
class CoproResult:
    """Result of a COPRO optimization step."""

    best_instruction: str
    best_score: float
    candidates: List[CoproCandidate]
    num_evaluations: int
    success_rate: float


class COPRO(Optimizer):
    """Collaborative Prompt Optimization.

    COPRO uses an LLM to generate improved instructions by analyzing
    patterns in successful and failed predictions. It generates multiple
    candidate instructions and evaluates them to find the best one.

    Example:
        >>> from msgflux.optim import COPRO
        >>> from msgflux.evaluate.metrics import exact_match
        >>>
        >>> optimizer = COPRO(
        ...     agent.parameters(),
        ...     metric=exact_match,
        ...     prompt_model=generator_model,
        ...     num_candidates=5,
        ...     seed=42,
        ... )
        >>> optimizer.step(trainset, valset)

    Args:
        params: Iterable of Parameters to optimize.
        metric: Metric function for evaluation.
        prompt_model: Module to use for generating candidate instructions.
        num_candidates: Number of candidate instructions to generate per step.
        max_success_examples: Maximum number of success examples to show.
        max_failure_examples: Maximum number of failure examples to show.
        task_description: Description of the task (optional).
        breadth: Number of candidates to evaluate before selecting best.
        depth: Number of refinement iterations.
        seed: Random seed for reproducibility.
    """

    def __init__(
        self,
        params: Iterable[Parameter],
        *,
        metric: Callable[[Example, Any], float],
        prompt_model: Optional[Module] = None,
        num_candidates: int = 5,
        max_success_examples: int = 5,
        max_failure_examples: int = 5,
        task_description: Optional[str] = None,
        breadth: int = 5,
        depth: int = 3,
        verbose: bool = False,
        seed: int = 0,
    ):
        defaults = dict(
            num_candidates=num_candidates,
            max_success_examples=max_success_examples,
            max_failure_examples=max_failure_examples,
            breadth=breadth,
            depth=depth,
            seed=seed,
        )
        super().__init__(params, defaults)

        self.metric = metric
        self.prompt_model = prompt_model
        self.num_candidates = num_candidates
        self.max_success_examples = max_success_examples
        self.max_failure_examples = max_failure_examples
        self.task_description = task_description or "Complete the given task."
        self.breadth = breadth
        self.depth = depth
        self.verbose = verbose
        self.seed = seed
        self.rng = random.Random(seed)

        # Progress tracking
        self._progress = OptimProgress(verbose=verbose)

        # Track optimization history
        self._candidates_history: List[List[CoproCandidate]] = []
        self._best_instruction: Optional[str] = None
        self._best_score: float = 0.0

    def step(
        self,
        trainset: List[Example],
        valset: Optional[List[Example]] = None,
        *,
        teacher: Optional[Module] = None,
        closure: Optional[Callable[[], float]] = None,
    ) -> Optional[float]:
        """Perform one COPRO optimization step.

        Args:
            trainset: Training examples for evaluation.
            valset: Validation examples (optional, uses trainset if None).
            teacher: Module to evaluate (optional).
            closure: Closure for loss computation (optional).

        Returns:
            The loss value if closure is provided, None otherwise.
        """
        self._step_count += 1

        if valset is None:
            valset = trainset

        # Start progress tracking
        self._progress.start(
            "COPRO",
            trainset_size=len(trainset),
            valset_size=len(valset),
            num_candidates=self.num_candidates,
            breadth=self.breadth,
            depth=self.depth,
        )

        # Get instruction parameter
        instructions_param = self._get_instructions_param()
        if instructions_param is None:
            self._progress.warning("No instructions parameter found")
            if closure is not None:
                return closure()
            return None

        # If no prompt model, skip optimization
        if self.prompt_model is None:
            self._progress.warning("No prompt model provided, skipping optimization")
            if closure is not None:
                return closure()
            return None

        # Evaluate current instruction on trainset
        self._progress.step("COLLECT EXAMPLES", 1, 3)
        current_instruction = instructions_param.data or ""
        success_examples, failure_examples = self._collect_examples(
            trainset, teacher, current_instruction
        )

        success_rate = len(success_examples) / len(trainset) if trainset else 0.0
        self._progress.substep(
            f"Collected {len(success_examples)} successes, {len(failure_examples)} failures"
        )
        self._progress.metric("Success rate", success_rate, 1.0)

        # Generate candidate instructions
        self._progress.step("GENERATE CANDIDATES", 2, 3)
        candidates = self._generate_candidates(
            current_instruction=current_instruction,
            success_examples=success_examples,
            failure_examples=failure_examples,
            total_examples=len(trainset),
            success_rate=success_rate,
        )
        self._progress.substep(f"Generated {len(candidates)} candidates")

        # Evaluate candidates on validation set
        self._progress.step("EVALUATE CANDIDATES", 3, 3)
        best_candidate = self._evaluate_candidates(candidates, valset, teacher)

        if best_candidate and best_candidate.score > self._best_score:
            self._best_score = best_candidate.score
            self._best_instruction = best_candidate.instruction

            # Update parameter
            if instructions_param.requires_grad:
                instructions_param.data = best_candidate.instruction

            self._progress.trial(TrialInfo(
                trial_num=self._step_count,
                total_trials=self._step_count,
                score=best_candidate.score,
                best_score=self._best_score,
                is_best=True,
            ))
        elif best_candidate:
            self._progress.trial(TrialInfo(
                trial_num=self._step_count,
                total_trials=self._step_count,
                score=best_candidate.score,
                best_score=self._best_score,
                is_best=False,
            ))

        self._candidates_history.append(candidates)

        # Finish with summary
        self._progress.finish(
            best_score=self._best_score,
            summary={
                "candidates_generated": len(candidates),
                "success_rate": f"{success_rate:.1%}",
            },
        )

        if closure is not None:
            return closure()
        return None

    def _get_instructions_param(self) -> Optional[Parameter]:
        """Get the instructions parameter from param_groups."""
        for group in self.param_groups:
            for param in group["params"]:
                if param.spec == PromptSpec.INSTRUCTIONS and param.requires_grad:
                    return param
        return None

    def _collect_examples(
        self,
        examples: List[Example],
        teacher: Optional[Module],
        current_instruction: str,
    ) -> Tuple[List[Tuple[Example, Any, float]], List[Tuple[Example, Any, float]]]:
        """Collect success and failure examples using the teacher module.

        Returns:
            Tuple of (success_examples, failure_examples)
        """
        success_examples = []
        failure_examples = []

        if teacher is None:
            # Without teacher, use labels as ground truth
            for ex in examples:
                success_examples.append((ex, ex.labels, 1.0))
            return success_examples, failure_examples

        for example in examples:
            try:
                prediction = teacher(example.inputs)
                score = self.metric(example, prediction)

                if score > 0.5:
                    success_examples.append((example, prediction, score))
                else:
                    failure_examples.append((example, prediction, score))
            except Exception:
                # Treat errors as failures
                failure_examples.append((example, None, 0.0))

        return success_examples, failure_examples

    def _format_examples(
        self, examples: List[Tuple[Example, Any, float]], max_examples: int
    ) -> str:
        """Format examples for the prompt."""
        if not examples:
            return "No examples available."

        selected = self.rng.sample(examples, min(len(examples), max_examples))
        formatted = []

        for i, (ex, pred, score) in enumerate(selected, 1):
            formatted.append(f"Example {i}:")
            formatted.append(f"  Input: {ex.inputs}")
            formatted.append(f"  Expected: {ex.labels}")
            formatted.append(f"  Prediction: {pred}")
            formatted.append(f"  Score: {score:.2f}")
            formatted.append("")

        return "\n".join(formatted)

    def _generate_candidates(
        self,
        current_instruction: str,
        success_examples: List[Tuple[Example, Any, float]],
        failure_examples: List[Tuple[Example, Any, float]],
        total_examples: int,
        success_rate: float,
    ) -> List[CoproCandidate]:
        """Generate candidate instructions using the prompt model."""
        # Format examples for the prompt
        success_str = self._format_examples(
            success_examples, self.max_success_examples
        )
        failure_str = self._format_examples(
            failure_examples, self.max_failure_examples
        )

        additional = ""
        if self.num_candidates > 2:
            additional = f"Continue with INSTRUCTION 3 through INSTRUCTION {self.num_candidates}."

        # Create the prompt
        prompt = INSTRUCTION_GENERATION_TEMPLATE.format(
            task_description=self.task_description,
            current_instructions=current_instruction or "(No current instructions)",
            total_examples=total_examples,
            success_rate=success_rate,
            success_examples=success_str,
            failure_examples=failure_str,
            num_candidates=self.num_candidates,
            additional_instructions=additional,
        )

        # Generate candidates
        try:
            response = self.prompt_model(prompt)
            candidates = self._parse_candidates(response)
        except Exception:
            # Fallback: return current instruction as only candidate
            candidates = [CoproCandidate(instruction=current_instruction)]

        return candidates

    def _parse_candidates(self, response: str) -> List[CoproCandidate]:
        """Parse candidate instructions from the model response."""
        candidates = []

        # Split by "INSTRUCTION N:" pattern
        parts = response.split("INSTRUCTION")

        for part in parts[1:]:  # Skip first empty part
            # Find the instruction text after the number and colon
            lines = part.strip().split("\n", 1)
            if len(lines) >= 2:
                # Skip the "N:" part and get the actual instruction
                instruction = lines[1].strip()
                if instruction:
                    candidates.append(CoproCandidate(instruction=instruction))
            elif lines:
                # Handle case where instruction is on the same line
                instruction = lines[0].split(":", 1)[-1].strip()
                if instruction:
                    candidates.append(CoproCandidate(instruction=instruction))

        return candidates

    def _evaluate_candidates(
        self,
        candidates: List[CoproCandidate],
        valset: List[Example],
        teacher: Optional[Module],
    ) -> Optional[CoproCandidate]:
        """Evaluate candidates and return the best one."""
        if not candidates or teacher is None:
            return None

        # Evaluate each candidate
        for candidate in candidates[: self.breadth]:
            score = self._evaluate_instruction(candidate.instruction, valset, teacher)
            candidate.score = score
            candidate.evaluated = True

        # Return best candidate
        if candidates:
            return max(candidates, key=lambda c: c.score)
        return None

    def _evaluate_instruction(
        self,
        instruction: str,
        valset: List[Example],
        teacher: Module,
    ) -> float:
        """Evaluate a single instruction on the validation set."""
        # Temporarily update instruction
        instructions_param = self._get_instructions_param()
        if instructions_param is None:
            return 0.0

        original_instruction = instructions_param.data
        instructions_param.data = instruction

        try:
            total_score = 0.0
            for example in valset:
                try:
                    prediction = teacher(example.inputs)
                    score = self.metric(example, prediction)
                    total_score += score
                except Exception:
                    pass

            avg_score = total_score / len(valset) if valset else 0.0
        finally:
            # Restore original instruction
            instructions_param.data = original_instruction

        return avg_score

    def get_best_instruction(self) -> Optional[str]:
        """Get the best instruction found so far."""
        return self._best_instruction

    def get_best_score(self) -> float:
        """Get the best score achieved."""
        return self._best_score

    def get_candidates_history(self) -> List[List[CoproCandidate]]:
        """Get the history of all generated candidates."""
        return self._candidates_history

    def state_dict(self) -> Dict[str, Any]:
        """Return optimizer state dictionary."""
        state = super().state_dict()
        state.update(
            {
                "best_instruction": self._best_instruction,
                "best_score": self._best_score,
                "seed": self.seed,
            }
        )
        return state

    def load_state_dict(self, state: Dict[str, Any]) -> None:
        """Load optimizer state dictionary."""
        super().load_state_dict(state)
        self._best_instruction = state.get("best_instruction")
        self._best_score = state.get("best_score", 0.0)
        self.seed = state.get("seed", self.seed)
        self.rng = random.Random(self.seed)

    # Async methods

    async def astep(
        self,
        trainset: List[Example],
        valset: Optional[List[Example]] = None,
        *,
        teacher: Optional[Module] = None,
        closure: Optional[Callable[[], float]] = None,
        max_concurrency: Optional[int] = None,
    ) -> Optional[float]:
        """Perform one COPRO optimization step asynchronously.

        Args:
            trainset: Training examples for evaluation.
            valset: Validation examples (optional).
            teacher: Module to evaluate (optional).
            closure: Closure for loss computation (optional).
            max_concurrency: Maximum concurrent evaluations.

        Returns:
            The loss value if closure is provided, None otherwise.
        """
        self._step_count += 1

        if valset is None:
            valset = trainset

        # Start progress tracking
        self._progress.start(
            "COPRO (async)",
            trainset_size=len(trainset),
            valset_size=len(valset),
            num_candidates=self.num_candidates,
            max_concurrency=max_concurrency or "unlimited",
        )

        instructions_param = self._get_instructions_param()
        if instructions_param is None or self.prompt_model is None:
            self._progress.warning("No instructions parameter or prompt model")
            if closure is not None:
                return closure()
            return None

        # Evaluate current instruction asynchronously
        self._progress.step("COLLECT EXAMPLES (async)", 1, 3)
        current_instruction = instructions_param.data or ""
        success_examples, failure_examples = await self._acollect_examples(
            trainset, teacher, current_instruction, max_concurrency
        )

        success_rate = len(success_examples) / len(trainset) if trainset else 0.0
        self._progress.substep(
            f"Collected {len(success_examples)} successes, {len(failure_examples)} failures"
        )

        # Generate candidates (sync - uses prompt_model)
        self._progress.step("GENERATE CANDIDATES", 2, 3)
        candidates = self._generate_candidates(
            current_instruction=current_instruction,
            success_examples=success_examples,
            failure_examples=failure_examples,
            total_examples=len(trainset),
            success_rate=success_rate,
        )
        self._progress.substep(f"Generated {len(candidates)} candidates")

        # Evaluate candidates asynchronously
        self._progress.step("EVALUATE CANDIDATES (async)", 3, 3)
        best_candidate = await self._aevaluate_candidates(
            candidates, valset, teacher, max_concurrency
        )

        if best_candidate and best_candidate.score > self._best_score:
            self._best_score = best_candidate.score
            self._best_instruction = best_candidate.instruction
            if instructions_param.requires_grad:
                instructions_param.data = best_candidate.instruction

            self._progress.trial(TrialInfo(
                trial_num=self._step_count,
                total_trials=self._step_count,
                score=best_candidate.score,
                best_score=self._best_score,
                is_best=True,
            ))
        elif best_candidate:
            self._progress.trial(TrialInfo(
                trial_num=self._step_count,
                total_trials=self._step_count,
                score=best_candidate.score,
                best_score=self._best_score,
                is_best=False,
            ))

        self._candidates_history.append(candidates)

        # Finish with summary
        self._progress.finish(
            best_score=self._best_score,
            summary={
                "candidates_generated": len(candidates),
                "success_rate": f"{success_rate:.1%}",
            },
        )

        if closure is not None:
            return closure()
        return None

    async def _acollect_examples(
        self,
        examples: List[Example],
        teacher: Optional[Module],
        current_instruction: str,
        max_concurrency: Optional[int] = None,
    ) -> Tuple[List[Tuple[Example, Any, float]], List[Tuple[Example, Any, float]]]:
        """Collect examples asynchronously."""
        if teacher is None:
            return [(ex, ex.labels, 1.0) for ex in examples], []

        async def evaluate_one(example: Example):
            try:
                if hasattr(teacher, "acall"):
                    prediction = await teacher.acall(example.inputs)
                else:
                    loop = asyncio.get_event_loop()
                    prediction = await loop.run_in_executor(
                        None, teacher, example.inputs
                    )
                score = self.metric(example, prediction)
                return (example, prediction, score)
            except Exception:
                return (example, None, 0.0)

        if max_concurrency:
            semaphore = asyncio.Semaphore(max_concurrency)

            async def bounded_eval(ex):
                async with semaphore:
                    return await evaluate_one(ex)

            tasks = [bounded_eval(ex) for ex in examples]
        else:
            tasks = [evaluate_one(ex) for ex in examples]

        results = await asyncio.gather(*tasks)

        success = [(ex, pred, score) for ex, pred, score in results if score > 0.5]
        failure = [(ex, pred, score) for ex, pred, score in results if score <= 0.5]

        return success, failure

    async def _aevaluate_candidates(
        self,
        candidates: List[CoproCandidate],
        valset: List[Example],
        teacher: Optional[Module],
        max_concurrency: Optional[int] = None,
    ) -> Optional[CoproCandidate]:
        """Evaluate candidates asynchronously."""
        if not candidates or teacher is None:
            return None

        for candidate in candidates[: self.breadth]:
            score = await self._aevaluate_instruction(
                candidate.instruction, valset, teacher, max_concurrency
            )
            candidate.score = score
            candidate.evaluated = True

        return max(candidates, key=lambda c: c.score) if candidates else None

    async def _aevaluate_instruction(
        self,
        instruction: str,
        valset: List[Example],
        teacher: Module,
        max_concurrency: Optional[int] = None,
    ) -> float:
        """Evaluate instruction asynchronously."""
        instructions_param = self._get_instructions_param()
        if instructions_param is None:
            return 0.0

        original = instructions_param.data
        instructions_param.data = instruction

        try:
            async def eval_one(example: Example):
                try:
                    if hasattr(teacher, "acall"):
                        pred = await teacher.acall(example.inputs)
                    else:
                        loop = asyncio.get_event_loop()
                        pred = await loop.run_in_executor(
                            None, teacher, example.inputs
                        )
                    return self.metric(example, pred)
                except Exception:
                    return 0.0

            if max_concurrency:
                semaphore = asyncio.Semaphore(max_concurrency)

                async def bounded(ex):
                    async with semaphore:
                        return await eval_one(ex)

                tasks = [bounded(ex) for ex in valset]
            else:
                tasks = [eval_one(ex) for ex in valset]

            scores = await asyncio.gather(*tasks)
            return sum(scores) / len(valset) if valset else 0.0
        finally:
            instructions_param.data = original
